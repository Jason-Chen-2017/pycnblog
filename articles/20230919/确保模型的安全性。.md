
作者：禅与计算机程序设计艺术                    

# 1.简介
  

安全ML（Safety-Machine Learning）是机器学习领域一个新兴研究方向，其目的是通过对模型的预测结果进行精心设计，使得其对人类而言“安全”、对计算机而言“可信”。在机器学习系统中引入安全机制可以提高系统的鲁棒性和安全性。本文将从机器学习角度出发，对现有的安全机制进行概述，并阐述一些安全ML相关的最佳实践方法。
# 2.基本概念和术语
# 模型可信程度：指模型对于人类的可信程度，取值范围为0~1，其中1代表完全可信，0代表完全不可信。模型可信程度越高，模型表现出的行为越接近人类真实的认知和行为，更有可能对用户的意图产生响应。
# 隐私泄露：指数据中敏感信息被泄露到第三方，可能造成个人信息泄露、财产损失、社会影响等严重后果。
# 数据扰动攻击：在训练过程中，数据经过一定处理或噪声添加后，可能会造成模型的泛化能力下降甚至崩溃。通过恶意构造的攻击样本，攻击者可以使用数据扰动攻击的方式，迫使模型做出错误的预测，从而影响模型的最终效果。
# 推理时间限制攻击：当模型需要对外提供服务时，可以通过设置推理时间限制，限制模型的响应速度，进而限制模型在实际生产环境中的应用。例如，在某种垃圾邮件分类任务中，如果模型的响应时间超过了设定的阈值，则会拦截掉该邮件并过滤掉。
# 标签伪造攻击：指对训练集中的标签进行人为操控，比如在垃圾邮件分类任务中，攻击者可以用正确的垃圾邮件替换成正常邮件，或在正常邮件中加入虚假的病毒。这种攻击手段能够削弱模型的泛化能力，同时也会对模型的预测能力产生负面影响。
# 对抗攻击：当有多个参赛选手竞争某个任务时，会发生对抗攻击。攻击者通过操纵模型的输入、输出，使得模型的预测结果发生变化，从而干扰了模型的正常工作。目前，针对对抗攻击的防御方法已经有了一些研究成果，如基于梯度方法的防御方案。
# 参数压缩攻击：当模型参数过多时，可以采用参数压缩攻击的方法，缩小模型参数的数量，进而增加模型的复杂度。
# 输入数据不一致攻击：指模型接收到不同于训练集的数据，导致模型对输入数据的预测结果出现偏差，甚至产生错误的输出。目前，有两种主要的方式来缓解此类攻击：一种是进行数据清洗和预处理；另一种是建立多个模型，分别针对不同的子集或分布情况进行训练。
# 模型部署后的安全问题：部署后的模型由于在线服务的需求，其自身的安全风险也随之而来。包括但不限于运行环境权限恶劣、模型参数泄漏、模型恶意攻击等。
# 参考文献：
[1] A.Durrani, M.Gal, and S.Kale. "Exploring the Security of Machine Learning Systems: An Empirical Study." IEEE Transactions on Dependable and Secure Computing (2019): 1-1.