
作者：禅与计算机程序设计艺术                    

# 1.简介
         

         RL在强化学习领域是一个非常热门的研究方向，很多工作都在着力于提升其能力。在2019年，DeepMind提出了一种新的方法——PPO，它通过在连续控制任务上训练深度策略网络（DPN）获得了成功。尽管DPN取得了不错的性能，但是还是存在一些问题。其中之一就是 reward shaping 方法，它通过预测未来的reward值来影响DQN网络对action选择的决策。然而，这种方式只能进行局部奖励缩放，使得整体奖励曲线难以很好地匹配真实收益函数。因此，作者提出了一种新的奖励修正方法——奖励调节（RM）。作者指出，RM的目的是为了让agent能够更好地利用学习到的价值函数估计状态-动作值函数。
         
         本文旨在将RM和VS结合起来，设计一种基于强化学习的连续控制方法——LQR-DDPG。首先，本文回顾了之前一些关于DPG的研究成果，例如DPG、DDPG、TD3、SAC等。然后，作者提出了一种新的奖励调整的方法——LQR，即线性逼近器。通过最小化LQR损失函数来实现对真实奖励函数的逼近。LQR允许agent利用价值函数信息来调整奖励信号，从而更好地利用学到的价值函数进行决策。
         
         LQR-DDPG把LQR应用到DDPG中，包括两个部分：一个是将LQR加入到actor loss function中作为奖励信号，另一个则是在Critic loss function中加入LQR损失函数的贡献。这种做法可以改善模型学习过程中的稳定性，因为奖励调整直接涉及到actor网络的输出。在实际的测试过程中，作者发现LQR-DDPG可以显著提高连续控制任务的性能。
         
         此外，LQR还被证明对于actor网络具有重要意义。作者提出了一个新颖的解决方案——使用模糊奖励（FM）来训练 actor，其含义是用非连续数据代表奖励，并添加一些噪声来模拟连续奖励信号。这种处理方法可以帮助 agent 掌握复杂动作空间的动态，同时不会引入额外的奖励超参数。模糊奖励可以有效地避免离散化操作带来的学习困难和稀疏性问题。
         
         在实验结果方面，作者在mujoco环境下验证了LQR-DDPG算法。作者首先探索了最新的基准测试问题——HalfCheetah上的挥臂离散控制问题。实验结果表明，LQR-DDPG在该问题上的性能优于DPN和DQN。此外，作者也在其他几个连续控制环境中评估了LQR-DDPG的效果。最后，作者进行了一系列的定性分析，比较了不同奖励调整方法的效果。
         
         作者的研究结果得到了业界的广泛关注。本文为连续控制任务的RL研究领域提供了一条崭新的研究道路。它开辟了新的方向，将实用的奖励调整方法集成到了目前最先进的DDPG方法中。LQR在深度强化学习中有着独特的作用，它对actor的性能起着至关重要的作用，也促使作者重新思考奖励机制。欢迎大家阅读全文，并给予宝贵的建议。