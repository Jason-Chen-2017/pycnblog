Self attention层可以理解为一种特殊的注意力机制，它可以让模型关注到输入序列中不同位置的相互关联性。它的基本思路是计算两个序列之间的相似度，然后将每个输入向量与该序列中所有其他输入向量进行计算。计算完之后，模型会获得一个注意力矩阵，根据注意力矩阵的结果，模型可以获取到输入序列的全局信息，进而可以决定当前时刻应该关注哪些输入。