7. Batch Normalization - Batch normalization is a technique that normalizes the inputs to each layer before activation. It helps to accelerate gradient descent and stabilize the training process. By applying batch normalization, we ensure that all the hidden units in our network behave similarly and avoid covariate shift, i.e., changes in the distribution of the input data with respect to change in weight updates. 