3. Rethinking Memory-Efficient Transformers for Large-Scale Language Modeling