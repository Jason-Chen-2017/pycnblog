第二项工作是介绍 Transformes 中的 self-attention。模型学习到输入序列和输出序列之间的全局关联性。它通过计算输入序列中每个词与其他所有词之间的注意力权重来实现这一点。