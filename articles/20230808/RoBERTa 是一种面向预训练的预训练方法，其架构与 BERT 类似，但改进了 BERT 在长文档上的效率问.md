RoBERTa 是一种面向预训练的预训练方法，其架构与 BERT 类似，但改进了 BERT 在长文档上的效率问题。其核心思想是通过引入可变长度的 attention mask ，解决了训练效率的问题。具体来说，通过随机遮盖模型输入的注意力部分来模拟较短长度的句子。这样做能够让模型能够捕获更长的上下文信息。