3. Teacher-student distillation: Distilling task knowledge from large teachers into small students for neural network compression