2. RoBERTa: 使用变长的 attention mask 来增强 Transformer 模型