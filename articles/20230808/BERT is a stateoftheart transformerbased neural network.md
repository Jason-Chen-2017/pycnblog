BERT is a state-of-the-art transformer-based neural network architecture designed for natural language processing tasks. It was released in March 2019 by Google Research and presents great advantages over traditional recurrent neural networks such as LSTM and GRU while handling long sequences of data well. Here, we will explore the inner workings of BERT and understand how it works under the hood. 