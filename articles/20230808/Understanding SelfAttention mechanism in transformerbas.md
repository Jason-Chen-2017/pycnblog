18. Understanding Self-Attention mechanism in transformer-based models