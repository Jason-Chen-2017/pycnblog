2. Training Neural Networks without Tears: A Guide to Optimization Strategies for Large Scale Deep Learning, Understanding the Effectiveness of Regularization in Training Neural Networks, Optimizing Gradient Descent for Convolutional Neural Networks, Accelerating Deep Learning Models via Microbatch Scheduling, Improving Generalization Performance by Avoiding Covariate Shift during Domain Adaptation, and Detecting Adversarial Examples Using Feature Squeezing and Spatial Smoothing.