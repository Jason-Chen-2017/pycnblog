Tokenization:将输入序列分解成一系列标记符号或词汇单元。