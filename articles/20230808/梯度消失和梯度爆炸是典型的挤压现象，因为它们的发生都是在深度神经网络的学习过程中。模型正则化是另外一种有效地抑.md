梯度消失和梯度爆炸是典型的“挤压”现象，因为它们的发生都是在深度神经网络的学习过程中。模型正则化是另外一种有效地抑制这种现象的方法。