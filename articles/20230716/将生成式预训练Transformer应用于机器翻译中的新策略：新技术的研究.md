
作者：禅与计算机程序设计艺术                    
                
                
近年来基于神经网络的自然语言处理技术在翻译领域发挥越来越重要的作用。在英语-中文机器翻译中，数据集的质量越来越高，现有的传统模型架构无法充分发挥其优势。因此，为了在翻译领域推动NLP技术的进步，研究人员们提出了两种不同方向的研究。一种方式是基于蒸馏(distillation)的方法，这种方法可以将大的神经网络模型的参数进行压缩，再通过一个小型模型进行模拟学习。另一种方式则是采用生成式预训练的方法。这种方法的基本思想是在大规模无监督的数据上进行预训练，然后将其作为初始化参数来训练小型的神经网络模型。目前，在机器翻译方面，基于蒸馏的研究取得了一些成果；但是，生成式预训练方法并没有成为主流的方法。然而，随着NLP技术的不断发展，出现了一系列新的技术，其中包括多任务学习、权重共享等，这些都对生成式预训练的效果提升了不少。


本文主要讨论生成式预训练Transformer应用于机器翻译中的策略——新技术的研究。生成式预训练的最初思想是利用大量无监督文本数据进行预训练，然后将其作为初始化参数来训练小型的神经网络模型。但是，这种方式存在两个弊端：首先，生成式预训练往往需要大量的计算资源来训练更大的模型，这限制了其适用范围；其次，大多数传统的预训练模型都是以单词级别建模的，而机器翻译任务需要对整个句子或者段落进行建模。基于此，作者提出了一种新型的预训练方案——LM-GPT，即语言模型+生成式预训练Transformer（Language Modeling + GPT-like Transformer）。


LM-GPT模型的预训练目标是能够对整个句子或者段落进行建模，而不是单个词。因此，该模型包含了一个编码器模块和一个解码器模块。编码器模块根据输入序列生成一个固定长度的上下文向量表示，解码器模块则从这个上下文向量表示开始生成输出序列。GPT-like Transformer是一个生成式预训练模型，它的参数是由语言模型（LM）在无监督环境下生成的。相比于传统的语言模型或BERT等模型，GPT-like Transformer采用了更多的层数、宽度和跨层连接，同时引入注意力机制来确保生成出的序列具有连贯性。图1展示了LM-GPT模型结构。




图1：LM-GPT模型结构示意图



LM-GPT模型的训练过程也需要大量的计算资源。作者使用了Nvidia V100 GPU服务器，每个GPU上同时训练两个模型——编码器和解码器。每轮迭代都需要处理若干百万甚至千万级的训练样本，因此训练时间较长。虽然速度慢，但这种方案能够获得相当准确的结果。与LM-LSTM模型比较，LM-GPT的准确率在某些评测指标上要高于它。


在实验中，作者探索了LM-GPT与BERT的差异。作者使用了OpenAI提供的WMT14英语-中文数据集，同时也使用了其他的开源数据集。结果表明，LM-GPT模型的准确率要优于BERT在F1和BLEU两个标准上的性能，而且在更严苛的评估指标SQuAD v1.1上的性能要稍微好些。另外，由于LM-GPT模型是单模态的，所以能够更好地捕获语句之间的关系，在训练阶段的收敛速度也要快于BERT。


本文总结了当前生成式预训练模型的局限性，并提出了一种新型的方案——LM-GPT，证明了这种方案能够在机器翻译任务上实现最佳的效果。与LM-LSTM模型和BERT模型进行了对比，发现LM-GPT能够在同样的计算资源下取得更好的性能。因此，生成式预训练Transformer的应用可以提高NLP技术的能力，为提升翻译质量和速度的发展注入了新的可能性。

