
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


虚拟现实（VR）技术主要应用于模拟人类的三维空间中的虚拟世界，可以让用户在真实环境中通过头部、手指或眼睛等控制设备，自由地行走、旋转、交互。当前虚拟现实技术仍处于起步阶段，市场容量相对较小，需求和成本都不高。随着科技的进步和创新，VR产业将会成为一个全新的行业。

虚拟现实技术之所以受到关注，还因为其具有创造力和独特的魅力。虚拟现实平台可由多个参与者共同建设，包括设计师、程序员、互动媒体和玩家。这些个体通过制作独特的VR项目，实现沉浸式、协同式、数字化、全息图式的增强现实（AR）效果。虚拟现实使人们在拥有自己独立的生活时，能够通过虚拟的环境和周边的物品进行沟通、探索和交流。

目前市面上关于VR技术的资源主要集中在两个方面：教育和游戏。教育领域的VR课程设计很受欢迎，应用也十分广泛。游戏领域则在继续火爆，比如今天的Ubisoft的虚幻引擎4发布了。如今VR正在成为越来越多的人们的选择。

虽然虚拟现实技术目前还处于起步阶段，但其潜在商业价值并不亚于其他一些技术。VR的应用场景非常丰富，涵盖娱乐、工作、体验、医疗、教育等多个领域。未来的研究和发展方向主要有两条：

1.基于虚拟现实的教育：目前国内的大型学校都在布局VR学习平台，VR助课软件也是众多教育公司的热门选择。通过虚拟的方式，人们可以在课堂中获得更加真实、融入式的教学体验。

2.基于虚拟现实的健康管理：虚拟现实的临床应用正在蓬勃发展，而VR设备的普及率也在提升。通过虚拟现实技术，可以让患者更容易地感受疾病状况，从而帮助医生做出精准治疗。

另外，VR行业还将成为下一个大热门词汇。业内人士表示，如果疫情再持续半年左右，无论是技术还是创投，都将迎来巨大的竞争。因此，在此期间，我们需要密切关注市场前景，积极参与相关的创业活动，充分发挥VR技术的作用，助推VR产业的发展。

# 2.核心概念与联系
以下介绍一些重要的概念和相关术语。
## 2.1 VR、AR、MR、XR
VR、AR、MR、XR都是不同类型的虚拟现实技术，它们之间具有一定区别和联系。以下简要介绍一下它们之间的关系。
- VR(Virtual Reality):虚拟现实。它是在计算机上仿真人类视觉、听觉、嗅觉及意识的能力。用户通过头戴显示器、头盔或其他装置，用眼睛、耳朵、手、脚或其他触觉器官来与真实环境进行交互。该技术允许人类高度的想象力，利用头部、手部、眼睛等外界输入设备与电脑进行交流。通过这种方式，虚拟现实呈现的是一个实际存在的、具有真实感的虚拟环境，使得人们能够透过镜像、延迟反馈、身体追踪、假体图像和语音合成等技术，逼真地感受真实世界。
- AR(Augmented Reality):增强现实。它是将现实世界与虚拟环境融合在一起的一种现代技术。增强现实是一种人机界面技术，它将物理对象与虚拟物体相结合，用于增强现实显示。这一技术能够让用户感受到事物的物理存在，让虚拟对象添加到现实环境当中，给人们带来非凡的沉浸感。在增强现实应用中，有各种各样的设备，比如手环、手机上的VR应用、电子墨水屏、一加、苹果公司的iPhone X及华为Mate X等。
- MR(Mixed Reality):混合现实。它是利用两者结合的虚拟现实技术。它与其他两种现实技术相比，最大的不同就是把虚拟现实和真实世界结合在一起。这项技术的一个重要特征就是可以同时呈现虚拟场景和现实世界，让用户感受到两种不同的、融合在一起的世界。
- XR(Extended Reality):可扩展现实。它是一个视觉同步技术，它让用户能够以全新的方式与虚拟现实进行交流。此种技术通过将虚拟与真实世界融为一体，让用户感觉到整个场景融为一体的新鲜感。它可让人类、机器、应用及物理世界在同一张画布上进行同步演示，并且具备高度的自主性。由于其全新、超前的理念，可扩展现实已经成为许多领域的先锋。

## 2.2 成像技术
图像技术对于虚拟现实技术的作用至关重要。图像技术能够捕捉到人的视觉、听觉、嗅觉及味觉等感官的信息，并转换为电信号或光信号，最终在人眼或耳蜗等输出设备上反映出来。图像技术有三种基本类型：立体摄影技术、光学成像技术、显示技术。
### 立体摄影技术
立体摄影技术是指通过使用多个摄像头组合成单个图像的技术。这种技术能够同时捕捉到环境中的所有元素。人在现实世界里看到的一切都会被这个技术记录下来。最著名的例子莫过于百度的“城市级大脑”项目。
### 光学成像技术
光学成像技术是指通过采用高频激光或者其他激光源对物体表面进行照射，并通过采集反射回波、散射回波、透射回波等多种信号来构造成像系统的技术。人类视觉的成像过程与光学成像技术密切相关。人眼对颜色的识别，依赖于对光谱范围内的各种波长的敏感度。例如红色的波长范围是630~750nm，在可见光中可见；紫色的波长范围是490~570nm，属于近红外线。光学成像技术的发明，意味着人类对外界环境的认知水平达到了前所未有的程度。
### 显示技术
显示技术又称为显示设备技术。它是指用于在视觉上的呈现信息的方法和技术。在现代虚拟现实技术中，通常采用各种显示技术，包括电脑显示屏、电子墨水屏、一体显示屏、全息显示屏、红外显示屏、激光显示屏等。

## 2.3 混合现实与增强现实
虚拟现实技术将人与计算机、物质、虚拟环境进行结合，体现出一种新的交互模式。混合现实技术通过采用纯粹的虚拟现实技术与增强现实技术的结合，让用户感受到真实与虚拟的融合，展现出更加真实、多维、创新的视角。

混合现实的目标是融合现实世界与虚拟环境，让真实世界与虚拟世界共存。其中，虚拟现实技术旨在利用计算设备模拟人的三维空间，并通过各种输入设备(如控制器、头盔、眼睛)与外部世界进行交互，提供沉浸式、协同式、数字化、全息图式的视觉体验。而增强现实技术则旨在将虚拟实体添加到现实世界中，为人们带来非凡的沉浸感。

增强现实技术的实现方法之一是使用虚拟现实技术对场景进行渲染，并将渲染结果与现实相结合。渲染指根据一组模型生成图像，而该图像与真实场景产生融合。由于实现难度较高，因此很多增强现实设备使用预渲染的方法来避免渲染时间过长。该方法将场景在某一位置渲染成一张图像，然后移动到其他位置。这样就可以使用户在几乎零延迟的时间内进行交互，达到更好的沉浸感。

由于混合现实的目的是融合真实世界与虚拟环境，因此需要考虑可穿戴式的虚拟现实设备。传统的虚拟现实设备是放在桌上、头戴显示器上等固定设备上。而可穿戴式的虚拟现实设备可以让用户在自己的身体里面放置，并通过各种输入设备与外部世界进行交互。可穿戴式的虚拟现实设备具有更大灵活性，适合于各种应用场景。

# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
## 3.1 分辨率与距离
虚拟现实的分辨率决定了场景的分辨率。一般来说，分辨率越高，场景看起来就越清晰。但是，因为场景看起来像真实的，所以它看起来就越远。要想让场景看起来像真实的，就需要调整距离。

而距离的调整，又分为调整视野大小和调整相机视角这两种方法。

调整视野大小的原理是缩小场景直径，使得用户只能看到场景的一部分。缩小的尺寸与场景的复杂程度成正比。调整的尺寸是固定的，不能太大，否则可能会影响用户的视线跟踪。调整视野大小的操作流程如下：

1.确定场景复杂度。
2.根据场景复杂度确定场景缩小的尺寸。
3.缩小场景直径。

调整相机视角的原理是调整相机指向的方向，使得用户能看清场景。调整的角度与用户视线的移动速度成正比。调整的角度也是固定的，不能太大，否则用户的注意力会分散。调整相机视角的操作流程如下：

1.确定视线跟踪的区域。
2.移动用户视线到跟踪区域中间。
3.调整相机指向方向。

## 3.2 深度摄像机与透视摄像机
虚拟现实中使用的摄像机可以分为深度摄像机与透视摄像机两种。

深度摄像机拍摄到的图像包含深度信息。每个像素都有一个对应的距离值，描述了距离相机的距离。如果像素的值越小，代表它与相机之间的距离越远；如果像素的值越大，代表它与相机之间的距离越近。

透视摄像机拍摄到的图像没有深度信息。它只有x、y轴坐标，没有距离信息。

为了解决缺乏深度信息的问题，虚拟现实中使用了深度摄像机。虽然它缺乏深度信息，但它的拍摄速度比普通摄像机快得多。而且，它能够提取到场景中的细节信息。不过，深度摄像机对场景的真实度要求较高，需要保证场景清晰且平整。同时，由于深度摄像机拍摄的是每个像素的距离值，导致图像的噪声比较大。因此，深度摄像机只能用于一些特定的场景，比如真实感的拍摄。

## 3.3 移动跟踪
移动跟踪是指虚拟现实的核心功能。在移动跟踪过程中，用户的头部、手、足、躯干等传感器都会收集并处理用户头部的运动数据，通过分析运动轨迹，计算出头部的位置。然后，系统会将头部的位置反馈给显示器，并反复循环播放。用户通过头部的位置观察到场景。

移动跟踪的原理如下：

1.首先，要确定需要跟踪的区域，通常是用户的头部区域。
2.接着，要确定跟踪算法。不同的跟踪算法会影响跟踪的精度。常用的跟踪算法有卡尔曼滤波、伪距匹配法、视觉里程计等。
3.在跟踪过程中，要利用摄像机捕获到的数据，如图像、深度图等，将其作为输入给跟踪算法。
4.跟踪算法会输出一个连续的、稀疏的头部姿态估计序列。
5.最后，系统将头部姿态估计序列显示在显示器上。

## 3.4 透视与投影
透视和投影是建立在图像重构的基础上的。图像重构是指将三维的场景图形投影到二维图像上去，得到的结果就是以二维像素点形式呈现的场景图形。

与透视相似，投影方法也分为正交投影和透视投影两种。

正交投影，顾名思义，就是将场景沿着一条直线来进行投影。投影的坐标轴与屏幕的坐标轴垂直，这样才能将三维场景转化为二维图像。投影的效果就是一条直线，即这条线代表了场景的最低层。

透视投影，即将场景投射到平面上去。当相机位于距离场景较远的时候，投影的效果类似于正交投影。但当相机移动到距离场景较近的地方，投影的效果就变得不同了。当相机和场景之间的夹角变小时，场景就被拉长，投影的效果就好看了。当相机和场景之间的夹角变大时，场景就会被压缩，投影的效果就很差了。

## 3.5 视频序列跟踪
视频序列跟踪，即将连续的视频序列中的每一帧都做跟踪处理。视频序列跟踪用于对动态对象的跟踪，比如人、车等。

该方法的原理如下：

1.首先，要确定需要跟踪的对象。
2.然后，要确定需要使用的跟踪算法。不同的跟踪算法会影响跟踪的精度。常用的跟踪算法有卡尔曼滤波、Hungarian算法、视觉里程计等。
3.在跟踪过程中，要利用视频序列中的每一帧图像进行跟踪处理。
4.在跟踪结束后，系统应该给出视频序列中跟踪的结果。

## 3.6 沉浸式渲染与虚拟现实体态
沉浸式渲染是指将虚拟现实体态与真实实体态融合在一起的渲染方式。这项技术能够将虚拟现实场景渲染到整个显示屏幕上，使得用户在虚拟现实中享受到真实的沉浸感。

该技术的实现方式是将虚拟现实场景的图像，与真实场景的图像混合在一起，显示在一个画面中，用户就像进入了一个全新的世界一样。

虚拟现实实体态的原理是由多种物理、图像材料组成的实体，它具有完美的形体、完整的材质，能够产生真实的反射、折射现象。它能模拟各种各样的材料属性，并具有较强的自然真实感。

沉浸式渲染的特点有三个：
1.真实感。沉浸式渲染将真实世界中的复杂物体模拟成可见的虚拟现实实体，让用户在虚拟现实世界中尽可能真实地感受到场景。
2.协同式。在沉浸式渲染中，两个实体可以交互互动，产生更加真实、动态的效果。
3.数字化。沉浸式渲染使虚拟现实中的物体可以被数字化、模型化，可以通过编程实现不同的交互方式。

# 4.具体代码实例和详细解释说明
下面是一些典型的代码实例。
## 4.1 创建虚拟现实场景
以下代码创建了一个简单的虚拟现实场景，包含一个圆柱体、球体和墙壁。其中，球体在移动鼠标指针的时候会绕着圆柱体转动。
```python
import openvr
from OpenGL.GL import *

width = 1280
height = 720
fb_id = None

def init_gl():
    global fb_id

    if not glfwInit():
        return False
    
    window = glfwCreateWindow(width, height, "My OpenVR Application", None, None)
    if not window:
        glfwTerminate()
        return False
    
    # Create the HMD
    hmd = openvr.VR_Init(openvr.VRApplication_Scene)
    if hmd is None:
        print("Unable to init VR runtime")
        glfwDestroyWindow(window)
        glfwTerminate()
        return False
    
    hmd_pose = openvr.TrackedDevicePose_t()
    
    controller_r = openvr.VRSystem().getTrackedDeviceIndexForControllerRole(openvr.TrackedControllerRole_RightHand)
    if controller_r!= -1:
        pose_r = openvr.VRSystem().getDeviceToAbsoluteTrackingPose(openvr.TrackingUniverseStanding, 0, [controller_r])
        if len(pose_r[controller_r].mDeviceToAbsoluteTracking):
            hmd_pose = pose_r[controller_r]
            
    projection_left = hmd.getProjectionMatrix(openvr.Eye_Left, 0.1, 1000.0)
    projection_right = hmd.getProjectionMatrix(openvr.Eye_Right, 0.1, 1000.0)
    
    depth_texture_left = glGenTextures(1)
    glBindTexture(GL_TEXTURE_2D, depth_texture_left)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)
    glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_COMPONENT24, width, height, 0,
                 GL_DEPTH_COMPONENT, GL_FLOAT, None)
    
    depth_texture_right = glGenTextures(1)
    glBindTexture(GL_TEXTURE_2D, depth_texture_right)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_BORDER)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_BORDER)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)
    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)
    glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_COMPONENT24, width, height, 0,
                 GL_DEPTH_COMPONENT, GL_FLOAT, None)
    
    framebuffers = (GLuint*2)(0, 0)
    glGenFramebuffers(2, framebuffers)
    glBindFramebuffer(GL_FRAMEBUFFER, framebuffers[0])
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D,
                           depth_texture_left, 0)
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D,
                           depth_texture_left, 0)
    glBindFramebuffer(GL_READ_FRAMEBUFFER, framebuffers[0])
    glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)
    glBlitFramebuffer(0, 0, width // 2, height,
                      0, 0, width // 2, height,
                      GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT, GL_NEAREST)
    glBindFramebuffer(GL_FRAMEBUFFER, framebuffers[1])
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D,
                           depth_texture_right, 0)
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D,
                           depth_texture_right, 0)
    glBindFramebuffer(GL_READ_FRAMEBUFFER, framebuffers[1])
    glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0)
    glBlitFramebuffer(width // 2, 0, width, height,
                      0, 0, width // 2, height,
                      GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT, GL_NEAREST)
    
    glfwMakeContextCurrent(None)
    
    return True

while not glfwWindowShouldClose(window):
    glfwPollEvents()

    for i in range(len(controllers)):
        poses = controllers[i].getDeviceToAbsoluteTrackingPose(openvr.TrackingUniverseStanding, 0, [])
        if len(poses[0].mDeviceToAbsoluteTracking):
            tracked_devices[i]["pose"] = poses[0]
        
    view_matrices = []
    proj_matrices = []
    
    vr_events = []
    for event in openvr.VRSystem().pollNextEvent():
        vr_events.append(event)
        if event.eventType == openvr.VREvent_ButtonPress and \
           event.trackedDeviceIndex == openvr.k_unTrackedDeviceIndex_Trackpad:
            if event.data.controller.button == openvr.k_EButton_SteamVR_Touchpad:
                move_forward = 0
            elif event.data.controller.button == openvr.k_EButton_Grip:
                jump = 1
            else:
                pass
        elif event.eventType == openvr.VREvent_ButtonUnpress and \
             event.trackedDeviceIndex == openvr.k_unTrackedDeviceIndex_Trackpad:
            if event.data.controller.button == openvr.k_EButton_SteamVR_Touchpad:
                move_forward = 0
            elif event.data.controller.button == openvr.k_EButton_Grip:
                jump = 0
            else:
                pass
        elif event.eventType == openvr.VREvent_MouseMove:
            x_delta = event.data.mouse.deltaX
            y_delta = event.data.mouse.deltaY
            
            trackpad = controllers[0].getAxis(openvr.k_eControllerAxis_Trigger)[0] + \
                        controllers[0].getAxis(openvr.k_eControllerAxis_TrackPad)[0] / 2.0
                    
            angle_rad = math.atan2(-trackpad, controllers[0].getAxis(openvr.k_eControllerAxis_Thumbstick)[0])
            cos_a = math.cos(angle_rad)
            sin_a = math.sin(angle_rad)
            dx = x_delta * cos_a + -y_delta * sin_a
            dy = -x_delta * sin_a + -y_delta * cos_a

            yaw += float(dx) / 500.0
            pitch -= float(dy) / 500.0
            
            pitch = max(-math.pi/2.0+0.1, min(pitch, math.pi/2.0-0.1))
            roll = 0.0
            
            qyaw = openvr.quat_from_euler(0.0, yaw, 0.0)
            qpitch = openvr.quat_from_euler(pitch, 0.0, 0.0)
            qroll = openvr.quat_from_euler(0.0, 0.0, roll)
            quat = openvr.quat_multiply(qyaw, openvr.quat_multiply(qpitch, qroll))
        
        elif event.eventType == openvr.VREvent_ScrollDiscrete:
            distance *= pow(1.1, event.data.scrollDelta)
        
        
    left_eye_view_matrix = openvr.Hmd_EyeViews()[0]
    right_eye_view_matrix = openvr.Hmd_EyeViews()[1]
    
    for eye in [openvr.Eye_Left, openvr.Eye_Right]:
        render_target_size = (int(width), int(height))
        
        texture_handle = openvr.Hmd_GetEyeTexture(hmd, eye)
        w, h = texture_handle.w, texture_handle.h
        
        glBindFramebuffer(GL_FRAMEBUFFER, framebuffers[eye])
        glViewport(0, 0, w, h)
        glClearColor(0.0, 0.0, 0.0, 0.0)
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)

        proj_matrix = openvr.Hmd_GetProjectionMatrix(hmd, eye, 0.1, 1000.0)
        proj_matrices.append(proj_matrix)
        
        glm.mat4 rot_matrix = glm.rotate(glm::mat4(), (float)glfwGetTime()*0.5f, glm::vec3(0.0f, 1.0f, 0.0f));
        glm.mat4 trans_matrix = glm.translate(glm::mat4(), glm::vec3(0.0f, -distance, 0.0f));
        
        view_matrix = openvr.Hmd_GetEyeToHeadTransform(hmd, eye).matrix() * rot_matrix * trans_matrix;
        view_matrices.append(view_matrix);
        
        pvm_matrix = proj_matrix * view_matrix;
        
        model_matrix = glm.identity<glm::mat4>();
        model_matrix = glm.translate(model_matrix, glm::vec3(0.0f, 1.0f, 0.0f));
        model_matrix = glm.scale(model_matrix, glm::vec3(0.5f));
        pmv_matrix = pvm_matrix * model_matrix;
        
        shader.use();
        shader.setMat4("projection", proj_matrix);
        shader.setMat4("view", view_matrix);
        shader.setMat4("model", model_matrix);
        cylinder.draw();
        
        if tracked_device["index"]:
            pose = tracked_device["pose"].mDeviceToAbsoluteTracking
            offset = glm.vec3(pose[0], pose[1], pose[2]+1.0)
            ball_pos = view_matrix * glm.vec4(offset, 1.0)
            ball_rot = glm.toQuat(glm.inverse(view_matrix) * glm.translation(ball_pos))
            ball_scale = glm.length(offset) / 1.5
            
            model_matrix = glm.identity<glm::mat4>();
            model_matrix = glm.translate(model_matrix, ball_pos);
            model_matrix = glm.scale(model_matrix, glm::vec3(ball_scale, ball_scale, ball_scale));
            model_matrix = glm.mat4_cast(ball_rot)*model_matrix;
            pmv_matrix = pvm_matrix * model_matrix;
            
            shader.setMat4("projection", proj_matrix);
            shader.setMat4("view", view_matrix);
            shader.setMat4("model", model_matrix);
            sphere.draw();
        
    glBindFramebuffer(GL_FRAMEBUFFER, 0)
    glViewport(0, 0, width, height)

    glfwSwapBuffers(window)
    
glfwTerminate()
```
## 4.2 点击式虚拟现实导航
以下代码实现了一个点击式虚拟现实导航。你可以将鼠标指针指向任意虚拟现实场景上的物体，左键点击，即可进入该物体所在的虚拟空间。你可以使用右键点击，切换至自由视角。
```python
import openvr
import pybullet as pb

from OpenGL.GL import *

width = 1280
height = 720
fb_id = None

clicked = False
grip_pressed = False
move_forward = 0
jump = 0

init_gl()

while not glfwWindowShouldClose(window):
    glfwPollEvents()

    for i in range(len(controllers)):
        poses = controllers[i].getDeviceToAbsoluteTrackingPose(openvr.TrackingUniverseStanding, 0, [])
        if len(poses[0].mDeviceToAbsoluteTracking):
            tracked_devices[i]["pose"] = poses[0]

    click_events = list(filter(lambda e: e.eventType==openvr.VREvent_MouseButtonDown or 
                                     e.eventType==openvr.VREvent_MouseButtonUp,
                               vr_events))
    
    grip_pressed = bool(controllers[0].getDigitalActionData(actions['grip']).bState)
    trigger_value = controllers[0].getAnalogActionData(actions['trigger']).x

    if trigger_value > 0.9:
        clicked = any([ce.button=='trigger' and ce.state for ce in click_events])
    
    left_hand_state = controllers[0].getComponentState(openvr.k_pchControllerComponent_Tip,
                                                         openvr.k_ulButtonStatus_Pressed | openvr.k_ulButtonStatus_Touched,
                                                         0)
    right_hand_state = controllers[0].getComponentState(openvr.k_pchControllerComponent_Touchpad,
                                                           openvr.k_ulButtonStatus_Pressed | openvr.k_ulButtonStatus_Touched,
                                                           0)

    if left_hand_state.bPressed:
        move_forward = 1
    elif left_hand_state.bReleased:
        move_forward = 0

    cam_position, cam_rotation = get_camera_transform()
    
    current_time = glfwGetTime()
    delta_time = current_time - last_frame_time
    last_frame_time = current_time
    
    pb.stepSimulation()
    
    display_gl()

pb.disconnect()
glfwTerminate()
```