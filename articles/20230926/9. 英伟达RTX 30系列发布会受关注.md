
作者：禅与计算机程序设计艺术                    

# 1.简介
  

随着数字技术的飞速发展、移动互联网、云计算等新兴技术的不断革新，英伟达公司也在不断地创新产品，推出基于GPU硬件加速图形处理技术，帮助客户实现更好的用户体验和业务效率。今天，英伟达推出的新一代产品RTX 30 Series于上午11点在NVIDIA新泽西州Palace Palace Residence举行发布会。
本文将主要介绍英伟达推出的基于NVIDIA Tesla T4架构的RTX 30 Series GPU处理器。
# 2.基本概念和术语说明
## 2.1 CUDA编程模型
CUDA是由NVIDIA提供的并行编程语言和开发环境。它允许开发者利用GPU的并行性，从而高效执行复杂的算法。
CUDA编程模型中，有一个显著特点就是端到端并行。开发者编写的代码首先被编译成字节码，然后再通过主机CPU把它传送给对应的GPU设备进行处理。这样就可以让GPU充分利用多核CPU的资源进行并行计算。
## 2.2 GPU工作流
GPU是一个纯粹的通用计算平台，用于处理各种并行任务。GPU能够同时执行多条线程（线程组），每个线程可以处理许多个数据项。为了提高性能，GPU采用了一种称为工作流的方式对线程进行编排。
GPU的工作流包括三个阶段：编译阶段、执行阶段和内存管理阶段。
- 编译阶段：首先，代码将根据用户指定的线程块尺寸和执行模式进行编译，生成指令流。然后，这些指令流被加载到GPU的高速缓存中，等待GPU执行。编译阶段通常只需要发生一次。
- 执行阶段：当指令流准备好被执行时，GPU中的ALU单元就开始从高速缓存中取出指令。根据指令的操作类型（如单精度或双精度浮点运算、加载和存储），ALU执行相应的操作。ALU的输出可以直接写入到输出缓冲区中，或者被传输到另一个寄存器中供其他ALU使用。
- 内存管理阶段：在实际应用中，GPU上的内存往往是十分宝贵的资源。因此，GPU提供了一套自动化的内存管理机制，对分配和释放内存非常有效。如果某些任务需要的内存超出当前可用内存，GPU还可以自动申请新的内存，从而避免内存碎片的问题。
## 2.3 并行编程模型
### 2.3.1 数据并行
数据并行指的是任务拆分成多个子任务，由不同的处理单元（如线程）分别处理。每个处理单元负责处理部分数据集，然后将结果写入全局变量中，整个过程具有无依赖关系。典型的数据并行算法有求和、排序、矩阵乘法等。
### 2.3.2 模板并行
模板并行指的是同一个函数被不同的参数值传入，不同参数值的处理都放在不同的处理单元中完成。典型的模板并行算法有分支界限搜索、动态规划、机器学习等。
### 2.3.3 指令级并行
指令级并行指的是程序中的每一条指令均可选定相应的处理单元进行处理，各个处理单元之间没有明确的数据依赖关系。通常情况下，指令级并行可以获得最佳性能，因为它可以充分利用处理单元的硬件优势。典型的指令级并行算法有向量相乘、FFT、图像处理等。
## 2.4 CUDA编程接口
CUDA编程接口提供了丰富的API，方便开发者编写并行程序。目前，CUDA支持C/C++、Fortran、Python等编程语言。这些接口都通过统一的驱动程序管理GPU资源，为开发者提供了便利。
## 2.5 NVIDIA GPUs
NVIDIA GPUs是英伟达公司自主研发的高性能处理器。它们由图形处理单元（Graphics Processing Unit, GPC）、核心芯片（Core Chip）和系统控制器（System Controller）三部分组成。
GPC的架构包括四种流处理器（Streaming Multiprocessors, SMPs）、六种常用功能单元（FUs: FP32, FP16, INT32, INT16, INT8, BF16）。
核心芯片由10至12个颗晶体管组成，每颗晶体管具有16~17个基本运算单元（Basic Arithmetic Units, BAU）。由于GPC和核心芯片共同协作完成复杂的图形处理任务，使得他们之间的通信延迟非常低。
系统控制器连接了所有硬件设备，并通过指令集并行（Instruction Level Parallelism, ILP）、乱序执行（Out-of-Order Execution, OOE）等技术优化性能。系统控制器还负责系统级任务（如启动设备、监控设备、管理电源），并确保安全稳定的运行。
# 3.核心算法原理和具体操作步骤以及数学公式讲解
RTX 30 Series的架构与前一代RTX 20系列相比有很大的改进，主要变化如下：
1. 架构升级：RTX 3000于2017年推出，是第一款支持图形和AI计算的新一代图形处理器。RTX 30系列则是在前述架构的基础上进行了深度升级，整体架构与RTX 20系列类似。RTX 3000于2020年发布，是支持Turing T4架构的第五代产品。RTX 30系列则由Tesla T4架构的2080 Ti、2080、2070、2060及更早版本组成，其中2080Ti是最强的版本，其拥有8GB GDDR6X、Tensor Core 张量加速、PCI Express 4.0 x16、ROCm等显著特征。 
2. 核心性能提升：Tesla T4架构的GPUs具有全新设计，配备2.4 TFLOPS FLOP，加速神经网络推理、视频渲染、游戏、可视化分析等领域的应用。此外，T4架构的GPU还有一系列新的性能特性，例如宽MEM延迟（WMem Latency）、Tensor Cores、Dynamic Wavefront Sizing (DWS)等。
3. 总线宽度扩展：RTX 30系列和RTX 3000系列采用全新架构，带宽达到了16 GB/s。此外，还引入了新的Tensor Cores架构，可实现复杂神经网络的并行计算。通过向传统GPU架构添加更多的Tensor Cores可同时解决内存带宽需求和计算性能需求，进一步提高GPU的利用率。
4. 支持容器：RTX 30系列GPUs不仅仅可以用于图形和计算密集型任务，还支持容器，通过容器技术，可以部署运行大量的并行任务。
## 3.1 计算核心架构
RTX 30系列的核心架构与前一代RTX 20系列一样，由四个“Stream Multiprocessors”（SMPs）和6个“Function Units”（FUs）组成，每一个SMP是一个线程块（Thread Block）包含两个核心（SMs），每个核心含有64个计算单元（CU）(核心核心配置)。其中FP32、FP16和INT8计算单元分别用于浮点运算、半精度浮点运算和整数运算。通过这种核心架构，可以有效地利用GPU的并行性，提升图形处理性能。
每个SM持有自己的1MB L2缓存，共享一个128KB的L1缓存。在每个CU中，还包括多个单精度、双精度、乘积、矢量加法等算术逻辑单元，以及读取和写入Cache的单元。另外，每个CU除了计算能力之外，还可以执行控制操作，如分支、跳转、循环、同步、内存操作等。CU的数量越多，就可以同时执行更多的线程。
## 3.2 Tensor Core计算
Tensor Core是RTX 30系列的重要特性，可以通过并行计算提升计算性能。Tensor Core由4个核心单元组成，具有较高的单精度吞吐量、深度（深度卷积）、宽度（Grouped Convolution）和内存访问速度，适合于深度学习算法。RTX 30系列支持的Tensor Core计算有深度学习、视频处理、机器翻译等领域。
Tensor Core的运算能力由Tensor Core的尺寸决定，尺寸可以从16x16（32倍）、8x8（64倍）、4x4（256倍）等。Tensor Core运算能力接近数据核心（Data Core）运算能力，两者兼顾性能。
图1展示了Tensor Core的结构示意图。其中输入数据经过Shuffle-Exchange层之后，进入8个16x16的核心单元进行运算，运算后的数据再经过Broadcast-Reduce层得到最终结果。数据通过DMA传送到GPU的主内存中。
## 3.3 深度学习计算加速
深度学习计算加速主要使用了Tensor Core加速。在训练神经网络模型时，GPUs可以同时处理多个训练样本，极大地减少了训练时间，提升了模型的训练效率。
传统的深度学习算法大多采用数据并行的方式，即将一个神经网络模型切割成多个小的网络，每个网络负责处理一定范围内的输入数据。但是这种方法存在很多缺陷，比如消耗过多的内存空间，不够灵活，容易出现模型退化的问题。RTX 30系列采用了“横向分流（Horizontal Data Parallelization）”的方法，将一个神经网络模型划分为多个子模块，每个子模块处理不同的输入数据，实现真正的“横向”并行，充分利用GPU的资源。
图2展示了深度学习加速的结构示意图。在这个示意图中，左侧子图展示了普通神经网络模型的串行计算方式，右侧子图展示了通过横向分流后的神经网络模型的并行计算方式。传统方法只能串行计算模型的所有计算节点，无法充分利用GPU的计算资源；通过横向分流的方法，模型被划分为多个子模块，每个子模块负责处理不同的输入数据，充分利用了GPU的计算资源，可以实现真正的并行计算。
## 3.4 图像处理加速
图像处理是计算机视觉领域的研究热点，特别是近年来基于深度学习的神经网络模型应用广泛。因此，在RTX 30系列中，还进行了图像处理加速，帮助客户在图像识别、内容生产等方面实现高效率。
RTX 30系列的图像处理能力依然采用Tensor Core的形式，只是在应用方面又加入了内存访问优化。具体来说，Tensor Core的尺寸为16x16，可同时执行4个核心单元的运算。与普通GPU相比，RTX 30系列的图像处理速度有显著提升。
图3展示了图像处理加速的结构示意图。图像处理通常涉及大量的图像运算，需要大量的内存计算。因此，为了充分利用GPU的内存计算能力，RTX 30系列采用了Shuffle-Exchange层对图像数据进行分块处理。图片在各个核之间进行移位交换，数据在每个核之间共享，可以有效地减少内存占用，提升运算性能。
## 3.5 桌面渲染加速
桌面渲染（Desktop Rendering）是计算机图形学中最常用的应用场景。RTX 30系列通过优化内存访问和计算性能，在桌面渲染上也取得了巨大的成功。
在RTX 30系列中，采用2.4TFLOPS的计算能力，已成为许多传统渲染引擎的瓶颈。为了提高桌面渲染的性能，RTX 30系列提出了Dynamic Wavefront Sizing（DWS）策略，可以在一定程度上优化GPU的渲染性能。DWS策略通过对线程调度和内存访问优化，可以将工作负载均匀分配到多个SM上，进一步提升渲染性能。
在RTX 30系列中，还开发了一系列新的渲染算法，比如支持动态分辨率（Dynamic Resolution）、屏幕空间高光（Screen Space Ambient Occlusion，SSAO）、反射探针（Reflection Probe，RP）、软阴影（Soft Shadow）等。
图4展示了桌面渲染加速的结构示意图。在该示意图中，左侧子图展示了传统GPU渲染方案，右侧子图展示了通过DWS优化后的渲染方案。传统方法需要将渲染图像划分为小的区域，进行串行渲染。DWS方法通过优化线程调度和内存访问，可以将渲染工作量均匀分配到多个SM上，进一步提升渲染性能。
# 4.具体代码实例和解释说明
以下是一些RTX 30系列的具体代码示例，供参考：
```
//Cuda编程模型
#include<cuda_runtime.h>
__global__ void add(int *a, int *b, int *c){
    c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x];
}
int main(){
    const int N = 10; //数据大小
    int *a, *b, *c; 
    cudaMalloc((void**)&a, sizeof(int)*N); //分配内存
    cudaMalloc((void**)&b, sizeof(int)*N);
    cudaMalloc((void**)&c, sizeof(int)*N);

    //初始化数据
    for(int i=0;i<N;++i){
        a[i]=i;
        b[i]=i*i;
    }
    //调用kernel
    add<<<dim3(N), dim3(1)>>>(a, b, c); 

    //获取结果
    int result;
    cudaMemcpy(&result, c, sizeof(int), cudaMemcpyDeviceToHost); //获取计算结果
    printf("%d\n", result);

    //释放内存
    cudaFree(a);
    cudaFree(b);
    cudaFree(c);
    
    return 0;
}
```
```
//计算核心架构
#include <stdio.h>  
#define BLOCKSIZE 1024  
  
__global__ void myKernel() {  
    unsigned int idx = threadIdx.x + blockDim.x * blockIdx.x;  
  
    float sum = 0.0f;  

    for (unsigned int j = idx; j < SIZE; j += BLOCKSIZE)  
        sum += exp(-j / 10.0f);  
  
    data[idx] = sum;  
}  

int main() {  
    const int SIZE = 1 << 20; //数组大小
    float *data; //设备端数据数组
    float *sumArray; //设备端求和数组
  
    //分配数据
    cudaMalloc((void**)&data, sizeof(float) * SIZE);
    cudaMalloc((void**)&sumArray, sizeof(float));
  
    //设置block和grid
    dim3 threadsPerBlock(BLOCKSIZE);
    dim3 numBlocks((SIZE + BLOCKSIZE - 1) / BLOCKSIZE); 
  
    //调用kernel
    myKernel <<<numBlocks, threadsPerBlock>>> ();  
  
    //获取结果
    cudaMemcpy(sumArray, data, sizeof(float), cudaMemcpyDeviceToHost);
    printf("Sum of exponential values is %f\n", sumArray[0]);
  
    //释放内存
    cudaFree(data);
    cudaFree(sumArray);
  
    return 0;  
}
```
```
//Tensor Core计算
#include <stdio.h>   
  
const int THREADS = 1024;  
const int ROWS = 1024;  
const int COLUMNS = 1024;  

//定义kernel函数
__global__ void matrixMultiplication(float *A, float *B, float *C) {
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    __shared__ float As[ROWS][COLUMS/THREADS];
    __shared__ float Bs[COLUMS][ROWS/THREADS];

    // Load input submatrix into shared memory
    if (tx < ROW && ty < COLUMN) {
        As[tx][ty] = A[tx][ty];
        Bs[tx][ty] = B[tx][ty];
    }

    // Synchronize to make sure the submatrices are loaded
    __syncthreads();

    // Multiply submatrices on device
    for (int k = 0; k < THREADS; ++k) {
        C[tx][k+ty*THREADS] = 0.0f;

        for (int m = 0; m < COLUMNS/THREADS; ++m)
            C[tx][k+ty*THREADS] += As[tx][k+m*THREADS] * Bs[k+m*THREADS][ty];
    }

    // Synchronize before using the result
    __syncthreads();
}

//计算函数
void multiplyMatrixOnGPU(float *A, float *B, float *C, int rows, int cols) {
    int size = rows * cols * sizeof(float);
    float *deviceA, *deviceB, *deviceC;

    // Allocate memory on the device
    cudaMalloc((void**)&deviceA, size);
    cudaMalloc((void**)&deviceB, size);
    cudaMalloc((void**)&deviceC, size);

    // Copy matrices to device
    cudaMemcpy(deviceA, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(deviceB, B, size, cudaMemcpyHostToDevice);

    // Invoke kernel function
    int gridSize = ((rows + THREADS - 1) / THREADS) * ((cols + THREADS - 1) / THREADS);
    dim3 blockSize(THREADS, THREADS);
    dim3 gridSize(gridSize);

    matrixMultiplication<<<gridSize, blockSize>>>(deviceA, deviceB, deviceC);

    // Get results back from device
    cudaMemcpy(C, deviceC, size, cudaMemcpyDeviceToHost);

    // Free allocated memory
    cudaFree(deviceA);
    cudaFree(deviceB);
    cudaFree(deviceC);
}

int main() {
    srand(time(NULL));

    float *A, *B, *C;
    int rows = ROWS;
    int columns = COLUMNS;

    // Initialize matrices
    A = new float[rows*columns];
    B = new float[rows*columns];
    C = new float[rows*columns];

    for (int i = 0; i < rows*columns; ++i) {
        A[i] = rand()%RAND_MAX;
        B[i] = rand()%RAND_MAX;
    }

    // Call multiplication function
    multiplyMatrixOnGPU(A, B, C, rows, columns);

    // Print first element of resulting matrix
    cout<<"The product value at index [0][0] is "<<C[0]*C[COLUMNS];

    delete[] A;
    delete[] B;
    delete[] C;

    return 0;
}
```
```
//图像处理加速
#include <stdio.h>   
#include <stdlib.h>   

#include <iostream>    
#include <chrono>      
#include <cuda.h>    

using namespace std::chrono;       


__global__ void processImage(uint8_t *input, uint8_t *output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x >= width || y >= height) {
        return;
    }

    output[y*width + x] = input[(height-1-y)*width + x]/2 + 128;
}



int main() {    
    const int IMAGEWIDTH = 640;
    const int IMAGEHEIGHT = 480;


    // allocate host memory for image buffers and set initial values
    uint8_t *hostInput = (uint8_t*)malloc(IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));
    uint8_t *hostOutput = (uint8_t*)malloc(IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));

    // fill random pixel values in input buffer
    for(int y=0; y<IMAGEHEIGHT; y++) {
        for(int x=0; x<IMAGEWIDTH; x++) {
            hostInput[y*IMAGEWIDTH + x] = rand()%256;
        }
    }

    // copy inputs to device
    uint8_t *deviceInput;
    uint8_t *deviceOutput;
    cudaMalloc(&deviceInput, IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));
    cudaMalloc(&deviceOutput, IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));
    cudaMemcpy(deviceInput, hostInput, IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t), cudaMemcpyHostToDevice);


    high_resolution_clock::time_point t1 = high_resolution_clock::now();

    // Launch the kernel with one thread per pixel
    dim3 blocksPerGrid(ceil(IMAGEWIDTH / 16.0f), ceil(IMAGEHEIGHT / 16.0f));
    dim3 threadsPerBlock(16, 16);
    processImage<<<blocksPerGrid, threadsPerBlock>>>(deviceInput, deviceOutput, IMAGEWIDTH, IMAGEHEIGHT);

    // Wait for kernels to finish executing
    cudaDeviceSynchronize();

    high_resolution_clock::time_point t2 = high_resolution_clock::now();


    // copy outputs back to host
    cudaMemcpy(hostOutput, deviceOutput, IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t), cudaMemcpyDeviceToHost);

    auto duration = duration_cast<microseconds>(t2 - t1).count()/1000000.0;

    // print average time taken by each thread to execute the kernel
    double totalTimeTakenByThreads = duration * blocksPerGrid.x * blocksPerGrid.y * threadsPerBlock.x * threadsPerBlock.y;
    cout << "Average time taken by each thread to execute the kernel:" << endl;
    cout << "\t" << totalTimeTakenByThreads/(double)(IMAGEWIDTH * IMAGEHEIGHT)<< " seconds."<<endl;

    
    // display original and processed images
    cv::Mat inputImg(cv::Size(IMAGEWIDTH, IMAGEHEIGHT), CV_8UC1, (void *)hostInput);
    cv::Mat outputImg(cv::Size(IMAGEWIDTH, IMAGEHEIGHT), CV_8UC1, (void *)hostOutput);

    cv::imshow("Original Image", inputImg);
    cv::imshow("Processed Image", outputImg);
    cv::waitKey();


    free(hostInput);
    free(hostOutput);

    cudaFree(deviceInput);
    cudaFree(deviceOutput);

    return 0;
}
```
```
//桌面渲染加速
#include <stdio.h>   
#include <stdlib.h>   

#include <iostream>    
#include <chrono>      
#include <cuda.h>     

using namespace std::chrono;       

typedef struct Vec3 {
    float x;
    float y;
    float z;
};

inline __host__ __device__ Vec3 operator+(const Vec3& u, const Vec3& v) {
    Vec3 r;
    r.x = u.x + v.x;
    r.y = u.y + v.y;
    r.z = u.z + v.z;
    return r;
}

inline __host__ __device__ Vec3 operator-(const Vec3& u, const Vec3& v) {
    Vec3 r;
    r.x = u.x - v.x;
    r.y = u.y - v.y;
    r.z = u.z - v.z;
    return r;
}

inline __host__ __device__ Vec3 operator*(float s, const Vec3& u) {
    Vec3 r;
    r.x = s * u.x;
    r.y = s * u.y;
    r.z = s * u.z;
    return r;
}

struct Ray {
    Vec3 origin;
    Vec3 direction;
};

struct Sphere {
    Vec3 center;
    float radius;
    bool inside;
};

struct Intersection {
    bool hit;
    Vec3 point;
    float distance;
    Vec3 normal;
};

__device__ __forceinline__ bool intersectSphere(Ray ray, Sphere sphere, Intersection& intersection) {
    Vec3 oc = ray.origin - sphere.center;
    float a = dot(ray.direction, ray.direction);
    float b = 2.0f * dot(oc, ray.direction);
    float c = dot(oc, oc) - sphere.radius*sphere.radius;
    float discriminant = b*b - 4.0f*a*c;
    if(discriminant > 0.0f) {
        float sqrtDiscriminant = sqrtf(discriminant);
        float t1 = (-b - sqrtDiscriminant)/(2.0f*a);
        float t2 = (-b + sqrtDiscriminant)/(2.0f*a);
        if(t1 > 0.0f && t1 <= 1e-3f || t2 > 0.0f && t2 <= 1e-3f) {
            float t = min(max(t1, 0.0f), 1e3f);
            intersection.hit = true;
            intersection.distance = t * ray.direction.length();
            intersection.normal = (intersection.point - sphere.center)/sphere.radius;
            intersection.point = ray.origin + t * ray.direction;
            return true;
        } else if(t1 > 0.0f && t1 <= 1e3f) {
            intersection.hit = true;
            intersection.distance = t1 * ray.direction.length();
            intersection.normal = (intersection.point - sphere.center)/sphere.radius;
            intersection.point = ray.origin + t1 * ray.direction;
            return true;
        } else if(t2 > 0.0f && t2 <= 1e3f) {
            intersection.hit = true;
            intersection.distance = t2 * ray.direction.length();
            intersection.normal = (intersection.point - sphere.center)/sphere.radius;
            intersection.point = ray.origin + t2 * ray.direction;
            return true;
        }
    }
    intersection.hit = false;
    return false;
}

__device__ __forceinline__ bool intersectScene(Ray& ray, Sphere*& scene, int numObjects, Intersection& intersection) {
    float closestSoFar = INFINITY;
    bool anyHit = false;
    for(int i = 0; i < numObjects; i++) {
        Intersection tempIntersection;
        if(intersectSphere(ray, scene[i], tempIntersection)) {
            anyHit = true;
            if(tempIntersection.distance < closestSoFar) {
                closestSoFar = tempIntersection.distance;
                intersection = tempIntersection;
            }
        }
    }
    return anyHit;
}

__global__ void renderImage(uint8_t* imageBuffer, int imageWidth, int imageHeight, Sphere* objects, int numObjects) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if(x >= imageWidth || y >= imageHeight) {
        return;
    }

    float aspectRatio = (float)imageWidth/(float)imageHeight;

    float fov = M_PI/3.0f;
    float halfViewHeight = tan(fov/2.0f);
    float halfViewWidth = aspectRatio * halfViewHeight;
    Vec3 lowerLeftCorner = Vec3(-halfViewWidth, -halfViewHeight, -1.0f);
    Vec3 horizontal = Vec3(2.0f*halfViewWidth, 0.0f, 0.0f);
    Vec3 vertical = Vec3(0.0f, 2.0f*halfViewHeight, 0.0f);
    Vec3 origin = Vec3(0.0f, 0.0f, 0.0f);

    Ray ray;
    ray.origin = origin;
    ray.direction = normalize(lowerLeftCorner + x*horizontal + y*vertical - origin);

    Intersection intersection;
    intersectScene(ray, objects, numObjects, intersection);

    if(!intersection.hit) {
        imageBuffer[y*imageWidth + x] = 0;
    } else {
        if(intersection.inside) {
            Vec3 color = Vec3(1.0f, 1.0f, 1.0f);
            Vec3 lightDir = Vec3(0.57735f, 0.57735f, 0.57735f);
            Vec3 normal = normalize(intersection.normal);
            float diffuseIntensity = max(dot(lightDir, normal), 0.0f);
            Vec3 colorWithLight = color * diffuseIntensity;

            imageBuffer[y*imageWidth + x] = round(colorWithLight.x * 255.0f);
        } else {
            Vec3 redColor = Vec3(1.0f, 0.0f, 0.0f);
            Vec3 greenColor = Vec3(0.0f, 1.0f, 0.0f);
            Vec3 blueColor = Vec3(0.0f, 0.0f, 1.0f);

            Vec3 color = Vec3(0.0f, 0.0f, 0.0f);
            Vec3 reflectDir = normalize(reflect((-intersection.direction), intersection.normal));

            Vec3 viewDir = Vec3(0.0f, 0.0f, -1.0f);
            float specularIntensity = pow(max(dot(viewDir, reflectDir), 0.0f), 32.0f);
            
            Vec3 colorWithSpecular = (specularIntensity > 0.0f)? (blueColor + greenColor + redColor)*(specularIntensity*0.8f) : redColor;

            imageBuffer[y*imageWidth + x] = round(colorWithSpecular.x * 255.0f);
        }
    }
}

int main() {
    const int IMAGEWIDTH = 512;
    const int IMAGEHEIGHT = 512;

    // create scene geometry
    Sphere* spheres = new Sphere[3];
    spheres[0] = {Vec3(0.0f, 0.0f, -1.0f), 0.5f, false};
    spheres[1] = {Vec3(0.0f, -100.5f, -1.0f), 100.0f, true};
    spheres[2] = {Vec3(1.0f, 0.0f, -1.0f), 0.5f, false};

    // allocate host memory for image buffer and initialize it
    uint8_t* hostImageBuffer = (uint8_t*) malloc(IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));

    // clear image buffer
    memset(hostImageBuffer, 0, IMAGEWIDTH * IMAGEHEIGHT * sizeof(uint8_t));

    // copy scene information to device
    Sphere* deviceSpheres;
    cudaMalloc(&deviceSpheres, sizeof(Sphere) * 3);
    cudaMemcpy(deviceSpheres, spheres, sizeof(Sphere) * 3, cudaMemcpyHostToDevice);

    // launch rendering kernel
    dim3 blocksPerGrid(ceil(IMAGEWIDTH / 16.0f), ceil(IMAGEHEIGHT / 16.0f));
    dim3 threadsPerBlock(16, 16);

    high_resolution_clock::time_point t1 = high_resolution_clock::now();

    renderImage<<<blocksPerGrid, threadsPerBlock>>>(hostImageBuffer, IMAGEWIDTH, IMAGEHEIGHT, deviceSpheres, 3);

    high_resolution_clock::time_point t2 = high_resolution_clock::now();

    // get elapsed time
    auto duration = duration_cast<microseconds>(t2 - t1).count()/1000000.0;

    // calculate performance metrics
    double totalPixelsRendered = IMAGEWIDTH * IMAGEHEIGHT;
    double megaPixelsPerSecond = totalPixelsRendered / duration / 1000000.0;

    cout << "Rendering took " << duration << " seconds." << endl;
    cout << "Rendered " << totalPixelsRendered << " pixels (" << megaPixelsPerSecond << " MP/s)." << endl;

    // display rendered image
    cv::Mat image(cv::Size(IMAGEWIDTH, IMAGEHEIGHT), CV_8UC1, hostImageBuffer);
    cv::imshow("Rendered Image", image);
    cv::waitKey(0);

    free(hostImageBuffer);

    cudaFree(deviceSpheres);

    delete [] spheres;

    return 0;
}
```