
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Semi-supervised learning (SSL) is a challenging and promising machine learning task that aims to leverage both labeled data and unlabeled data for better generalization performance on the target task. However, the conventional SSL methods typically employ supervised pre-training of deep neural networks, which requires large amounts of annotated data to learn reliable features from raw images. In this paper, we propose a new SSL method called Semi-Supervised Learning via Disentangled Representations (SSLDR), where instead of traditional supervised training, our model learns disentangled representations by leveraging multiple views of input data without any label information. This leads to an end-to-end framework that can work with small amount of noisy labels or even noisy image pairs. We also provide theoretical analysis to demonstrate the properties of disentangled representation and prove its efficacy in solving semi-supervised learning problems. Finally, we evaluate the proposed SSL method on several popular benchmark datasets including CIFAR-10/100, SVHN, STL-10, and ImageNet, and show significant improvement over state-of-the-art approaches.
This article introduces a novel SSL approach called SSLDR that explores disentangled representation as a fundamental concept for improving SSL performance. Specifically, our model builds upon the insight that different aspects of an image are often related but not fully captured by single modality, such as texture and contextual relationships between pixels. By decomposing each input image into these shared and independent components, our network learns robust features that are disentangled along various dimensions. To train our model, we present two effective ways of generating noisy pseudo-labels based on disentangled representations: global negative sampling strategy and hardness thresholding strategy. These techniques effectively balance the tradeoff between accuracy and computational efficiency during training. Experiments demonstrate that SSLDR achieves competitive results against several state-of-the-art SSL algorithms, especially when dealing with limited annotated data or noisey labels. Our source code will be made publicly available. 

# 2.基本概念术语说明
We use the following definitions to describe SSLDR:
1. Input image x ∈ R^{H×W×C} represents the RGB pixel values of an H × W × C image, where H, W, and C denote height, width, and number of color channels respectively.
2. Output variable y ∈ {0,1}^K represents the binary class labels of an input image. K is the total number of classes. For simplicity, we assume there is only one output node corresponding to positive examples while all other nodes correspond to negative examples.
3. Local view $v_l$ and Global view $v_g$, where $\mathbf{x}_l = \sigma(\Phi(x))$ and $\mathbf{x}_g = \sum_{i=1}^{N}\frac{\exp(-\|\mathbf{f}_i-\mathbf{x}_l\|^2/\epsilon)} {\sum_{j=1}^{M}\exp(-\|\mathbf{f}_j-\mathbf{x}_l\|^2/\epsilon)}, N$ and M represent the numbers of local and global feature vectors respectively. $\Phi$ is the base convolutional layer followed by pooling layers, and $\sigma$ is the softmax activation function. The parameter $\epsilon$ controls the smoothness of decision boundary, and it is usually set to be around half of the receptive field size of the last conv layer. Therefore, if the last conv layer has a receptive field size of FxF, then $\epsilon$ should be set to be $(F/2)^2$.
4. Disentangled Representation D(x; v) is defined as the element-wise product of local view $v_l$ and global view $v_g$ obtained from input image $x$: $$D(x;v)=v_l\odot v_g$$
5. Latent Space Z ∈ R^{K∗d} represents the latent space of the classifier where d is the dimension of disentangled representation. It consists of d-dimensional vectors arranged in K groups such that the i-th group contains all d-dimensional vectors associated with the i-th class. Initially, Z is initialized randomly using a Gaussian distribution or learned from scratch depending on whether prior knowledge about semantic structure exists or not. During training, Z is optimized using stochastic gradient descent (SGD).

# 3.核心算法原理和具体操作步骤以及数学公式讲解
## Introduction
In this section, we first introduce the problem setting and key challenges of SSL and SSLDR. Then, we explain how our SSLDR algorithm works and what makes it different from existing SSL methods. Afterwards, we discuss more details regarding training and evaluation strategies used in our method. Lastly, we summarize and conclude the paper.