
作者：禅与计算机程序设计艺术                    

# 1.简介
  

&emsp;&emsp;Transformer模型(Vaswani等人，2017)是目前最热门的基于注意力机制的深度学习模型之一，其在多任务学习、机器翻译、自动问答等NLP领域都取得了优秀的成绩。但由于Transformer的复杂结构，导致其对内存占用和计算时间的需求较高。因此，研究人员们设计出了Positional Encoding方案来解决这一问题，即通过编码输入序列中各个位置之间的关系来提升模型的表达能力。本文将以此篇论文的内容为基础，系统性地进行Positional Encoding相关的理论及实践研究。

&emsp;&emsp;对于基于序列到序列(Seq2seq)模型的NLP任务，包括语言模型、机器翻译、文本摘要、问答系统等，同样需要考虑词序信息。传统的做法是在训练数据中添加位置编码(positional embedding)，即给每个单词的embedding加上一个位置向量。这种方法简单直观，但是效果一般，且容易造成梯度消失或爆炸的问题。而Transformer通过增加位置编码模块，将位置信息融入到神经网络的输入中，从而能够提升模型的表现力和上下文理解能力。

&emsp;&emsp;Positional Encoding可以看作一种自回归的方式，也就是说，当某个位置的信息被用来预测下一个位置时，这个位置所对应的信息会影响当前位置的预测结果。从理论上来说，位置编码使得模型不仅能够捕捉到序列中存在的依赖关系，还能够捕捉到不同位置之间的相互作用。例如，位置i的信息对于预测位置j的信息也十分重要。

# 2.基本概念术语说明
## 2.1 Transformer概述
&emsp;&emsp;Transformer是一种基于注意力机制的Seq2Seq模型，它主要由encoder和decoder组成。为了更好地理解Transformer，首先引入一些基本概念。
### 2.1.1 Self-Attention
&emsp;&emsp;Attention mechanism指的是计算某些元素对另一些元素的相关性，并根据相关性对元素的表示进行重塑。Transformer模型中的Self-Attention是指在相同的时间步长内，计算各个位置之间关联性的过程。Self-Attention的关键思想是将注意力集中于源序列的当前状态，而非全局考虑整个目标序列。Self-Attention可看作是编码器输出与解码器输入之间的交互映射。

### 2.1.2 Encoder-Decoder
&emsp;&emsp;Encoder-Decoder结构指的是将Seq2Seq模型拆分为编码器（Encoder）和解码器（Decoder）两个独立的子模型，其中编码器负责生成含有完整信息的源序列表示，而解码器则将该表示用于产生目标序列的一系列输出。

### 2.1.3 Position Embedding
&emsp;&emsp;Position Embedding是在输入序列中加入位置信息的一种方式。每条序列都是由固定长度的数字标记表示的。比如，对于英语序列，如果使用1-based编号，那么第一单词的编号就是1，第二单词的编号就是2，依次类推。在构建输入序列时，不需要指定具体位置的标记，而是直接给予每个位置一个唯一的位置向量作为标记。这样做的原因是，当序列长度较短的时候，如小于等于几百，那么采用1-based编号就够用；而当序列长度较长时，比如几千、几万，采用1-based编号可能会造成溢出，甚至导致模型无法正常训练。因此，通常会使用更小的位置嵌入，如位置向量与其他特征组合后的One-Hot向量等。

## 2.2 Positional Encoding
&emsp;&emsp;Positional Encoding又称作相对位置编码，是一个在序列中加入位置信息的策略。Positional Encoding可以认为是一种在Transformer中的Self-Attention操作的特殊情况，它的实现方法是，对输入的embedding向量进行加权，使得其与距离当前位置越近的位置具有更高的权重，同时远离当前位置的位置所占的权重越低。

形式化定义如下：
$$PE_{(pos,2i)}=\sin{(pos/10000^{2i/d_model})}$$
$$PE_{(pos,2i+1)}=\cos{(pos/10000^{2i/d_model})}$$

其中，$PE_{(pos,2i)}$ 和 $PE_{(pos,2i+1)}$ 分别表示第 $pos$ 个位置对应的 $i$ 个维度上的Sin函数和Cos函数值。这里的 $pos$ 表示当前位置，$d\_model$ 表示模型的输入维度大小，默认情况下是 $512$。

下面举例说明：假设模型的输入维度大小为 $512$，那么 $PE_{(1,2i)}, PE_{(1,2i+1)}$ 是 $PE_{(2,2i)}, PE_{(2,2i+1)}$ 的正弦和余弦函数。

$$\begin{align*}
PE_{(1,2i)} &= \sin{(1/10000^{(2i)/512})} \\
&= \sin(0.0001) \\
&\approx -0.0001 \\
PE_{(1,2i+1)} &= \cos{(1/10000^{(2i)/512})} \\
&= \cos(0.0001) \\
&\approx 0.9999 \\
\end{align*}$$

$$\begin{align*}
PE_{(2,2i)} &= \sin{(2/10000^{(2i)/512})} \\
&= \sin(\frac{2}{100}) \\
&\approx -0.9951 \\
PE_{(2,2i+1)} &= \cos{(2/10000^{(2i)/512})} \\
&= \cos(\frac{2}{100}) \\
&\approx 0.0981 \\
\end{align*}$$

可以看到，对于距离当前位置很近的位置，Sin函数的值接近于$-1$，而距离当前位置很远的位置，Sin函数的值接近于$1$，因此这两者的权重比较小。相反地，对于远离当前位置的位置，它们的权重也很小，所以Sin函数的值将远大于$1$，Cos函数的值则将远小于$0$。这样，经过相对位置编码之后，不同的位置之间的相关性将得到关注，Transformer可以更有效地利用位置信息。