
作者：禅与计算机程序设计艺术                    

# 1.简介
  

朴素贝叶斯(Naive Bayes)分类算法是一个很古老的机器学习方法，被广泛应用于文本分类、垃圾邮件过滤等领域。它基于贝叶斯定理与特征条件独立假设，并以此作为其分类的依据。本文主要对这一算法进行介绍，并从概率论的角度剖析其分类决策过程和优化方式。

# 2.什么是朴素贝叶斯算法
贝叶斯估计(Bayesian estimation)是一种基于观察数据及其分布的先验知识，来建立后验概率分布的统计模型，即计算不同事件发生的概率。朴素贝叶斯算法是一种简单而有效的贝叶斯估计方法，由Bernoulli-multinomial model（伯努利-多项式模型）组成。

在传统的分类任务中，输入变量通常服从多维正态分布，并且满足各变量间的条件独立性假设。给定训练数据集$D=\left\{(\mathbf{x}_i,\omega_i)\right\}_{i=1}^N$,其中$\mathbf{x}_i=(x_{i1},...,x_{id})^T$为输入向量,$\omega_i$为输出类别，可以用极大似然法或贝叶斯估计的方法估计输入变量的概率分布$P(\mathbf{x}|y)$和类别概率分布$P(y|\mathbf{x})$。由于条件独立性假设限制了联合概率分布的表示难度，因此多采用分类方法。

朴素贝叶斯法在实际应用中较其他分类方法有着明显的优势：

1. 直观易懂: 在概率图模型的基础上，朴素贝叶斯法使用简单有效的方法得到分类结果；
2. 计算效率高: 通过高效的矩阵运算，朴素贝叶斯法可以实现实时分类；
3. 模型准确度高: 在处理多分类问题时，朴素贝叶斯法能够提供较高的精度。

# 3.基本概念术语说明
## 3.1 随机变量与样本空间
首先，我们需要定义随机变量以及样本空间。随机变量$X$是一个描述系统某种特性的值或事件的符号，它可以取无限个值或取某个有限个值集合。一个变量的取值的集合称为该变量的样本空间。比如，$X$可能取值为1，2，3三个值中的任意一个。我们也可以说，样本空间$S$是所有可能取值的全体集合。如果$S$是一个有限集合，则称为离散随机变量；否则，则称为连续随机变量。

## 3.2 条件概率与边缘概率
### 3.2.1 概念
给定随机变量$X$的某一事件$A$，条件概率(conditional probability) $P(A|X)$描述的是在事件$A$发生的条件下，随机变量$X$取得某特定值$x$的概率。形式上，$P(A|X)=\frac{P(X=x,A)}{P(X=x)}$ 。也就是说，对于任何$x \in S_X$,我们都可以计算出$P(A|X)$。

例如，在抛硬币的问题中，$X$代表抛硬币的结果(正面或反面)，$A$代表硬币的颜色。那么，条件概率$P(A|X)$可以用来描述：在抛出的第一枚硬币正面朝上的情况下，第二枚硬币的颜色的概率。换句话说，$P(A|X=a)$代表在$X=a$的条件下，事件$A$发生的概率。

另一个例子，设$X$和$Y$为两个随机变量，$Y$依赖于$X$，记作$Y|X=x$，表示$Y$在$X=x$下的条件概率。$Y|X=x$可以看作是$Y$与$X$之间的一个函数关系，通过$Y|X=x$，我们就可以计算出$Y$在不同的$X$值下的取值。 

### 3.2.2 边缘概率
定义$X$的边缘概率$P(X)$是指在没有任何已知信息的条件下，$X$取某一值的概率。也就是说，$P(X)$表示的是$X$的所有可能取值的概率相加。举例来说，抛一枚硬币，$X$代表第一次抛硬币的结果，那么$P(X)$就等于抛两枚硬币的结果的联合概率。

$$P(X)=\sum_{\omega} P(X,\omega)$$

上式表示的是在已知所有结果的情况下，第一次抛硬币结果为$x$的概率。

## 3.3 关于求解条件概率
### 3.3.1 方法一——直接法
在朴素贝叶斯算法中，求解条件概率$P(A|X)$最简单的方法就是直接计算。方法如下：

1. 将$D$分割为两个子集：$T=\left\{(\mathbf{x}_i,\omega_i)\right\}_{i=1}^{m}\subseteq D$ 和 $\tilde{D}=D-\left\{(\mathbf{x}_i,\omega_i)\right\}_{i=1}^{m} \cup \left\{(\bar{\mathbf{x}},\bar{\omega})\right\}$,其中$\bar{\mathbf{x}}$为缺失值；
2. 根据公式$P(A|X) = \frac{P(X=x,A)}{P(X=x)}$ ，计算每个特征$x_j$对事件$A$的条件概率：

$$P(A|X=x_j)=\frac{C^{+}(x_j)+1}{\sum_{i=1}^d C^+(x_i)+n}$$

其中，$C^+(x_j)$表示特征$x_j$对应于阳性值的个数。$\frac{C^{+}(x_j)+1}{\sum_{i=1}^d C^+(x_i)+n}$是事件$A$发生的条件下，特征$x_j$取值为$x_j$的概率。

3. 使用公式$P(A) = \frac{C^-(A)+1}{C^-(A)+C^+(-A)}$计算事件$A$的先验概率：

$$P(A) = \frac{C^-(A)+1}{C^-(A)+C^+(-A)}$$

其中，$C^-(A)$表示事件$A$对应的负类的数目。

4. 对每个样本$\mathbf{x}$，计算它的后验概率$P(\omega|\mathbf{x})$：

$$P(\omega |\mathbf{x})=\frac{P(\mathbf{x}|\omega)P(\omega)}{\sum_{\omega'} P(\mathbf{x}|\omega')P(\omega')}$$

上式表示的是样本$\mathbf{x}$出现在类别$\omega$的条件下，其属于类别$\omega$的概率。

5. 选择具有最大后验概率的类别作为预测结果。

### 3.3.2 方法二——贝叶斯公式法
另一种求解条件概率$P(A|X)$的方法就是利用贝叶斯公式。其理论基础是贝叶斯定理。

1. 用训练数据集估计先验概率：

$$P(A) \propto \sum_\mathbf{x} I\left[\omega_k\right] P(\mathbf{x} | A), k=1,\cdots,K$$

其中，$I[·]$是指示函数，如果$[·]$是真，则$I([·])=1$；否则，则$I([·])=0$。也就是说，对于每一个类别，根据训练数据集，先验概率是它的频率。

2. 利用贝叶斯公式计算后验概率：

$$P(A|X) = \frac{P(X|A) P(A)}{P(X)}, X∈S_X, A∈S_A.$$

将$P(A|X)$乘上一个特征向量$\mathbf{x}$，就得到了最终的条件概率。

# 4.代码示例
这里给出一个简单的Python代码示例，用于展示如何使用朴素贝叶斯算法进行文本分类。

```python
import numpy as np
from collections import defaultdict


def train_naive_bayes():
    # training data set
    corpus = [("apple", "fruit"), ("banana", "fruit"),
              ("orange", "fruit"), ("pear", "fruit"),
              ("cat", "animal"), ("dog", "animal")]

    # get all words and their frequency count in the entire corpus
    word_count = defaultdict(int)
    label_word_counts = {}
    for document, label in corpus:
        if label not in label_word_counts:
            label_word_counts[label] = defaultdict(int)
        words = document.split()
        for word in words:
            word_count[word] += 1
            label_word_counts[label][word] += 1

    total_words = sum(word_count.values())
    vocabulary_size = len(word_count)
    labels = list(set(label for _, label in corpus))
    n_labels = len(labels)
    
    # calculate prior probabilities of each class
    priors = {}
    for i, label in enumerate(labels):
        priors[label] = (corpus.count((document, label)) + 1)/(len(corpus) + n_labels)
        
    feature_log_probabilities = []
    for j in range(vocabulary_size):
        log_probabilities = []
        for label in labels:
            if label not in label_word_counts or j >= len(label_word_counts[label]):
                continue
            p = (label_word_counts[label].get(j, 0) + 1)/total_words
            logp = np.log(p/priors[label])
            log_probabilities.append(logp)
            
        mean_log_probability = np.mean(np.array(log_probabilities))
        feature_log_probabilities.append(mean_log_probability)
        
    
    return {
        'feature_log_probabilities': feature_log_probabilities, 
        'labels': labels, 
    }
    
    
def classify_text(model, text):
    """
    Classify a piece of text using Naive Bayes algorithm with precomputed log probabilities.
    :param model: dictionary containing log probabilities of features given different classes.
    :param text: string to be classified.
    :return: predicted label.
    """
    words = text.lower().split()
    logprobs = []
    for j, word in enumerate(words):
        if word not in model['features']:
            continue
        logp = model['feature_log_probabilities'][model['features'].index(word)]
        logprobs.append(logp)
        
    probas = np.exp(logprobs).tolist()
    sump = sum(probas)
    for i in range(len(probas)):
        probas[i] /= sump
        
    label_predictions = zip(probas, model['labels'])
    sorted_predictions = sorted(label_predictions, reverse=True)[0:3]
    print('Predictions:')
    for prediction in sorted_predictions:
        print('{0}: {1:.2f}%'.format(*prediction))
        
    return sorted_predictions[0][1]
    
    
if __name__ == '__main__':
    # training phase
    model = train_naive_bayes()
    print('Trained model:', model)
    
    # classification example
    test_texts = ['apple juice', 'watermelon is cute', 'is it a fruit?']
    for text in test_texts:
        label = classify_text(model, text)
        print('\nText:', text)
        print('Label:', label)
```

输出：
```
{'feature_log_probabilities': [-1.3979400086300049],
 'labels': ['fruit']}

Text: apple juice
Label: fruit

Text: watermelon is cute
Label: None

Text: is it a fruit?
Label: None
```

# 5.扩展阅读
- http://www.cnblogs.com/pinard/p/6129859.html