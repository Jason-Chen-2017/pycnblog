
作者：禅与计算机程序设计艺术                    

# 1.简介
  

“激活函数”这个概念对很多人来说都不陌生，而在深度学习领域应用的激活函数也越来越多样化、广泛。本文将介绍一下常用的几种激活函数及其特点，希望能够对读者有所帮助。
# 2. 基本概念术语说明
## 2.1 激活函数
**激活函数（activation function）**是指非线性函数，它用于将输入信号转换为输出信号。常用的激活函数主要分为两类：
* 一类是**sigmoid函数**：它是一个S形曲线，输入信号随着时间推移逐渐变平，从而使得神经元的输出值逼近于0或1。sigmoid函数通常用作输出层激活函数。
* 一类是**tanh函数**：它的输出范围是-1到1，类似于sigmoid函数，但是它的中心位置处于零。tanh函数常用于隐含层激活函数。

## 2.2 池化层
池化层(pooling layer)是指通过某种方式提取特征的操作，常用的方法包括最大池化和平均池化，目的是为了降低网络参数数量，并减少过拟合。
最大池化：以特征图中某个区域内的所有元素作为池化窗口，选出该窗口内元素的最大值作为该区域的代表值。
平均池化：与最大池化相反，以该区域内所有元素的均值作为代表值。
## 2.3 BN层
BN层(Batch Normalization Layer)，缩写为BN，是一种在卷积神经网络（CNN）中使用的技巧。它可以使得神经网络训练过程中的梯度更稳定，防止梯度爆炸或消失。一般在卷积层或全连接层后面添加BN层。BN层分为两个阶段：预测阶段和更新阶段。预测阶段不需要学习，直接进行正则化处理；更新阶段利用训练数据，计算每个样本的归一化因子。
## 2.4 激活函数选择
以下是一些常用的激活函数及其优缺点：

1.**Sigmoid 函数**

优点:

- 简单，易于计算
- 在(-∞，+∞)范围内连续可导，输出值在(0，1)之间，因此能有效地抑制输出值过大导致的梯度消失或爆炸现象
- 函数的输出值是一个概率值，便于分类和回归任务
- 函数的输出值只有0和1两个极端值，比较适合二分类问题。

缺点:

- 函数输出值的大小在一定程度上受限于输入值的大小，输入偏差大的情况下，输出值容易饱和或者产生错误分类，即使神经网络学习能力强，也是影响不好的因素之一。

2.**tanh 函数**

优点:

- 比 sigmoid 函数的输出值更加平滑
- tanh 函数的输出值的范围更小，在(-1, +1)之间，因此在处理输入偏差大的情况下比 sigmoid 函数有更好的抗扰动能力
- 对称性较强，因此可以把所有负值转化为很小的正值，避免了 sigmoid 函数在输入负值时输出较大的零值的情况

缺点:

- 函数输出值的变化率和 sigmoid 函数一样快，导致网络训练过程容易被困住或收敛缓慢。

3.**ReLU 函数**

优点:

- 函数的表达式比较简单，计算量小，速度快
- ReLU 函数是神经网络中最常用的激活函数之一，它的输出值恒为非负值，即使在训练过程中出现爆炸现象，也可以快速收敛
- ReLU 函数在零边界处导数为零，因此可以保证网络的稳定性

缺点:

- 当输入值突然变化时，ReLU 函数可能不会立刻起作用，因为它有一个非常弱的抵抗性，因此在训练初期需要较长的时间才能稳定
- 如果训练过程中出现负值，那么这一段就不会再得到更新，而这些负值将永远存在下去。

4.**ELU 函数**

优点:

- 虽然 ELU 函数是 ReLU 函数的改良版，但比 ReLU 更加平滑
- ELU 函数是在 sigmoid 函数和 tanh 函数之间的折衷，既能抑制振荡，又能保留局部尖锐的梯度，从而保证网络的鲁棒性。

缺点:

- ELU 函数在零边界处导数不够平滑，可能会出现 “堵塞” 现象
- ELU 函数对称性不如 ReLU ，导致模型收敛过程会出现明显波动，难以收敛。