
作者：禅与计算机程序设计艺术                    

# 1.简介
  

协方差(covariance)就是两个随机变量之间的线性关系。协方差衡量的是两变量之间变化的方向和变动幅度。当一个变量的变动导致另一个变量发生改变时，称之为共线性关系(co-linearity)。通常用样本协方差表示变量之间的相关程度，反映了变量之间存在多少线性关联。

协方差矩阵(covariance matrix)是对多维随机变量协方差的一种描述，由n个不同随机变量的协方差构成，记作C=(c_{ij}),i=1,2,...,n,j=1,2,...,n。其中，c_{ij}表示第i个随机变量与第j个随机变量之间的协方差。协方差矩阵是一个方阵，对角线元素为各个随机变量的方差，非对角线元素为协方差。

而协方差矩阵的特征值和特征向量也是矩阵分析中的重要概念，它们可以帮助我们更好地理解随机变量之间的联系。矩阵的特征值(eigenvalue)就是矩阵各个特征向量的模长，特征向量(eigenvector)则对应着特征值的方向。因此，协方差矩阵的特征值和特征向量能够告诉我们随机变量之间的联系和相关关系。

当然，协方差矩阵有很多应用场景，例如混合效应、自回归效应、长期经济衰退等。

# 2.基本概念与术语
## 2.1 矩阵
矩阵(matrix)是一个矩形数组，在数学中用符号$A \in \Bbb R^{m\times n}$表示，其中$m$和$n$分别为行数和列数。常见的矩阵有行向量、列向量、转置矩阵、单位阵等。
## 2.2 对称矩阵
如果矩阵$A$满足$A=A^T$，即$A$等于其转置矩阵（记作$A^\intercal$），那么它就是对称矩阵。记作$A^T=\overline{A}$。
## 2.3 正定矩阵
如果矩阵$A$满足$x^TAxx=(x^TAx)^T$(即$x^TA^Tx=x^TAx>0$)，那么它就是正定的。
## 2.4 负定矩阵
如果矩阵$A$满足$x^TAxx=(x^TAx)^T$(即$x^TA^Tx=x^TAx<0$)，那么它就是负定的。
## 2.5 奇异矩阵
如果矩阵$A$不是秩为n-1的满秩矩阵（即不存在n个线性无关的基），那么它就是奇异矩阵。
## 2.6 向量空间
向量空间(vector space)是一个集合，包括向量及加法、标量乘法以及一些运算规则。特别地，矢量空间中的向量具有封闭性、封环性和直观性。
## 2.7 特征值与特征向量
对于方阵$A$，如果存在$\lambda_i (i=1,2,\cdots,n)$和$u_i (\forall i)$，使得$Au_i=\lambda_iu_i$，那么，$A$就有一个固定的特征向量$u_i$和相应的特征值$\lambda_i$。特征值和对应的特征向量构成了$A$的特征向量组(eigenvectors)。记作$A=\sum_{\lambda_i}\lambda_iu_i$。如果特征值只有一个，则称为实对称矩阵，否则称为复对称矩阵。

# 3.协方差矩阵
## 3.1 计算方法
协方差矩阵$C$是一个方阵，对角线上为各个变量的方差，非对角线上为变量之间的协方差。计算协方差矩阵的方法有几种：
1.直接计算：从数据集D中计算每个变量的均值$m_i$以及样本方差$S^2_i$（或分母为n-1）以及每个变量与其他变量的偏离平方和。将这些信息用矩阵形式表示：
$$C = \frac{1}{n-1} D'D$$

2.利用矩阵微积分求导法则：利用矩阵的微积分的求导法则，通过计算协方差矩阵的行列式以及对角线元素，推导出协方差矩阵的表达式。

3.利用中心化的矩阵方法：将数据集D中心化（减去各变量的均值）后，再计算协方差矩阵。

## 3.2 几个例子
### 3.2.1 只有两个变量的数据集
假设只有两个变量x和y。数据集D中所有数据点的坐标如下：
$$D=[(1,3),(2,4),(3,5),(4,6)]$$
根据公式2.1，计算各个变量的均值：
$$\begin{aligned} m_x &=\frac{1+2+3+4}{4}=2 \\ m_y &=\frac{3+4+5+6}{4}=4 \end{aligned}$$
计算每个变量的样本方差：
$$\begin{aligned} S^2_x&=\frac{(1-2)^2+(2-2)^2+(3-2)^2+(4-2)^2}{3}\\ &=\frac{4}{3}\\ S^2_y&=\frac{(3-4)^2+(4-4)^2+(5-4)^2+(6-4)^2}{3}\\ &=\frac{16}{3}\\ \end{aligned}$$
计算得到协方差矩阵：
$$C=\frac{1}{n-1} \begin{pmatrix} 
4&\quad-\frac{8}{3}\\ 
-\frac{8}{3}&\quad\frac{16}{9}
\end{pmatrix}$$

### 3.2.2 有两个变量的数据集
假设有两个变量x和y。数据集D中所有数据点的坐标如下：
$$D=[(1,3),(2,4),(3,5),(4,6)]$$
根据公式2.1，计算各个变量的均值：
$$\begin{aligned} m_x &=\frac{1+2+3+4}{4}=2 \\ m_y &=\frac{3+4+5+6}{4}=4 \end{aligned}$$
计算每个变量的样本方差：
$$\begin{aligned} S^2_x&=\frac{(1-2)^2+(2-2)^2+(3-2)^2+(4-2)^2}{3}\\ &=\frac{4}{3}\\ S^2_y&=\frac{(3-4)^2+(4-4)^2+(5-4)^2+(6-4)^2}{3}\\ &=\frac{16}{3}\\ \end{aligned}$$
计算得到协方差矩阵：
$$C=\frac{1}{n-1} \begin{pmatrix} 
4&\quad-\frac{8}{3}\\ 
-\frac{8}{3}&\quad\frac{16}{9}
\end{pmatrix}$$

### 3.2.3 有三个变量的数据集
假设有三个变量x, y和z。数据集D中所有数据点的坐标如下：
$$D=[(1,3,2), (2,4,-1),(3,5,3),(4,6,-2)]$$
根据公式2.1，计算各个变量的均值：
$$\begin{aligned} m_x &=\frac{1+2+3+4}{4}=2 \\ m_y &=\frac{3+4+5+6}{4}=4 \\ m_z &=\frac{-1+-1+3+-2}{4}=-\frac{4}{3} \end{aligned}$$
计算每个变量的样本方差：
$$\begin{aligned} S^2_x&=\frac{(1-2)^2+(2-2)^2+(3-2)^2+(4-2)^2}{3}\\ &=\frac{4}{3}\\ S^2_y&=\frac{(3-4)^2+(4-4)^2+(5-4)^2+(6-4)^2}{3}\\ &=\frac{16}{3}\\ S^2_z&=\frac{(-1)-(-1)+3+(-2)} {3}\\ &=\frac{8}{3}\\ \end{aligned}$$
计算得到协方差矩阵：
$$C=\frac{1}{n-1} \begin{pmatrix} 
4&-\frac{8}{3}&-\frac{16}{3}\\ 
-\frac{8}{3}&16&\quad-\frac{4}{3}\\ 
-\frac{16}{3}&\quad-\frac{4}{3}&2
\end{pmatrix}$$


# 4.协方差矩阵的特征值和特征向量
## 4.1 特征值与特征向量的定义
对于方阵$A$，如果存在$\lambda_i (i=1,2,\cdots,n)$和$u_i (\forall i)$，使得$Au_i=\lambda_iu_i$，那么，$A$就有一个固定的特征向量$u_i$和相应的特征值$\lambda_i$。特征值和对应的特征向量构成了$A$的特征向量组(eigenvectors)。记作$A=\sum_{\lambda_i}\lambda_iu_i$。如果特征值只有一个，则称为实对称矩阵，否则称为复对称矩阵。

## 4.2 特征值和特征向量的计算
计算特征值和特征向量有两种方法：
1.直接求解：通过对协方差矩阵进行分解，求解矩阵的特征值和特征向量。
2.利用特征值分解：将协方差矩阵写成特征向量与特征向量组的乘积。

### 4.2.1 通过对协方差矩阵进行分解
对于协方差矩阵$C$，存在唯一的特征向量$v$和对应的特征值$\lambda$，使得
$$C v = \lambda v$$

先对矩阵$C$进行初等行变换，令$C = P^{-1} CP'$，其中$P$为一个正交矩阵。则
$$C C^\intercal v = PC^\intercal P v = \lambda v$$
因为$C$的秩为n，所以$PC^\intercal P$是一个$n$阶的可逆矩阵。则
$$\begin{bmatrix} u_1 & \cdots & u_n\end{bmatrix}=\begin{bmatrix} 1 & & 0\\ \vdots & & \\ 0 & & 1\end{bmatrix}^{-1} P^{-1} \begin{bmatrix} c_{11} & c_{12} & \cdots & c_{1n}\\ \vdots & \ddots & \\ c_{n1} & \cdots & c_{nn}\end{bmatrix}$$
其中，$u_k$为第$k$个特征向量，$\{\lambda_i\}_{i=1}^n$为$\lambda$的特征值。

### 4.2.2 利用特征值分解
将协方差矩阵写成特征向量与特征向量组的乘积形式：
$$C = V\Lambda V^\intercal$$
其中，$V$为n维列向量组，每一列是一个特征向量；$\Lambda$为对角矩阵，对角线上的元素是特征值，从大到小排列。

若$\Lambda$是实对称矩阵，则其特征向量$v$满足
$$C v = \lambda v,$$
其中$\lambda$为特征值，$v$为特征向量。

若$\Lambda$是复对称矩阵，则其特征向量$v$满足
$$Cv = \Re(\lambda) e^{\Im(\lambda)} v.$$
其中，$\Re(\lambda)$为实部，$\Im(\lambda)$为虚部。

因此，若协方差矩阵为实对称矩阵，可将特征值作为实数，将特征向量组作为矩阵，并用矩阵乘法计算各个变量之间的协方差；若协方差矩阵为复对称矩阵，可将特征值分解为实部与虚部两个部分，并用复数乘法计算各个变量之间的协方差。

# 5.代码实现
```python
import numpy as np 

def calculate_cov_matrix(data):
    mean = data.mean(axis=0) # axis=0 means column-wise calculation of mean
    centered_data = data - mean 
    cov_mat = centered_data.T @ centered_data / (len(centered_data)-1)
    return cov_mat
    
def compute_eig(matrix):
    eigenvalues, eigenvectors = np.linalg.eig(matrix)
    idx = eigenvalues.argsort()[::-1]   
    eigenvalues = eigenvalues[idx]  
    eigenvectors = eigenvectors[:,idx]  
    print('Eigenvalues:', eigenvalues)    
    print('Eigenvectors:\n', eigenvectors)
    
if __name__ == '__main__':
    x = [1,2,3,4]
    y = [3,4,5,6]
    
    # Example 1: two variables dataset
    X = np.array([x]).T 
    Y = np.array([y]).T
    D = np.concatenate((X,Y), axis=1)
    print("Data set:")
    print(D)

    cov_mat = calculate_cov_matrix(D)
    print("\nCovariance Matrix:")
    print(cov_mat)
    compute_eig(cov_mat)

    # Example 2: three variables dataset
    z = [-1, -1, 3, -2]
    Z = np.array([z]).T
    D = np.concatenate((X,Y,Z), axis=1)
    print("\nThree Variables Dataset")
    print(D)

    cov_mat = calculate_cov_matrix(D)
    print("\nCovariance Matrix:")
    print(cov_mat)
    compute_eig(cov_mat)

    # Example 3: negative definite covariance matrix
    A = [[2,-1],
         [-1,2]]
    cov_mat = np.array(A)
    print("\nNegative Definite Covariance Matrix:")
    print(cov_mat)
    compute_eig(cov_mat)

    # Example 4: positive definite covariance matrix
    B = [[1,0.5],[0.5,1]]
    cov_mat = np.array(B)
    print("\nPositive Definite Covariance Matrix:")
    print(cov_mat)
    compute_eig(cov_mat)
```