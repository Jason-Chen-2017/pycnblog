
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在机器学习和深度学习领域，激活函数（activation function）是一个非常重要的概念。它将神经网络中节点的输出从输入空间映射到输出空间。本文将对常用的激活函数进行分类介绍，并对其优缺点作出评述。

激活函数在神经网络中的作用主要有以下三个方面：

1.解决梯度消失和梯度爆炸的问题；

2.使得神经网络模型更加非线性、复杂，能够拟合任意数据集；

3.对优化算法起到正则化的作用，增加模型泛化能力。

常用激活函数可以分为三类：

1.线性激活函数（Linear Activation Function）：最简单的激活函数之一。直接将神经元的输出作为输出信号。缺点是容易造成欠拟合或过拟合。

2.非线性激活函数（Non-linear Activation Function）：包括sigmoid函数、tanh函数、ReLU函数等。这些函数的特点是具有非线性，能够近似表示任意函数。

3.softmax函数：多用于多分类问题中，用于将输出值转换为概率值。

接下来，我们将详细介绍这些激活函数。

# 2.基本概念
## 2.1 激活函数
在机器学习中，激活函数（Activation Function）就是神经网络中用于引入非线性因素的函数。在神经网络中，激活函数通常用来对中间层神经元的输出进行变换，或者称为非线性映射，从而使得神经网络的表达能力不仅局限于输入变量之间的线性组合，还能利用更多复杂的模式。

为了能够理解为什么需要激活函数，我们先看一下感知机。

## 2.2 感知机
感知机（Perceptron），是神经网络的基础模型之一。它的基本结构由两部分组成，输入层和输出层。其中，输入层的每个神经元都与一组权重相连，而输出层只有一个神经元。输入层向量乘以相应的权重后，再经过一个激活函数（如Sigmoid函数），得到输出。简单来说，感知机就是一个单层的神经网络模型。


如上图所示，感知机把输入通过加权求和后的结果送入激活函数（如Sigmoid函数），然后根据激活函数的输出，输出相应的结果。因此，输出层的神经元个数只能有一个，即只能输出一个实数。

但是这样的话就限制了感知机的表达能力。为了增加模型的表达力，就需要引入非线性函数作为激活函数。举个例子，假设输入是二维特征，如果没有激活函数，那么感知机只能做出一条直线作为决策边界，而不能有效地拟合复杂的数据集。所以，在使用感知机时，一般都会加上非线性激活函数，如Sigmoid函数、ReLU函数等。

## 2.3 激活函数及其特点
### 2.3.1 Sigmoid 函数
sigmoid函数也叫做S型函数，是一种典型的S形曲线函数。sigmoid函数表达式如下：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$

这个函数是一个非线性函数，经常用于激活神经网络的输出单元。其优点是输出值在(0,1)范围内，是一种 smooth 函数，易于求导，且计算简单。

在深度学习中，sigmoid函数通常被用作输出单元的激活函数。它可以把输出压缩到0到1之间，并强制输出总和为1。另一方面，它可以将任何实数的输入值压缩到0到1之间，并且对于大于等于零的值保持较高的值，对于小于零的值，其输出会趋近于0，因此可以实现非线性。

sigmoid函数的缺点主要是：

1.输出受到x值的大小变化影响很大，在某些情况下可能导致输出“饱和”，导致信息丢失或者神经元死亡。

2.sigmoid函数易出现梯度消失或梯度爆炸的情况，当神经网络深度太大，或者学习速率设置不当时，经常发生这种现象。

3.sigmoid函数在训练过程中难以优化，容易出现 vanishing 或 exploding gradients 的情况，导致神经网络无法收敛。

### 2.3.2 tanh 函数
tanh函数是 sigmoid 函数的平滑版本。tanh 函数表达式如下:

$$tanh(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{(e^x - e^{-x})}{(e^x + e^{-x})}$$

tanh 函数和 sigmoid 函数类似，也是一种非线性函数，但是 tanh 函数输出范围为[-1, 1]，其优点是，输出值在(-1,1)范围内，是一种 smooth 函数，易于求导，且计算速度快。与 sigmoid 函数不同的是，tanh 函数的输出总是处于两个固定点的交叉点上，因此可以避免前者输出饱和或梯度消失的问题。同时，tanh 函数是双曲正切函数，即 y = tan x = sinh (x) / cosh (x)，所以，当 x=0 时，y=0 ，所以 tanh 函数在取值为零时不会产生 NaN。

tanh 函数的缺点同样是存在梯度消失或梯度爆炸的问题，不过比 sigmoid 函数更严重一些。另外，当 x 在 [-1, 1] 区间外时，tanh 函数的输出可能出现指数爆炸或者指数跌倒，因此也不能用于处理那些极端值。

### 2.3.3 ReLU 函数
ReLU函数（Rectified Linear Unit）又名修正线性单元（Rectified Linear Activation Unit）。它是一个非常流行的激活函数。其函数形式为 max(0, z)。这里，z 是输入信号经过非线性函数处理之后的值。ReLU 函数的基本思想是，只要某个神经元的输入大于0，则该神经元的输出就会被激活。在实际应用中，ReLU 函数常用来构建神经网络。

ReLU 函数的优点是，其计算简单、计算快速，避免了 sigmoid 函数中的梯度消失和梯度爆炸的困扰。并且，其函数特性是 monotonic ，也就是说，它不是凹的或者凸的，因此，更利于保持梯度稳定，提高模型的鲁棒性。

但随着深度的增长，ReLU 函数容易导致梯度消失或者梯度爆炸的问题，这是因为其梯度很小，因此在深层网络中，容易出现 vanishing or exploding gradients 。

### 2.3.4 softmax 函数
softmax 函数是多分类问题的激活函数，它可以把输出值压缩到0到1之间，并且使得输出值之和为1。在神经网络的最后一层，通常会使用 softmax 函数作为输出激活函数。

softmax 函数的表达式如下：

$$P(y_i|x,\theta) = \frac{e^{z_{yi}}}{\sum_{j} e^{z_{yj}}}$$

softmax 函数是一个归一化函数，其输出的值落在 (0,1) 区间，且所有输出值的总和等于 1。这个函数的输入为输出层神经元的输入值 x 和参数向量 $\theta$ ，输出为每一类的概率。由于 softmax 函数有关概率分布，因此可以用来解决多分类问题。

softmax 函数的优点是，它解决了多分类问题，输出值范围为 (0,1) ，且所有输出值的总和等于 1 ，因此可以有效的用于多分类问题。

然而，softmax 函数也存在着很多缺点。首先，它计算量比较大，当分类数量较多时，计算开销较大。其次，它是非线性函数，容易造成输出值过于激进或过于冷淡，对优化算法的性能有一定影响。第三，当标签集中类别数量不均衡时，softmax 函数容易偏向于占主导地位的类别，导致模型的偏差较大。

# 3.优缺点
## 3.1 sigmoid 函数
### 3.1.1 sigmoid函数优点
- 输出范围在0~1，值域范围广，适合于处理各种预测任务；
- 有利于进行概率预测；
- 有利于分割数据，对输入数据分布的形状不敏感；
- 可以轻易求导，运算速度快；
- 在生物神经网络模型中有广泛应用。

### 3.1.2 sigmoid函数缺点
- sigmoid函数属于S型曲线，当输入值特别大或特别小时，sigmoid函数的输出会迅速飘升或迅速缩小，导致模型的训练过程非常困难。因此，在实际应用中，应该根据实际情况来选择合适的激活函数；
- sigmoid函数的输出值和导数值都在0~1之间，因此对于大于等于零的值保持较高的值，对于小于零的值，其输出会趋近于0，因此无法对抗梯度消失或者梯度爆炸；
- 训练过程中，sigmoid函数的梯度容易受到其他单元的影响，因此易出现 vanishing or exploding gradients 。

## 3.2 tanh 函数
### 3.2.1 tanh函数优点
- tanh函数的值域范围在-1到1，具有柔性，能够对数据分布形态变化具有良好的抵抗力；
- 使用tanh函数作为激活函数的神经网络，容易实现深层次的特征抽取；
- tanh函数易于求导，而且计算速度较快。

### 3.2.2 tanh函数缺点
- tanh函数属于双曲正切函数，易产生梯度弥散现象，导致神经元死亡；
- 当输入值特别大或特别小时，tanh函数的输出可能会发生指数爆炸或者指数跌倒，从而导致数值溢出或者 NaN 错误；
- tanh函数输出范围在-1到1之间，因此，对于大于等于零的值保持较高的值，对于小于零的值，其输出会趋近于0，因此无法对抗梯度消失或者梯度爆炸。

## 3.3 ReLU 函数
### 3.3.1 ReLU函数优点
- 易于计算，能够加快模型的训练过程；
- 有利于防止梯度消失和梯度爆炸；
- 在深度模型中被广泛使用。

### 3.3.2 ReLU函数缺点
- ReLU函数具有等高线性，能够更好地抓住有效区域；
- 对输入数据零的敏感度不足；
- 如果模型的某一层因为输入输出的关系而无法学习到足够的信息，则会造成这一层的神经元死亡。

## 3.4 softmax 函数
### 3.4.1 softmax函数优点
- softmax函数不需要对输入进行归一化处理，能够处理更广泛的预测任务；
- softmax函数输出的概率值代表了对应类的置信度，因此，可以直观地了解模型对于特定输入的预期程度；
- 输出值可以反映模型对各个类别的识别能力。

### 3.4.2 softmax函数缺点
- softmax函数的计算复杂度比较高；
- softmax函数输出的概率值代表了对应类的置信度，但是，它并不能表示出真实的置信度级别；
- 如果模型的某一层因为输入输出的关系而无法学习到足够的信息，则会造成这一层的神经元死亡。