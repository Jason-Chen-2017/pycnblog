
作者：禅与计算机程序设计艺术                    

# 1.简介
  

​			激活函数是指神经网络中节点输出值的非线性变换，也就是将输入信号通过一个非线性函数进行加工处理，然后在传给下一层神经元时得到输出值。一般来说，激活函数的种类很多，包括Sigmoid、tanh、ReLU等。本文将结合具体案例详细阐述关于激活函数和损失函数的知识。
# 2.基础概念
## 2.1 激活函数（Activation Function）
​		激活函数又叫非线性函数，其作用就是对输入的实值进行转换，从而在一定程度上抑制或放大梯度，使得神经网络中的各个节点能够拟合任意曲线。其中，常用激活函数有Sigmoid、tanh、ReLU等。以下简单介绍一下这几种激活函数：

1. Sigmoid 函数：

   sigmoid函数形状类似钟型曲线，在区间(-inf，+inf)上的值域是(0,1)，取值为：

   $f(x)=\frac{1}{1+e^{-x}}$

   当x取到无穷大时，sigmoid函数趋于0；当x取到-无穷大时，sigmoid函数趋于1。

2. tanh 函数：

   tanh函数形状类似钟型曲线，但它的函数范围在(-1,1)之间，并且在两端点处饱和，取值为：

   $f(x)=\frac{\sinh{(x)}}{\cosh{(x)}}=\frac{\exp{(x)}}{\exp{-(x)}}$

3. ReLU (Rectified Linear Unit) 函数:
   
   relu函数是一种激活函数，其基本思想是：如果神经元的输入信号小于零，则置零；否则，令该信号不变。relu函数具有平滑性、非饱和性、单调性、可微性等优点，是目前常用的激活函数之一。

   $f(x)=max\{0, x\}$

   当输入信号x大于0时，relu函数输出信号等于输入信号；当输入信号x小于0时，relu函数输出信号等于0。
   
## 2.2 损失函数（Loss Function）
​		损失函数用来衡量模型预测结果与实际结果之间的差距，它可以用来反映模型的性能好坏，所以它也是非常重要的。损失函数的选择对训练过程的稳定性及最终模型效果都有着至关重要的影响。常用的损失函数有均方误差（Mean Squared Error，MSE）、交叉熵（Cross Entropy）、二次代价（Quadratic Cost）等。
​	
1. MSE （均方误差）：
   
   均方误差又称“欧氏距离”，是最简单的损失函数之一。对于目标变量y，它表示模型对样本预测值$\hat y_i$ 和真实值$y_i$ 的差距，取平均值的平方根作为损失函数。

   $\begin{align*}
   L&=\frac{1}{m}\sum_{i=1}^my_i^2-\frac{1}{m} \sum_{i=1}^m(\hat{y}_i - y_i)^2 \\ &=\frac{1}{2m}(\sum_{i=1}^m(\hat{y}_i - y_i)^2+\sum_{i=1}^m\epsilon_i^2), \quad m = 总样本数, \epsilon_i 是噪声。
   \end{align*}$

   当输入误差向量满足独立同分布假设（IID）时，即每组样本都满足独立同分布且具有相同方差的假设，那么上式就可以写成如下形式：

   $\begin{equation*}
   L=(\hat{y}-y)^T (\hat{y}-y).
   \end{equation*}$

   

2. Cross Entropy（交叉熵）：
   
   交叉熵是一个常用的损失函数，用来度量两个概率分布p和q之间的距离，其表达式为：

   $\begin{equation*}
   H(p, q)=-\frac{1}{N}\sum_{n=1}^{N}[y_n\log \hat{y}_n+(1-y_n)\log(1-\hat{y}_n)]
   \end{equation*}$

   交叉熵可以有效地衡量模型预测的不确定性，模型越困难，对应的损失越大。

3. Quadratic Cost（二次代价）：
   
   二次代价是指对网络输出的估计误差求平方后再求和，其表达式为：

   $\begin{equation*}
   J=\frac{1}{2m}\sum_{i=1}^{m}(y^{(i)}-a^{[L](i)})^2
   \end{equation*}$

   二次代价不仅考虑了输出的差距，而且还考虑了误差项的方差，因此可以更好地匹配数据集的特征，并且在数据量较少或者出现不可避免的噪声时也能起到正则化的作用。