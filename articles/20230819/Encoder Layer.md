
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Encoder层由多层Transformer模块组成，每一层包括两个子层（Multi-Head Attention及Feed Forward）、残差连接、Layer Normalization，可以对输入序列进行编码，输出一个定长表示。多层Encoder堆叠起来可以实现复杂的特征提取能力。
Encoder层将输入序列经过Embedding映射到连续空间中，接着输入到第一个子层(Multi-head Attention)，得到query、key和value三者，然后通过自注意力机制获取输入序列的表示。
Attention后面再接了一个Feed Forward层，该层采用两层全连接网络，其中第一层的输出维度等于第二层的输入维度，中间不加激活函数，输出维度也不变，即两个全连接层共享参数，其作用类似于残差连接。最后，将得到的特征进行Layer Normalization，使得每个样本的表示都处于零均值和单位方差的正态分布，从而减少模型的抖动现象。这样，就完成了第一次的Encoder Layer的计算过程。


# 2.基本概念术语说明
## Transformer结构图示
如上图所示，整个Transformer结构分为Encoder和Decoder两部分。其中Encoder主要进行序列处理任务，由多个Encoder Layer堆叠而成；Decoder主要进行序列生成任务，用同样的结构堆叠几个相同的Encoder Layer。
## Multi-Head Attention
多头自注意力机制，也称作“多头关注”，是Attention的一种改进方法。多头自注意力机制在多头之间引入独立的注意力机制，解决了单头自注意力机制存在的问题——当模型学习到不同上下文之间的依赖关系时，可能会失去有效信息的表达能力。
## Scaled Dot-Product Attention
缩放点积注意力机制，也称作“缩放点积注意力”，是在Attention中加入缩放因子之后，能够缓解梯度消失或爆炸问题。同时，为了防止因词向量过小引起的注意力难以传递，还引入了投影矩阵。
## Residual Connection and Layer Normalization
残差连接和层归一化，是两种非常重要的技巧，能够增强模型的非线性表示能力，并降低训练难度。
残差连接，顾名思义就是将前面的运算结果的输出作为输入送给后面的运算层，这种方式相当于直接跳过一些层，简化了神经网络的结构。
层归一化，是在神经网络的输出中添加噪声，使之具有零均值和单位方差的分布，这是为了避免模型的抖动现象。
## Positional Encoding
位置编码，是一种编码方案，是指在原始序列的位置信号被引入注意力机制之前，对序列的位置信息进行编码。
论文中使用的位置编码方式是基于正余弦函数，形式如下：PE(pos, 2i) = sin(pos/(10000^(2*i/dmodel))) ， PE(pos, 2i+1) = cos(pos/(10000^(2*i/dmodel))) 。其中pos代表位置信息，2i和2i+1分别代表偶数和奇数位置，dmodel为模型的嵌入维度大小。这种方式能够对位置信息进行编码，使得不同位置上的词向量能够充分地表示位置关系，从而能够更好地学习长期依赖关系。
## Embedding Layer
嵌入层，是将输入序列进行数字化，将其转化为连续的词向量。常用的嵌入层有Word2vec、GloVe等。
## Feed Forward Network
前馈网络，又称为神经网络中的全连接层，是一个二元激活函数，由两个完全相同的神经元构成，即输入层和输出层，用于对输入数据做变换，转换后的结果被送回给前一层。FFN网络通常包含三个全连接层，每层包括两个隐藏层，一个激活层。
## Dropout
Dropout，是一种正则化方法，在训练过程中随机让某些神经元的输出变为0，使得模型变得不可靠，同时减轻模型过拟合的风险。Dropout一般应用于全连接层或卷积层后面。