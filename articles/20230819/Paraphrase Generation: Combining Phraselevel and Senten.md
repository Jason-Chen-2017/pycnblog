
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Paraphrase generation (PPG) is a natural language processing task that aims to generate new text based on the input text by replacing some words or phrases with their paraphrases. This task can be applied in various scenarios such as query reformulation, dialogue rephrasing, summarization of long texts, etc., which are widely used in industry applications including customer service, chatbots, automatic translation, news aggregation, etc. The state-of-the-art PPG systems usually focus on generating high-quality paraphrases while preserving the original meaning of the input text. However, there have been few works addressing this issue and combining sentence-level and phrase-level representations has not received sufficient attention. In this paper, we present an approach to combine both phrase-level and sentence-level representations for improving PPG performance. Specifically, we propose a novel method called CLAM (Combining Latent Alignments and Masked Language Modeling), which combines two types of embeddings into one unified model using attention mechanisms. We first extract phrase-level embeddings from unsupervised phrase encoder models and use them as key vectors to align each word with its corresponding phrase representation. Then, we train a masked language model over the aligned inputs to predict the missing tokens. Finally, we pass these predictions through a post-processing layer before outputting the final paraphrase. Experimental results show that our proposed method outperforms several strong baselines including syntax-based methods, statistical machine translation techniques, and neural machine translation approaches. Additionally, we demonstrate how our approach compares favorably against the previous state-of-the-art system GPT-3 and shows how it improves upon its syntactic abilities. Overall, we believe that our work provides a promising way towards achieving significant improvements in PPG performance with minimal modifications to existing architectures.

# 2.关键词
Phrase level representations; Sentence level representations; Paraphrase generation; Latent alignment; Masked language modeling; Transformers

# 3.引言
Paraphrase generation (PPG) refers to the task of creating a new text based on an input text, replacing certain words or phrases with similar but different expressions or meanings. It plays a crucial role in a variety of NLP tasks such as query reformulation, dialogue rephrasing, summarization of long texts, among others. Despite being a challenging problem due to the complexity and ambiguity involved in transforming sentences into other meaningful forms, recent advances in deep learning have significantly improved the accuracy of current PPG algorithms. There have been numerous studies attempting to improve PPG performance through incorporating more complex features such as dependency parsing or sentiment analysis, but relatively little research focuses solely on designing better feature engineering strategies or exploring potential interactions between phrase-level and sentence-level representations. 

In this study, we explore the impact of combining phrase-level and sentence-level representations in PPG systems. To address this issue, we propose a novel method named CLAM (Combining Latent Alignments and Masked Language Modeling). CLAM uses a transformer architecture and incorporates latent alignment modules that jointly embeds the same information at both the word and phrase levels. During training, CLAM learns to assign each word to the nearest phrase embedding, allowing it to encode its semantic context without relying on handcrafted features. At test time, CLAM generates paraphrases conditioned on the assigned phrase embeddings, enabling it to produce fluent and accurate paraphrases even when dealing with idiomatic expressions and multiword units like proper nouns. Our experiments demonstrate that CLAM outperforms the state-of-the-art baseline models in terms of BLEU scores, and also demonstrates competitive performance compared to GPT-3 in several metrics. Further analyses highlight that CLAM effectively captures multiple linguistic aspects such as pragmatic roles, modifiers, negation, intensifiers, contexts, and sequence structure, making it particularly suitable for handling diverse inputs and producing sophisticated outputs.