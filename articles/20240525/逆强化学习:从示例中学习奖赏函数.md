逆强化学习（Inverse Reinforcement Learning, IRL）是强化学习（Reinforcement Learning, RL）的一个子领域，旨在从观察中学习一个未知的奖赏函数。IRL 的目标是确定一个模型，使得给定一个奖赏函数，模型可以生成与观察到的行为一致的策略。IRL 问题可以分为两类：确定性IRL和概率IRL。

确定性IRL（Deterministic IRL）假设一个确定性的环境，其中每个状态只对应一个最佳动作。确定性IRL 的主要挑战是找到一个状态到动作的映射，该映射应该能最大化奖赏。确定性IRL 的典型方法有最大熵法（Maximum Entropy）和最大熵策略梯度（Maximum Entropy Policy Gradients）。

概率IRL（Probabilistic IRL）假设一个概率性环境，其中每个状态对应多个可能的动作。概率IRL 的主要挑战是找到一个概率分布，使得给定一个状态，模型可以生成与观察到的行为一致的概率分布。概率IRL 的典型方法有贝叶斯推理（Bayesian Inference）和动态 Programming（Dynamic Programming）。

逆强化学习的典型应用包括机器人学习、游戏策略学习等。