
作者：禅与计算机程序设计艺术                    

# 1.简介
  

小波分析（Wavelet Analysis）是一种信号处理方法，它通过对信号进行分解，将其分解成不同频率成分，即各种波形。在信号处理过程中，小波分析经常被用来检测、预测或者分类图像、声音等多种数据的模式。相比传统的傅里叶变换（Fourier Transform），小波分析有着更高的灵活性和适应性，能够捕获更多高频细节，也因此而得名。小波分析被广泛应用于图像、声音、物理、生物医学等领域。在本文中，我们将讨论小波近似支持向量机（Wavelet Approximation Support Vector Machine, WASVM）是一种基于小波分析的支持向量机分类器。WASVM的优点是可以有效地解决样本不均衡的问题，而且对于异常值不敏感。
# 2.小波分析
## 2.1 定义
小波分析由香农和海顿（Haar Wavelets）创立，属于信号处理中的一种分解或重构方法。简单来说，就是对函数或者信号进行一系列的小型操作，得到其组成各个小块子函数或者子信号，这种操作具有自相关性，对高频分量抓取的能力强。
## 2.2 尺度空间与系数空间
在小波分析中，首先将原始信号按照不同的尺度进行分解，然后得到分解后的信号。每个尺度都是一个集合，表示了在这个尺度下原始信号中所含有的信息量大小。尺度空间（Scale Space）则是指不同尺度上的信号集合，这些信号集合的组合构成最终的系数空间。
## 2.3 小波基函数
在小波分析中，常用的小波基函数包括离散小波和正交小波。离散小波一般采用正交离散小波基，而正交小波则指有限体小波，其正交性保证了信号在任意尺度下的分解结果是正交的。在实际应用中，常用的小波基函数有 Daubechies 小波（DB）、Haar 小波、Biorthogonal 小波和 Symlet 小波等。
### DB小波
Daubechies小波（又称DB小波）是由德国数学家J.L.Banks于1984年提出的一种小波，它也是最著名的小波之一。该小波基由两个单位权值的离散小波组成，故其基函数为：
$$
    c_0 = \frac{1}{2} \\
    c_{j+1} = \sqrt{\frac{2}{\pi}}(\cos(j\pi/(N+1)) - (-1)^j\sin(j\pi/(N+1)))\\
    j=0,\cdots,(N-1), N>0
$$
其中 $c_0$ 和 $c_{N}$ 为归一化因子。
### Haar小波
Haar小波是另一种常用的小波基，它由两个正交单位小波组成，故其基函数为：
$$
    c_0=\frac{1}{\sqrt{2}}, \quad c_{n+1}=c_n=-\frac{1}{\sqrt{2}}\sqrt{(2n)!}\cdot(-1)^n\cdot\omega^{2n}, n=0,1, \ldots
$$
其中 $\omega=\exp(\frac{2\pi i}{N})$ 是自然基矢量，$i$ 表示虚数单位，$N$ 表示基函数个数。
### Biorthogonal 小波
Biorthogonal 小波是用两组正交的小波基 $b(a,b)$ 构造出的小波基，其基函数如下：
$$
    b(a,b)=\sqrt{\frac{2}{N+1}}\begin{cases}(2ab-a-b)\cos(\theta)+(a^2-b^2)\sin(\theta)&\text{if }a+\beta\leq N, b+\alpha\leq N\\
    0&\text{otherwise}\\
    \end{cases}
$$
其中 $\theta=\arcsin(\frac{2ab-a-b}{a^2-b^2}), \alpha=\frac{a}{N}, \beta=\frac{b}{N}$。
### Symlet 小波
Symlet 小波是通过求导形成的小波基，且称为自相关小波（Autocorrelation Wavelets）。其基函数如下：
$$
    c_n = \frac{1}{\sqrt{n!}} (\frac{d^{|k|}c_{n+k}}{dx^{|k|}}) \quad k=0,1, \ldots, |n|-1
$$
其中 $\frac{d^{|k|}c_{n+k}}{dx^{|k|}}$ 指 $c_{n+k}$ 在 $x^{|k|}$ 次项的导数。Symlet 小波一般用于分解和重构双边谱，通过对基函数和滤波器的选择合理实现小波分析的各项功能。
## 2.4 小波分析的特性
在小波分析中，有以下几个特性：

1. 可分解性：小波基的选择会影响信号的分解结果。一般情况下，信号分解后的各个小部件之间不会互相关，但在一定程度上会有相关性。

2. 不变性：小波分析的过程会导致信号的形状发生变化，但不会影响其信息量。

3. 可重构性：通过小波分析得到的信号可以通过逆滤波恢复。

4. 自相关性：小波分析的输出结果本身就是小波函数。

5. 分辨率控制：可以通过设置不同尺度的分辨率来控制小波分析的精度。

6. 对比度增强：小波基的选择会增强图像或信号的对比度。

## 2.5 小波近似支持向量机
WASVM 的基本思路是：先利用小波分析将输入数据从原来的高维空间映射到低维空间，再用小波函数拟合局部边缘，最后将数据投影到低维空间的局部子空间上，训练出支持向量机进行分类。WASVM 将数据从二维映射到了低维，并同时考虑了局部特征和全局特征，在损失函数中引入了拉普拉斯分布。WASVM 有如下优点：

1. 可以处理非线性的数据，并且不需要手工去设计特征；

2. 通过局部和全局的小波函数进行特征学习，可以获得更好的泛化性能；

3. 可以自动地降低维度，使得模型参数规模更小，计算速度更快；

4. 支持向量机在分类时能够实现非线性的决策边界，防止过拟合；

5. 模型训练的效率非常高，适合大数据集和多核CPU环境；

6. 使用小波函数拟合边缘可以简化模型，减少参数量，提升效率。

# 3. 基本概念及术语说明
## 3.1 符号说明
* $N$ : 样本数量
* $M$ : 样本维度
* $X$ : 输入样本，维度为 $(N, M)$ ，其中每行为一个样本，列为对应特征的特征值
* $\phi(\cdot)$ : 小波函数，$\phi: [0, 2\pi] \rightarrow R$ ，若 $\phi_m (t) \in L^{2}(\mathbb{R}^{M})$ ，则称 $\phi_m(t)$ 为 $\phi$ 的一个小波变换
* $W$ : 小波权重矩阵，$\forall m \neq l, w_{ml} = 0$, 且 $w_{ll} \geq 0$ 。大小为 $(M, N_{\lambda})$ ，其中 $N_{\lambda}$ 表示正交小波基的个数。
* $P$ : 投影矩阵，大小为 $(N_{\lambda}, K)$ ，其中 $K$ 表示分类数量
* $f(\cdot)$ : SVM 分类器，$f: R^M \rightarrow \{1,\cdots,K\}$, 输出为类别标签
* $D$ : 数据集，$D = \{(x^{(i)}, y^{(i)}):i=1,\cdots,N\}$, 每条数据包括输入样本 $x^{(i)} \in R^M$ 和类别标签 $y^{(i)} \in \{1,\cdots,K\}$
* $\Lambda$ : 小波基函数，$\Lambda=(\phi_m(t)|m=1,\cdots,M)$ ，其中 $\phi_m(t)$ 是第 $m$ 个尺度的小波函数。
* $\hat{Y}_l$ : 训练集 $D$ 中第 $l$ 类的判定结果
* $\bar{Y}_l$ : 测试集 $T$ 中第 $l$ 类的判定结果
## 3.2 SVM 的原理
SVM 是支持向量机的缩写，支持向量机是一种二类分类器，它的主要目的是寻找一个超平面将所有样本完全正确分开。输入空间到特征空间的变换$\phi$是一个非线性变换，目的是为了找到一个能将原数据转换为高维空间内一个特征方向上的线性可分离超平面的映射，使得分类决策的边界比较鲁棒。SVM 使用拉格朗日乘子法求解目标函数，求解的方法是使用二次罚函数。

假设输入空间$X$是 $\mathbb{R}^p$, 其中 $p > 1$ ，$X=\{x_1, x_2, \cdots, x_n\}$ 为样本点集合。对每个输入实例 $x_i \in X$ ，赋予其相应的实例标签$y_i \in Y$ 。输入空间中的样本点通过一个映射函数$\phi:\mathcal{R}^p \rightarrow \mathcal{R}^q$ 变换到新的特征空间$\mathcal{H}=\{h_1, h_2, \cdots, h_m\}$ 中。$\mathcal{H}$ 中的每个 $h_i \in \mathcal{H}$ 都是从 $\mathcal{R}^p$ 到 $\mathcal{R}$ 的仿射变换，其定义如下：
$$
    h_i = \sum_{j=1}^p a_{ij} x_j + b_i
$$
其中 $\mathbf{a}=(a_{ij}|i=1,\cdots,p;j=1,\cdots,p)$ 和 $\mathbf{b} =(b_i|i=1,\cdots,m)$ 是变换矩阵和偏置向量。假设有训练数据 $(x_1, y_1),(x_2, y_2),\cdots,(x_n, y_n)$ ，希望构建一个线性可分离超平面，将样本点 $x_i$ 映射到相应的超平面的那一侧，使得超平面越靠近数据点 $x_i$ ，分类误差越小。于是定义拉格朗日函数
$$
    \mathcal{L}(\mathbf{w}, b, \mathbf{a}, \mathbf{b}, \xi) = \frac{1}{2}||\mathbf{w}||^2 - \sum_{i=1}^n \xi_i[y_i(\mathbf{w}^T\phi(x_i)+b)-1]+\mu\sum_{i=1}^n\xi_i
$$
其中 $\mathbf{w}=(w_1,w_2,\cdots,w_q)$ 是超平面的法向量， $b$ 是超平面的截距，$\mathbf{a}$ 和 $\mathbf{b}$ 是映射函数的参数，$\mu>0$ 是惩罚系数，$\xi_i$ 是拉格朗日乘子。拉格朗日函数刻画了如何将样本点投影到超平面上，且期望最小化分类误差，同时引入了软间隔约束，以便于处理噪声点。

给定拉格朗日函数，求解的方法是采用对偶问题：
$$
    \min_{\mathbf{w},b,\mathbf{a},\mathbf{b}}-\frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^n\xi_i-\sum_{i=1}^n\alpha_i[y_i(\mathbf{w}^T\phi(x_i)+b)-1]\\\qquad+\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \phi(x_i),\phi(x_j)\rangle\\\qquad+\sum_{i=1}^nl_i(\alpha_i-\alpha'_i)
$$
其中 $C>0$ 是惩罚参数，$l_i(u)=max\{0,u\}$ 是 Hinge loss 函数，$-1,1$ 之间的数字分别表示超平面的左右两侧，$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$ 是拉格朗日乘子向量，$\alpha'=\arg\min_\alpha Q(\alpha)$ 是对偶问题的解。Q 函数刻画了在给定的拉格朗日乘子 $\alpha$ 下，目标函数的期望值，也就是说，对目标函数求期望，使得其更易优化。要使目标函数最小化，就可以让 Q 函数接近无穷大。

具体的求解方法是用拉格朗日对偶性的方法，先固定 $b$ ，令 $\Delta_i=y_i(\mathbf{w}^T\phi(x_i)+b)-1<0$ ，则有
$$
    \xi_i+y_i(\mathbf{w}^T\phi(x_i)+b)-1-\sum_{j=1}^n\alpha_j\langle \phi(x_i),\phi(x_j)\rangle>=0
$$
其中第二个等号表示条件 $-\alpha_j\alpha_j y_iy_j\langle \phi(x_i),\phi(x_j)\rangle\leq0$ ，可以知道如果 $\xi_i+y_i(\mathbf{w}^T\phi(x_i)+b)-1+\eta\left[\sum_{j=1}^n\alpha_j\alpha_j y_iy_j\langle \phi(x_i),\phi(x_j)\rangle\right]>0$ ，那么就违反了 $\sum_{j=1}^n\alpha_iy_j=0$ 和 $\sum_{j=1}^ny_i\alpha_j\leq C$ 的约束条件，也就是说 $\eta$ 会使得 $-\xi_i-\sum_{j=1}^n\alpha_j\langle \phi(x_i),\phi(x_j)\rangle>0$ ，也就会违反条件 $\alpha_i\geq0$ ，所以对所有的 $i$ 来说，有
$$
    \xi_i \geq max\{0,-1+y_i(\mathbf{w}^T\phi(x_i)+b)+\eta\left[\sum_{j=1}^n\alpha_j\alpha_j y_iy_j\langle \phi(x_i),\phi(x_j)\rangle\right]\}
$$
把上述不等式带入目标函数，得到
$$
    \min_{\mathbf{w},b,\mathbf{a},\mathbf{b}}-\frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^n\xi_i+\eta\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \phi(x_i),\phi(x_j)\rangle\\\qquad+\sum_{i=1}^nl_i(\alpha_i-\alpha'_i)
$$
其中 $\eta$ 叫做松弛变量，通过对目标函数的二阶导的最小值，可以在不满足约束条件的情况下，最小化目标函数。对目标函数求极小，就可以得到对偶问题的解 $\mathbf{w}^*, b^*, \alpha^*$ 。
## 3.3 正则化参数 C
SVM 引入了软间隔约束，通过软间隔约束允许一些错误分类的样本点。但是，如果忽略掉这些误分类的样本点，可能会导致模型的过拟合。为了解决这个问题，需要引入正则化参数 $C$ 。$C$ 越大，模型对误分类的容忍度越高，也就意味着对误分类的样本点越严厉。$C$ 越小，模型对误分类的容忍度越低，容易受到样本噪声的影响，也就导致模型欠拟合。因此，选择合适的值的 $C$ 对模型的性能起着至关重要的作用。