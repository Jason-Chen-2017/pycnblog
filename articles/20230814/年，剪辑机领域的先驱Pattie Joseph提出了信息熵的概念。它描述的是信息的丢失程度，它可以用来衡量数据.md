
作者：禅与计算机程序设计艺术                    

# 1.简介
  

信息熵（Information Entropy）又称信息度、信息期望值、信息增益、互信息等，是一个用于评价随机变量不确定性或随机事件发生概率分布不确定性的数学指标。特别地，它描述了一个给定随机变量随机事件发生的概率分布的信息量。
信息熵可由香农（Mathematician John von Neumann）于1948年提出，其后由卡尔·雅克比（Claude Shannon）及其助手沃尔夫（Robert Walford）等人进行了深入研究并发展。
信息熵作为一种度量，其计算方法与概率论、数理统计学中的其他信息量量化方法不同。信息熵是基于概率分布而非样本空间的。因此，它可以应用于连续型数据（如声音信号），也可以应用于离散型数据（如文本、图像）。
# 2.基本概念术语说明
## 2.1 概率分布与熵
在信息论中，随机变量的概率分布（probability distribution）是指一个随机变量可能出现的取值的概率。概率分布用公式表示为：
$$p(x)=\frac{f(x)}{F(x)}=\frac{\text{Probability of } x}{\text{Total probability}}$$
其中，$f(x)$ 是随机变量 $X$ 的取值为 $x$ 的频数，$F(x)$ 是所有可能取值的概率总和。如果概率分布是连续的，则概率密度函数（probability density function）$f(x)$ 称为概率质量函数（mass function）。
随机变量的熵（entropy）是指使得 $X$ 分布最混乱的程度。随机变量 $X$ 的熵定义为：
$$H(X)=E\left[-\log_{b} p(x)\right]=-\int_{\infty}^{\infty}\frac{f(x)}{b}\log_b f(x)dx$$
其中，$b$ 是待定底，当 $b=e$ 时，熵的单位为比特（bit），当 $b=2$ 时，熵的单位为阶。若 $X$ 为离散随机变量，则 $f(x)$ 和 $dx$ 可以忽略。
设 $X$ 有 $k$ 个不同的取值，则随机变量 $X$ 的概率分布可以记作 $p=(p_1,\dots,p_k)$ ，即 $p_i$ 表示随机变量 $X$ 在第 $i$ 个可能取值上的概率。随机变量 $X$ 的概率分布通常用 $X$ 的离散概率分布或连续概率分布表示。
## 2.2 信息熵与互信息
熵 H 与互信息 I 关系如下：
$$I(X;Y)=H(X)-H(X|Y)$$
其中，$I(X;Y)$ 是关于随机变量 $X$ 和 $Y$ 的互信息。互信息衡量两个变量之间的相关性。假设 $X$ 和 $Y$ 分别服从参数分别为 $\theta_X$ 和 $\theta_Y$ 的分布，则它们的联合分布可以记作 $p_{XY}(x,y)$ 。根据贝叶斯公式，$p_{XY}(x,y)=p_X(x)p_Y(y|\frac{x}{\sum_\chi x})$ 。由于条件概率 $p_Y(y|\frac{x}{\sum_\chi x})$ 中的常数项，因而该分布不是唯一确定的，因此，为了方便计算，常常采用加权平均的方式对它求解。于是，$p_{XY}(x,y)$ 可以用加权平均的形式表示：
$$p_{XY}(x,y)=\frac{1}{Z}\exp(\theta_X^T \cdot x+\theta_Y^T \cdot y+\frac{1-e^{-\eta(x,y)}}{\eta(x,y)})$$
其中，$\theta_X$ 和 $\theta_Y$ 分别表示 $X$ 和 $Y$ 的期望向量；$\eta(x,y)$ 是归一化因子，它保证该分布是凸的，且满足 $0\leqslant\eta(x,y)\leqslant1$ 。
对上式两边同时取对数：
$$\ln p_{XY}(x,y)=\ln\frac{1}{Z}-\ln Z+\theta_X^T \cdot x+\theta_Y^T \cdot y+\ln\eta(x,y)+\frac{1-e^{-\eta(x,y)}}{\eta(x,y)}$$
由链式法则，得到：
$$\ln p_{XY}(x,y)=\theta_X^T \cdot x+\theta_Y^T \cdot y+\ln\eta(x,y)+\ln[1+e^{\ln\eta(x,y)}\cdot e^{-\ln\eta(x,y)}]+C$$
其中，$C$ 是常数项。对上式右端同时乘以 $-\eta(x,y)$ 可得：
$$-\eta(x,y)\cdot[\ln p_{XY}(x,y)-\theta_X^T \cdot x-\theta_Y^T \cdot y]-\eta(x,y)\cdot C+\ln\eta(x,y)-\ln[1+e^{\ln\eta(x,y)}\cdot e^{-\ln\eta(x,y)}]=0$$
也就是说，两个随机变量的联合分布只依赖于两个随机变量的期望值和归一化因子。因此，通过观察两个变量的联合分布，就能够确定它们之间的信息量。
## 2.3 概念扩展
### 2.3.1 相对熵（KL散度）
对于两个分布 $p$ 和 $q$ （$p$ 为真实分布，$q$ 为近似分布），相对熵 KL 散度（Kullback-Leibler divergence）定义为：
$$D_{KL}(p||q)=\int_{-\infty}^{\infty}p(x)\log\frac{p(x)}{q(x)}dx$$
根据概率论知识可知，K-L 散度为非负数，当且仅当 $p$ 为 $q$ 的最佳重构时，即存在一个编码方案将样本点按照 $q$ 分配到 $p$ 中最优的方式。也就是说，如果分布 $p$ 通过编码方案 $c$ 将分布 $q$ 重构成分布 $p$ ，那么就可以利用 $D_{KL}(p||q)$ 来衡量两个分布之间的距离。但这个方案不一定存在。相对熵 KL 散度还有一个重要作用是计算两个分布之间的相似性。当且仅当 $D_{KL}(p||q)=0$ 时，分布 $p$ 就是分布 $q$ 。
### 2.3.2 小结
1. **信息熵**：它描述的是信息的丢失程度，描述随机变量或者随机事件发生的概率分布的纯度。
2. **概率分布**：概率分布是随机变量的取值的概率，由概率质量函数或概率密度函数表示。
3. **熵**：熵是描述概率分布不确定性的度量，熵越小，随机变量的不确定性越低。
4. **互信息**：互信息是用来描述两个随机变量之间相互作用的信息量，衡量两个变量之间的关联程度。