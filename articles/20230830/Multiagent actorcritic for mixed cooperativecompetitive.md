
作者：禅与计算机程序设计艺术                    

# 1.简介
  

本文提出了一种基于多智能体的联合Actor-Critic方法来解决多方博弈问题，这种方法可以有效处理混合合作性/竞争性环境中的交互行为，并且能够在复杂的多智能体和奖励设置下，在最坏情况下（不平稳）仍然保证收敛性能。作者从强化学习和基于值函数的方法中找到了共同之处，并开发了一个简洁而有效的Actor-Critic框架，它能够同时处理多个智能体的策略，并利用策略自适应调整来解决非均衡的问题。

# 2.相关工作
关于多智能体Actor-Critic算法存在的一个很重要的研究领域就是机器人学习，因为机器人往往由多种不同类型的控制组件组成，因此需要考虑到它们的协作和竞争关系。在这一领域已经有一些成熟的模型，比如有中心化的大脑和分布式的感知器官等。这些模型通过模仿大脑的功能来实现联合控制。但是由于各个智能体之间的动态互动特性，使得这种控制方式难以满足实际应用需求。

基于同样的原因，还有一些基于混合动力学的模型被提出，这些模型将多种物理学家的观点融入到一起，尝试建立一个理论系统来描述复杂的交互过程。但是由于缺乏可微分的优化算法，只能在许多特殊情况下得到比较好的结果。

前面介绍的两种模型都可以看作是多智能体的Actor-Critic算法的变体。这里要说的是Barto等人在2017年的Nature paper《Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments》里提出的模型。该文章认为，传统的单智能体Actor-Critic模型虽然已经成功地用于许多离散任务，但是对于连续和高维的任务来说效果不佳。其主要原因是因为Actor-Critic的更新依赖于当前状态和历史轨迹，不能直接处理连续和高维输入的问题。为了克服这个障碍，他们提出了一个新的框架，即分布式策略梯度的集合Actor-Critic框架。

另外，Kaplan等人也提出了一个类似的框架，称之为基于模型预测的Actor-Critic方法（Model-Based Predictive Actor-Critic）。该方法允许智能体之间具有更紧密的联系，但同时也带来了一系列的负面影响，如噪声和延迟。

综上所述，当前多智能体Actor-Critic算法主要集中在单智能体模型，且仅仅考虑了限定类型的环境。Barto等人的文章继承了这些模型的优点，采用分布式策略梯度的集合框架，旨在解决混合合作性/竞争性环境中的交互行为。

# 3.模型概述
## 3.1 问题背景
多智能体Actor-Critic(MADDPG)方法是一个训练多智能体策略的强化学习算法。它可以用于解决多种类型的游戏环境，包括策略游戏、零和游戏、增强型多目标游戏等。MADDPG可以有效地处理复杂的多智能体和奖励设置，并可以在不平稳情况下保持最佳性能。

假设我们有$n$个智能体，每个智能体有自己的策略$\mu_i(s)$，决策空间为$S_i\subseteq \mathcal{X}$，状态$s_i$表示智能体$i$的状态；动作空间$A_i\subseteq \mathcal{U}(s_i)$，动作$a_i$表示智能体$i$采取的动作。我们还假设有一个全局状态$s'$和奖励函数$R(\tau)\forall\tau = \{s_1^{(t)}, a_1^{(t)}, r_{1:T}\}$。$\tau$代表一系列时间步，$(t=1,...,T)$，$\tau=(s_1^{(t)},a_1^{(t)},r_{1:T})$。其中，$T$表示总步数，$r_{1:T}$表示每个时间步的奖励列表。如果满足适当条件，则每个$\tau$都对应一个终止状态$s_f$，即$\forall t>T:\quad s_t^*=s_f$。所有智能体在该环境的行动和状态遵循如下约束：

1. **合作约束** (cooperative constraint): $C=\{(c_1,\cdots,c_n)|c_i\in S_i\times A_i\forall i\in\{1,\cdots,n\}}$ 表示全局合作约束集，表示每个智能体的所有可能的合作关系。
2. **竞争约束** (competition constraint): $\bar{C}=\{(b_1,\cdots,b_n)|b_i\in S_i\times A_i\forall i\in\{1,\cdots,n\}}$ 表示全局竞争约束集，表示每个智能体的所有可能的竞争关系。
3. **唯一收益性** (monotonicity of utility): 每个人都对自己历史局面有一个清晰的Utility Function，称为$u_i:\mathcal{X}\rightarrow \mathbb{R}^{+}$，且满足$\text{argmax}_{\pi\in \Pi_i}[\sum_{\tau \in \tau_\pi} R(\tau)]\leq u_i(s_i)$。$\Pi_i$ 表示智能体$i$的策略集。
4. **奖励约束** (reward constraints): 在联合动作中，每个动作都应该满足指定的奖励约束，否则奖励将会被忽略掉。奖励约束通常都是定义在全局观察到的合作或竞争关系上的。因此，奖励约束集 $\Theta=\{(r_{ij}\mid j\in[1, n]\cup [n+1])| r_{ij}=r_{ik}}$, 表示每个智能体之间的奖励约束。

## 3.2 联合Actor-Critic方法
### 3.2.1 Actor网络
在Actor-Critic方法中，actor网络用来生成行为策略，输出一个在给定的状态下能够执行的动作。它的结构由三个全连接层组成，分别有隐藏单元数$h_1, h_2, h_3$，激活函数为ReLU。输入是所有智能体的状态向量$s=[s_1;...;s_n]$，输出是一个规范化的动作向量$a=[a_1;...;a_n]^T$，要求$a_i\in [-1, 1]$。状态向量的第一个元素表示环境状态，后面的元素表示其他智能体的状态。网络的输出通过激活函数归一化到一个范围内。
$$a = g(\phi(s))$$
其中$g$为ReLU函数，$\phi$为状态特征映射函数。输出层的权重和偏置参数是学习的对象。

### 3.2.2 Critic网络
Critic网络用来评价智能体在给定的状态下的Q值，它由两个全连接层组成，分别有隐藏单元数$h'_1, h'_2$，激活函数为ReLU。输入是所有智能体的状态向量和行为向量$s'=[s'_1;...; s'_n], a=[a_1;...;a_n]^T$。输出是一个状态价值$V(s')$和动作价值$Q(s',a)$的元组，即$Q=[Q(s'_1,a_1);...; Q(s'_n,a_n)]$和$V=[V(s'_1);...; V(s'_n)]$。状态价值是所有智能体状态的期望，动作价值是相应动作在所有智能体状态下的期望。
$$Q = f(\psi(s',a))$$
$$V = f(\theta(s'))$$
其中$f$为激活函数，$\psi$为状态动作特征映射函数，$\theta$为状态特征映射函数。输出层的权重和偏置参数也是学习的对象。

### 3.2.3 更新策略
首先，根据当前全局状态和动作进行环境响应，生成下一步所有智能体的状态和奖励信息。然后，计算TD误差，采用Actor-Critic更新策略更新Actor和Critic网络的参数。

#### 3.2.3.1 TD误差计算
对于每个智能体$i$及当前状态$s_i$，将其执行的动作记作$a_i$，下一个状态记作$s_{i'}$，奖励记作$r_i$。则TD误差的计算如下：
$$y^{i}_{j} = r_i + \gamma Q(s_{i'}, a^{\pi^\star}_i') - Q(s_i, a_i)$$
其中$\gamma$为折扣因子，$\pi^\star$表示最优策略。

#### 3.2.3.2 策略梯度更新
对于所有智能体，更新Actor网络参数：
$$\nabla_{\theta_i} J_i(\theta_i) = \frac{1}{m} \sum_{k=1}^m \sum_{j\neq i} \left[\nabla_{a_j} Q(s_j, \mu(s_j;\theta_j))\right] \odot [\nabla_{\theta_j} Q(s_j, a_j;\theta_j)]^\top$$
其中$m$为mini-batch大小。

### 3.2.4 模型参数更新
在Actor和Critic网络更新完毕之后，更新模型参数：
$$\theta_i \leftarrow \theta_i + \alpha \cdot \delta_\theta_i,\quad \alpha > 0$$
$$\pi_i \leftarrow \arg\max_{\mu_i} \left[\sum_{\tau \in \tau_\pi} R(\tau)\right]$$