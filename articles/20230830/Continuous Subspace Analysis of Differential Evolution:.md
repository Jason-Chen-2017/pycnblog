
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Differential evolution (DE) is a popular stochastic optimization algorithm that has been successfully applied in many fields such as engineering design, operations research, finance, and computer science to solve complex problems with high-dimensional spaces and large numbers of decision variables. The DE algorithm consists of selecting parents from the population using two randomly chosen individuals, mutating them according to various mutation strategies, calculating their fitness values by evaluating the objective function on these new solutions, selecting the best individual among them for reproduction, and then combining this selected parent with one of its children generated through mutation. 

In recent years, several modifications have been proposed to the basic idea of differential evolution, including support vector machines (SVM), robust optimizers, Bayesian optimization, etc., which aim at addressing some critical issues in current DE algorithms. For example, SVM can be used as an alternative strategy when generating offspring candidates. Robust optimizers use non-linear constraints or penalties during mutation and selection stages to improve the convergence rate and stability of the algorithm. By incorporating a prior distribution into the parameter space, Bayesian optimization enables the optimizer to search for better hyperparameters that are likely to produce better outcomes.

However, despite the numerous advances in DE algorithms, there remains some limitations in terms of performance, scalability, and interpretability. One major issue is that the convergence behavior of traditional DE algorithms is usually sensitive to the choice of parameters such as mutation probabilities, crossover probabilities, and selection strategies, which makes it difficult to find suitable settings that optimize the global optimum efficiently. In addition, the employed basis functions often do not provide sufficient information about the underlying problem structure and thus may lead to suboptimal results. To address these challenges, several recent works propose continuous subspace analysis techniques, which leverage the statistical properties of continuous functions to construct candidate solutions instead of using fixed bases or discrete levels. These techniques combine elements of artificial intelligence, data analytics, machine learning, statistics, and physics to analyze the space of potential solutions and obtain more informative candidate solutions.


The purpose of this article is to review the most recent developments in continuous subspace analysis of differential evolution (CSA-DE), highlighting the key insights and techniques that have significantly improved the performance, scalability, and interpretability of traditional DE algorithms. We will begin by reviewing the background knowledge and concepts related to CSA-DE, followed by presenting the core algorithm and technical details. Next, we will demonstrate how to apply CSA-DE in practice, discuss open problems and future directions, and finally draw out a conclusion and perspectives for further research. Finally, we hope that our work could inspire other researchers to explore the exciting field of continuous subspace analysis of DE and promote more practical applications of DE in real-world problems. 



# 2.前景介绍
Differential evolution (DE) is a popular stochastic optimization algorithm that has been successfully applied in many fields such as engineering design, operations research, finance, and computer science to solve complex problems with high-dimensional spaces and large numbers of decision variables. The DE algorithm consists of selecting parents from the population using two randomly chosen individuals, mutating them according to various mutation strategies, calculating their fitness values by evaluating the objective function on these new solutions, selecting the best individual among them for reproduction, and then combining this selected parent with one of its children generated through mutation. 


In general, the standard formulation of the DE algorithm involves four steps: (1) generate random initial guesses x(i); (2) evaluate f(x(i)) to get fitness value; (3) select pairs of parents based on tournament selection or linear ranking; (4) compute offspring y = g(x(i)) where g() is a mutation operator such as polynomial mutation, blend crossover, or exponential. The final step combines the selected parent with one of its children generated through mutation to create a new solution x(i+1). After repeating this process multiple times, each generation produces a series of candidate solutions, and the most fit one serves as the optimal solution. This iterative process continues until the termination criterion is met, i.e., either a specified number of generations is reached, or no significant improvement over a certain period occurs.

Recently, numerous modifications have been proposed to the basic idea of differential evolution, including support vector machines (SVM), robust optimizers, Bayesian optimization, etc., which aim at addressing some critical issues in current DE algorithms. Specifically, SVM can be used as an alternative strategy when generating offspring candidates, while robust optimizers use non-linear constraints or penalties during mutation and selection stages to improve the convergence rate and stability of the algorithm. By incorporating a prior distribution into the parameter space, Bayesian optimization enables the optimizer to search for better hyperparameters that are likely to produce better outcomes. However, despite the numerous advances in DE algorithms, there remains some limitations in terms of performance, scalability, and interpretability. Among others, the convergence behavior of traditional DE algorithms is usually sensitive to the choice of parameters such as mutation probabilities, crossover probabilities, and selection strategies, which makes it difficult to find suitable settings that optimize the global optimum efficiently. Moreover, the employed basis functions often do not provide sufficient information about the underlying problem structure and thus may lead to suboptimal results. 

To address these challenges, several recent works propose continuous subspace analysis techniques, which leverage the statistical properties of continuous functions to construct candidate solutions instead of using fixed bases or discrete levels. These techniques combine elements of artificial intelligence, data analytics, machine learning, statistics, and physics to analyze the space of potential solutions and obtain more informative candidate solutions.

Among all the existing methods for analyzing continuous subspaces, continuous variable transformation (CVT) is a promising approach that transforms the original continuous space into another latent continuous space. It maps the input vector xi onto a new set of transformed coordinates xt that captures the essential characteristics of the input space without losing any relevant features. Then, a classifier learns to map xt back onto the target class labels, making predictions on new inputs. The mapping between the original and latent space is learned automatically and adaptively using neural networks.

Another method called kernel ridge regression (KRR) uses a special kernel function to transform the continuous input space into a higher dimensional feature space, where a regularized least squares regression model is trained to predict the output targets. The resulting predictor can handle arbitrary smoothness in the original input space and therefore provides a flexible way of handling highly nonlinear problems. Another variant of KRR called thin plate splines is even more expressive and efficient than ordinary KRR but requires specialized training procedures to avoid overfitting. Despite the success of kernel methods in solving nonlinear problems, they still suffer from the curse of dimensionality and require careful parameter tuning and high computational costs to scale up to large datasets. Therefore, CVT and KRR offer complementary approaches to enhance the flexibility, efficiency, and accuracy of DE algorithms.


In summary, the primary motivation behind developing CSA-DE is to achieve better performance, scalability, and interpretability compared with traditional DE algorithms while leveraging the power of continous variable transformations and kernel models. We believe that these techniques will pave the road towards applying DE in real-world problems, particularly those involved with highly nonconvex and ill-posed objective functions and large numbers of decision variables.