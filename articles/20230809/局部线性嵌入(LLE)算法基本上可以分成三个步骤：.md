
作者：禅与计算机程序设计艺术                    

# 1.简介
         
 在无监督学习领域中，线性可分离混合模型（LCM）是一种非常流行的降维方法。但是，在高维空间中训练LCM存在两个主要问题：首先，数据点的密集分布往往使得欧氏距离变得很大，而LCM对数据的敏感度依赖于距离；其次，对于非凸数据，LCM训练可能陷入局部最优，导致泛化能力差。为了解决以上两个问题，提出了局部线性嵌入（Locally Linear Embedding，LLE）算法，它基于局部权重矩阵将高维空间中的数据点投影到低维空间中。
         LLE算法包括两步：第一步是计算核函数，第二步是最小角回归算法。
         ## 1.背景介绍
         局部线性嵌入算法LLE是一种无监督降维的方法，其基本思路是通过一种称为核技巧的形式将高维空间的数据点映射到低维空间中。核技巧将原始空间中的输入空间映射到一个特征空间，其中每个点都由一组关于其邻居的函数值构成。然后将这个特征空间映射到另一个低维空间中。
         为什么要用核函数呢？简单来说，核函数是一种能够有效处理非线性关系的核函数，比如高斯核、多项式核等等。通过核函数把原始输入空间映射到特征空间后，再利用最小角回归算法对低维空间进行寻找，就可以得到局部嵌入后的结果。
        - **核函数（Kernel Function）**：
          LLE算法的核心是计算核函数。核函数是一个非参数型的函数，它能够将原始输入空间映射到一个特征空间中。核函数有很多种，但由于不同的核函数适用于不同的数据类型，所以需要根据数据的特点选择合适的核函数。核函数计算公式如下：
          K(x, y)=exp(-||x-y||^2/sigma^2)，sigma表示核函数的参数，也叫径向基函数。
        - **最小角回归算法（Mahalanobis Distance Regression）**：
          Mahalanobis距离是一种测度两个随机向量间的距离的标准。如果两个向量在某个方向上没有相关性，那么它们之间的距离就会越来越大，反之，如果两个向量在该方向上高度相关，那么他们之间的距离就会相对较小。因此，当应用到高维空间的数据时，Mahalanobis距离可以用来衡量两个点是否属于同一个簇。
           假设将原始空间的数据点映射到特征空间后，得到的数据点为X，而对应的类别标签为y。现在希望找到一组参数W，能将X投影到低维空间中。假定有k个高斯分布构成高斯混合模型，那么可以通过最大似然估计法求得W。具体地，定义：
          P(Y=j|X)=p(Y=j)/sum(i=1 to k)p(Y=i)。则有：
          logP(X)=logp(Y=1)+logp(Y=2)+...+logp(Y=k)-k*E_w[||X-Wz||^2]
          对各个分量求偏导，令其等于0，并令E_w[||X-Wz||^2]=0，则可以得到下面的优化问题：
          argmax W*, sum(i=1 to m)(logp(Y=1)*z_1+(k-1)*logp(Y=k)*z_m)+(m-1)*(k-1)/(k+m-2)*logp(Y=(k+m-2))*wz-(m/(k+m-2))*(1/2)*||w||^2
          可以看到，优化目标就是找到使得整个数据集概率最大的超平面。
          