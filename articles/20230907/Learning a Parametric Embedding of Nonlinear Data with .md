
作者：禅与计算机程序设计艺术                    

# 1.简介
  


在无监督学习过程中，如何把非线性数据降维到一个合适的空间上是一个至关重要的问题。最近几年出现了一些方法，如谱聚类、核化学习、图嵌入等，它们的效果都很好，但这些方法都是基于人造数据的经验而得出的，对于真实世界的数据不一定有效。另一种更加现代的方法，则是使用Johnson-Lindenstrauss transform(JLJ)。

JLJ是一种非线性变换，它通过构造一个低秩矩阵将高维空间中的样本映射到低维空间中。该低秩矩阵的分解可以得到一些简单的和非线性的基函数。这样的基函数对降维后的空间中的样本有一定的非线性拟合能力，且不需要进行参数调优，因此非常方便。由于JLJ是一阶的，因此也很容易计算。

本文主要研究如何用JLJ进行数据的降维，并提出了一个有效的模型，即利用JLJ的原始理论，去学习非线性数据的低秩嵌入，即将高维数据压缩到一个低秩子空间中，同时保持非线性关系。

# 2.相关工作

1997年，Johnson和Lindenstrauss提出了非线性数据处理的第一个方法，即利用矩阵分解的思想，通过学习几个低秩矩阵和基函数，来将高维数据压缩到低维空间中。他们还证明了这种方法可以在不同程度上捕获数据的局部和全局信息。

2003年，Kang等人提出了对偶形式的矩阵分解方法，通过最小化对偶损失函数而不是原始损失函数，来学习非线性数据的低秩嵌入。

2008年，Tipping和Bishop等人提出了通过正交约束的方法，来学习非线性数据的低秩嵌入。但是，这种方法不能保证获得最佳的基函数的复杂度。

2010年，Cheng等人提出了极小均方差分解（EMD）的方法，通过最大化嵌入的重构误差来学习非线性数据的低秩嵌入。但是，这种方法需要对数据进行采样，且效率比较低。

2012年，Liu等人提出了改进的JLJ方法，利用多项式时间复杂度的算法，从理论上证明了其可行性和稳定性。但这一理论上的推导只局限于一阶映射。

2013年，Bergqvist等人在两篇文章中详细阐述了JLJ方法。其中一篇是关于离散信号的，另一篇则讨论了连续信号的情况。

2015年，Shi等人提出了学习JLJ方法，在理论上证明了其收敛性和有效性。此外，他们还提供了一个有效的模型——基于Wigner-Ville分布的JLJ方法，来应用到图像数据中。

# 3.模型概览

给定输入$X\in R^{N \times d}$，希望通过学习映射$\phi:R^d \rightarrow R^k$来将其压缩到一个低维空间$Z\in R^{N \times k}$中，同时保持数据的非线性关系。假设输入数据满足某种统计规律$p_X(x)$，而我们想要学习的是$\phi$的一个近似，使得$p_{\phi}(z) = p_{X}(\phi^{-1}(z))$尽可能地接近真实的分布$P(x)$。

为了达到这个目标，我们首先定义一个超平面$H$：
$$ H=\{x : x^Tx=k\} $$

并且选取基函数$b_j(\cdot), j=1,\ldots, m$满足：
$$ \|b_j(x)-b_l(y)\|=max\{ \|x-\mu_j\|, \|y-\mu_l\|, \|x-\mu_l\|, \|y-\mu_j\| \} \quad for all l=1,\ldots,m; and all i,j, where \mu_j=(\sum_{i=1}^N b_j(x_i)/N, \ldots )^T.$$

注意到基函数满足完备性条件，即对于任意两个基函数$\tilde{b}_j, \tilde{b}_l$, 满足：
$$ \|(\tilde{b}_j+\delta b)_i-\tilde{b}_l+\delta b_i-b_l\|^2\leq c_1\|b_j-b_l\|^2+c_2\|b_i-b_l\|^2+c_3\|b_i-b_j\|^2,$$

其中$\|\cdot\|$表示F范数，$c_1>0, c_2>0, c_3>0$为权重系数。

则基函数的线性组合$\sum_{j=1}^mc_jb_j(\cdot)$，定义为：
$$ Z_h=H\bigg(\sum_{j=1}^mc_jb_j(X)\bigg)^T.$$

现在考虑两种情况下的嵌入结果：

## （1） 维数k等于样本维数d时

此时，基函数$\{\phi(e_i):i=1,\ldots, d\}$直接为原始数据的一部分。可以直接求解$Z_h$，因为它只能由线性组合的基函数决定。

## （2） 维数k<d时

此时，基函数$\{\psi_j(x):j=1,\ldots, k\}$被限制在$H$上的某个子空间中，可以视作一个低维非线性变换。令：
$$ A_h=B_hD_hb_h(X), B_h=[\phi(x_1), \ldots, \phi(x_N)]^T, D_h=diag([\lambda_1,\ldots,\lambda_k]), h\in H,$$

则$A_h$的秩为$rank(D_h)=k$，且$A_h$有$k$个奇异值$\{\lambda_1>\lambda_2>\cdots >\lambda_k\}$, 且对每个$j=1,\ldots, k$, 有$col(A_h)_j = col(D_hb_h(X))_j$, $\forall j$.

要将$A_h$转换为$Z_h$，可采用如下优化问题：
$$ argmin_Z ||Z - X||_F^2 + \tau(D_h)(I-DD_h)^{-1}\|A_h-Z\|_F^2 $$

这里，$\tau(D_h)$是控制正则化强度的参数。当$k\geq d/2$时，$\tau$接近于0；否则，$\tau$就增大了，使得优化问题变得比标准最小二乘法复杂度更低。

显然，当$k\leq d/2$时，优化问题的解一定存在。如果$\tau(D_h)>0$, 那么问题是凸的，可以使用一些优化算法（如Adam）来求解。否则，如果$\tau(D_h)=0$, 那么问题是仿射的，我们可以采用如下近似解：
$$ Z_h\approx X^TD_h^{-1}\phi(X).$$