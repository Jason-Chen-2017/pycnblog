
作者：禅与计算机程序设计艺术                    

# 1.简介
  


随着高维数据的出现，以及人们对大数据分析方法的需求越来越高，如何对数据进行降维成为一个重要课题。降维有很多方法，如主成分分析、线性判别分析等，但这些方法往往不能达到最优的效果。为了解决这一难题，张量分解（tensor decomposition）在人工智能、生物信息学、统计学习、模式识别等领域都有广泛应用。

张量分解是指将任意一个张量分解为两个或多个矩阵相乘所得。张量分解可以在无损的情况下降低原始张量的秩，同时保持原有的主要特征。张量分服有很多种不同的形式，如CP分解、Tucker分解等。本文将详细介绍Tensor decomposition的基本概念，并基于PCA、CP Decomposition、TSVD等三个典型的张量分解算法进行阐述。最后，将比较这三个算法之间的差异，并给出TSVD的一个具体例子。

# 2.基本概念、术语及符号说明

2.1 张量（tensor）

张量是指数组和其中的元素之间具有相关关系的多维数组。比如一张图片就是一个三阶张量，其中第一阶表示像素值，第二阶表示行，第三阶表示列。而机器学习中，输入输出的数据也是作为张量存在的。

2.2 秩（rank）

秩是张量的线性独立向量的个数。张量的秩可以看作它丢弃了的特征数量。例如，一个3阶张量的秩就等于3，因为它保留了所有图像的信息。而一个2阶张量的秩就等于1，因为它只保留了一阶信息——即每个像素的灰度值。

2.3 约化（canonical form）

约化指的是将张量从一个指定的秩降到另一个指定秩，这种约化称为约化算子。约化算子保证了张量在约化前后秩相同，因此是一种正则化手段。约化算子还会将张量分解为两个矩阵相乘所得的形式，这也是张量分解的一种形式。一般来说，张量分解得到的两个矩阵相乘形式的张量也叫做约化张量。

2.4 谱空间（spectral space）

谱空间是一个标准的规范化坐标系，由一组正交基张成，并且每一个向量在这个坐标系下都对应着一个复数值。例如，如果张量X的秩k，那么它的谱空间的维度就等于k。这里谱空间与特征空间不同，它是张量在某些特征值对应的特征向量构成的空间。

2.5 单位阵（identity matrix）

单位阵是一个对角线为1，其他元素为0的方阵。

2.6 对角化（diagonalization）

对角化是指将张量用对角线上的元素来表示。一般来说，对角化可以分为奇异值分解（SVD）和Cholesky分解两种方式。

2.7 Tucker分解

Tucker分解是一种对张量的三阶张量进行分解的方法，它把张量分解为三个矩阵相乘所得的形式。假设有张量X(I1×I2×I3)，其中I1、I2、I3分别表示其中的三个索引，那么Tucker分解就是求三个矩阵W1、W2、W3，满足下面的约束条件：

X = W1 ⊗ (W2 ⊗ W3)

对角化可知，X = Q1 ⊗ (Q2 ⊗ Q3) ⊗ QT，且QT是单位阵，Q1、Q2、Q3分别是对角阵。利用约化运算，Tucker分解可以获得X在W1、W2、W3上分解的结果X1、X2、X3，以及三个张量W1、W2、W3之间的约化张量Y。Tucker分解的缺点是要求三个矩阵相乘所得的形式，可能会造成奇异值过多的问题，但这是可以通过减少参数个数来缓解的。所以，Tucker分解适用于较大的张量，且要求精确的近似结果。

# 3.核心算法原理和具体操作步骤

## （1）PCA(Principal Component Analysis)

PCA的全称是主成分分析，是一种简单而有效的无监督降维技术。PCA通过寻找数据的最大方差方向，将数据投影到该方向上去，达到降维的目的。如下图所示：


1. 首先计算协方差矩阵，用XTX表示
2. 求解协方差矩阵的特征值和特征向量，得到PCs
3. 选择前k个最大的特征值对应的特征向量，作为新的PCs
4. 用PCs投影原始数据得到新的数据X'，得到的新数据仅保留前k个特征向量

解释一下PCA的几个步骤：

1. 把数据按行看作观测变量，用Sx表示协方差矩阵
2. Sx是样本协方差矩阵，Sxx是总体协方差矩阵
3. 通过svd求解Sx的特征值和特征向量
4. 在解释数据之前，先确定要保留的主成分数目K
5. 取Sx的前K个特征值对应的特征向量组成新的空间W
6. 将数据投影到W上，得到W上的投影Y
7. Y只是X的一部分，但是已经比X小很多

## （2）CP Decomposition

CP decomposition是一种用于张量分析的一种矩阵分解技术。其基本思想是在原始张量的各个分量间建立依赖关系，然后按照依赖顺序将张量分解为多个矩阵相乘得到的形式。CP decomposition的过程如下图所示：


1. 初始张量A(I1*I2*...*In)根据传感器之间的互补影响，分解成几个block Ai=(ai1*aij*...*ain)i=1:M, j=1:N, k=1:L
2. 迭代计算每个Ai块的变化率dia(i), dij(j), dik(k)=aiij*aik+aijk*aik-aiji*aik, dik表示了第i个block对第j个block的独立性
3. 根据dii, dij, dik构造W(i,i)*W(j,j)*(W(k,k)-1)*W(l,l)张量
4. 沿着第i个方向进行分解，得到Wi(I2*...*In)。之后将它们相乘得到新的张量W(m,n)*(W(p,q)-1)*W(r,s)，其中m, n, p, q, r, s都是任意的索引
5. 恢复原始张量

解释一下CP Decomposition的几个步骤：

1. 原始张量A按感应器维度划分为多个block，这些block是独立的
2. 使用CP Decomposition，对于Ai块，构造一个二阶张量Bij(aiij*aik+aijk*aik-aiji*aik)，记录该block对其它block的依赖关系
3. 对所有的block，将相应的张量连接起来，形成全连接张量，然后对该张量进行对角化
4. 分解得到的三个矩阵表示各个block的变换，与依赖关系相匹配的block表示为独立的block，最终恢复原始张量

## （3）TSVD(Truncated SVD)

TSVD是用于压缩高维数据的一种矩阵分解方法。当数据维度很高时，通常希望减少存储和计算的时间。TSVD的主要思路是截断奇异值分解（SVD），从一定数量的奇异值开始积累，直至最小奇异值的数量达到所需数量。

TSVD的过程如下图所示：


1. 对样本X进行中心化处理
2. 从奇异值分解得到U，V，Σ
3. 取Σ的前r个奇异值和相应的奇异向量组成新的奇异子空间Υ
4. 将X转换到Υ子空间，得到Z，其中X=UΣV^T
5. 将Z的奇异值分解得到U‘，Σ‘，V’
6. 取Σ‘的前q个奇异值和相应的奇异向量组成新的子空间Uπ
7. Z是原始数据经过压缩后的形式，Uπ是U‘的前q个奇异向量组成的子空间，Σ‘的前q个奇异值组成奇异值，Uπ是U‘的前q个奇异向量组成的子空间，Uπ*Uπ^T=I

解释一下TSVD的几个步骤：

1. 样本X进行中心化处理
2. 对样本X进行奇异值分解，得到样本X的左奇异矩阵U和右奇异矩阵V，以及奇异值矩阵Σ
3. 取Σ的前r个奇异值和相应的奇异向量组成新的奇异子空间Υ
4. 将X转换到Υ子空间，得到Z
5. 对Z进行奇异值分解，得到样本Z的左奇异矩阵U‘，右奇异矩阵V’，以及奇异值矩阵Σ‘
6. 取Σ‘的前q个奇异值和相应的奇异向量组成新的子空间Uπ
7. Z是原始数据经过压缩后的形式，Uπ是U‘的前q个奇异向量组成的子空间，Σ‘的前q个奇异值组成奇异值，Uπ是U‘的前q个奇异向ved-vectors组成的子空间，Uπ*Uπ^T=I