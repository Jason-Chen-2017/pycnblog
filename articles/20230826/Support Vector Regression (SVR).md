
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Support vector regression（SVR）是一种用于回归分析的机器学习方法，其目标函数是使预测值与实际值尽可能接近，同时保证误差或损失函数最小化。支持向量机（SVM）是另一种流行的机器学习算法，也可用来做回归，但它侧重于分类而不是回归。SVM使用核函数将输入空间中的数据点映射到高维空间中，因此能够在非线性情况下找到最优解。而支持向量回归（SVR），是在SVM的基础上增加了惩罚项，用以解决回归问题中的样本不均衡的问题。

# 2.基本概念
## 支持向量机（Support Vector Machine, SVM）
SVM通过求解一个最大间隔分离超平面将无标签的数据集划分为两个子集：边界上的正类(positive class)，和边界上的负类(negative class)。对于给定的输入x，SVM模型会计算该点属于哪一类的概率，概率越大，则判断出的类别就越准确。那么如何找到这样的超平面呢？SVM采用拉格朗日对偶性法求解，首先构造出对应的拉格朗日函数L(w,b,xi)=(1/2)*||w||^2-q(i)*(y_i*(w*xi+b))，其中w为分离超平面的法向量，b为截距项，q(i)是一个松弛变量，当样本yi(xi)>=1时，q(i)=0；当样本yi(xi)<=-1时，q(i)=C；当样本yi(xi)在(-1,1)范围内时，q(i)=0。然后求解拉格朗日方程组，将其转换为等价形式，得到原始问题的对偶问题：

max L(w,b,xi)<=0 for all i=1 to m, -xi_i*y_i<0 and xi_i>0 and C>=0

s.t w(T*y)<0, y_i(T*y_i)==1 for all i=1 to n, |xi|<C for all xi.

这里T*y表示任意超平面（如l=0时的超平面），-y_i(T*y_i)表示分类正确的样本个数，y_i(T*y_i)>1表示分类错误的样本个数，|xi|<C表示xi满足约束条件。这个对偶问题可以直接用KKT条件求解。

## 拉格朗日对偶性
拉格朗日对偶性，又称“软”间隔法、“松弛”条件，是一种求解凸二次规划（convex quadratic programming）问题的方法。把二次规划问题的对偶问题作为原问题的下界或者上界，原问题的最优解应该落在这个下界或者上界之内，而且不严格等号（<==>）。拉格朗日对偶性是优化理论中重要的工具，很多著名的最优化问题都可以转化为这一类问题进行求解。

假设给定一问题P，原问题具有最优解，即原问题的一个最优子问题Q的最优解，对应的充分必要条件（sufficiency condition）是：如果Q的最优解是最优的，那么原问题的最优解也是最优的。相应地，若原问题的最优解不是最优的，那么一定存在某个最优子问题Q，使得Q的最优解不是最优的，进而导致原问题的最优解不是最优的。所以，拉格朗日对偶性是检验一个最优化问题是否存在局部最优解的有效方法。

假设原问题是一个二次型：min f(x), x∈ Rn，x为n维向量。利用拉格朗日乘子法，定义拉格朗日函数L(x,λ) = f(x)+Σ_{i=1}^m λ_i[h_i(x)-1]，其中λ=(λ_1,...,λ_m)^T为m个未知的拉格朗日乘子，每个λ_i>=0。为了使得问题变成对偶问题，需要满足约束条件：

1) 若i≠j，则有L(x,λ)_i≥L(x,λ)_j+1, i=1,...,m;
2) Σ_{i=1}^m λ_i=0;
3) h_i(x)>=0, i=1,...,m.

第1条称作松弛条件，表示拉格朗日函数的一阶导数单调递增；第2条称作互补松弛条件，要求拉格朗日乘子的和等于零；第3条称作规范可行条件，表示函数的每一值域非负（包括零）。拉格朗日对偶问题就是将原问题的对偶问题定义为：max ‖g‖, g(x,λ) = ‖f(x)+Σ_{i=1}^m λ_i[h_i(x)-1]-μ‖, μ>=0。令g'(x*)=0，由松弛条件可知g(x*,λ*)=inf{L(x,λ)|x'*z>=c}, x'∈ Rn，z=(z_1,...,z_m)^T，c=max{(z_1,...,z_m)}, g(x*,λ*) = max{z'*H_k(x')+c(−λ_*)}.

对于二次型问题，拉格朗日对偶问题是一个极小极大问题，也可以看作是原问题的一个下界。由于存在许多限制条件，求解过程较为复杂。但是，拉格朗日对偶性还可以扩展到其它类型的优化问题，比如线性规划问题、非凸二次规划问题等。