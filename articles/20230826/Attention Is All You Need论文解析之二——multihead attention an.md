
作者：禅与计算机程序设计艺术                    

# 1.简介
  

MT(Machine Translation)领域的最热门paper之一"Attention is all you need"已于近两年发表。本篇博客将阐述该paper的基本思路、原理和应用。


什么是Multi-Head Attention？
从字面上理解，Multi-Head Attention就是多头注意力机制。传统的注意力机制（Attention Mechanism）能够让网络在输入序列中关注到其中某些特定的信息，而Multi-Head Attention可以视为一种改进方案，通过多个子空间间的相互联系实现特征的交叉关联。对于机器翻译任务来说，在单个子空间中的注意力机制可能难以捕获复杂的词语之间的关系，因此需要多个子空间并行处理不同的特征维度。



多头注意力机制如何工作？
从原理上讲，每一个子空间都是一个小型的自回归神经网络（RNN），在每个子空间中，模型能够捕捉输入序列中不同位置之间的依赖关系。与传统的注意力机制相比，多头注意力机制能够捕捉到输入序列中不同子空间的相关性。如下图所示，传统的注意力机制只有一个子空间，它只能捕捉到整个输入序列中的依赖关系；而Multi-Head Attention包含三个子空间，分别处理不同位置、句法和语言的相关性。


注意力机制如何应用到机器翻译任务中？
机器翻译任务包括两种模式：编码器－解码器模式和生成式的模式。在编码器－解码器模式下，需要将源语言序列通过编码器映射成固定长度的表示，再将固定长度的表示送入解码器进行解码。解码器会根据编码器的输出以及其他条件（如当前翻译位置等）确定相应的翻译目标。而在生成式的模式下，则不需要预先设计解码器，只需在训练过程中根据模型的输出生成翻译结果即可。对于机器翻译任务来说，生成式的方法更加实用。

生成式的多头注意力机制如何工作？
对于生成式多头注意力机制，其基本思路与传统的注意力机制类似，即每次计算当前时刻应该关注哪些词汇。但是，生成式的多头注意力机制的主要区别是，生成过程不是由单一的隐状态向量决定的，而是由多头的隐状态向量组合得到的。具体地，在每个时间步t，生成模型会首先产生t时刻的隐状态向量h_t^i = f(x_{t:T}, h_{t-1}^i)，其中f为非线性激活函数。然后，生成模型会将h_t^1，h_t^2，...,h_t^{K}作为多头注意力模块的输入。

多头注意力模块的原理很简单，它首先将h_t^i与各个子空间的查询矩阵Wq进行矩阵乘法，计算出查询向量q_t^(k)。随后，它还需要对查询向量进行规范化处理，然后再与值矩阵Wq与键矩阵Wk一起做矩阵乘法，计算出注意力权重，权重的值通常采用softmax函数。最后，它会将不同子空间的注意力权重拼接成最终的注意力向量α_t^k，并与值向量Wv一起做矩阵乘法，获得t时刻的输出向量o_t^(k)。

通过使用多头注意力模块，生成模型能够捕捉到不同子空间之间的依赖关系。生成模型在每个时间步t都会接收到来自前面的时间步的信息。但为了提升效果，还可以通过其它方法（如残差连接或高效的运算方式）来增强生成模型的性能。