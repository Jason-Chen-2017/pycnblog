
作者：禅与计算机程序设计艺术                    

# 1.简介
  
型张量分解(Introduction-based tensor decompositions)
这类方法不需要进行正则化，但是对原始数据进行了假设，因此计算复杂度会比较高。经典算法有PARAFAC、Tucker分解、HOOI分解等。这些方法对于矩阵比较大的情况下，可以有效地实现降维。例如，使用PARAFAC进行图像压缩，使用HOOI分解进行文本检索。但是，由于其对原始数据的假设，当原始数据不满足特定假设时，其结果可能不准确。
# 2.正则化张量分解(Regularized tensor decompositions)
这类方法通过引入额外的正则项来解决原始数据的限制。经典算法有CP，Tucker分解，HOOI分解，以及其他相关方法。这些方法能够处理原始数据的奇异值和模式信号，并且对噪声有很好的鲁棒性。如PARAFAC，HOOI分解能够很好地对小样本数据进行降维，而Tucker分解适用于大型稀疏数据。HOOI分解在现实应用中也有广泛的应用，如推荐系统中的协同过滤和多模态数据分析。
# 3.模型学习张量分解(Model learning tensor decompositions)
这类方法通过直接学习潜在变量模型来确定分解的最优参数。经典算法有Hosvd、BPP、KSVD等。这些方法能够捕捉到原始数据的更大规模结构信息，具有很高的解释性和预测能力。而且，它们可以在不显式地指定分解维度的情况下，自动选择合适的维度。目前，最大似然估计的方法，如EM算法，也是一种模型学习方法。
# 4.增强型张量分解(Enriched tensor decompositions)
这类方法在主流方法上添加了一些额外的约束条件来提升分解质量。经典算法有三角分解、高斯张量分解等。这些方法能够捕捉到不同子空间之间的关系，并且能够在一定程度上避免因子呈现冗余或混叠现象。如Tucker分解和HOOI分解都属于这一类。
总结一下，张量分解的方法大体上可以分为四种类型:简介型、正则化型、模型学习型、增强型。其中，模型学习型的模型学习方法（例如EM）在实际中应用得非常少，主要作为补充理论知识来研究张量分解的方法。简介型的方法缺乏解释力和预测力，但它对原始数据的限制较少；而正则化型的方法在保证高效率的同时，还能够捕捉到丰富的结构信息；增强型的方法能够捕捉到不同子空间之间的关系并尽量减少因子冗余，适用场景包括稀疏数据。所以，掌握这些方法的基本原理及应用方式，是理解张量分解背后的原理和运作机制的关键。






作者：张晶帆
链接：https://www.jianshu.com/p/3fd87abfc3e9
來源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



## 张量分解的分类

张量分解，又称张量化特征提取，指的是利用张量来对数据进行低维表示。一般来说，张量分解可以分为三个阶段：

1. 简介型张量分解：基于简介的人工设计假设进行张量分解。
2. 正则化张量分解：通过添加惩罚项和约束条件来对张量进行正则化。
3. 模型学习张量分解：通过直接学习潜在变量模型来确定张量分解的最优参数。

在这三个阶段之间，又可以衍生出更多不同的张量分解方法。接下来，我将简要地对各个分类方法进行介绍。


### 1. 简介型张量分解

这是最古老的张量分解方法。在该方法中，假设原始数据可以分解成两个因子$A$和$B$，即$\forall X\in R^{m \times n}, A\approx B=AX$。这种假设虽然十分简介，却能够有效地实现降维。首先，考虑一下向量长度的定义，设$x_i=(a_{ij})_{j=1}^n, i=1,\cdots,m$, $y_j=(b_{ji})_{i=1}^m, j=1,\cdots,n$，那么由正交性，有$(x^Ty)=\sum_{i=1}^{m}\sum_{j=1}^{n}(x_i^Ty_j)=\sum_{j=1}^{n}\left(\sum_{i=1}^{m} x_i^Ty_j\right)$。因此，张量$X$的特征值$\lambda_k=\sigma_k^2=||U_k\Sigma_k V_k||$，其中$U_k,V_k$分别是特征向量，而$\Sigma_k$是一个对角阵，且$\Sigma_k$的第$k$个元素对应着特征值$\lambda_k$。假设原始数据$X$可以分解成$A$和$B$，那么令$Y=AB$，有$Y=AX$，其中$\{U_k\}_{k=1}^r, \{V_k\}_{k=1}^q$分别是$X$的左特征张量和右特征张量，而$\{\lambda_k\}_{k=1}^{r+q}$则是$X$的特征值。显然，张量$X$的分解$\{A\}$, $\{B\}$可以看作是在特征张量空间$\mathcal{K}_{\rm F}=\mathbb{R}^{n\times m}$的基底变换。因此，简介型张量分解所需计算的时间复杂度是$\mathcal{O}(mnr^2+\max\{mr^3,nq^3\})$。

### 2. 正则化张量分解

正则化张量分解相比于简介型张量分解，除了假设数据可以被分解成$A$和$B$之外，还采用了额外的正则项或者约束条件，从而能够处理稀疏和缺失的数据。一种典型的正则化张量分解方法是CP(canonical polyadic，中心多项式)模型，它的目标是寻找最佳的单隐层神经网络(neural network with one hidden layer)权重$W$。其表达式为：

$$min_{\Theta,\Phi} ||Y-\hat Y(X)||+\alpha \Omega(\Theta)+\beta \Omega(\Phi),\quad s.t.\ l_{\min }<\Theta<l_{\max },\ p_{\min }<\Phi<p_{\max}$$

其中，$\hat Y(X)$是利用张量分解得到的分解结果；$\Theta, \Phi$分别是神经网络的权重；$l_{\min},l_{\max}$分别是权重的下界和上界；$p_{\min},p_{\max}$分别是正则项$\Omega$的下界和上界；$\alpha,\beta$是正则项的参数。正则化张量分解的计算时间复杂度为$\mathcal{O}(mnrp^3+\frac{(m+n)}{p}r^2+\frac{(m+n)^2}{p^2}q^2+\frac{mn}{\rho q^2})$。

除此之外，还有一些进阶的正则化张量分解方法，比如HOOI分解，就是以局部线性嵌入的形式对数据进行降维。另外，还有一些张量分解模型采用额外的结构约束，例如带界约束。这些方法既考虑了对原始数据进行额外的约束，又兼顾了复杂度和表达力。


### 3. 模型学习张量分解

模型学习张量分解借鉴了自编码器的思想。首先，对原始数据构建一个图模型。然后，利用图模型中的潜在变量进行学习，学习得到的模型参数能够解释原始数据。

已有的模型学习张量分解方法有两种类型，一种是通过拉普拉斯变换进行学习，另一种是通过最大熵模型进行学习。前者假设数据可分解成模型中已知的节点的组合，后者假设数据的概率分布由已知的图模型来描述。

最大熵模型对数据进行建模，构造了一个具有随机性的复杂分布，令模型的似然函数最大，因此，模型学习张量分解可以推广到各种复杂的模型，如混合模型、贝叶斯网络等。

李宏毅先生等人开发的最大熵张量分解法，其目标是找到一组分解因子，使得对任意给定的范数或者距离度量，我们都能找到对应的张量分解。其表达式如下：

$$min_{U,D,V}\sum_{i=1}^{N}\sum_{j=1}^{M}|Y_{ij}-UDV^\top|+\alpha \Omega_\Theta(U)+\beta \Omega_\Theta(V)+\gamma \Omega_\Theta(D)+\epsilon (\|\Theta\|_F^2+\|\Phi\|_F^2)$$

其中，$Y\in R^{N\times M}$是原始数据矩阵；$U\in R^{N\times K}$, $V\in R^{M\times K}$, $D\in R^{(N+M)\times (N+M)}$ 是张量分解的参数；$\{\theta_i\}$ 和 $\{\phi_j\}$ 是图模型的参数；$\epsilon>0$ 是正则化项；$\Omega_\Theta(U)$ 表示对$U$施加的正则化项；$\|\cdot\|_F^2$ 为Frobenius范数。最大熵张量分解法可以看作是在最大熵模型上的简化，利用边缘概率分布和图模型参数来代替隐变量，从而提高模型的表达能力。计算时间复杂度为$\mathcal{O}(Nk^3+(N+M)k^2r^2)$。