
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在机器学习任务中，大多数情况下都属于监督学习或者半监督学习，也就是输入数据既包括标签信息，也包括样本特征信息。但是在一些特殊的场景下，比如欺诈检测、网络攻击检测等，数据既不包括标签信息，又没有可用的标签。这种情况下，如何利用数据特征，设计出一个模型，能够将欺诈行为和正常样本区分开？One-class learning (OCL)就是解决这样的问题。它可以利用无标签的数据集，通过学习判别模型来实现对异常数据的分类。

2019年Google提出的Projected Gradient Descent (PGD)算法是一个成功的OCL算法。PGD算法最大的优点就是在保证模型鲁棒性的同时，还可以避免模型过拟合。目前已经有很多基于PGD的OCL方法被提出来，包括Deep Anomaly Detection、Isolation Forest、Local Outlier Factor、Manifold Regularization等。然而，最近，来自于斯坦福大学的何强博士在其文章《One Class SVM》中提出了一个新的基于距离度量的OCL方法——Mutual Similarity Discrepancy（MSD）。相比于其他的方法，MSD的方法更加准确地刻画了样本之间的距离，并且可以有效地处理样本维度较高的情况。

3.问题描述
通常来说，OCL问题主要分为两类：分类问题和回归问题。在分类问题中，目标是在已知正常样本分布的情况下，利用给定的无标签样本，对它们进行正常样本和异常样本的区分。在回归问题中，目标是预测未知样本的某种属性值，即要对给定的无标签样本集进行属性值的预测。

Mutual Similarity Discrepancy (MSD) 方法是由何强博士在2019年提出的一种基于距离度量的OCL方法。MSD方法首先计算出样本与所有其他样本的余弦相似度矩阵$M_{ij}=\frac{f(x_i)^Tf(x_j)}{\|f(x_i)\|\|f(x_j)\|}$。然后，它构造了一个新的数据集，里面包含的是样本$x_i$到所有其他样本$x_j$之间距离$d(x_i, x_j)$的$k$近邻样本集合，其中距离函数为欧氏距离。对于每一个训练样本$x_i$，MSD方法分别计算它的距离$d(x_i, x_j)$，选取距离最小的$k$个样本作为它的$k$-NN集合，并用这$k$个样本构建一个子空间$\mathcal{U}_i = \{u\in \mathbb{R}^n | u^T f(x_i)-u^T f(y_l)>0,\forall l \in \mathcal{N}_{i, k}\}$，其中$\mathcal{N}_{i, k}$表示$x_i$的$k$近邻样本的索引号集合。该子空间内的样本的平均值可以视为$x_i$的质心，$k$-NN集合中的所有样本的平均值则可以视为$x_i$的质心的标准差。

具体来说，对于每个训练样本$x_i$，MSD方法计算得到的$k$-NN集合为$\mathcal{N}_{i,k}= \{j:d(x_i, x_j)<d(x_i',x_j') \forall j\neq i\}$，其中$x_i'$为第$i$个训练样本所属的簇中质心，根据样本的距离远近，$d(x_i, x_j)$可以分为以下四种情况：
$$d(x_i, x_j)=\max\{d(x_i, y), d(x_j, y)\}$$
此时$x_i$和$x_j$分别属于不同的簇；
$$d(x_i, x_j)=min\{d(x_i, y):y\in\mathcal{U}_i\}$$
此时$x_i$属于簇$\mathcal{U}_i$，$x_j$不属于该簇；
$$d(x_i, x_j)=min\{d(x_i, y)\}$$
此时$x_i$和$x_j$为同一簇；
$$d(x_i, x_j)=0$$
此时$x_i$和$x_j$无任何关系。

MSD方法的损失函数为如下形式：
$$L= -\sum_{i=1}^m [w_0+w^T f(\phi_{\mathcal{N}_{i,k}}(x_i))]_{+\infty}, s.t.\ \forall i, w>0$$
其中$[\cdot]_{+\infty}$表示强对偶作用，$\phi_{\mathcal{N}_{i,k}}(x_i)$表示$x_i$在子空间$\mathcal{U}_i$下的坐标。损失函数可以看作是在满足约束条件$\forall i, w>0$的前提下，优化$\max_{w_0,w}\{-\sum_{i=1}^m [\max_{1\leq l\leq m}[w_0 + w^T f(\phi_{\mathcal{N}_{i,k}}(x_l))]]_{+\infty}-\lambda\|w\|_1\}$。$\lambda$是参数，用于控制模型的复杂度，值越小表示模型越复杂。

总之，MSD方法的特点是利用样本距离矩阵，将正常样本和异常样本划分成不同的簇，并根据簇的距离度量构造特征向量。它通过学习低纬嵌入空间上的线性判别模型，来进行异常样本的分类。

4.算法过程详解
MSD方法的算法过程如下：

1）计算样本之间的余弦相似度矩阵$M$。

计算样本间的余弦相似度矩阵可以采用SVD分解或直接通过样本向量乘积的方式获得。在本文中，我们采用后者的方法计算样本间的相似度矩阵。

2）构造一个数据集，里面包含的是样本到所有其他样本的距离$d(x_i, x_j)$，及样本的簇标签$y_i$。

MSD方法首先计算出样本$x_i$与所有其他样本的余弦相似度矩阵$M_{ij}$。然后，将每一个样本与其余所有样本构成一个二元组$(x_i, x_j)$，其中$x_j$对应着$x_i$的相似度最高的样本，记为$(x_i,x_{(i,j)})$，将其余的$(x_i, x_j)$从样本集中删除。最后，对于所有的$(x_i,x_{(i,j)})$，标注为相同簇的$x_j$，不同簇的$x_j$都视为噪声。

3）计算特征向量，构造低纬嵌入空间。

为了将样本的特征转换到低纬空间上，MSD方法利用PCA降维的方式获得低纬嵌入空间。在PCA的基础上，MSD方法可以添加一个正则化项，防止过拟合。MSD方法的最终决策边界由决策函数$g(x;\theta)$决定，其中$x$为输入，$\theta$为模型参数。

4）训练模型，估计超平面和模型复杂度。

MSD方法的损失函数为：
$$L=-\sum_{i=1}^{m}{[w_0+\sum_{j\in N_i}(w^T f(x_j))-\max_{1\leq l\leq m}(w^T f(x_l)))]^{+}}$$

为了求解模型参数$w$，MSD方法采用批量梯度下降法。

对于训练集中，标记为同一簇的样本，损失函数采用一元logistic loss。对于训练集中，标记为不同簇的样本，损失函数采用一个单位范数惩罚项。综合两个损失函数，得到全局的损失函数。

模型训练结束之后，通过阈值$\tau$将异常样本进行标记，即可完成分类。