
作者：禅与计算机程序设计艺术                    

# 1.简介
  

## 一、研究方向
> 概率图模型(Probabilistic Graphical Model, PGM)是一种用于表示和分析因果关系和概率分布的数学模型，其特点是结构化地描述复杂系统中的随机变量及其依赖性，能够高效地处理多种数据类型的数据。由于结构清晰，信息量丰富，并且易于理解和实现，PGM已成为很多领域的重要工具。在社会科学、生物信息学、计算机视觉等诸多应用中，PGM都得到了广泛应用。本课程将从概率图模型的基本理论出发，教授学生学习如何构建和分析概率图模型，掌握如何使用PGM进行人工智能、自然语言处理、模式识别、生物信息分析等方面的研究工作。
## 二、研究人员
- 课程负责人：吴国栋
- 助教老师：陈向东、张宇宽、刘腾飞
- 实验室指导老师：陈林、李明炯、王佳卿
- 本课题组成员：刘逸杰（博士后）、李达（博士后）、朱光亚、徐国峥、蒋阳、郭彦佐（博士）、李兆基（博士）。
- 在校优秀毕业论文：“基于概率图模型的网络数据预测研究”（李达、朱光亚、孙越博）、“深度学习下的预测框架及案例研究”（刘逸杰、李达、孙越博）、“基于网络制约下的微博舆情分析”（陈林、李明炯、周康熙）、“基于文本生成的网络舆情传播预测”（蒋阳、张宇宽、郭彦佐）。
# 2.概率图模型
概率图模型（Probabilistic Graphical Model, PGM）是一种用来表示和分析系统概率分布以及由此产生的各种联合分布的图模型。它是基于贝叶斯公式和马尔可夫网格（Markov random field, MRF）提出的，因此得名为概率图模型。PGM最大的优点在于它的直观性。它是用图形的方式表示系统的随机变量以及它们之间的依赖关系。通过这种方式，可以很容易地看出各个变量之间的关联关系以及依赖性，进而进行推断。另一方面，它还具备较强的建模能力，能够对复杂系统的概率分布进行建模，提供广泛的应用。比如，在自然语言处理、模式识别、生物信息分析、机器学习、网络流量预测等领域都有着广泛的应用。
## 2.1 模型表示方法
一个概率图模型通常由若干个节点和若干条边组成，每个节点代表一个随机变量，每个边代表这个随机变量之间的依赖关系或函数关系。概率图模型一般分为两种： directed graph 和 undirected graph 。当模型为 directed graph 时，称为有向图模型；当模型为 undirected graph 时，称为无向图模型。无向图模型简单地表示两个节点之间的相关性，而有向图模型除了表示相关性外，还可以表示因果性。因此，在实际使用中，往往需要结合具体的问题来选择不同的模型形式。
### 2.1.1 有向图模型
对于有向图模型，节点表示随机变量，边表示随机变量之间的条件概率分布。如图所示：


上图中，左侧节点表示观测到的随机变量，右侧节点表示未观测到的随机变量，中间节点表示隐藏的随机变量。观测到的随机变量直接对应真实值或者不可观察到的随机变量。隐藏的随机变量是由其他变量组成的函数。根据图结构，可以很容易地判断出隐藏变量和观测到的变量之间的依赖关系。

### 2.1.2 无向图模型
对于无向图模型，任意两个节点之间都存在一条有向边。如图所示：


上图中，节点表示随机变量，边表示随机变量之间的相关性。无向图模型比有向图模型更加简单，但缺乏依赖性，难以捕获复杂的依赖关系。
## 2.2 参数学习与推断
概率图模型的核心是参数学习与推断。参数学习可以从已知数据中学习到参数的最佳估计，进而进行推断。概率图模型的学习目标可以分为两类：全局学习和局部学习。全局学习要求所有变量的参数都要学习出来，即对所有的变量进行参数的最优化。局部学习则只对部分变量进行参数学习，使得学习结果具有一定的局部性质。下面我们先了解一下如何对概率图模型进行全局学习，然后再讨论局部学习。
### 2.2.1 全局学习
全局学习的方法包括朴素贝叶斯法（Naive Bayes）、隐马尔可夫模型（Hidden Markov Model，HMM）、Expectation Maximization（EM）算法等。这些方法都假设数据服从多项式分布，而且假设各变量间的相关性可以用一个共轭正定矩阵来描述。如图所示：


朴素贝叶斯法采用的是简单的贝叶斯定理，假设各特征之间相互独立。HMM采用动态规划算法来求解状态转移矩阵和混淆矩阵。EM算法利用拉普拉斯近似（Laplace approximation）来计算期望，迭代求解参数。

### 2.2.2 局部学习
局部学习是指只对部分变量进行参数学习，并保留其余变量的参数不变。局部学习的方法可以分为三种：因子分析（Factor Analysis）、线性混合模型（Linear Mixture Model）、神经网络（Neural Networks）。因子分析和线性混合模型假设变量的协方差矩阵等于某个低维矩阵乘以单位阵，因此只能学习到低阶的信息。神经网络是深度学习的基础，具有高度非线性的特性，能够捕捉到复杂的非线性依赖关系。如图所示：


因子分析可以用来进行主题模型，线性混合模型可以用来进行聚类，而神经网络可以用来进行分类、回归等任务。其中，因子分析又可以细分为主成分分析（Principal Component Analysis，PCA），潜在语义分析（Latent Semantic Analysis，LSA），因子分析的混合版本（Nonparametric Factor Analysis）可以用来发现结构和函数之间的关系。