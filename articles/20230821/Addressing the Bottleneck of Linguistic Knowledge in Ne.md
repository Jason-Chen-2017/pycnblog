
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Neural machine translation (NMT) has achieved significant progress recently with the development of advanced deep learning models and techniques. However, a major limitation is that the trained model only captures limited linguistic knowledge about languages, making it difficult to translate between unrelated languages or domains. This article addresses this problem by proposing an approach for jointly training NMT on multiple monolingual corpora, which enables translating between more than two related languages. We present a new model architecture called joint multi-lingual transformer (JM-T), which can be trained end-to-end without using any parallel data. Experiments show that JM-T outperforms state-of-the-art multilingual neural models such as mBART while being capable of handling non-English languages. In addition, we explore the possibility of jointly training NMT with other sources of supervision, including language modeling, named entity recognition, and machine translation, and demonstrate their potential contribution to improving performance even further. Finally, we discuss the limitations of our approach and propose directions for future research. 

# Abstract
Neural machine translation (NMT) has achieved significant progress recently with the development of advanced deep learning models and techniques. Despite its success, however, it suffers from a major drawback: its learned representation of source and target languages are heavily constrained due to the use of a single monolingual corpus. To address this issue, we propose a novel technique called joint multi-lingual transformer (JM-T) for jointly training NMT on multiple monolingual corpora. The key idea behind JM-T is to learn a shared vocabulary across all languages through a unified embedding layer, and then train individual transformers for each language separately based on separate monolingual corpora. These transformers share common layers but have unique parameters specific to their respective languages. By doing so, JM-T is able to handle more complex translations between unseen language pairs. We evaluate JM-T on six language pairs covering diverse languages and show that it achieves competitive results compared to state-of-the-art approaches while significantly reducing the number of parameters needed to achieve comparable performance. Furthermore, we show that joint training with additional sources of supervision can improve the overall accuracy of NMT even further, especially when combined with pretraining techniques like language modeling. Overall, these experiments suggest that NMT may benefit greatly from exploring alternative paradigms for training, which could include incorporating information from multiple sources of supervision or introducing specialized models for different types of tasks, such as language modeling, named entity recognition, and machine translation.

Keywords: neural machine translation, multi-linguality, multilingual training, joint training, pretraining

Introduction and Related Work
Machine translation (MT) refers to the task of automatically converting text from one natural language into another. It has been applied successfully in various fields ranging from spoken communication to technical documentation. MT systems typically consist of two components: a statistical translation model that assigns probability scores to all possible translations given a source sentence, and a word-alignment algorithm that determines how words in the source sentence correspond to those in the target sentence. Both components depend critically on large amounts of labeled training data, which consists of millions of parallel sentences in both the source and target languages. As the size and quality of available training data increases, MT performance continues to increase, despite the fact that computational resources have become increasingly scarce. One approach towards mitigating the limited availability of training data is to leverage monolingual data, i.e., data extracted directly from a particular language without any parallel sentences. However, training such models requires very high quality parallel data that often remains hard to obtain.

The recent advancement in neural networks coupled with advances in deep learning techniques led to the emergence of neural machine translation (NMT). Despite its impressive performance, there remain many challenges remaining: insufficient amount of labeled training data, the bottleneck of linguistic knowledge, and slow convergence speed. The latter is particularly troublesome in cases where larger models and longer input sequences are used. Additionally, there exists no easy way of leveraging parallel data to combine them with monolingual data during training.

In this work, we address these problems by proposing a methodology for jointly training neural machine translation (NMT) on multiple monolingual corpora. Our proposed approach combines the strengths of joint training, specifically the ability to exploit cross-lingual correlations among data points, with the scalability and ease-of-use benefits of single-language NMT. Specifically, we introduce a new model architecture called joint multi-lingual transformer (JM-T), which exploits features such as attention mechanisms, automatic tokenization, and multi-head self-attention to enable joint training of NMT on multiple monolingual corpora. We also experiment with several strategies for dealing with non-English languages, including rewriting rules and character-level encoding. We empirically evaluate JM-T on several language pairs and find that it performs well relative to existing approaches while requiring fewer resources and reducing model complexity. Moreover, we show that joint training with additional sources of supervision, such as language modeling, named entity recognition, and machine translation, can yield substantial improvements in translation quality. Based on these observations, we identify three main directions for future research in NMT: (i) investigating different ways of combining monolingual and parallel data, (ii) applying syntactic transformations to enhance the linguistic context and capture morphological variations, and (iii) addressing the issues caused by sparsity and noise in the training data, such as negative sampling and domain adaptation.