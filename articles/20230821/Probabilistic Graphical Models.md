
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Probabilistic graphical models (PGMs) 是一种建立在贝叶斯网络之上的概率模型，它将观测变量、隐藏变量以及它们之间的依赖关系用图结构表示出来，并对图结构上定义的分布做条件推断。因此，PGM 将复杂多变的问题转换成简单易处理的形式，可以用来解决很多实际问题。例如，假设要对某个社会网络中两个人的联系进行建模，可以用一个有向无环图来表示每个人的关系，而边的权重则代表他们之间交往的频率。通过最大化边的权重来最大化网络的整体连接性。同时，也可以利用无向图结构来表示不确定性（noise），即两个变量间可能存在因果关系但是却不能从数据中直接观察到。PGM 在数据采集、数据处理、建模、学习、预测等各个环节都有着广泛应用。如今已经成为机器学习领域里的一个重要工具。本文主要讨论PGM相关的基本概念及其应用。
# 1.1. 概念
## 1.1.1. 随机变量
首先，需要给出什么是随机变量。在数理统计中，随机变量(Random Variable)是概率论中的基本概念。顾名思义，它是某件事情的结果，其取值可以是若干个元素中的一个或多个。例如，抛一次骰子，正面朝上就是随机变量，其取值为1。抛几次骰子就会产生多个随机变量，比如第一个骰子正面朝上，第二个骰子正面朝上，第三个骰子正面朝上。每个随机变量可以赋予不同的取值，而这些取值的概率都是相同的，即按照一定比例出现。由于每个随机变量都带有随机性，所以需要研究这个随机变量背后的机制。
## 1.1.2. 概率分布
在描述随机变量时，一般会涉及到它的取值的概率分布。概率分布通常采用概率密度函数或概率质量函数的形式来表示。其中，概率密度函数描述了随机变量取值所在的连续区域的概率密度，也就是说，在某个确定的取值附近的概率值高；概率质量函数则是描述了离散的取值所具有的概率，即概率质量。在PGM中，我们一般只使用概率密度函数来表示随机变量的分布。概率密度函数的形式如下：
$$p(x)=P\{X=x\}$$
其中，$X$是一个随机变量，$x$是一个特定的取值。比如，抛一枚硬币，正面朝上的概率为$p(x)=0.5$；抛两枚硬币，正面的概率分别为$p(x_1), p(x_2)$。
## 1.1.3. 联合概率分布
对于一个有向无环图（DAG）G=(V,E)，如果G中的每个节点v都对应一个随机变量X(v)，那么我们就得到了一个具有以下形式的联合概率分布：
$$p(\{X(v)\}|\{\epsilon_{ij}\}, \theta )=\frac {p(\{X(v)\}, \epsilon_{ij}, \theta )} {\int_{\Theta }p(\{X(v)\}, \epsilon_{ij}, \theta | \{\epsilon_{ij}\})d\{\epsilon_{ij}\}} $$
其中，$\epsilon_{ij}$表示从节点i到节点j的边对应的噪声，$\theta$表示其他参数。$\Theta$是一个参数空间。$p(\{X(v)\}| \{\epsilon_{ij}\}, \theta )$表示根据当前参数的设定，计算所有变量的值。$p(\{X(v)\}, \epsilon_{ij}, \theta)$表示给定噪声的情况下，所有变量的值。
## 1.1.4. 指导分布
给定一个DAG G和一些已知的边的值，假设$X(v)$和$Y(v)$都是二元随机变量，其中$X(v)$表示从节点v指向其后继节点的边的数量，$Y(v)$表示从v出发的边的数量。如果我们希望知道$Y(v)$，我们可以使用迭代的方式来更新$X(v)$，直至收敛到一个足够好的估计。当迭代结束时，$Y(v)$的估计值将对应于$p(Y(v))$的期望值。这个过程被称作推导分布(marginalize distribution)。假设已经计算出了$p(X(v), Y(v)), X(v)$和$\epsilon_{ij}$，就可以计算出：
$$p(Y(v)| \{X(v)\}_{v \in V}, \epsilon_{ij}, \theta ) = \sum_{h} p(Y(v)| h, X^{*}_{v}(h), \epsilon_{ij}, \theta )p(X^{*}_{v}(h)|\{X(v)\}_{v'}, \epsilon_{ij}, \theta )$$
其中，$p(Y(v)| h, X^{*}_{v}(h), \epsilon_{ij}, \theta )$表示根据隐含变量的值$h$和已知的边情况$\epsilon_{ij}$，计算$Y(v)$的概率分布。$p(X^{*}_{v}(h)|\{X(v)\}_{v'}, \epsilon_{ij}, \theta )$表示$X^{*}(v)$的条件分布，依赖于已有的变量$X(v')$。注意，这里省略了子集索引$v'$。通过求和，我们消除了隐含变量的所有约束，从而获得了$Y(v)$的精确分布。
## 1.1.5. 边缘概率分布
给定一个DAG G和已知的边缘概率分布$p(Y|D)$，如果我们想知道某个节点到其父节点的边的数量，或者从某个节点出发的边的数量，就可以利用条件独立性来求解。如果$Z$和$X$都是变量，且$Z$独立于其他变量，且$Z$仅依赖于某些节点，且$X$仅依赖于某些节点，则称$X$由$Z$生成，记作$Z \perp X | \{ Z^{\prime}=z^{\prime}\}$. 在这种情况下，我们可以使用已知边缘概率分布计算$X$的边缘概率分布。首先，计算子图G'，仅保留依赖于$Z$的节点和它们之间的边，然后计算G'的联合概率分布：
$$p(Z, \{X(u)\}_{u \in V'} |\{\epsilon_{ij}\}, \theta )=\frac {p(Z, \{X(u)\}_{u \in V'}, \{X(v)\}_{v \in V^{\prime}}, \epsilon_{ij}, \theta )} {\int_{\Theta }p(Z, \{X(u)\}_{u \in V'}, \{X(v)\}_{v \in V^{\prime}}, \epsilon_{ij}, \theta | \{\epsilon_{ij}\})d\{\epsilon_{ij}\}}$$
然后，利用马尔科夫性质计算边缘分布：
$$p(X(u)|Z,\{\epsilon_{ij}\},\theta)=\sum_{z^{\prime}}\frac {p(Z=z^{\prime}, X(u), \epsilon_{ij}, \theta)} {\sum_{\tilde{z^{\prime}}} p(Z=\tilde{z^{\prime}}, \{X(u)\}_{u \in V'}, \epsilon_{ij}, \theta )} \cdot p(Z=\tilde{z^{\prime}}|Z=z^{\prime},\{X(v)\}_{v \in V^{\prime}},\epsilon_{ij},\theta)$$
最后，利用对称性计算整个图的边缘分布：
$$p(X(u)=k_u|D,\epsilon_{ij},\theta)=\frac {\sum_{z^{\prime}} p(Z=z^{\prime},X(u)=k_u,\epsilon_{ij},\theta)}\left[\sum_{z^{\prime\prime}} p(Z=z^{\prime\prime},X(u)=k_u,\epsilon_{ij},\theta)\right]$$