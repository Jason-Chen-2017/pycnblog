
作者：禅与计算机程序设计艺术                    

# 1.简介
         
2019年10月，一篇名为《Understanding Visual Representations by Inverting them》的论文发布了，这篇文章的作者提出了一种新的视觉表示学习方法——逆向视觉表示学习(Inverse Visual Representation Learning)，并证明这种方法可以有效地提取高阶特征，并在许多视觉任务上表现优异。近几年，基于图神经网络(Graph Neural Networks)的视觉任务已经取得了很好的成果，如图像分类、目标检测、分割、以及视频理解等。而这些任务都离不开对全局信息的建模能力，因此，如何将全局信息融入到局部特征中，对于提升视觉任务的性能至关重要。本文研究了如何将局部特征的特征图映射到全局表示空间，并在图神经网络中进一步使用全局表示进行推理的模型——GNN-FiLM。
         本文主要贡献如下：
         (1) 提出了一个全新的基于图神经网络的视觉推理模型——GNN-FiLM，该模型能够对视觉数据进行学习，从而获得全局可通用的、高度泛化的视觉表示。
         (2) 在多个视觉任务上评估了GNN-FiLM模型，证实其在性能上的优势。
         (3) 提供了一个有效的可视化工具箱，通过动画演示了GNN-FiLM在各种视觉任务中的工作机制。
         # 2.相关工作
         目前，大量的工作都集中于三种视觉表示学习方法：CNN、RNN、GNN。CNN通过卷积神经网络(Convolutional Neural Network)提取局部特征；RNN采用递归神经网络(Recurrent Neural Network)处理时序信息，捕获动态特性；GNN利用图结构来建模全局关系，捕获不同对象之间的相互作用。而本文的核心工作是设计一个全新的基于图神经网络的视觉推理模型——GNN-FiLM。

         相关工作：

         （1）CNN-RNN 模型

         R-CNN [31]是第一个完全卷积的区域卷积网络（fully convolutional region-based convolutional neural network），其目的是对输入图像进行目标检测和定位。它首先用一个预训练的AlexNet网络提取出固定大小的特征图，然后再通过循环神经网络对每一个网格点进行空间自适应的特征编码。R-CNN后来被VGG [32]取代，并得到广泛应用。然而，由于这个模型的复杂性和计算成本，其仅限于小目标检测领域。

         SSD [7]是受R-CNN启发而设计的一个基于深度可分离卷积的单阶段物体检测器（single-stage object detector）。它直接输出检测框的位置及类别置信度，并且不需要像R-CNN那样独立地预测每个类别的回归参数。SSD具有很高的准确率，是最新且最具代表性的目标检测模型之一。

         DGCNN [29]提出了一种用于3D点云数据的图神经网络，将每个点划分成不同的空间邻域，然后再对每个邻域内的点执行卷积操作。DGCNN引入了对局部点云特征的空间感知和全局上下文信息，有效地学习到全局的语义表示。

         （2）FCN [4]通过全连接神经网络对输入图像进行语义分割，得到每个像素所属的语义类别。但是，FCN只能同时对一张图像进行语义分割，不能充分利用全局信息。为了解决这一问题，DenseNet [15]使用一个双向连接的网络来处理图像的全局上下文信息，通过特征重用促进不同层之间的信息共享，因此能够提取到丰富的全局特征。 DenseNet-FCN [35]结合了DenseNet的全局上下文特征和FCN的局部特征，实现了端到端的语义分割。

         FCRN [12]是第一篇使用深度学习对固体渲染进行建模并进行实时反映的文章。它使用卷积神经网络生成前景概率分布，然后通过反向传播更新渲染参数，最大化前景概率分布与实际渲染结果之间的一致性。

          （3）Graph Attention Netowrks [19]和Transformer [34]都是用于文本序列建模的模型。他们使用图结构来编码文本并关注不同子句之间的关系，从而可以更好地捕获长序列的信息。Transformer则把注意力机制应用到图像的特征抽取过程中，从而学习到全局的视觉信息。

           