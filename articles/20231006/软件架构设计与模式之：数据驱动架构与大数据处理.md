
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


作为一名技术专家、程序员、软件系统架构师，我相信每一个技术人员都有自己擅长的领域。例如，你是一个Java工程师，那么你擅长Java语言编程、Spring框架应用和架构设计；如果你是一个架构师，你可能擅长云计算、分布式系统、高可用性架构等等。因此，作为一名技术专家，我们首先需要选择一个擅长领域，进而在这个领域上深入学习、实践，并通过知识积累提升自己的职级。

以大数据处理为例，从企业运维角度出发，大数据解决了海量的数据存储和处理难题。如何有效地把海量的数据进行高效地分析、挖掘和存储？如何实现对数据的快速查询、检索？这是一个十分重要的技能。所以，对于大数据处理技术的研究与实践可以说是技能的基石。

实际上，从计算机科学的层面来说，“数据”这一概念也是一种抽象的概念，它既包含高速存储的数据（如磁盘），也包含离线的数据（如数据库）。同时，数据本身还存在多种形式，如文本、图像、音频、视频、网页等。数据是各种信息的源头，也是各种应用服务和工具的基础。正因为如此，“数据”这一概念具有高度的抽象性，它使得各种不同类型的数据处理、分析、挖掘、存储都具有共同的需求。因此，理解“数据”这一概念以及它们之间的关系是非常关键的。

如何才能充分利用大数据的价值，就成为每个技术人员必不可少的技能。只有了解大数据背后的基本原理及其应用场景，才能够更好地理解大数据技术的优势所在，进而充分发挥它的作用。只有正确认识到大数据是基于海量的数据采集、处理和存储，并且随着时间推移不断产生新的信息，才会有动力去研究、开发和应用大数据相关技术，创造出更多的商业价值。

本文的主要目的就是讨论大数据技术在软件系统架构中的应用。由于软件系统架构设计往往是构建大型复杂系统的基石，理解大数据技术在软件系统架构中扮演的角色至关重要。如何利用大数据技术促进业务发展，如何提升数据处理能力和资源利用率，这些都是大数据架构师应该具备的基本功底。

# 2.核心概念与联系
## 数据（Data）
数据（Data）是计算机科学的一个术语，泛指与计算密切相关的信息。数据可以是数字或符号等任何形式，既可以静态的物质形态（如文字、图片、音频、视频），也可以动态的过程（如行为数据、环境数据）。一般情况下，数据具有独立于上下文、独立于处理过程和硬件设备的特性，可以由多种方式生产出来。

数据具有极大的多样性和丰富性，存在着各种形式、属性和结构。许多计算机科学、统计学和社会科学研究者关注的问题都与数据相关。比如，生物医学、金融、经济、生态、人口统计、航空交通、地震危机等领域都涉及大量的数据处理、存储和分析。而大数据则是对海量数据的一次次深度挖掘、提炼、转换、汇总，最终呈现出大数据海洋中的数据“珠宝”。

## 大数据
大数据是由海量的数据组成的一种新型信息时代。它是指过去几十年间收集、积累、存储、传输的海量数据，包括各种类型的数据，如结构化数据、非结构化数据、半结构化数据、网络日志、图像、视频等。由于数据的规模大、范围广、维度多，很难用传统的数据处理方法对其进行有效、可靠地分析、挖掘和处理。

目前，大数据技术已经成为各行各业的一项重要技能，包括金融、电子商务、互联网、制造、制药、交通、能源、航空、农业、政务、医疗等领域。

## 软件架构
软件架构是一个既定且稳定的系统的结构、功能、规则、约束和边界，用来描述软件的静态结构、静态特性、动态特性以及它们之间在某一特定时刻的相互作用关系。软件架构是计算机科学、软件工程、管理学和经济学的重要分支，它不仅体现了软件产品的静态和动态特性，而且可以预测软件产品的未来发展方向和演变趋势。

## 数据驱动架构
数据驱动架构（Data-Driven Architecture，简称DDa）是一种基于数据的软件架构风格，其核心特征是通过数据采集、处理和分析来支持业务决策，有效地提升业务价值。

这种架构风格适合处理海量数据，因为处理海量数据通常需要复杂、重复性的工作，而采用数据驱动架构可以自动化地完成这些任务，缩短开发周期，并降低维护成本。

数据驱动架构的核心要素如下：

1. 数据采集：包括数据生成、获取、导入、存储等环节，负责将原始数据经过清洗、转换、规范化等方式整理成适用于分析的格式。
2. 数据处理：包括数据清洗、转换、规范化、过滤、合并、归纳等环节，负责将采集到的数据按照分析要求进行过滤、转换、加工，从而获得数据的分析价值。
3. 数据分析：包括数据挖掘、机器学习、推荐系统等，负责对处理后的数据进行分析，找出其中的模式、关联和信息。
4. 数据展示：包括数据可视化、业务决策支持等，通过各种数据展现形式将分析结果呈现给用户，支持业务决策。

## Hadoop MapReduce
Hadoop MapReduce是Apache Hadoop项目中最常用的分布式计算框架。它提供了一种简单且高效的方式来处理大规模数据集。MapReduce是一种并行化的编程模型，其中包含两个阶段：Map阶段和Reduce阶段。

Map阶段接收输入数据，按照一定的计算逻辑转换为键值对形式，然后发送给Reduce阶段。Reduce阶段对Map输出的键值对进行汇总，得到最终的结果。

在Hadoop框架中，MapReduce被广泛地应用在各种数据分析领域，如日志文件分析、网络流量监控、Web页面索引、广告点击跟踪、文本搜索、图谱计算、主题模型等。

## Apache Spark
Apache Spark是一个开源的大数据处理框架，其设计目标是快速执行迭代式数据处理作业，能够应对即席查询、交互式查询、批量分析等各种应用场景。

Spark的核心特点有：

1. 高性能：Spark利用Scala、Java、Python等多门语言提供高性能API。
2. 统一计算模型：Spark支持SQL、DataFrames和DataSet等统一的计算模型，支持批处理、实时处理、流处理。
3. 可伸缩性：Spark可以方便地扩展到大数据集群中，适合处理任意规模的数据。
4. 高容错性：Spark具备完善的错误恢复机制，在失败时自动重新调度任务。

## 海量数据处理的挑战
海量数据处理面临的主要挑战如下：

1. 存储与计算：如何高效地存储海量数据，保证数据的准确性，并且处理实时更新的数据。
2. 查询分析：如何快速、准确地检索、分析海量数据。
3. 网络通信：如何在异构系统之间高效地共享数据。
4. 分布式计算：如何有效地运行海量数据处理应用程序。

# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
## 概念介绍
### 数据模型
数据模型是指数据的表示、组织、处理方式以及控制。数据模型分为实体-联系（Entity-Relationship, ER）模型、对象-关系（Object-Relational, OR）模型、半结构化模型、文档模型和星型模型等。

ER模型是一种抽象的数据模型，它将实体、属性和关系作为基本要素，实体用来描述客观事物，属性用来描述实体的特征，关系用来描述实体之间的联系。

OR模型是一种逻辑模型，它采用关系表来表示实体、关系和属性。关系表是由二维数组表示的表格，每一行代表一个实体，每一列代表一个属性，每一对实体之间的关系对应于表格内的一条记录。

半结构化模型是指数据模型中包含不确定性和多样性，不确定性指的是某个字段的值取值范围不固定；多样性指的是同一个表里面的记录有不同的结构。

文档模型是指以文档的形式存储数据，它将多个实体按一定顺序组织起来，每个实体表示为一个文档。文档模型的优点是查询容易，缺点是不利于复杂查询。

星型模型是指数据模型的一种特殊形式，它将实体间的联系建立在一个中心实体之上，这样可以直接反映出多个实体间的复杂关系。

## 数据仓库

数据仓库是基于企业数据资产，集合各种存储渠道和技术，为决策支持部门提供不同角度的数据支持。数据仓库的四个特征是：面向主题，集成性，时效性，一致性。

### 数据集成
数据集成是将多个来源的数据存入同一个数据仓库，从而支持业务智能分析和数据报告，提高数据分析效率。

数据集成的方法有三种：星型模型、维度建模和事实表。

#### 星型模型
星型模型的特点是将所有实体间的联系集中在一个中心实体之上，通过一个中心表将所有相关数据连接在一起。

#### 维度建模
维度建模是根据业务的逻辑，将实体划分成多个维度，每个维度单独作为一个表进行保存。

#### 事实表
事实表是指根据实体之间所拥有的关系和联系，按照事实表的格式，将所有相关数据存入同一个表。

### 时效性
数据仓库的时效性是指数据发生变化之后，如何及时更新数据仓库中的数据，确保数据分析的实时性。数据仓库中通常会存储静态数据和流动数据，静态数据是不经常发生变化的数据，例如产品目录、客户信息等，流动数据是经常发生变化的数据，例如订单、销售额等。

对于静态数据，可以不设置时效性策略，只需要保存最近的数据版本即可。对于流动数据，可以通过以下方式实现时效性策略：

1. 插入最新数据：每次新数据进入数据仓库时，都将其插入到对应的表中。
2. 更新历史数据：当数据发生变化时，先记录更改之前的历史数据，再插入最新的数据。
3. 生成快照数据：可以将某些数据按固定周期生成快照，保存整个数据集的状态。
4. 定义窗口函数：允许对数据进行滚动聚合，将某段时间内的数据聚合到窗口中，如日、周、月、季度、年。

### 一致性
数据仓库的一致性是指数据修改之后是否立即更新到数据仓库中，或者在什么条件下才更新到数据仓库中。

数据仓库的一致性有两种级别：强一致性和弱一致性。

1. 强一致性：当数据发生更新后，立即写入数据仓库中，保证数据实时性和完整性。
2. 弱一致性：当数据发生更新后，不会立即写入数据仓库中，而是在一段时间后，或数据量达到一定阈值时，将数据更新到数据仓库中，但并不是完全的实时和完整。

### ETL(Extract Transform Load)流程
ETL（extract transform load）流程是指将数据从源端提取、清洗、转换，并加载到目标系统中。ETL流程能够有效地对数据进行筛选、清洗、规范化，以及异常值的处理，还可以将数据进行合并、分解、归纳、汇总等操作。

ETL流程分为三个步骤：抽取、转换、加载。

#### 抽取数据
抽取数据包括从各种来源（如Oracle、MySQL、MongoDB、CSV等）中提取数据，抽取到数据仓库的中间区域。

#### 清洗数据
清洗数据是指将数据进行有效的过滤、清洗、验证等操作，以满足数据仓库的需求。

#### 转换数据
转换数据是指将清洗好的数据转换为分析需要的格式，例如将日期格式转换为整形的时间戳。

#### 加载数据
加载数据是指将数据从数据仓库的中间区域加载到目标系统中，通常目标系统为关系数据库或数据湖。

## 数据挖掘

数据挖掘（英语：data mining）是一种基于数据库、网络和统计学的科学技术，用于发现隐藏在数据中的模式、关联和意义。数据挖掘可用于分析大型复杂数据集、提高业务决策和优化营销活动。数据挖掘方法有：探查性数据分析、预测性数据分析、关联分析、分类与聚类、聚类分析、协同过滤、序列分析、因子分析、关联规则、异常检测、文本挖掘、神经网络和支持向量机。

### 探查性数据分析
探查性数据分析（EDA）是指通过对数据进行探索、分析、检验等过程，以发现数据的规律和趋势。

EDA可以用于预览数据，识别数据结构、字段、数据类型、缺失值、唯一标识符、外键等，并可以按照指定的模式探索数据。探查性数据分析可以帮助理解数据的特点，从而更好的理解业务、数据处理的需求，以及改进数据质量。

### 预测性数据分析
预测性数据分析（PDA）是指对未来的数据进行预测和评估，以制定相应的策略、方案。

PDA通过对历史数据进行分析，发现其中的趋势，并预测未来可能出现的情况。PDA的目标是通过对历史数据进行分析，建立数据模型，预测未来的趋势，并基于此做出相应的策略调整。

### 关联分析
关联分析（association analysis）是指通过分析数据之间的联系，找出它们的关联关系。

关联分析可以帮助商业团队发现客户群中的共同偏好、分析市场趋势，以及发现潜在的商业机会。关联分析基于规则、统计和图表来实现，包括强关联分析、传递关联分析、频繁项集分析、卡方检验等。

### 分类与聚类
分类与聚类（classification and clustering）是指按照一定的标准对数据进行划分，使数据按一定规律聚集到一起。

分类与聚类算法有基于距离的、基于密度的、基于模型的、基于概率的等。分类与聚类的目的是为了提高数据分析的效率和可读性，帮助商业团队发现数据的特征，并进行有效的商业决策。

### 聚类分析
聚类分析（clustering analysis）是指将数据集分成几个簇或类，数据对象的分布区别于其类别的特征。

聚类分析的步骤有：特征选择、划分簇、评估结果、模型调整。聚类分析是一种无监督的机器学习算法，通过特征向量的空间分布对数据集进行聚类，将相似的数据聚在一起。

### 协同过滤
协同过滤（Collaborative filtering）是指推荐引擎将用户喜欢的商品推荐给其他类似用户。

协同过滤算法包括基于用户的推荐算法、基于物品的推荐算法、基于用户兴趣的推荐算法。协同过滤算法可以用于推荐系统、商品推荐、股票投资建议、网上购物、社交网络分析、游戏推荐等方面。

### 序列分析
序列分析（Sequence Analysis）是指对一系列数据进行顺序分析，识别其中的模式，并发现其中的趋势。

序列分析方法包括时序分析、动态时间 warping、趋势分析、聚类分析、回归分析、马尔可夫链蒙特卡罗方法等。时序分析是指根据时间先后顺序对数据进行分析，找到数据中的模式和趋势。

### 因子分析
因子分析（Factor Analysis）是一种多维数据分析技术，通过分析自变量和因变量之间的关系，提取出更多的有意义的信息。

因子分析的步骤有：数据准备、方差分析、截面分析、降维、主成分分析、加载ings法、因子旋转法等。因子分析是一种无监督的机器学习方法，通过降维的方法，发现数据的低维子结构。

### 关联规则
关联规则（Association Rule）是指在数据集中寻找有关联关系的物品对。

关联规则的基本假设是如果一个事务出现在某种上下文中，那么另一个事务也出现在这种上下文中。因此，关联规则挖掘的目的是寻找频繁的模式，发现隐藏在数据中的关联规则。关联规则方法有挖掘算法、搜索启发式算法、强连贯性算法。

### 异常检测
异常检测（outlier detection）是指根据数据集的统计分布、统计规律，识别异常值，并给予警告或处置。

异常检测方法包括平滑插值法、基于密度的异常检测、基于回归的异常检测等。平滑插值法是指根据当前数据集，估计出接近该数据的噪声点，并根据该噪声点进行插值。基于密度的异常检测是指根据数据集的密度分布，判断数据是否属于正常分布。基于回归的异常检测是指根据一阶或二阶的线性回归模型，检测数据集中的异常点。

### 文本挖掘
文本挖掘（Text Mining）是指从大量的文本数据中发现隐藏的模式、结构和意义。

文本挖掘方法有词性标注、词干提取、主题模型、文档聚类、信息抽取、情感分析、名人异性关系挖掘、知识图谱等。文本挖掘是一门面向人工智能、计算机科学和语言学的交叉学科，涉及统计学、信息检索、语义理解、图论、机器学习、计算机系统、语言学等多领域。

### 神经网络
神经网络（neural network）是一种基于模拟大脑神经元网络的计算模型，能够模拟人类大脑的神经网络活动。

神经网络方法包括神经网络学习、径向基函数网络、Hopfield网络等。神经网络是一种非监督的机器学习方法，通过构建模仿人脑神经网络的计算模型，能够识别复杂的数据集，并发现数据的模式。

### 支持向量机
支持向量机（support vector machine，SVM）是一种二类分类器，是支持向量机模型的基础。

支持向量机方法包括核函数、软间隔最大化、求解凸二次规划问题等。支持向量机模型能够处理线性可分的数据集，并可以有效地进行分类、回归和预测。

## 机器学习

机器学习（英语：machine learning）是指让计算机学习并适应数据的能力。机器学习由人工智能和计算智能两部分组成。人工智能包括：搜索、决策、推理、学习、归纳、归档、交流、感知、表达、理解等。计算智能包括：计算、推理、数学、统计、优化、数据挖掘、深度学习、机器视觉、自然语言处理、图像处理、语音识别、推荐系统、知识图谱等。

机器学习的三个重要特征：泛化能力、概率模型和学习算法。

### 泛化能力
泛化能力（generalization ability）是指机器学习模型的能力，在新的数据上或新环境下，依旧可以较好的完成预测和分类任务。

机器学习模型的泛化能力依赖于训练数据集的大小、训练算法的选择、训练参数的设置、正则化的使用等。

### 概率模型
概率模型（probabilistic model）是指根据已知事件发生的可能性来描述某一现象的发生。

概率模型分为判别模型和生成模型。判别模型假设数据是符合某一特定的分布，例如二项分布模型、多项分布模型、伯努利分布模型等；而生成模型则假设数据服从某种未知分布，例如高斯混合模型、隐马尔可夫模型等。

### 学习算法
学习算法（learning algorithm）是指使用已知数据集，基于学习目标和假设，通过一步步修正模型参数，从而学习数据的特征、结构、模式，并预测未知数据。

学习算法的种类有：监督学习算法、非监督学习算法、半监督学习算法、强化学习算法、集成学习算法。监督学习算法包括线性回归、朴素贝叶斯、决策树、随机森林、支持向量机、K均值、K近邻、Adaboost等；非监督学习算法包括聚类、密度聚类、关联分析、协同过滤、最大熵模型、层次聚类等；半监督学习算法包括EM算法、IBCM算法、HMM算法、Gibbs抽样、度量学习、LLE算法等；强化学习算法包括动态规划、蒙特卡洛树搜索、Q-learning算法等；集成学习算法包括Bagging、Boosting、Stacking、多样性增强、惩罚项等。

# 4.具体代码实例和详细解释说明
## 数据存储方案
假设我们有一个商城网站，需要处理用户上传的文件，有以下需求：

1. 用户上传的文件最多不能超过10GB。
2. 文件应该按照时间戳进行分文件夹存储。
3. 当文件超过100MB时，需要分块进行上传。
4. 客户端应该及时收到文件上传进度和结果通知。
5. 服务端应该尽量减少磁盘IO，提高上传速度。

假设使用的文件存储方案是HDFS，它提供了高容错性、高可用性、海量数据存储的能力。HDFS可以配置副本数量，通过副本实现数据的容灾。

下面是解决以上需求的代码实现：

### 文件存储客户端
文件上传客户端可以使用多线程异步上传的方式，可以保证客户端实时上传进度，以及上传速率的提升。

```java
public class FileUploadClient {
    private static final int MAX_BLOCK_SIZE = 10 * 1024 * 1024; // 10MB

    public void uploadFile(String localPath, String remotePath) throws Exception{
        long startTime = System.currentTimeMillis();

        Path srcPath = new Path(localPath);
        Configuration conf = new Configuration();
        FileSystem fs = FileSystem.get(conf);

        List<Block> blocks = generateBlocks(srcPath, MAX_BLOCK_SIZE);
        
        for (int i = 0; i < blocks.size(); i++) {
            Block block = blocks.get(i);

            byte[] buffer = readFileBytes(block.getPath());
            
            InputStream inputStream = new ByteArrayInputStream(buffer);
            FSDataOutputStream outputStream = fs.create(new Path(remotePath + "/" + block.getBlockId()));

            IOUtils.copyBytes(inputStream, outputStream, conf, false);
            outputStream.close();
        }

        long endTime = System.currentTimeMillis();

        System.out.println("Time elapsed: " + (endTime - startTime));
    }

    private List<Block> generateBlocks(Path path, int maxBlockSize) throws IOException {
        List<Block> result = Lists.newArrayList();

        long fileSize = Files.size(path);
        int numBlocks = Math.max((int)Math.ceil((double)fileSize / (double)maxBlockSize), 1);

        RandomAccessFile file = null;
        try {
            file = new RandomAccessFile(path.toString(), "r");

            for (int i = 0; i < numBlocks; i++) {
                long offset = randomOffset(fileSize, maxBlockSize);
                long length = Math.min(randomLength(fileSize - offset, maxBlockSize), maxBlockSize);

                if ((offset + length) > fileSize) {
                    break;
                }
                
                String blockId = UUID.randomUUID().toString();

                Block block = new Block(blockId, filePath(path), offset, length);
                result.add(block);
            }
        } finally {
            if (file!= null) {
                file.close();
            }
        }

        return result;
    }

    private long randomOffset(long fileSize, int blockSize) {
        return (long)(Math.random() * (fileSize - blockSize + 1));
    }

    private long randomLength(long remainingFileSize, int blockSize) {
        double p = remainingFileSize / blockSize;
        double q = Math.pow(Math.E, -p);
        return (long)(q * blockSize);
    }
    
    private String filePath(Path path) {
        URI uri = path.toUri();
        String authority = uri.getRawAuthority();
        String pathStr = uri.getPath();
        
        StringBuilder builder = new StringBuilder();
        if (!Strings.isNullOrEmpty(authority)) {
            builder.append("//").append(authority).append("/");
        }
        
        if (!Strings.isNullOrEmpty(pathStr)) {
            builder.append("/").append(pathStr);
        }

        return builder.toString();
    }

    private byte[] readFileBytes(Path path) throws IOException {
        return Files.readAllBytes(path);
    }
}
```

### 文件存储服务端
文件上传服务端主要包括文件的存储、上传确认、块拆分和重组等模块。

```scala
class HDFSFileManager extends FileManager with Logging {
  override def saveFile(filePath: String): Boolean = {
    val src = new Path(filePath)
    val dest = getHdfsPathWithTimeStampFolder(src.getName())

    var fs:FileSystem = null
    try {
      val conf = new Configuration()
      fs = FileSystem.get(dest.toUri(), conf)

      if (!fs.exists(dest)) {
        fs.mkdirs(dest)
      }

      copyFileToHDFS(filePath, dest, fs)

      true
    } catch {
      case e:Exception =>
        logError(s"Failed to store $filePath", e)
        false
    } finally {
      if (fs!= null) {
        try {
          fs.close()
        } catch {
          case _:Throwable =>
        }
      }
    }
  }

  private def getHdfsPathWithTimeStampFolder(fileName: String) = {
    import java.text.SimpleDateFormat
    import java.util.{Calendar, TimeZone}

    val timeStamp = Calendar.getInstance(TimeZone.getTimeZone("UTC"))
    timeStamp.setTimeInMillis(System.currentTimeMillis())
    val dateFormat = new SimpleDateFormat("_yyyyMMddHHmmss")
    val timeStampFormat = dateFormat.format(timeStamp.getTime())

    new Path(ConfigFactory.load().getString("hdfs.url"), s"/files/user/$timeStampFormat/" + fileName)
  }
  
  private def copyFileToHDFS(filePath: String, hdfsDest: Path, fs: FileSystem) = {
    val bufferSize = ConfigFactory.load().getInt("hdfs.buffer.size")
    val in = new BufferedInputStream(new FileInputStream(filePath), bufferSize)
    val out = fs.create(new Path(hdfsDest, new Path(filePath).getName()), false, bufferSize, fs.getDefaultReplication(hdfsDest), bufferSize)
    
    try {
      IOUtils.copyBytes(in, out, bufferSize, false)
    } finally {
      try {
        out.close()
      } catch {
        case _: Throwable => 
      }
      
      try {
        in.close()
      } catch {
        case _: Throwable => 
      }
    }
  }
}
```

### 文件上传进度监控
客户端需要及时收到文件上传进度，方便用户实时查看上传情况。这里可以使用心跳机制，定期发送上传进度消息到服务端。

```java
public interface FileUploaderListener {
    void onProgressUpdate(int progress);
    void onUploadCompleted(boolean success);
}

public abstract class FileUploader implements Runnable {
    protected String mLocalPath;
    protected String mRemotePath;
    protected volatile boolean mRunning = true;
    protected FileUploaderListener mListener;

    public FileUploader(String localPath, String remotePath, FileUploaderListener listener) {
        this.mLocalPath = localPath;
        this.mRemotePath = remotePath;
        this.mListener = listener;
    }

    @Override
    public final void run() {
        while (mRunning &&!Thread.currentThread().isInterrupted()) {
            try {
                Thread.sleep(1000);

                int progress = getCurrentProgress();
                if (progress >= 100 || isCancelled()) {
                    notifyCompletion(true);
                    break;
                } else {
                    mListener.onProgressUpdate(progress);
                }
            } catch {
                case _: InterruptedException =>
                  Thread.currentThread().interrupt();
                  break;
            }
        }
    }

    protected abstract int getCurrentProgress();

    protected abstract boolean isCancelled();

    protected synchronized void notifyCompletion(boolean success) {
        if (mListener!= null) {
            mListener.onUploadCompleted(success);
        }
        mListener = null;
    }

    public final void cancel() {
        mRunning = false;
    }
}
```