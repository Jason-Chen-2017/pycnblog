
作者：禅与计算机程序设计艺术                    

# 1.简介
  

张量（tensor）是一个很广义的概念，它指代了一个向量、矩阵或多重数组中的元素的集合。一般来说，张量可以理解成具有张量积结构的数组，即把一个n维向量拓展到n+1维空间，将得到一个n+1维数组。常见的张量包括如下几种类型：

1. 标量（scalar）张量：一个元素的集合。如矩阵的主元就是标量张量。
2. 矢量（vector）张量：一组元素构成的线性方程组的解的集合。如矩阵的列向量就是矢量张量。
3. 矩形张量（matrix）：二维张量，在两个坐标轴上取值的多项式函数组成的矩阵。
4. 三重张量（three-dimensional tensor）：在三个坐标轴上取值的多项式函数组成的数组。

在现实生活中，很多数据都具有多维的结构，例如时间序列，图像，语音信号等。这些数据往往具有不同的复杂性，包括相关性和非相关性，不同的规模，不同维度等。所以，如何有效地从这些高维数据中发现有用信息并降维处理是数据分析的关键。张量分解就是一种降维的方法。
张量分解最早由斯坦福大学教授莫纳托克说出，他认为矩阵可以通过分解成几个相互正交的子矩阵来描述整个矩阵。他提出了一种新型的张量分解模型，称之为奇异值分解。奇异值分解可以分解任意阶张量，且其分解形式简单直观。而且，奇异值分解在很多情况下是唯一的或者最佳的。因此，张量分解成为研究张量的一种有效工具。

# 2.基本概念及术语
## 2.1 张量
张量是由元素组成的一个n维数组，张量可以看作是一个向量、矩阵或多重数组中的元素的集合。根据其元素的个数，张量又分为以下三类：

1. 一阶张量（scalar tensor）：一个数字，如2；
2. 二阶张量（vector tensor）：一组数字构成的向量，如(1,2,3)；
3. 三阶张ivalues of tensors with three dimensions are typically vectors or matrices of rank higher than two and can be considered as a tensor in their own right. Examples include the covariance matrix of data, which is a third order tensor; and a network weight matrix, which is also a fourth order tensor (i.e., it has four indices). 

张量的每个元素都可以表示为一个符号和相应的索引组成的表达式，即a_ijk = f(i,j,k)。其中，f()代表某个具体的函数，而i, j, k则对应于三个坐标轴上的位置。因此，张量的每个元素都由三个坐标轴上的位置确定。在二维的张量中，一个元素用两个坐标轴的下标表示，其余两个坐标轴的下标设定为0。

举个例子，下图展示了一个三阶张量A=(a_{ijk})，其每一个元素都可以用三个坐标轴上的位置（i，j，k）来表示。其中，i、j、k的范围分别是[1,3]、[1,3]、[1,3]。我们还可以用一个下标形式表示A的第(i,j,k)个元素，如A_{ijk}。


## 2.2 基底和基向量
假设我们有一个n维张量，其元素由n个基底组成，也就是说，该张量由n个不同的基向量的积来表示。张量的每一维都有一个对应的基底，并且相同位置的元素对应相同的基底。对于张量的每一个基底，都可以用一个基向量来表示。

## 2.3 模式和模式空间
张量A的一个模式（singular value）是一个与另一个张量B（通常为单位张量I，维度等于A的秩）的乘积，即S=AA^T（A^T表示A的转置）。如果把张量A分解为多个模式，那么这个过程就称为张量分解。

模式空间（singular space）是张量的张量空间的一个基，它由所有与张量A具有相同模式的张量构成，即U=col{u}_i（i从1到秩），其中u_i是A的第i个特征向量（eigenvector）。这意味着如果我们把张量A分解为多个特征向量的乘积，那么这个特征向量就属于模式空间。由于不同基底对应不同模式，张量的特征向量本身就可以视为特征值（eigenvalue）。

# 3.核心算法原理
张量分解的基本想法是在原始张量中找到尽可能少的维度的低秩近似。具体做法是先求张量的特征值，然后按降序排列，选出前k个特征值对应的特征向量组成的子空间U，最后用U来近似原始张量。这样做的好处是保留了原始张量中的主要信息，而且不损失任何重要的结构。

张量分解的两种基本方法是奇异值分解和谱方法。奇异值分解直接得到了特征值和特征向量，而谱方法要求用户指定某个最低的范数阈值来控制最大的维度。

## 3.1 奇异值分解（SVD）
奇异值分解（singular value decomposition，SVD）是一种矩阵分解的方法，通过将矩阵分解成两个正交矩阵的乘积而产生。张量也可以进行SVD，但需要增加额外的基底才能实现。

假设M是一个m行n列的矩阵，我们希望得到它的SVD分解AMU=USV^T，其中U是一个m行k列的正交矩阵，S是一个k行k列的对角矩阵，其对角线元素为奇异值，V是一个k列n行的正交矩阵。如果将M想象成由m个列向量构成的列空间，那么U就是代表列空间的基向量。SVD可以分解任意阶张量。

为了证明SVD的正确性，首先考虑矩形的情况，即M是一个m行n列的矩阵，我们要证明存在一个奇异值分解。我们可以使用Gram-Schmidt正交化算法来构造U，使得U满足如下条件：

U(1,:)=m_1 e_1 / || m_1 e_1||，...，U(k,:)=m_k e_k / || m_k e_k||，其中m_i是第i个奇异值，ei是标准正交基向量。U的其他列可以由如下的Gram-Schmidt算法求出：

j!=1: ui(j) = u_j(j)- \sum^{j-1}_{i=1}\left<ui(i),u_j(j)\right> u_i(j)，其中\left<,\right>是内积。

通过上述算法，我们知道，U是一个k行m的矩阵，且满足约束条件：

- U(i,:)是一个标准正交向量，即U(i,:)*U(i,:)'=[I], i=1~k
- 第i个奇异值为sqrt(\frac{\lambda_i}{\sum_{j=1}^k|\lambda_j|}), i=1~k

可以看到，利用Gram-Schmidt正交化算法，U的第i列的长度等于它的第i个奇异值，而其他列的长度等于零。因此，如果M是一个m行n列的矩阵，那么U是一个m行k的正交矩阵，而S是一个对角矩阵，其对角线上的值为奇异值，而V是一个k行n的正交矩阵。

然后，我们就可以写出矩阵的乘积形式：

MM^T=M*(UV)^T*U^TM=VSU^TU^TMSV^T=VSS^TV^T

其中，M^T表示M的转置。根据矩阵乘法结合律，我们可得：

M=UMSV

通过上面的推导，我们已经知道了SVD分解的定义以及证明了其正确性。

## 3.2 谱方法
谱方法（spectral method）也是一种矩阵分解的方法。不同的是，谱方法是通过对矩阵的所有奇异值进行排序的方式，而不是仅仅对矩阵的主对角线上的元素进行排序。这种方式不需要构造额外的基底。

谱方法的基本思路是将原始矩阵投影到一个最小维度的子空间。对于任意给定的矩阵M，其能分解为：

M=WSW^T，其中W是一个m行r列的正交矩阵，S是一个r行r列的对角矩阵，其对角线上的元素为奇异值，W^T表示W的转置。

所以，我们可以把矩阵M分解成三个矩阵的乘积，即M=PDW。其中，D是一个m行m列的对角矩阵，其对角线上的元素为M的最大奇异值。D的作用是保证每一个投影向量的长度是相同的，从而保证最后得到的矩阵的列向量长度是相同的。对于任意给定的投影向量p，都有dp>=0，所以对D进行归一化后，每一列都是正交的，即：

p^Tp=1。

然后，我们就可以得到新的投影矩阵P，以及对角阵D，从而完成矩阵的分解。

谱方法的缺陷是要求用户指定某个最低的范数阈值来控制最大的维度。因此，谱方法主要用于了解矩阵的局部信息，以及需要快速求解矩阵的一些特征值。但是，由于仅对矩阵的一部分进行分解，因此无法获得全局的信息。

# 4.具体操作步骤
1. 对原始矩阵进行SVD分解，得到奇异值分解的结果。
2. 选择秩r，并将前r个奇异值所对应的奇异向量的列组成的矩阵W选出，记为M。
3. 使用W对原始矩阵进行投影得到M。
4. 返回M和原始矩阵之间的误差。

下面我们通过实例来演示这一方法。