
作者：禅与计算机程序设计艺术                    

# 1.简介
  

集成学习(ensemble learning)是指将多个分类器或回归模型结合起来，从而提升预测能力和减少方差的一种机器学习方法。本文主要关注Bagging、Boosting两个常用的集成学习算法，并通过实践实例进行讲解。
# 2.Bagging与Boosting的概念
## 2.1 Bagging与随机森林
### （1）Bagging
Bagging(Bootstrap aggregating)，中文译名为自助法。它是一种集成学习方法，由多棵决策树组成，对每棵决策树进行训练时，都会采用不同的样本数据（bootstrap）。不同决策树之间具有不同的数据分布。最终通过平均或投票等方式得到最终的预测结果。
举个例子：假设有10个样本，用全体样本训练10棵决策树，可以得到10个决策树。在预测新样本时，可以将这10个决策树输出的概率值加权得到最终的预测值。
### （2）随机森林Random Forest
Random Forest是一个基于树的bagging集成学习方法，由多棵决策树组成，每棵决策Tree都有随机的特征选择过程。因此，不同决策树之间具有不同的特征分布。通过这种方式，可以降低决策树之间过拟合的风险。
## 2.2 Boosting
Boosting也叫串行传播，是一种迭代学习的方法，它是基于前一次错误的学习结果，去修正当前模型的一种算法。Boosting在训练过程中，不断地将模型拟合得更好，最终形成一个强大的模型。Boosting的主要思路是在每一步中，优化前一次预测的不准确性，增加新的弱分类器。
### （1）Adaboost
Adaboost是最早提出的boosting算法，它是一种迭代算法，每次选取一个最佳的弱分类器，并且通过调整每个基分类器的参数，使分类效果尽可能的提高。其基本思想就是用错误率最小作为目标函数，希望通过改变基分类器的权重，得到一个更好的集成模型。

Adaboost在基分类器的选择上，是按照一定的权值分布来选择的。首先，所有样本被赋予相同的权重；然后，对于第t轮迭代，根据样本的权值分布，为每个样本分配一个带权值的错误率，弱分类器C_t(x)被训练，它根据前面基分类器的错误率来确定自己的权重。最后，集成分类器f(x)=∑wtCi*C_i(x)是所有弱分类器的线性组合，其中wi是对应的弱分类器的权重。

### （2）Gradient boosting
Gradient boosting又称梯度提升，是Adaboost的一个改进版本。与Adaboost相比，Adaboost只考虑了分类误差，而 Gradient boosting还考虑了残差误差。具体做法是，把前面的基分类器的输出结果r(x)看作是每个样本的残差误差，然后拟合一个简单的基分类器，学习这个基分类器的系数α，通过将前面所有基分类器的输出结果乘以相应的系数和，加和得到下一个基分类器的输入，直到残差误差足够小或者达到最大迭代次数。

Gradient boosting算法主要分两步，第一步学习基分类器，第二步拟合这些基分类器。在第一步，基分类器是线性模型，例如Decision Tree和Logistic Regression；在第二步，学习到的基分类器的参数成为基模型参数，用于后续的迭代。Gradient boosting算法产生的集成模型是一系列基模型的加权和。