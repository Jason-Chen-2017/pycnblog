
作者：禅与计算机程序设计艺术                    

# 1.简介
  

决策树（decision tree）是一个基于树结构的机器学习算法，用于分类或回归问题，能够输出一系列的判断条件，并据此做出预测或决策。
在机器学习中，决策树模型是一种监督学习方法，它可以对输入数据进行分析、处理、标记，然后基于数据中的规则和经验，构造出一个模型，根据这个模型对新的输入进行预测或决策。其特点包括：易于理解、可解释性强、对异常值不敏感、处理连续变量能力较强、容易实现、并行计算、适应多种问题。
决策树算法的主要优点如下：

1. 简单：决策树模型是一种比较直观的可视化方法，它的决策过程很容易被人类所理解，并且模型本身也比较简单。同时，决策树算法一般都十分容易理解，对数据的要求也不高，因此，决策树模型能很好的解决实际问题。

2. 准确率高：决策树模型能比较好的将数据划分成不同的类别或者不同的集合，因此，它对于复杂的数据集也比较有效。另外，决策树算法本身就具有一定的局部近似性质，因此，它对少量的训练样本也能得到相当不错的结果。

3. 可处理多维数据：决策树模型能够处理多维数据，并能够在不同特征之间的选择最优分割点。因此，决策树模型能够自动地从原始数据中发现特征之间的关系，并能够对数据进行降维、压缩，提升数据分析效率。

4. 非线性可分支持: 决策树模型可以处理非线性的数据，并且能够在不同的子空间中检测到特征间的非线性关系，因此，它可以有效地处理多种类型的非线性问题。

# 2. 概念术语
## 2.1 数据集(dataset)
决策树算法所处理的数据集合，通常是一个表格型的数据结构，包含若干个输入属性和一个输出属性。
## 2.2 属性(attribute)
数据集中用来描述客体的某些性质，如人的年龄、身高、财产状况等。每个属性对应着一个或多个取值。
## 2.3 取值(value)
属性的一个特定值。
## 2.4 样本(sample)
数据集中一个特定的记录，由一组属性及其对应的值构成。
## 2.5 类(class)
样本的输出值，代表样本的分类。
## 2.6 训练集(training set)
用以训练决策树模型的数据集，也就是说，训练集中包含训练数据和标签。
## 2.7 测试集(test set)
用以测试决策树模型性能的数据集，它与训练集互斥，而且测试集中不能包含训练集的数据。
## 2.8 特征(feature)
指代数据集中输入属性的一项。
## 2.9 父节点(parent node)
某个结点的直接上级结点，也称父亲结点。
## 2.10 孩子结点(child node)
父结点下直接的后代结点，也称子女结点。
## 2.11 叶结点(leaf node)
没有孩子的结点。
## 2.12 节点(node)
父节点、孩子节点及叶节点统称为节点。
## 2.13 分支(branch)
从根结点到叶结点的一条路径。
## 2.14 路径长度(path length)
从根节点到目标节点的边数。
## 2.15 深度(depth)
从根结点到最近叶结点的距离。
## 2.16 宽度(width)
在同一层次上所有结点的个数。
## 2.17 属性值(attribute value)
在训练集中每个样本都有各自的值，这些值就是属性值的具体表示。
## 2.18 基尼系数(Gini coefficient)
度量二分类问题中，样本被错误分类的概率，它等于往集合中随机抽取两个样本，其中一半属于类1，另一半属于类2，那么抽取到的这两个样本分别被错误分类的概率就是基尼系数。
## 2.19 信息熵(Information entropy)
度量一个事件发生的不确定性程度，如果随机变量X的取值仅有两种可能，即X=x1和X=x2，那么X的信息熵定义为−p(x1)log2p(x1)-p(x2)log2p(x2)，其中p(x)是X=x的概率。