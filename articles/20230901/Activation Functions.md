
作者：禅与计算机程序设计艺术                    

# 1.简介
  

深度学习的发展离不开激活函数(Activation Function)的应用。激活函数能够在神经网络中引入非线性因素，使得网络能够更好地拟合复杂的数据，并提升其泛化能力。本文将从以下方面介绍激活函数：
- 激活函数的种类及其特点；
- 最常用的激活函数（Sigmoid、tanh、ReLU、Leaky ReLU等）的特点及作用。
# 2. 激活函数的种类及其特点
## 2.1 Sigmoid 函数
### 定义
Sigmoid函数是一个S形曲线，表达式如下：
$$\sigma(x)=\frac{1}{1+e^{-x}}=\frac{\text{e}^x}{\text{e}^x+\text{e}^{-\left|x\right|}}$$
### 性质
- 对称性：$f(-x)=1-f(x)$;
- 归一化：$\int_{-\infty}^\infty \sigma(x)\,\text{d}x=1$;
- 可导性：$\sigma^{\prime}(x)=\frac{\text{e}^{x}}{(1+\text{e}^{x})^2}=f(x)(1-f(x))$。
### 使用场景
- 在二分类问题中，输出值通常是概率值，因此Sigmoid函数可以作为输出层的激活函数；
- 在多分类问题中，由于需要对多个类别进行分类，因此多用于输出层的最后一层；
- 另外，Sigmoid函数在处理输入数据时具有自然的生长行为，因此也被用作激活函数的一个替代品，如Softmax函数。
## 2.2 Tanh 函数
### 定义
Tanh函数又叫双曲正切函数，表达式如下：
$$tanh(x)=\frac{\text{e}^x-(\text{e}^{-|x|}+1)}{\text{e}^x+\text{e}^{-|x|}-1}$$
### 性质
- 对称性：$tanh(-x)= - tanh(x)$;
- 归一化：$\int_{-\infty}^\infty tanh(x)^2\,\text{d}x=1$;
- 周期性：$tanh(x+n\pi)=tanh(x)$。
### 使用场景
- Tanh函数在较大的范围内都有着良好的特性，它能够在一定程度上抑制梯度消失现象，因此被广泛地使用于循环神经网络的输出层；
- 有助于缓解梯度爆炸或梯度消失问题，加快网络训练速度。
## 2.3 ReLU 函数
### 定义
ReLU函数又称修正线性单元，即Rectified Linear Unit (ReLu)。ReLU函数的表达式如下：
$$
\begin{aligned}
&ReLU(x)= max(0, x)\\[2ex]
&y=\operatorname*{argmax}_{i}\left\{a_i x+b_i\right\}\\[2ex]
&\text { if } a_i x+b_i>0, y=x \\[\smallskipamount]
&\text { otherwise }, y=0
\end{aligned}
$$
### 性质
- 非饱和：ReLU函数是非饱和的，即对于任何实数，如果该数字大于等于0，则输出对应的ReLU函数的值不会超过该数字；
- 单调性：ReLU函数的输出大于等于0，因此是单调递增的；
- 抗饱和性：ReLU函数对抗饱和性很强，它能够避免“死亡死亡活活”的问题，而且它的梯度非常容易求出。
### 使用场景
- ReLU函数一般作为输出层的激活函数；
- 如果输入信号的部分响应比较小，那么采用ReLU函数可以起到稀疏连接的效果，从而减少参数量和计算量，并且只对部分节点产生影响，因此可以有效防止过拟合；
- 可以用于卷积神经网络的隐藏层，因为它具有非线性性和平滑性，因此能够有效提取特征信息。
## 2.4 Leaky ReLU 函数
### 定义
Leaky ReLU函数是一种修正版的ReLU函数，由泄漏指数α控制。泄漏指数越高，相比于ReLU函数，越多的部分会变成0。表达式如下：
$$LeakyReLU(x)=\left\{
        \begin{array}{}
            ax & (\text{for } x<0) \\
            0 & (\text{otherwise }) \\
        \end{array}
    \right.$$
其中，α表示泄漏指数，α=0表示恒等映射，α→∞时，函数表现类似于ReLU函数。
### 性质
- Leaky ReLU函数对负输入的敏感度比ReLU函数要低一些；
- Leaky ReLU函数的参数更少，运算效率更高。
### 使用场景
- Leaky ReLU函数适用于需要保证某些节点的一定输出或保持局部稳定性的场景；
- 由于其结构简单、参数少、运算快速，因此在实际项目中可以作为替代ReLU函数，以获得更好的性能。
# 3. 总结
激活函数是深度学习模型中不可缺少的一环，用来控制各个神经元的输出值，其作用主要是为了使神经网络能够更好地拟合复杂的数据、提升其泛化能力。不同的激活函数之间又存在着不同的特点和使用场景。本文介绍了两种常用的激活函数——sigmoid函数与tanh函数，并阐述了它们的特点、使用场景以及区别。后续还将介绍Leaky ReLU函数，展示其与ReLU函数的不同之处，并给出其适用的场景。