
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


In recent years, a variety of optimization algorithms have been proposed for solving various types of non-convex optimization problems, such as linear programming (LP), quadratic programming (QP), second-order cone programming (SOCP) and semidefinite programming (SDP). In this paper, we will focus on two specific algorithms: stochastic average gradient descent (SAGA) [1] and Nesterov's accelerated gradient descent (NAG) [2]. These algorithms belong to the family of quasi-Newton methods that use iterative approximations to approximate the Hessian matrix or the Newton direction in order to achieve fast convergence to local optima. However, it is not clear how these algorithms relate to traditional first-order methods like conjugate gradients or Gauss-Seidel method used for convex optimization problems, which can converge much faster than their counterparts using quasi-Newton techniques. Moreover, when applied to certain types of non-convex problems with strong global structure, they may exhibit different behaviors compared to their corresponding first-order counterparts due to their intrinsic nature of exploring regions with high curvature. In this work, we aim to answer these questions by comparing and contrasting the theoretical properties of SAGA and NAG, as well as their practical performance on a number of test functions with strong global structure, including linearly constrained quadratic programs (LCQPs), semidefinite programming (SDPs), logistic regression, maximum entropy models, and separable approximations of generalized Lagrangians. We also explore some possible connections between SAGA and other related methods, such as FTRL, Adam, Adagrad, and RMSprop. Finally, based on our results, we propose future research directions in this area. 

# 2.核心概念与联系
## Quasi-Newton Methods
Quasi-Newton methods are popular numerical optimization algorithms that rely on iteratively approximating the inverse hessian matrix or the search direction in order to improve convergence speed and accuracy. The most commonly used approach for approximating the Hessian is to use an approximation matrix $\hat{H}$ formed from limited memory updates, such as updating only the last few rows and columns of the matrix at each iteration. This technique is known as the BFGS scheme or Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. Other common approaches include limited-memory variable metric (L-VM) and rank-one methods, among others. In all cases, the goal is to find an approximation $h$ to the exact solution $x^*$ within a given tolerance level $\epsilon$. The convergence rate of these methods depends crucially on the quality of the approximation, both in terms of accuracy and stability. For example, if the problem involves ill-conditioned matrices or very large numbers of variables, then even simple iterative steps could lead to slow convergence rates. Thus, efficient implementation strategies and adaptive switching mechanisms are critical to ensure reliable convergence across a wide range of applications.

One class of important quasi-Newton methods belongs to the family of trust-region methods, which are motivated by the idea of constructing an appropriate step length that guarantees convergence to a saddle point of the objective function without entering the basin of attraction associated with any local minima. One widely used subfamily of trust-region methods is the tractable variants of the BFGS and L-BFGS algorithms, which maintain a lower bound on the curvature of the objective function around the current iterate. It is demonstrated that the unconstrained minimization problem can be equivalently expressed as an optimization over a set of hyperplanes spanned by the eigenvectors of the hessian matrix or those of its square root. Therefore, the choice of scaling and rotation of the underlying vectors plays a significant role in determining the effective dimensionality of the optimization problem and thus affects the convergence behavior of the algorithm. In practice, most standard trust-region methods employ line searches along the negative gradient direction, but alternative choices may lead to improved performance depending on the particular characteristics of the problem. Nonetheless, while much progress has been made recently towards developing scalable implementations of these methods, the fundamental connection between quasi-Newton methods and trust-region methods remains elusive. In this work, we start by reviewing some of the key ideas behind quasi-Newton methods and their practical implications, followed by a detailed comparison between SAGA and NAG. 


## Stochastic Average Gradient Descent (SAGA) Algorithm
The SAGA algorithm was introduced by Zhang et al. [1] as a modification to the traditional version of gradient descent called momentum SGD [3], where instead of computing the gradient directly from the loss function, we compute a stochastic estimate of the gradient. Specifically, we randomly sample a subset of data points during each iteration, and use them to form an empirical estimate of the gradient vector using the formula:

$$\widehat{\nabla f(w_t)}_{i} = \frac{1}{\sqrt{m}}\sum_{j=1}^m \nabla f(\widetilde{w}_t^{(j)})_i $$

where $w_t$ denotes the current parameter values, $\widetilde{w}_{t}^{(j)}$ represents the value of parameters after applying one update step to mini-batch $j$, m is the size of the batch, and $i$ indicates the i-th component of the gradient vector. By averaging over multiple batches, we obtain a more robust estimate of the true gradient, particularly useful in situations where the dataset cannot fit into main memory. The weight decay term is added outside the sumation sign, so we do not need to explicitly add it to each element of the gradient vector. 

Using this empirical gradient estimate, we can now apply the following update rule to take a gradient step towards the local minimum:

$$ w_{t+1} = w_t - \alpha_t \widehat{\nabla f(w_t)} $$

Here, $\alpha_t$ is a learning rate schedule that controls the step size, typically chosen as a function of the iteration index t. Similar to standard momentum SGD, the idea is to keep track of a velocity v_t and project it onto the tangent space of the surface defined by the current gradient estimate and the second derivative of the loss function at the current position w_t:

$$ v_{t+1} = \beta v_t + (\eta-\beta)\widehat{\nabla^{2}} f(w_t)^{-1}\widehat{\nabla f(w_t)} $$

Here, $\beta$ is the momentum term and $\eta=\beta+\gamma/|\widehat{\nabla^{2}} f(w_t)|$ determines whether we are making a corrective move or diverging away from the optimum. This velocity is incorporated into the next update step to give us a better understanding of the geometry of the optimization landscape. 

To summarize, the SAGA algorithm maintains an online estimate of the gradient, allowing it to make corrections to the current iterate based on a weighted combination of the current estimate and previous updates. It uses a trust region constraint to avoid divergence and ensures that the gradient estimate stays relatively stable throughout training, making it suitable for handling complex, non-convex optimization problems. Since it estimates the gradient indirectly through mini-batches, it requires less computation than full-batch gradient descent and is able to handle datasets that exceed main memory capacity.


## Nesterov's Accelerated Gradient Method (NAG) Algorithm
NAG [2] is another popular quasi-Newton method specifically designed for convex optimization problems, based on Nesterov's thoughts about mirror descent. The basic idea behind NAG is similar to that of SAGA, but it uses the "directional" variant of momentum, which means that rather than computing the gradient directly at the new position (which leads to cyclic oscillations), we compute it at a nearby point inside the trust region and predict how far we would move there under normal conditions. This predicted movement is then incorporated into the next update step to give us a better sense of where we should move next, rather than moving blindly toward the optimal solution. 

Specifically, NAG maintains an auxiliary variable x_tp (called "lookahead") that lies approximately on the decision boundary between the feasible set and the trust region. At each iteration t, we evaluate the objective function at several candidate positions and choose the one that maximizes the lookahead condition. Specifically, let $w_{t-1}, h_t$ represent the current location and direction respectively, and let $y_t$ be a random step in the opposite direction of $h_t$:

$$\widehat{\theta}_{t-1}(y_t)=\arg\max_{\theta}\left[\mathcal{L}(\theta)+\beta_2||\nabla_\theta\mathcal{L}(\theta)||^2_2+\frac{1}{2\rho}\cdot y_ty_T\right]$$

Here, $\mathcal{L}(\theta)$ represents the loss function evaluated at parameter $\theta$, $\beta_2$ scales the smoothness penalty, $\nabla_\theta\mathcal{L}(\theta)$ represents the gradient vector evaluated at $\theta$, and $y_t$ is obtained by subtracting $h_t$ from $x_t$ twice, resulting in a step halfway between $x_t$ and $x_{t-1}$. Note that this expression does not depend on the actual gradient estimate itself, but only evaluates the expected improvement in the next best position. Letting $\widehat{\theta}_{t-1}(y_t)$ indicate the predicted location if we were to follow the search direction $h_t$, we construct the following estimate of the gradient at $x_t$:

$$ \widehat{\nabla f(x_t)}_{k} = \frac{1}{\sqrt{m}}\sum_{j=1}^m \nabla f(\widetilde{\theta}_{t-1}(y_t))_k $$

We further divide this estimate by the square root of the trust region radius r to get a normalized gradient estimate, giving us:

$$ \widehat{\tilde{\nabla}}_t = \dfrac{\widehat{\nabla f(x_t)}}{\sqrt{r}} $$

Note that here we assume that the initial guess for $x_t$ falls into the feasible set and satisfies the trust region constraint, but this assumption might no longer hold true later down the road. After choosing the new step direction $d_t$, we combine it with the velocity $\widehat{\tilde{v}}$ and the momentum term $\beta$ to obtain the final update:

$$ x_{t+1}=x_t+\alpha_t d_t+\beta_t\widehat{\tilde{v}} $$

Again, the learning rate and momentum terms are controlled by the step sizes alpha_t and beta_t, respectively, which depend on the iteration index t. Unlike SAGA, NAG relies solely on the fact that the gradient is locally convex to guarantee convergence to a global optimum, and performs well on many test functions with strong global structure, especially those involving regularizers like sparse priors or penalties on individual coefficients. Together with SAGA, it forms part of a family of hybrid optimization algorithms that strike a balance between efficiency and effectiveness for a wide range of problems.