
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


目前视觉 transformer 在深度学习领域中应用广泛，并且在各种任务上都取得了优秀的成绩。然而，transformer 模型在实时处理要求高、计算资源消耗大等方面也存在一些限制。因此，近年来，多种低延迟的轻量级模型被提出，如 MobileNetV3 和 EfficientNet，它们通过网络分解和其他方式来降低计算复杂度和内存占用，但仍然受限于浮点运算。最近几年，随着机器学习计算能力的不断增强以及半导体芯片的普及，人们越来越关注基于硬件加速的新型计算技术。相比之下，传统的浮点运算与数据类型一直占据着深度学习开发者的主要研发资源。尽管计算机硬件的快速发展促进了科技的进步，但是其仍然有很多性能瓶颈。在基于硬件的方案中，CNN、Transformer 和其他神经网络层等硬件加速的模块在很大程度上依赖于浮点数运算。因此，为了实现更精确的视觉 transformer 模型，本文提出了一种新的可微型视觉转换器架构——带固定点运算的深度神经网络（DNN）中的精确量化。

在本文中，作者将探索当前视觉 transformer 的浮点运算约束并提出一种新的可微型视觉转换器架构——带固定点运算的深度神经网络（DNN）中的精确量化。基于该架构，作者提出了一个集成电路级别的基于硬件的 DNN 模型，可以进行视觉 transformer 的所有基本操作，而不需要任何浮点运算。该架构利用了面向定制芯片的定点计算技术，旨在降低神经网络的计算精度损失。其精确性源自于 DNN 中的多个层的准确计算，而且这种准确性可以通过低阶近似获得。

本文的贡献如下：
- 提出了一种新的可微型视觉转换器架构——带固定点运算的深度神经网络（DNN）中的精确量化，它利用了面向定制芯片的定点计算技术，通过较少的误差得到了相同的性能保证，而且能处理更高维度的输入。
- 通过对视觉 transformer 前馈过程的分析，揭示了其具有高度非线性特性，这使得很难对其进行有效的量化处理。
- 使用定点计算技术将视觉 transformer 的计算精度提升到一个足够精确的水平。
- 对该方法的效率进行了评估，验证了定点 DNN 模型的可行性。

2.核心概念与联系
固定点（fixed point）是指在数字电路设计中使用的一种数值表示方式，其中每一个存储单元都有着固定的点位置，从而使得小数点不会移动或者只会移动到某些特定的位置，这样可以保证精确度。因此，当使用定点数的处理单元完成计算时，可以不必考虑浮点数的舍入误差，从而保证精度。在本文中，我们将使用定点数的方式模拟浮点数运算的大部分功能，包括加减乘除等基本算术运算。

视觉 transformer 是一种深度学习模型，用于对图像进行分类和回归任务。它的核心操作是基于 self-attention 模块的编码器-解码器结构。不同于标准的 CNN 或 RNN，它可以同时处理序列和图像数据的特征抽取和表示，因此具有优势。本文所述的精确化视觉 transformer 架构与传统的浮点运算视觉 transformer 架构之间存在根本差别。传统的视觉 transformer 架构由浮点运算构建，在实际部署中，只能使用浮点数作为参数和中间变量的存储。但是，在现代计算平台上，可以充分利用定点技术提升视觉 transformer 的计算性能。

传统的浮点运算视觉 transformer 有以下几个缺点：
- 无论是在推理阶段还是训练阶段，其计算速度都无法满足实时要求。
- 浮点运算存在累积误差，导致深度学习模型的精度下降。
- 需要大量的内存空间来存储中间结果，这严重限制了模型的规模。

精确化视觉 transformer 将视觉 transformer 的计算拆解为两个子部分：前馈子层（feedforward sublayer）和自注意力子层（self-attention sublayer）。前馈子层类似于普通的卷积层或全连接层，可以对输入数据做变换。自注意力子层则采用 self-attention 机制，可以学习全局特征。此外，除了两个子层之外，还加入了专用的量化和校准模块。量化模块通过生成近似函数（approximation function）来降低中间结果的精度损失，校准模块则用于消除量化过程中引入的误差。作者认为，该方法能够有效地降低视觉 transformer 模型在计算上的需求，从而为各种任务提供更好的实时性能。

3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
由于视觉 transformer 是一个具有高度非线性特征的模型，所以即便是在浮点数运算的情况下，其计算的误差也是非常巨大的。因此，我们希望通过研究如何对视觉 transformer 中的操作进行定点化，来提升模型的计算精度。

首先，让我们来看一下视觉 transformer 中的前馈子层（feedforward sublayer）：


该子层由两层感知机组成，其中第一层具有 ReLU 激活函数；第二层没有激活函数，输出为长度为 d 的向量。本文所述的精确化视觉 transformer 架构的关键在于，替换掉标准的 ReLU 函数为定点版本，这样就可以达到定点运算的目的。

另一方面，自注意力子层（self-attention sublayer）需要对整个序列中的每个 token 进行 attention 操作。与之前的模型一样，这里也使用 multihead attention 来对序列中的信息进行建模。

接下来，让我们来看一下精确化视觉 transformer 中自注意力子层的计算过程。假设我们有一个序列 S = [x_1, x_2,..., x_{T}]，其中 xi ∈ R^n 为序列中的第 i 个元素。第 j 个 head 的 attention weight 可以计算为：

$$
\alpha_j(h_i)= \frac{exp(E_ih_i)^{\frac{1}{\text {dim}_k}}}{\sum_{l=1}^{L}\left(exp(E_{il}h_{i})^{\frac{1}{\text {dim}_k}}\right)}
$$

这里，$E_ih_i$ 和 $E_{il}$ 分别代表 jth head 对于 xi 和 lth element 的注意力权重。L 表示 multihead 的个数，$\text {dim}_k$ 表示特征维度。使用 logit 函数来归一化注意力权重。

然后，使用 softmax 函数将注意力权重转换为注意力概率分布。softmax 函数的参数是矩阵 A = [[W^{Q}(h_1), W^{K}(h_1)],..., [W^{Q}(h_T), W^{K}(h_T)]] 和向量 v=[v_1,..., v_{T}], 其中 $W^{Q}, W^{K}$ 分别是 Q 和 K 的矩阵投影矩阵，v 是 bias 向量。softmax 函数的输出为 attention vector z：

$$z=\sigma(\beta+A\cdot v), \quad \sigma(x)=\frac{1}{1+\exp(-x)}$$

这里，β 为缩放系数。

最终，输出的计算公式为：

$$o_i=z_i \odot h_i + (1-\alpha_i)\odot o_i,\quad \odot 表示逐元素相乘$$

其中 $\odot$ 表示 Hadamard product，即对应元素相乘，$z_i$ 和 $h_i$ 表示第 i 个 token 的 attention vector 和 token 对应的隐藏状态。$o_i$ 表示第 i 个 token 的输出。

接下来，我们看一下精确化视觉 transformer 中的量化模块（quantization module），该模块负责对中间结果进行精确化。假设我们有一个参数 W，其要被量化到整数 k 位精度。那么，我们首先定义一些符号：

- α : quantization step size ，即量化步长，等于 range / 2^(k-1)。
- β : offset，负责对整数进行量化，等于 2^(k-1) * round(input / alpha)，其中 round() 函数用来实现四舍五入。
- delta : 噪声幅度，即量化结果的最大误差，等于 range / 2^(k-1) 。
- r : noise power，均值为 0，方差为 q/α * ln(2)(q-1)/delta 。

具体来说，我们对参数进行如下变换：

$$
q_\text{int}=round(input/\text{stepsize})\text{stepsize}+\text{offset} \\
noise=(random(-r,r))^\prime \\
q_{\text{float}}=q_\text{int}-\text{offset}-noise*stepsize
$$ 

其中，$(-r,r)$ 为均匀分布。

最后，我们通过求导的方法计算参数 W 的梯度，并更新参数。

综上所述，对于自注意力子层和前馈子层，作者提出的定点计算架构可以极大地降低模型的计算精度损失。其具体操作步骤如下：

1. 对输入数据进行预处理：如对图像进行归一化，划分 batch 和 sequence，等等。

2. 对输入数据进入视觉 transformer 结构进行处理，并产生中间结果。

3. 对中间结果进行量化，并通过参数 W 更新。

4. 重复步骤 2 和 3，直到达到目标精度。

5. 返回预测结果。

总结来说，基于定点技术的视觉 transformer 架构，可以有效地降低计算资源消耗，提升视觉 transformer 模型的计算性能，并达到实时的要求。