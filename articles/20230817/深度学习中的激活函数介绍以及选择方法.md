
作者：禅与计算机程序设计艺术                    

# 1.简介
  

关于激活函数（activation function）在深度学习领域的介绍，一直都是研究热点。从不同角度阐述了激活函数的作用，分类及其应用场景，详尽地介绍了激活函数在神经网络中的各种形式及优缺点。本文将对深度学习中常用的激活函数进行介绍及比较分析，并且介绍一些更适合于不同的场景的激活函数。

## 激活函数是什么？
在深度学习过程中，激活函数是每层节点输出值的非线性映射，可以理解成是一个神经元的生物学反应。比如，一个输入信号值越大，则该神经元的输出值也越大；而输入信号值越小，则该神ュー神经元的输出值也越小。因此，激活函数的引入可以使得神经网络能够解决非线性拟合的问题。

激活函数的目的就是为了让神经网络输出值具有非线性特征，从而使模型能够处理复杂、非线性的数据集。根据激活函数的类型及其特点，深度学习模型可以分为以下几类：

1. 线性模型：最简单的一种模型，只需要简单加权求和即可完成计算；
2. 感知机模型：感知机模型是最早提出的神经网络模型，它通过输入变量之间的线性组合来实现简单分类功能；
3. 普通神经网络：普通神经网络即多层感知机模型，可以同时学习多个线性模态或非线性变化，具有较高的灵活性；
4. CNN卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN），是一种特殊的神经网络，用于图像识别领域。CNN通常采用卷积层+池化层+全连接层结构，能够有效提取图像特征并进行分类预测；
5. RNN循环神经网络：循环神经网络（Recurrent Neural Network，RNN），是一种可以捕获序列相关性信息的神经网络。RNN模型能够捕获时间依赖性、存储历史信息，对时序数据有很好的表现力。

## 激活函数的种类
### sigmoid 函数（S型函数）
sigmoid函数又称阶跃函数，是指把输入信号线性变换到0-1之间，然后再将输出值转化为输出概率。sigmoid函数的表达式为：

$$h = \frac{1}{1 + e^{-z}}$$

其中$z$为线性输入：$\mathbf{x} = (x_1,\dots, x_n)$，权重参数$\mathbf{w}$，偏置项$b$，如下图所示：


它的特点是：
- 在一定范围内有着良好的输出(区间在0到1)，对于二分类问题来说，sigmoid函数就可以作为激活函数；
- 梯度直观，易于计算，而且不容易饱和，可以使得后续的训练过程快速稳定；
- 当输入的绝对值较大时，sigmoid函数输出接近0或1，对计算资源要求不高；
- 对输出值的梯度值比其他激活函数要窄；
- 可以将输入信号转换为概率输出，也可以用作分类函数；
- sigmoid函数也叫做S形函数，它属于双曲正切函数族，由于其函数形状与正弦曲线非常相似，故又被称为s曲线或双曲正切曲线。

### tanh 函数（双曲正切函数）
tanh函数是指把输入信号线性变换到-1到1之间，然后再将输出值转化为输出概率。tanh函数的表达式为：

$$h = \frac{\mathrm{exp}(z)-\mathrm{exp}(-z)}{\mathrm{exp}(z)+\mathrm{exp}(-z)}=\frac{\mathrm{e}^{2x}-1}{\mathrm{e}^{2x}+1}$$

其中$z$为线性输入：$\mathbf{x} = (x_1,\dots, x_n)$，权重参数$\mathbf{w}$，偏置项$b$，如下图所示：


它的特点是：
- 输出范围为-1到1之间；
- tanh函数的输出值范围是[-1,1]，输出刚好是输入值的变换结果，因此能够保留输入的信息，而不像sigmoid函数那样压缩到0-1区间；
- tanh函数具有良好的单调性，导数不会出现零膨胀或者爆炸；
- tanh函数的输出值趋近于0，不易过度饱和，使得梯度计算容易；
- tanh函数对参数初始化比较敏感；
- tanh函数能够将输入信号转换为概率输出；
- tanh函数也叫做双曲正切函数，特别是在输出值大于1时，它的输出会因子的大小超过1，导致输出变成无穷大。

### ReLU函数（修正线性单元）
ReLU函数是指Rectified Linear Unit，即修正线性单元，其是深度学习中使用的一种激活函数，其输出值不断地在0和x之间移动。其定义为：

$$h=max(0, z)=\left\{
    \begin{array}{}
        0, & z<0 \\
        z, & z\geqslant0 \\
    \end{array}
\right.$$

其中$z$为线性输入：$\mathbf{x} = (x_1,\dots, x_n)$，权重参数$\mathbf{w}$，偏置项$b$，如下图所示：


它的特点是：
- Rectified Linear Units (ReLUs) 因其名字包含“rectifier”而得名，意味着它是一种修正线性单元。虽然名字带有“线性”，但是它实际上是利用“修正”的方式达到线性的效果。
- ReLU函数是目前使用最广泛的激活函数之一。它激活强度不断减弱，直到某个阈值，之后就保持恒定的不变，这样可以避免神经元的死亡现象。
- ReLU函数的输出值都处于非负区间，因此可以保证没有任何负的影响。
- ReLU函数的输出也是逐元素计算，可以进行并行运算；
- 没有零梯度，也没有饱和现象；
- 参数初始化不必担心；
- ReLU函数可进行快速计算。但是当负输入较多时，ReLU函数可能造成梯度消失，从而影响收敛速度。

### Softmax 函数
Softmax函数是指把输入信号线性映射到0-1之间，然后归一化为概率分布。Softmax函数的表达式为：

$$softmax(\mathbf{z})_{i}= \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}} $$

其中$\mathbf{z}=(z_1,...,z_K)^T$为输入向量，$i$为第$i$个类别，$\sum_{j=1}^Ke^{z_j}$表示$e^{z_j}$之和，用来确保输出值总和为1。

Softmax函数的特点是：
- softmax函数也是一种激活函数；
- 虽然它具有几个线性激活阶段，但却采用了归一化的方法，因此可以得到概率分布；
- 如果有K个类别，那么soft函数就会输出K维的向量，并且所有元素的值都在0-1之间；
- softmax函数一般用来产生类别概率分布。

### Swish 函数
Swish函数是指自然语言处理任务中常用的激活函数。它是一个非线性函数，表达式为：

$$swish(x)=x\sigma(x)={\frac {x}{1+e^{-x}}}$$

其中$\sigma(x)$是sigmoid函数，swish函数可以通过sigmoid函数的组合来实现。这种实现方式能够较好地解决深度学习模型中梯度饱和的问题，尤其是在梯度下降过程中。

Swish函数的特点是：
- swish函数具有非线性特性；
- swish函数在低频部分具有平滑、连续的行为，在高频部分会出现梯度的急剧减小现象；
- swish函数的表达式比较简单，能够直接看出它如何进行激活计算；
- 参数初始化较为简单。

### Mish 函数
Mish函数是由作者独立于他人的论文所提出的，其表达式为：

$$mish(x)=x\tanh({ln(1+e^x)})$$

与swish函数一样，mish函数也是基于sigmoid函数构建的非线性函数。与swish函数的主要差异在于：mish函数的曲线拟合能力更强，能够更好地拟合复杂的函数。

Mish函数的特点是：
- mish函数也是一种非线性函数；
- mish函数的表达式更为简单，但是由于计算代价高昂，在运算速度方面仍然优于sigmoid和swish函数；
- 作者认为mish函数的设计思想是融合sigmoid和swish函数的长处，适合于在CNN等神经网络模型中使用。

## 使用哪个激活函数才最合适呢？
很多情况下，我们无法确定应该选用哪种激活函数，只能通过试错法进行尝试和比较。下面我们从三个方面给出建议：

1. 数据分布：如果数据具有零均值和方差，推荐使用sigmoid函数。否则，推荐使用tanh函数；
2. 优化目标：如果是回归问题，则推荐使用线性函数。如果是分类问题且类别数较少，则推荐使用softmax函数；
3. 计算量：如果需要高速计算，则推荐使用relu、leaky relu或elu函数；如果想要降低内存占用，则推荐使用swish函数或mish函数。