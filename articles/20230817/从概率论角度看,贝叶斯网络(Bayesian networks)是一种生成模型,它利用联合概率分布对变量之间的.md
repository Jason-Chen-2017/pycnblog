
作者：禅与计算机程序设计艺术                    

# 1.简介
  

贝叶斯网络由三元组(V, E, C)，其中V表示节点集合、E表示边集，C表示概率分布函数。可以将概率分布写成如下形式：

P(X=x1,..., X=xn) = P(X=x1) *... * P(X=xn)

其中，Xi表示随机变量，xi取值于Xi的一系列取值{xi1, xi2,..., xim}。所以贝叶斯网络就是定义了计算联合概率分布P(X1,...,Xn)的概率分布函数。贝叶斯网络可以用于：

- 概率推断：给定某些已知条件下变量X的取值,贝叶斯网络可以计算出X的其他取值及对应的概率值。如通过一条信息可以推断另一条信息的可信程度。
- 结构学习：贝叶斯网络是无向图结构，且可以从数据中学习到网络的拓扑结构、边的方向、节点的子集等。
- 参数学习：贝叶斯网络的参数估计可以利用极大似然法或EM算法实现。
- 模型比较：同一个贝叶斯网络可以针对不同的任务进行训练得到不同的参数。因此，可以比较不同任务的结果，以确定最好的模型。

通常来说，贝叶斯网络有两种结构：隐马尔科夫网络(Hidden Markov Model, HMM)和精确的贝叶斯网络(Exact Bayes Nets)。

本文主要关注的是贝叶斯网络的概率推断方法。

# 2.基本概念
## 2.1 概念
贝叶斯网络由两部分组成：

- 一套离散随机变量：用X1, X2,..., Xn表示，每个随机变量可以取若干个值。比如，面试题目可能涉及四道选修课的课程是否及格、某个人对一个项目的评价打分等。
- 一些依赖关系：用二元关系(Xi, Xj)表示，表示随机变量Xi与随机变量Xj存在某种依赖关系。这些依赖关系可以是直接相关的，如一个人的年龄与他的成绩成正比；也可以是间接相关的，如一张地图上两个城市之间的距离依赖于交通状况等。
贝叶斯网络模型同时考虑两个因素：

- 观测数据的状态：也就是说，已知随机变量X的取值时，其余变量X的值是多少？这时称为“观测”；
- 在给定某些已知条件下，如何对未知变量X的值进行预测？这时称为“推断”。

由此可以得出以下公式：

P(X=x1,..., X=xn|do(A)) = ∏_{i=1}^{n} P(X_i=x_i|pa_i)P(X_i∈c_i|do(A))

其中，do(A)表示已知条件A；P(X=x1,..., X=xn|do(A))表示已知条件A下的观测分布；X_i表示第i个变量；pa_i表示前驱节点，即X_i的父节点；c_i表示该变量可以取值的集合；P(X_i∈c_i|do(A))表示X_i在已知条件A下的条件分布。

## 2.2 变分推理
贝叶斯网络的核心思想是：如果已知观测数据的状态，则可以根据贝叶斯网络的定义计算联合概率分布。而实际应用中，我们并不知道所有变量的取值，而只关心其中的一部分或者几个变量。这样，就需要进行“变量消除”，即假设已知的变量取值与其他变量的取值之间存在某种关系，然后去掉其他变量，只留下相关变量即可。由于已知条件的限制，得到的概率分布会更加准确。这种方法叫做“变分推理”，其基本思路是：通过变量的约束，逼近真实分布，从而获得有限的关于已知变量取值的分布情况。

如图所示，先假设我们已经知道X1，那么通过这一变量，可以计算出X2、X3、X4的值，因为根据我们的假设，X2、X3、X4的值都与X1有一定联系。依次类推，当我们知道了所有的变量值时，就可以根据贝叶斯网络计算出整个分布。这种推理方式叫做“局部变量消除”，局部变量指已知的变量，而全局变量指未知的变量。

变分推理有什么意义呢？举例来说，现在有一个医院诊断患者生病的概率为0.01。如果知道患者年龄、体重、收缩压、舒张压、脂蛋白等诊断症状，则可以通过贝叶斯网络计算出其生病的概率。但是，如果没有足够的数据，只能基于这个信息预测患者可能会生病，但不能具体预测哪些生理症状。通过局部变量消除的方法，我们可以根据诊断症状里包含的信息预测患者可能生病的症状。这样，我们就可以尽量减少数据量，从而提高效率和准确性。

# 3.核心算法
## 3.1 归一化因子（normalization factor）
对于一个贝叶斯网络，存在一个归一化因子Φ(C)，它是观察到X的一条路径的概率乘积。有时也写作Z(C)，表示在已知条件C下，X的所有路径上的概率之积。这个值可以表示为：

Z(C) = Π_k Π_s p(e_ks)(C), k=1,...,K; s=1,...,S

其中，p(e_ks)是第k条路径e_ks上的事件发生的概率；e_ks是一个长度为s的路径，e_ks=(X_1^k,...,X_s^k); K是所有可能的路径个数；S是最大的路径长度。我们希望找到一个观测分布q，使得q(e_ks)/Z(C)最大。

## 3.2 前向传播（forward propagation）
前向传播是求解条件概率P(X=x1,..., X=xn|do(A))的一种算法。它的过程是：

- 初始化：令Q(X^{1})={x1}, q(X^{1})=1/n, 其中n为变量X的总数。
- 对s=1,...,S进行迭代：
  - 对每一条路径e_ks=(X_1^k,...,X_s^k)，遍历所有的可能的取值，计算该路径上的事件发生的概率q(e_ks)：
    - 如果e_ks包含变量X_i，那么计算p(X_i=xj|pa_i, do(A)), pa_i表示前驱节点，即X_i的父节点；
    - 如果e_ks不包含变量X_i，那么计算p(X_i|pa_i, do(A))。
  - 更新Q(X^{s+1})：令Q(X^{s+1})=Q(X^{s}) union e_ks中所有包含变量的节点。
  - 根据更新后的Q(X^{s+1}), 更新q(X^{s+1})：
    - 通过P(X_i∈c_i|do(A))更新q(X_i)：
      - 如果X_i在路径e_ks出现过，那么通过该路径上的事件发生的概率q(e_ks)乘以P(X_i∈c_i|do(A))。
      - 如果X_i不在路径e_ks出现过，那么用1/(S-s+1)*p(X_i|pa_i, do(A))代替。
    - 用q(X^{s+1})除以Ψ(Q^{s+1}, do(A))进行归一化。
  - 将q(X^{s+1})保留作为下一次迭代的初始值。
- 最终的结果是：Z(C) = Π_{k=1}^K Π_{s=1}^Sp(e_ks)(C)。

## 3.3 后向传播（backward propagation）
后向传播也是求解条件概率P(X=x1,..., X=xn|do(A))的一种算法。它的过程是：

- 初始化：令Q(X^{S})={X_1,...,X_n}, q(X^{S})=1, 表示从观测变量X到潜在变量Xi的所有路径的数量都是相同的。
- 对s=S-1,...,1进行迭代：
  - 对每一条路径e_ks=(X_1^k,...,X_s^k)，遍历所有的可能的取值，计算该路径上的事件发生的概率q(e_ks)：
    - 如果e_ks包含变量X_i，那么计算p(X_i=xj|pa_i, do(A)), pa_i表示前驱节点，即X_i的父节点；
    - 如果e_ks不包含变量X_i，那么计算p(X_i|pa_i, do(A))。
  - 更新Q(X^{s-1})：令Q(X^{s-1})=Q(X^{s}) intersect e_ks中所有包含变量的节点。
  - 根据更新后的Q(X^{s-1}), 更新q(X^{s-1})：
    - 利用P(X_i∈c_i|do(A))更新q(X_i)：
      - 如果X_i在路径e_ks出现过，那么通过该路径上的事件发生的概率q(e_ks)乘以P(X_i∈c_i|do(A))。
      - 如果X_i不在路径e_ks出现过，那么用1/(S-s+1)*p(X_i|pa_i, do(A))代替。
    - 用q(X^{s-1})除以Ψ(Q^{s-1}, do(A))进行归一化。
  - 将q(X^{s-1})保留作为下一次迭代的初始值。
- 最终的结果是：Z(C) = Π_{k=1}^K Π_{s=1}^Sp(e_ks)(C)。