
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


图神经网络（Graph Neural Networks）已经被越来越多研究者关注。近年来基于图结构的数据有着广泛的应用，例如推荐系统、社交网络分析、生物信息分析等。为了能够充分地利用这些数据，图神经网络需要学习出良好的表示方法。传统的机器学习方法对图数据处理存在诸多局限性，因此，深度学习模型应运而生。本文就是要研究如何训练一个深度学习模型来对图进行表示学习。图结构数据的基本特征包括节点之间的连接关系、节点的特征、边的特征等。一般来说，图数据的表示学习主要有三种方式：基于邻接矩阵的方法、基于特征的方法和基于空间或时空的方法。本文以无监督的方式对基于特征的方法进行探索。
# 2.核心概念与联系
图(graph)由点和边组成，点之间用边相连，点、边、和边上的属性可以看做图的特征。图中一般有两种节点类型，即节点和子图，节点是图中的顶点，子图是指子图形态相同的图。边是连接两个节点的线段。在图学习过程中，希望对节点和边的特征进行建模，同时保留其在图中的相互关系。因此，图神经网络的输入是图，输出则是一个节点或子图的表示向量或特征。对节点和边的表示可以使用高维空间或者低维稀疏向量表示，不同的图神经网络模型可以选择不同的表示方式。

图神经网络的核心算法主要包括三类：传播模型、聚合模型和生成模型。传播模型是指用于描述节点间关系的非参数模型；聚合模型则是在不同层级上对节点的表示进行融合；生成模型则试图通过生成高质量的图结构来帮助训练模型学习到有效的结构信息。不同的模型可以组合起来，构成一个完整的图神经网络。下面重点介绍其中两种模型：第一种是图卷积网络GCN，第二种是注意力机制Graph Attention Network GAT。这两种模型都是典型的无监督学习模型，分别通过对节点和边进行特征学习和分类任务来实现对图的表示学习。

# Graph Convolutional Networks (GCNs)

图卷积网络GCN是最早提出的无监督学习模型，其核心思想是把图的邻接矩阵当作卷积核，使用拉普拉斯差商作为激活函数，并进行图的卷积操作，得到节点的表示。下图展示了GCN的结构示意图：

图卷积网络的基本过程如下：

1. 对图进行邻接矩阵编码，如用拉普拉斯差商进行编码；
2. 计算图卷积核K=∑a~l×ai；
3. 通过K作用在每个节点的特征上，得到新的特征表示z~i；
4. 使用激活函数进行非线性变换，得到最终的节点表示h~i；
5. 将所有节点的表示连接起来，得到整个图的表示h。

图卷积网络的一个优点是不要求图的全局连接，可以在节点、子图甚至整个图上进行表示学习。但是，它也有一些缺陷，比如无法捕获全局的异质性，无法捕获节点的历史行为以及局部的拓扑结构信息。

# Graph Attention Networks (GATs)

注意力机制Graph Attention Networks（GATs）是另一种常用的无监督学习模型。它的特点是利用节点和边的注意力机制来建模节点之间的关系。图Attention模型的基本结构如下图所示:

GATs包括两部分：第一部分是特征生成模块FGM，其负责将节点和边的信息整合到一起，生成全局的上下文信息。第二部分是注意力分配模块AM，利用边上的注意力机制来获得节点之间的关系。GATs可以生成具有更高容量和语义信息的节点表示。

GATs相比于GCN有以下优势：

1. 更强大的局部感知能力，能捕获到节点之间的复杂关系；
2. 可以对节点的历史行为进行建模；
3. 不需要全局连接信息。