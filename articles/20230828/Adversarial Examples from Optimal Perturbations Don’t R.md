
作者：禅与计算机程序设计艺术                    

# 1.简介
  

人工智能领域一直在追求自动化，深度学习的发展促使传统机器学习模型逐渐向深度学习迁移，但是面对着更加复杂的任务，深度学习模型的性能仍然不能完全满足需求。因此，研究者们不断探索新的攻击方法，更好地保护模型的安全性。近年来，许多研究者都提出了基于梯度的方法来生成鲁棒性较强的图像扰动。这些方法通过对扰动施加合适的正则化项来保证鲁棒性，但是却很难处理其他类型的扰动。比如，在缺乏信息的情况下生成具有目标特定的特征的扰动。本文试图解决这一问题，提出一种不需要训练 discriminator 的攻击方法，可以有效生成特定类别的对抗样本。
# 2.相关工作及主要贡献
现有的一些针对深度学习模型的攻击方法都需要训练一个 discriminative model ，如GAN（Generative Adversarial Networks），通过最小化生成器 G 生成真实数据分布 P(X) 和伪造数据分布 Q(G(z)) 的差距，从而训练生成模型 G 。但这种训练方式过于复杂，且对于某些类型的扰动效果并不理想。另一种常用的攻击方法则是在已有的可信模型上进行投射，得到的是不可靠的输出，无法真正显示出模型对输入数据的理解能力。为了实现对抗样本的快速生成，一些研究者提出了通过优化一些目标函数来完成扰动生成，如 L_0、L_2 范数的最小化等。但是这些方法存在的问题也很突出，首先，由于所有扰动的范围相当广，没有明确指定对哪个类别产生对抗扰动。其次，这些方法仅适用于小样本数据集，计算量过大。最后，这些方法生成的扰动往往容易被模型识别为正常样本，而不是目标类别。
本文的主要贡献如下：
- 提出了一个不需要训练 discriminator 的对抗样本生成方法。本文的新方法不需要给定正确标签就能够生成对抗样本，即可以通过对扰动施加合适的正则化项来保证鲁棒性，同时还可以在一定程度上处理其他类型的扰动。
- 通过定量分析和比较，证明了所提出的这个方法比之前的攻击方法都要好。
- 将此方法应用到分类任务中，取得了比之前的一些方法更好的结果。
# 3. 概念、术语定义
## adversarial examples
adversarial examples 是一种对抗样本，它对输入模型进行了恶意修改，使得模型误分类为目标类别。通常来说，对抗样本是一种黑盒攻击方式，即只能知道模型对原始输入 x 的预测结果 y，却不能直接获得模型的内部参数 w 或中间结果 z，所以生成对抗样本需要借助于其他手段，例如梯度、插值等。

## optimal perturbation
optimal perturbation 是一个最优的扰动，即使在最坏的情况下，也会使得模型的预测发生变化。对于输入 x 属于 class C 的样本，如果存在一个扰动 ε ，使得模型在ε作用后预测为目标类别，称ε 为 optimal perturbation。

## optimization-based attack method
optimization-based attack method 是指通过优化某个目标函数来生成对抗样本。通常的做法是选择一个扰动 ε ，使得ε越小，模型在ε作用后的预测变化就越小。可以用 L0 norm 或者 L2 norm 来定义目标函数，其中 L0 norm 表示扰动中非零元素的个数，L2 norm 表示扰动平方的期望值。

## regularization term for perturbation
regularization term for perturbation 是对扰动施加的正则化项，包括了噪声扰动、裁剪扰动、缩放扰动等。目前已有的一些优化目标函数，如 FGSM (Fast Gradient Signed Method)，在对抗样本生成时，一般都会采用一些正则化项来保障鲁棒性。

## deep neural network
deep neural network （DNN）是指具有多个隐藏层的神经网络。

# 4.核心算法原理和具体操作步骤
## Core idea and hypothesis
本文的核心观点是，对于目标类别的图像 x ，如果存在一个扰动 ε ，使得模型在ε作用后预测为目标类别，那么该扰动 ε 可以被看作是最优的。因此，可以将这个过程表示为一个优化问题，找到对输入的 ε 方向上的最优扰动，然后根据最优扰动生成对抗样本。这种方法不需要训练 discriminator，只需要优化目标函数，而且可以有效处理不同类型的扰动，比如缺失信息或高维度数据。

### 流程图

1. 对原始图像进行初始化；
2. 在恒定的半径内随机选取一个初始点；
3. 使用 Adam optimizer 训练两个子网络：discriminator 和 generator；
4. 训练结束时，将生成器 G 拓展到整个图像空间；
5. 用生成器 G 生成扰动 ε；
6. 将 ε 乘以合适的系数得到最终扰动；
7. 添加扰动后重新输入模型，得到对抗样本。

## 具体操作步骤
下面详细介绍下上述流程的具体操作步骤。

### Step 1: 初始化原始图像
设 x 为原始图像，构造一个对应于目标类别的扰动 ε ，使得该扰动会导致模型预测为目标类别。

### Step 2: 随机选取初始点
对原始图像进行初始化，随机选取一个起始点 s = [s1, s2,..., sn] 。

### Step 3: 使用 Adam optimizer 训练两个子网络
为了生成对抗样本，作者使用 Adam optimizer 训练两个子网络：discriminator 和 generator。Discriminator 负责判断输入是否为正常图像，Generator 则将正常图像作为输入，生成具有恶意性的扰动图像。Discriminator 和 Generator 分别由两层 Dense 网络构成，两个网络都使用 Adam optimizer。

#### 训练 discriminator
对于输入的正常图像 x，把 x 送入 discriminator 网络。训练 discriminator 有两种情况：
- 如果 discriminator 认为 x 为正常图像，则更新权重，使得后面生成的对抗样本变得越来越真实；
- 如果 discriminator 认为 x 为恶意样本，则更新权重，使得后面生成的对抗样�变得越来越假。

#### 训练 generator
对于输入的正常图像 x，把 x 送入 generator 网络。将生成器 G 拓展到整个图像空间，用生成器 G 生成一张扰动图像 ε。把 x 和 ε 结合起来，得到一个扰动图像 z = x + ε 。再用生成器 G 生成一个判别结果 d(z)，如果 d(z) 为目标类别，则重复第 3 步，直到 d(z) 不等于目标类别。

### Step 4: 把生成器 G 拓展到图像空间
为了生成具有恶意性的图像，作者使用生成器 G 生成一张扰动图像 ε ，但是实际上生成器 G 只产生在 [-1,1] 之间的值，这与输入图像 x 在 [0,1] 之间是不同的。为了生成具有恶意性的图像，作者用插值的方式把生成的 ε 插值到图像空间里。

### Step 5: 生成对抗样本
把 ε 乘以一个适当的系数 a ，得到最终的扰动图像 z' = x + a*ε 。将 z' 送入模型，得到模型的预测结果。如果模型预测结果为目标类别，则停止，否则继续迭代。返回第 2 步，随机选取新的起始点。

至此，作者成功地生成了一个具有恶意性的图像。但还有很多问题没有解决，如：如何选择适当的系数 a? 如何保证对抗样本是稳定的? 是否存在图像旋转等扰动？ 

# 5. 未来发展趋势与挑战
虽然当前的论文已经取得了一定的效果，但仍有很多问题没有解决，比如：
- 如何设计衡量模型欺骗性的评价标准？
- 如何优化插值的方式？
- 更多的扰动类型如何处理？
- 攻击方式是否可以泛化到其它领域？

总之，本文提出了一种新的攻击方法，即不需要训练 discriminator ，可以有效地生成针对目标类的对抗样本。作者从理论和实践角度阐述了这项工作的原理和局限性，也提供了一些后续研究方向。