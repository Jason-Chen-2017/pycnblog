
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Self-attention机制是一个计算复杂度为$O(N^2)$，参数复杂度为$O(N^2d)$的模块。在自然语言处理任务中，很多模型都采用了self-attention机制作为主要特征提取方式。比如BERT、GPT等模型都是将多层self-attention堆叠得到最后的输出表示。随着多头注意力（multi-head attention）方法的出现，很多论文开始使用这种方法来提升自然语言理解性能。本文的主要目标就是介绍一下Multi-Head Self-Attention Network (MSAN)，它是一种基于self-attention机制的多头注意力结构。


Self-Attention机制主要由Kim、Yang及其同事于2017年提出，它通过关注当前位置与周围位置之间的关联性来生成查询结果。在自然语言处理任务中，它的作用是找到不同位置的信息联系，并产生一个全局的整体表示。与传统的CNN、LSTM、GRU等编码器-解码器结构相比，self-attention结构的特点在于更高的并行化能力和建模效率，且不需要复杂的设计。但是，由于其计算量过大，因此self-attention并不能直接替代CNN、LSTM等结构。


MSAN是一种多头注意力结构，它能够在保持计算复杂度和参数复杂度不变的前提下提升自然语言理解的性能。它的核心思想是把输入序列分成多个子序列，然后在每个子序列上独立地进行self-attention，最后再拼接这些序列上的表示作为最终输出表示。这样做的好处是可以提取到不同子序列之间的关联信息，从而使得模型能够学习到更多有效的特征。


另外，MSAN还引入了可学习的权重矩阵，这使得模型能够根据上下文信息选择最相关的子序列。通过这种方式，MSAN可以在不降低模型的准确率的情况下，增加模型的表现力。


为了提升模型的鲁棒性和泛化能力，本文作者对实验设置、数据集、训练策略、模型架构等方面进行了充分的研究。通过对不同的网络架构、损失函数、正则化方法、超参数的调优等方面进行实验分析，作者发现MSAN在各种任务中的性能都要优于传统模型。同时，对于长文本序列的预测，MSAN也表现优异。

# 2.基本概念术语说明
## self-attention

​    Self-attention mechanism is a computational complexity of $O(N^2)$ and parameter complexity of $O(N^2d)$ where $N$ is the sequence length, $d$ is the model dimensionality. It attends to its own input by generating queries, keys and values for each position in the input sequence. The output at each time step can be computed using these values based on their similarity with other positions in the input sequence. The key idea behind self-attention is that it allows models to focus on different parts of an input sequence rather than just relying on individual units or features. In traditional encoder-decoder structures like CNNs, LSTMs, GRUs etc., self-attention is limited due to the large computation cost but still performs well for tasks like machine translation. 

In this paper, we will introduce a novel architecture called multi-head self-attention networks (MSAN) which uses multiple parallel layers of self-attention mechanisms to extract more informative features from an input sequence. MSAN consists of two stages - pre-processing stage and processing stage. 


## Input sequence
Input sequences are usually longer than the target sentences used in training neural language models. These sequences contain information such as contextual features along with the actual text. The size of the input sequence directly affects the number of parameters in the model, therefore, it has been shown that increasing the input sequence length increases the performance of the model. This leads us to question whether increasing the input sequence length always improves the overall performance of the model or not. However, if we increase the input sequence length without considering how many tokens should go into one sentence, then our model will become too complex and may fail to capture relevant relationships among words. Therefore, we need to divide the long input sequence into smaller subsequences so that we only attend to a subset of the entire sequence instead of the entire sequence itself. 

We call these smaller subsequences "chunks" since they represent meaningful groups of tokens that appear together frequently. For example, in English, chunks could be paragraphs, chapters, or sentences. Each chunk is processed independently by self-attention and then combined to form the final representation. We also add learnable weights to selectively attend to different chunks during the pre-processing stage.


## Chunks/Subsequences
A chunk is a set of contiguous tokens that appears together frequently. When splitting a long input sequence into small subsequences, we have to choose the appropriate chunk size. If the chunks are too small, then the model loses valuable information about global interactions between words. On the other hand, if the chunks are too big, then the model becomes less efficient and requires a lot of memory to store all the intermediate representations generated by each chunk. A typical value of the chunk size is around 512 tokens. 

For efficiency reasons, we typically use mini-batches while training our models. Within each batch, we randomly sample a subset of chunks that cover most of the input sequence. This helps ensure that every token in the input sequence gets considered by the model. While doing inference, we simply process the full input sequence by feeding the whole sequence through all the chunks sequentially. 

By adding learnable weights during the pre-processing stage, we allow the model to adjust its attention towards different types of subsequences within the input sequence. For instance, if there are no pronouns in the input sequence, then we might want to make sure that the model does not attend to any non-contextual subsequence containing them. Similarly, if there are fewer adjectives than verbs in the input sequence, then we might want to allocate some extra weight to the latter type of chunk. By incorporating these learned weights, the model is able to adapt its attention over the input sequence accordingly and improve its ability to generalize to unseen examples.

## Multi-head attention
Multi-head attention refers to an extension of self-attention that involves dividing the input sequence into multiple subsequences, performing self-attention on each subsequence individually, and then combining the resulting representations to form the final output representation. The intuition behind multi-head attention is that each head focuses on a distinct part of the input sequence and thus captures complementary aspects of the input sequence simultaneously. We hypothesize that this approach helps to enhance the expressiveness of the input sequence representation because it enables the model to consider multiple pieces of information present in the same place in the input sequence. Furthermore, we argue that learning separate attention heads can effectively reduce interference between the heads due to shared weights, leading to better transferability across related domains. 

To implement multi-head attention, we first split the input sequence into multiple subsequences called heads. Each head contains a fixed sized slice of the input sequence, which corresponds to a specific aspect of the input sequence being focused upon. We then perform self-attention on each head separately and concatenate the results to get the final output representation. The output vector for each head can either be weighted by learned weights or concatenated before applying activation functions. Finally, we combine the output vectors obtained from each head to obtain the final output representation by averaging or concatenating them depending on the choice of hyperparameters.

The main advantage of multi-head attention compared to standard self-attention is that it enables the model to gain insights from different aspects of the input sequence simultaneously, improving robustness and accuracy. Additionally, by dividing the input sequence into multiple subsequences, we enable the model to scale up the effective model depth and the diversity of attention patterns that it learns. Finally, the additional weight matrices introduced by multi-head attention help to address the vanishing gradients problem associated with long inputs, making it easier for the model to train even when the input sequence length is very large.