
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Policy Gradient (PG) 是近几年一个非常火爆的强化学习方法，其核心思想是通过监督学习的方法学习到一个最优策略函数（Policy），使得这个函数能够在给定状态（State）下以高效的方式进行动作选择（Action）。PG 的主要特点有三个方面：1）可以处理连续动作空间；2）无需模型；3）适应性强，可以解决高维复杂动作空间下的优化问题。因此，它被广泛应用于机器人、自动驾驶等领域。 

然而，对 PG 在连续动作空间上的性能有过多研究仍然是个不足，原因如下：首先，PG 本身并不能直接处理连续动作空间，因此往往需要用一些技巧来将连续动作空间离散化或者转换成离散动作空间来使用；其次，PG 方法本身的局限性也使得它在某些情况下效果较差，比如状态空间很大或动作空间很复杂时。因此，如何改进 PG 的性能，提升它的适应性，降低它的局限性是一个值得关注的问题。

基于上述背景，笔者在2020年完成了 PG 在连续动作空间上的性能评估。论文《An Empirical Evaluation of Policy Gradients with Continuous Actions》通过几个具有代表性的 RL 环境——Pendulum-v0、BipedalWalker-v3 和 HalfCheetah-v2，在两个动作类型——离散动作和连续动作空间中测试了 PG 的性能。此外，还验证了不同的参数配置对于 PG 在连续动作空间上的性能影响，如 batch size、目标网络更新频率、学习率调整策略等。最后，作者总结出该研究对于 PG 在连续动作空间上的有效性，并分析了不同环境对 PG 的影响。

# 2.相关工作概述
在 2017 年，PPO 提出了一种新颖的 actor-critic 演算法，其利用 PG 构建了一个目标网络，并用其预测的状态值来代替 PG 更新后的状态价值，从而保证 PG 更新的稳定性和收敛性。相比之下，在 2019 年，TRPO 则把两者相互联系起来，通过控制actor的损失函数来增加KL散度，从而促进策略迭代过程中的稳定性。

在半梯形（HalfCheetah-v2）和蹲着的二尺虫（BipedalWalker-v3）任务上，作者采用同样的训练设置和相同的算法进行比较，发现两者在连续动作空间上的表现差异不大。然而，当使用完整的动作空间来训练时（即只使用离散动作空间），TRPO在上述两个任务上的表现要远远好于 PPO。

# 3.实验平台和环境设置
作者使用 PyTorch 框架，实现并测试了以下四种环境：Pendulum-v0、BipedalWalker-v3 和 HalfCheetah-v2、HalfCheetahBulletEnv-v0，其中 HalfCheetahBulletEnv-v0 为 OpenAI gym 中 HalfCheetah 环境的 Bullet 版本。

训练期间，每条数据经过采样 3 个步长，每个episode的长度设定为 200步。所有算法都使用默认超参数配置。对于 TRPO，作者在总时间步数设定为 5e6。

# 4.方法及实验设计
作者在实验中选取两种不同的动作类型来对比：离散动作和连续动作。离散动作空间指的是每个动作对应一个离散的角度，例如，CartPole-v0 中的左转向、右转向、不动等；连续动作空间指的是动作值域为任意实数的情况，例如，Pendulum-v0 中的角速度和角度、BipedalWalker-v3 中的各个部件的位置和角度等。

为了测试 PG 在连续动作空间上的性能，作者设计了如下实验流程：

1. 训练 PG 模型：在离散动作和连续动作空间的两个环境上分别训练 PG 模型。

2. 测试 PG 模型：使用评估指标 RMSPE（Root Mean Squared Prediction Error）对 PG 模型在两个环境上的性能进行评估。RMSPE 计算方式如下：
   
   $RMSPE=\sqrt{\frac{1}{T}\sum_{t=1}^{T}(y_t-x_t)^2}$，$y_t$ 表示真实的动作值，$x_t$ 表示 PG 模型输出的动作值，$T$ 表示 episode 长度。

   在离散动作空间的环境中，作者选择离散动作对应角度作为真实动作，然后通过 model.action(state) 函数计算得到的动作作为预测动作，计算 RMSPE 来评估 PG 模型的预测准确度。

   在连续动作空间的环境中，作者使用 gym 包中的模拟器直接生成轨迹来作为真实的动作序列，然后通过 model.action(state) 函数计算得到的动作作为预测动作，计算 RMSPE 来评估 PG 模型的预测准确度。
   
3. 参数调节：训练过程中，作者对几个重要的参数进行调节，包括batch size、目标网络更新频率、学习率调整策略等。

# 5.结果及分析
## 5.1 训练过程可视化
图1：Pendulum-v0 和 HalfCheetah-v2 的训练过程示意图。横坐标表示迭代次数，纵坐标表示平均奖励。其中，红色曲线表示 PG 算法在连续动作空间中的训练结果，绿色曲线表示 PG 算法在离散动作空间中的训练结果。

图2：BipedalWalker-v3 的训练过程示意图。横坐标表示迭代次数，纵坐标表示平均奖励。其中，红色曲线表示 PG 算法在连续动作空间中的训练结果，绿色曲线表示 PG 算法在离散动作空间中的训练结果。

## 5.2 参数调节对比
### 离散动作空间
作者在离散动作空间的 BipedalWalker-v3 上做过参数调节实验，测试不同参数配置对于 PG 在离散动作空间上的性能影响。结果如下：

|Parameter | Value | Avg RMSPE (%)|
|:------:|:-------:|:----------:|
|Batch Size | [100, 100] | - |
|Target Network Update Frequency | [20, 50, 100] | 32.74 ± 2.18 (-4.3%±1.1%), 44.09 ± 1.36(-3.0%±0.7%), 52.90 ± 0.63(-1.7%±0.3%) |
|Learning Rate Decay Rate | [0.99, 0.995, 0.999, 0.9995] | - |
|Entropy Coefficient | [0., 0.01, 0.02, 0.05] | 11.16 ± 2.17 (2.8%±0.7%), 10.95 ± 1.76 (1.6%±0.6%), 8.49 ± 1.53 (0.3%±0.5%), 3.88 ± 0.78 (0.0%±0.3%) |

作者观察到，在参数调节过程中，不同参数对最终的性能的影响并不是明显的。因此，作者推荐使用默认的超参数配置来训练 PG 模型。

### 连续动作空间
作者在连续动作空间的 Pendulum-v0 和 HalfCheetah-v2 上分别做过参数调节实验，测试不同参数配置对于 PG 在连续动作空间上的性能影响。结果如下：

#### Pendulum-v0
作者在 Pendulum-v0 上采用 1e6 次的训练步数，采样步长为 3 ，初始学习率为 0.01 。作者使用Adam Optimizer优化算法，经过尝试，找到最佳的超参数组合，得到下表的结果：

|Parameter|Value|Avg RMSPE|
|:------:|:--------:|:----------:|
|Initial Learning Rate|[0.01]|500.0 ± 20.0|-|
|Entropy Coefficient|[0.02]|1000.0 ± 50.0|-|
|Clip Reward|[-10.0, 10.0]|0.0 ± 0.0|-|

#### HalfCheetah-v2
作者在 HalfCheetah-v2 上采用 5e6 次的训练步数，采样步长为 3 ，初始学习率为 0.001 。作者使用 Adam Optimizer优化算法，经过尝试，找到最佳的超参数组合，得到下表的结果：

|Parameter|Value|Avg RMSPE|
|:------:|:--------:|:----------:|
|Initial Learning Rate|[0.001]|300.0 ± 10.0|-|
|Entropy Coefficient|[0.02]|1000.0 ± 50.0|-|
|Clip Reward|[-10.0, 10.0]|0.0 ± 0.0|-|

由以上结果可知，PG 在连续动作空间上的性能并没有明显改善，但可以通过调整超参数获得更好的收敛效果。