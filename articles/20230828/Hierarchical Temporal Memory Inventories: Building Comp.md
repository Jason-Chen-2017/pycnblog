
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Hierarchical Temporal Memory (HTM) is a computational model of cognitive architecture that enables learning and reasoning based on sequential data, in contrast to classical neural networks which operate on scalar inputs or spike trains. HTMs have proven to be effective for solving complex problems such as pattern recognition, prediction and control tasks by processing sequential input patterns in an efficient way. 

However, the key challenge of building large-scale HTMs with billions of neurons remains how to represent and store long sequences of input-output pairs efficiently while still maintaining scalability and efficiency. Among other issues, this representation problem can lead to high memory usage and slow execution times when dealing with millions or even billions of neurons. This becomes particularly critical for applications where the sequence length may vary significantly between examples, such as speech recognition or natural language understanding.

To address these challenges, we present Hierarchical Temporal Memory Inventories (HTMI), a new approach to build complex HTM models from simple input-output pairs without using any explicit temporal structure information. Instead, HTMI constructs multiple HTMs at different granularities over short time intervals called inventories, each trained separately using only a small subset of the available training data points. The resulting hierarchical organization of HTMS allows them to adaptively learn complex representations of longer sequences while remaining computationally efficient and scalable enough to handle large datasets. 

In addition, HTMI provides several advantages compared to conventional approaches, including improved robustness due to its adaptive nature and ability to adapt to different types of input data distributions and varying lengths of sequences. We evaluate our method against existing HTMs and convolutional neural networks on various real-world benchmark tasks, showing that it consistently outperforms both baselines in terms of accuracy and scalability. Finally, we demonstrate how HTMI can effectively deal with dynamic input distribution changes during inference, leading to significant improvements in performance.

# 2.背景介绍
The evolution of artificial intelligence has led to numerous breakthroughs in machine learning and deep learning techniques. However, they often require massive amounts of labeled training data to achieve satisfactory results. These developments motivated researchers to consider alternative ways to learn complex data structures from unstructured and heterogeneous sources of knowledge, such as raw data streams.

One promising approach to enable learning from streaming data is Hierarchical Temporal Memory (HTM). It operates on sequential input data by considering the past, current, and future contexts around individual events, similar to how humans process visual information and make inferences about ongoing events. HTMs are highly flexible and can capture both local and global dependencies within the input data, enabling them to solve challenging problems such as predicting stock prices, recognizing patterns in text corpora, and controlling robotic systems. Despite their potential power, however, building large-scale HTMs requires careful consideration of algorithm design and storage requirements, limiting their practical use beyond simple classification and regression tasks.

To address this limitation, recent works have proposed methods to construct HTMs incrementally over short time windows called "inventories". These inventories consist of collections of HTMs at different levels of granularity, each specialized for handling a specific range of time scales, allowing them to adaptively learn complex representations of larger sequences without relying on the availability of explicit temporal structure information. By organizing HTMs into multiple layers, HTMI can provide substantial benefits over traditional approaches like bagging and boosting, reducing the risk of overfitting and improving generalization capabilities. Additionally, HTMI offers automatic detection of variable-length input sequences and adapts the inventory sizes accordingly, providing better robustness against noisy or incomplete input signals. 

Despite their significant impact, HTMI remains underexplored, mostly due to two factors. Firstly, most previous work focuses on developing algorithms for constructing one single HTM at a fixed level of granularity, whereas HTMI considers HTMs at multiple levels of granularity simultaneously. Secondly, there exist few benchmarks specifically designed to measure the quality of HTM-based models in situations where the input distribution changes dynamically over time. To fill this gap, we propose a novel benchmark suite, DYNAMIC HTM BENCHMARKS, consisting of dynamic variants of standard sequence modeling tasks, such as speech recognition or natural language understanding, where the input signal evolves over time. Our experiments show that HTMI outperforms state-of-the-art HTMs on all test cases, including those that require online adaptation to change queries over time.