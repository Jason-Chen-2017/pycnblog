
作者：禅与计算机程序设计艺术                    
                
                
支持向量机（Support Vector Machine）是一种分类模型，它是一种优秀的机器学习技术。在实际应用中，该模型可以有效地解决多维空间的线性、非线性分类问题。它的主要特点如下：
- 高度灵活的核函数：支持向量机不仅能够处理线性可分问题，而且还可以根据训练数据中输入数据的非线性关系对数据进行自动化的映射。通过选择合适的核函数，支持向量机可以在高维空间内实现非线性分类。目前常用的核函数包括线性核函数、径向基函数、多项式核函数等。
- 对异常值不敏感：支持向量机采用了惩罚项的方法使得模型对异常值不敏感。当模型判断某个样本不是训练集中的正负例时，就会给予它一个小于等于1的惩罚值。因此，异常值不会影响最终的分类结果。
- 支持向量到直线的最佳超平面：支持向量机的最终分类结果往往是一个凸优化问题，通过求解相应的最优化问题，就可以得到最佳的超平面。而支持向量机通过将输入空间中的每个点都映射到特征空间，从而可以更准确地找到最佳的超平面。
基于以上特点，支持向量机已经被广泛用于图像识别、文本分类、生物特征分析、股票市场分析、网络安全等领域。
# 2.基本概念术语说明
## 2.1 概念及术语
### 2.1.1 SVM
支持向量机（Support Vector Machine）是一种二类分类器，它由两组几何间隔边界定义的区域组成，间隔边界线分割空间为两类空间。其学习策略是最大化两个类的间隔边界的距离，并使得这两个类的间隔尽可能大。间隔最大化导致存在一些样本点处于间隔边界上或靠近间隔边界的方向，称为支持向量(support vector)，即使把这些样本点去掉也不能改变决策面。

### 2.1.2 定义域、范围、特征空间
定义域：输入空间X={x|x∈R^n}；输出空间Y={y|y∈{-1,+1}}。其中，X为输入空间，Y为输出空间，x ∈ R^n 为输入向量，y ∈ {-1,+1} 为输出变量。

范围：由于SVM是二类分类器，因此Y只能取两种取值{+1,-1}。

特征空间：特征空间F={f|f(x)≤0, x∈X}, 在输入空间X的基础上给出了一个新的空间。对于某些输入向量x，在特征空间F下，存在着一个超平面w*x+b=0，通过这个超平面可以将X划分为不同的区域。如果超平面的法向量w≥0，则把x分配到一类，否则分配到另一类。因此，特征空间就是从输入空间X到超平面的映射。

### 2.1.3 超平面
超平面：在特征空间F下的一个曲面，由w*x+b=0表示，w为单位向量，b为偏置。超平面决定了特征空间F下点x到超平面的距离，在超平面上的点叫做支持向量。

### 2.1.4 支持向量
支持向量：在二类分类问题中，超平面有一个重要作用：给定任意一个新的数据点，可以计算该点到超平面的距离。而只有距离超平面很远的样本才是决定分类的关键。因此，距离超平面很远的点就成为支持向量。

## 2.2 数学公式
### 2.2.1 二类分类问题的拉格朗日形式
给定训练数据集T={(x_i, y_i)}, i=1,2,...,N，其中xi∈R^n为输入向量，yi∈{-1,+1}为输出变量。目标是求解能将xi映射到超平面上的最优参数w*x+b, s.t.对所有i,y_i*(w*x_i+b)>1,i=1,2,...,N。其中，y_i*(w*x_i+b) > 1,i=1,2,...,N是拉格朗日乘子。

此处省略公式，直接进入优化问题的求解过程。

### 2.2.2 拉格朗日对偶问题
为了求解上面拉格朗日问题，需要先求解原始问题的对偶问题：

max L(w,b) = max Σ_{i=1}^{N}[y_i(w*x_i+b)-1]+λ∑_{j=1}^N[||w||^2]
s.t. y_i*(w*x_i+b)>1,i=1,2,...,N, ||w||<=C, |β|<C.

其中，λ>=0为正则化系数，C>0是松弛变量。此处省略约束条件的具体含义。

### 2.2.3 KKT条件
KKT条件是求解最优化问题的必要条件之一，具体地，假设最优化问题满足KKT条件：

1. 残差：对于所有的i,0<α_i<C,有r_i=y_i*(w*x_i+b)-1。

2. 互信息：对于所有的i!=j,有：I(y_i;y_j)=−ln((1-η)/η)/(2*π*η^2)*(1/φ(ε)+(ε^2/(2*φ(ε))-1)*δ(z_i,z_j))/ϕ(θ(ε)), φ(ε)和ϕ(θ(ε))分别是密度函数和累积分布函数。

3. 奥卡姆剃刀：至少要有一个α_i非零，并且β不等于0。

在SVM的最优化问题中，经过KKT条件检验后，可以求解原始问题的对偶问题。

## 2.3 求解SVM的优化问题的几种方法
### 2.3.1 分页算法
分页算法（PAC）：利用逐步迭代的方式，一步一步逼近全局最优解。它首先初始化一些可行的解，然后对每次迭代进行三个步骤：

1. 拟期望步长法（Proximal Gradient Descent with Line Search）。首先随机选取一个初始点，然后沿着梯度下降的方向走一段距离，再用线搜索的方法来调整步长，保证收敛到最优解。这个方法的好处在于快速，只需极少的时间即可收敛到最优解。

2. 交替方向法（Coordinate Ascent）。首先确定哪个变量具有正方向，第二步确定下一个变量，第三步重复。

3. 满足条件的解法（Feasible Solution）。当第一次收敛到一个可行解之后，就可以将其作为起始点，继续进行下一步迭代。每一步迭代都会增加计算量，但每一步的收敛率会随着时间的推移而减小，最终会收敛到一个满意的解。

### 2.3.2 拉格朗日对偶问题
SMO（Sequential Minimal Optimization）算法是用于求解SVM的优化问题的一种方法。它的基本思路是在一定的次序序列里，每次固定两个变量，求解其他变量的值。由于每次变量的选择都是独立的，因此能较好的控制目标函数的变化。同时，通过引入惩罚项，使得稀疏解比较难出现，从而达到稀疏化的效果。

### 2.3.3 拉格朗日对偶问题的求解算法
# 3.案例实操——利用SVM进行手写体数字识别

