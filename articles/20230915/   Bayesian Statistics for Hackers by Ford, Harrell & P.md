
作者：禅与计算机程序设计艺术                    

# 1.简介
  

## 1.1 Bayes概率公式的由来
贝叶斯定理（Bayes' theorem）是一个关于conditional probability的经典理论。在其最早的形式中，贝叶斯定理描述了在已知某事件A发生的情况下，如何计算后验概率P(B|A)。该定理的重要性不亚于它的历史地位。它首次提出是在19世纪末期，由艾尔弗雷德·贝叶斯、约翰·哈依利、乔治·皮耶尔开创。贝叶斯定理的证明曾长期困扰着统计学家们。直到19世纪末，才有人基于上述定理，提出了著名的“贝叶斯公式”，解决了贝叶斯定理的计算复杂度太高的问题。随后的几百年里，贝叶斯定理一直是统计学研究的基石之一。
## 1.2 基本概念术语说明
### 1.2.1 概率空间
从数学上来说，一个概率空间（probability space）由两个集合组成：事件集合Ω和样本空间Σ。事件Ω是一个样本空间Σ上的子集，表示可能的事件。例如，在抛硬币问题中，Ω为{正面，反面}，Σ为{0，1，2，3，4，5，6}。
### 1.2.2 随机变量
设X为一个随机变量，并假设其取值可以用p_x(x)来表示。则：
1. X是Ω的非空子集。也就是说，X中的每一个元素都属于Ω；
2. 如果X的任何两个元素之间都没有联系，那么它们的联合分布就是全概率公式。即：
p(x,y)=p(x)p(y)
其中p(x), p(y)分别为X，Y的概率分布，且两者满足：
* 对于任意事件z∈Ω，p(x,y)>0；
* p(x∩y)=p(xy)，即两个事件同时发生的概率等于单独发生的概率之积；
* (x∪y)-p(x∩y)=p(x)+p(y)-p(xy) ，即事件X或Y发生而另一个事件不发生的概率等于两个事件都不发生的概率加起来减去两个事件同时发生的概率；
3. X的条件概率分布为：
p(y|x)=p(y,x)/p(x)
其中，p(y,x)表示事件X和Y同时发生的概率；p(x)表示事件X发生的概率；p(y|x)表示事件Y在事件X发生的条件下发生的概率。换句话说，条件概率是表示事件Y依赖于事件X的概率。
### 1.2.3 边缘概率
设X为一个随机变量，P(X=x)表示事件X=x发生的概率。则：
* 边缘概率P(X=x)是非负实数。
* 对任意事件Y，有：
P(X=x|Y)=P(X=x, Y)/P(Y)
* X的边缘分布（marginal distribution）是指对所有其他变量的边缘化（marginally）。即：
p(x)=\sum_{y}\int_{Y}p(x,y)dy
其中，π(x,y)表示事件X=x并且Y=y同时发生的概率。
### 1.2.4 独立性
若两个随机变量X和Y相互独立，则存在一个函数f，使得：
p(x, y)=f(x)f(y)
其中，f(x)表示随机变量X的概率密度函数。换句话说，两个随机变量X和Y之间只有相关性，与他们各自单独发生的概率无关。如果两个随机变量X和Y独立，那么根据贝叶斯定理的独立性假设，它们的联合分布就是：
p(x, y)=p(x)p(y)
# 2.基本算法与例子
## 2.1 朴素贝叶斯分类器
### 2.1.1 朴素贝叶斯分类器模型
假设给定训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}，其中xi=(x1,x2,...,xn)称为输入向量，yi∈{c1, c2,..., ck}是输出类别标签。朴素贝叶斯分类器的目标是学习一个函数h：X→C，即输入向量到输出类别的映射，其中C={c1, c2,..., ck}为类的集合。h(x)的输出表示输入向量x属于哪个类别C中的概率最大。具体地，h(x)可以通过以下算法得到：

1. 根据训练数据集T估计先验概率分布Pi(i):
    * Pi(ci)=N(c=ci)/(N)，其中Ni表示属于类别ci的训练样本数目。
2. 利用贝叶斯定理估计条件概率分布Pj(j|i):
    * Pj(xj|ci)=((xij∈Ci)*1+(xij∉Ci)*0 + alpha)/(Ni+alpha*k)
      where:
        xij∈Ci 表示第j个特征属于第i个类别
        Ni    表示第i个类别的样本数量
        k     表示总的特征数量
        alpha 是Laplace平滑项参数
3. 最终，朴素贝叶斯分类器的决策规则是：
    * h(x)=argmax[P(Y=cj|X=x)]
      where:
        j = 1, 2,..., k   # 所有特征索引
        cj = 1, 2,..., m  # 每个类别索引
### 2.1.2 朴素贝叶斯分类器应用
#### 2.1.2.1 spam邮件过滤
给定一封新的邮件，判断它是否为垃圾邮件，可以运用朴素贝叶斯分类器。首先收集一份具有垃圾邮件和非垃圾邮件的大型邮件数据集，然后将邮件分为两种类型：文本格式和HTML格式。这些邮件中都含有一些词汇，如“viagra”、“free”等。对于训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈{html, text}, yi∈{spam, not_spam}。通过朴素贝叶斯分类器，我们可以对新邮件进行分类。首先根据训练数据集估计先验概率分布Pi(i):
* Pi(spam)=SpamCorpus/TotalCorpus
* Pi(not_spam)=NotSpamCorpus/TotalCorpus
其中，SpamCorpus表示垃圾邮件的数据个数，NotSpamCorpus表示正常邮件的数据个数，TotalCorpus表示邮件数据的总个数。接着，利用贝叶斯定理估计条件概率分布Pj(j|i):
* Pj(vj|ci) = ((vj∈Ci)*1 + (vj∉Ci)*0) / (Sum of all features in email from Ci and Spam emails)
其中，vj代表email的第j个feature, Sum of all features in email from Ci and Spam emails代表与第i类同时出现的特征的个数（记为Nij）。
最后，朴素贝叶斯分类器的决策规则是：
* h(x) = argmax[P(Y=spam|X=text)P(X=text)]
  * argmax表示选择概率最大的那个类别作为分类结果
  * P(Y=spam|X=text) = P(X=text|Y=spam) * P(Y=spam) / P(X=text)
    * P(X=text|Y=spam) = Count of all words from email which are present in spam emails but not present in normal emails / Number of spam words
    * P(Y=spam) = Pi(spam)
    * P(X=text) = P(X=text|Y=spam) * P(Y=spam) + P(X=text|Y=normal) * P(Y=normal)
  * P(X=text) = Probability that given word v appears in any document out of total number of documents
  
#### 2.1.2.2 缺失数据处理
对缺失数据进行处理的方法有很多种，其中朴素贝叶斯分类器是一种常用的方法。假设要处理的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}，其中xi为包含缺失值的输入向量，yi∈{c1, c2,..., ck}。由于数据缺失，我们无法直接训练朴素贝叶斯分类器，但是可以通过利用贝叶斯定理估计条件概率分布Pj(j|i)，估计缺失值处的条件概率分布，再结合所有特征的条件概率分布，估计缺失值处的类别。具体地，过程如下：

1. 在训练数据集T中，假设xi有mi个缺失特征。
2. 根据训练数据集T估计先验概率分布Pi(i):
    * Pi(ci)=N(c=ci)/(N)，其中N为训练数据集T中属于类别ci的样本数目。
3. 通过贝叶斯定理估计条件概率分布Pj(j|i)的准确方式，需要引入拉普拉斯平滑项alpha:
    * Pj(xj|ci)=((xi(j)∈Ci)*1 + (xi(j)∉Ci)*0 + alpha)/(Nij + alpha*ki)
      where:
        xi(j)∈Ci 表示第j个特征属于第i个类别
        Nij      表示第i个类别中特征xi(j)的个数
        ki       表示第i个类别的特征数量
        alpha    是Laplace平滑项参数
4. 为了估计Pj(j|i)，可以采用EM算法（Expectation Maximization algorithm），即重复迭代两个步骤直至收敛。
   a. E-step: 在E步，利用当前的参数估计缺失特征的条件概率分布Pj(j|i)。具体地，利用贝叶斯定理，求：
     Pj(xj|ci)=((xi(j)∈Ci)*1 + (xi(j)∉Ci)*0 + alpha)/(Nij + alpha*ki)
   b. M-step: 在M步，根据E步所获得的条件概率分布Pj(j|i)重新估计参数Pi(i)和alpha。具体地，估计：
     * Pi(ci)=N(c=ci)/(N)
     * alpha = Laplace smoothing parameter estimation method such as Add-one smoothing or Jeffrey's prior
   c. 判断是否收敛，如果达到预设的停止条件，则结束迭代。
5. 最终，朴素贝叶斯分类器的决策规则是：
    * h(x)=argmax[P(Y=cj|X=x)]
      where:
        j = 1, 2,..., mi   # 所有特征索引
        cj = 1, 2,..., m  # 每个类别索引

## 2.2 隐马尔可夫模型
### 2.2.1 隐马尔可夫模型简介
隐马尔可夫模型（hidden Markov model，HMM）是关于时序数据的一套统计模型，主要用来做序列标注任务，即给定观测序列O，推断出隐藏状态序列Q。用观测序列表示序列中每个位置的观测值，用状态序列表示序列中每个位置的隐状态。HMM由初始状态分布π、状态转移概率矩阵A、观测概率矩阵B组成。具体来说，假设观测序列为O=(o1, o2,..., on)，状态序列为Q=(q1, q2,..., qn)，其中qi∈{1, 2,..., K}，表示第i个观测值对应的隐状态。HMM的目标是求出观测序列O的联合概率分布P(O, Q)。HMM的训练目标是极大似然估计：

L(π, A, B|O) = ∏i∑n􏰄∏jπ(qj)A(qi,qj)B(oi|qj)

其中，A是K×K的矩阵，表示状态转移概率，pi(qj)是初始状态分布，B是K×V的矩阵，表示观测概率，Bi(oj|qj)是观测值oj对应的状态qi的观测概率。
### 2.2.2 HMM应用举例
#### 2.2.2.1 手写数字识别
假设有一个包含28×28像素的图片，可以用不同颜色或纹理的黑点或白点来表示图像中的黑色或白色像素，也可以用灰度值表示每个像素的亮度。可以将这个二维图像看作一个序列，每个元素对应一幅图上的一个像素点。使用隐马尔科夫模型，可以对此序列进行建模，并找出隐藏状态序列。具体地，给定观测序列O，定义状态为{0, 1, 2,..., 9}，K=10，对应10个可能的数字。则有：
* π=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  # 初始状态分布
* A=[[0.7, 0.1, 0.1], [0.1, 0.7, 0.1], [0.1, 0.1, 0.7]]  # 状态转移概率
* B=[[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], 
     [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],
    ...
     [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]]  # 观测概率
则，可以计算：
* P(O,Q) = ∏i∑n􏰄∏jπ(qj)A(qi,qj)B(Oi|qj)
* P(O,1) = π1*B(1)^1 * (B(2)|B(1))^0 * (B(3)|B(1),B(2))^0.... B(n)|B(1),B(2),..., B(n-1))^0
* P(O,2) = π2*B(1)^0 * B(2)^1 * (B(3)|B(2))^0 *(B(4)|B(2),B(3))^0.... B(n)|B(2),B(3),..., B(n-1))^0
*...
* P(O,10) = π10*B(1)^0 * B(2)^0 * B(3)^0 * B(4)^0 *... B(n)^0 

#### 2.2.2.2 拼写检查
给定一个错别字的单词w，拼写检查系统希望正确的拼写词w'，并给出改进建议。可以将拼写检查问题看作一个序列标注任务，即给定正确的拼写词w及候选拼写词列表W，推断出正确的拼写词。可以使用隐马尔可夫模型对拼写检查问题建模。给定观测序列O=(o1, o2,..., on)，状态序列Q=(q1, q2,..., qn)，K=(|w|+1)表示不同的状态。则有：
* π=[1/(|w|+1), 1/(len(W)+1),..., 1/(len(W)+1)]  # 初始状态分布
* A=[[(1/(|w|+1))*prob(a->a), prob(b->a)*1/len(W),..., prob(t->a)*1/len(W)], 
     [(prob(a->b)*1/len(W)), (1/(|w|+1))*prob(b->b), prob(c->b)*1/len(W),..., prob(t->b)*1/len(W)],
    ...
     [(prob(a->t)*1/len(W)), prob(b->t)*1/len(W),..., prob(t->t)*1/len(W)*(1/(|w|+1))]
   ...]  # 状态转移概率
* B=[[prob(o1|a), prob(o2|a),..., prob(on|a)], 
     [prob(o1|b), prob(o2|b),..., prob(on|b)],
    ...
     [prob(o1|t), prob(o2|t),..., prob(on|t)] 
    ]  # 观测概率
则，可以计算：
* P(O,Q) = ∏i∑n􏰄∏jπ(qj)A(qi,qj)B(Oi|qj)
* P(O,a) = (1/(|w|+1))*prob(o1->a)
* P(O,ab) = prob(o1->a)*prob(o2->b) * (1/len(W))
*...
* P(O,w') = sum(prob(o1->wi-1)*prob(o2->wi)*...prob(on->wn)

#### 2.2.2.3 汽车电池寿命预测
给定一辆车的状态，包括故障信息、速度、位置等，可以用隐马尔可夫模型对其寿命进行预测。给定观测序列O，定义状态为{0, 1, 2, 3}，K=4，分别表示不正常、正常、易耗品、过保修状态。则有：
* π=[0.3, 0.3, 0.3, 0.1]  # 初始状态分布
* A=[[0.8, 0.1, 0.1, 0.0], [0.1, 0.8, 0.0, 0.1], [0.0, 0.1, 0.8, 0.1], [0.1, 0.1, 0.1, 0.8]]  # 状态转移概率
* B=[[0.9, 0.05, 0.05, 0.0], [0.05, 0.9, 0.05, 0.0], [0.05, 0.05, 0.9, 0.0], [0.05, 0.05, 0.05, 0.9]]  # 观测概率
则，可以计算：
* P(O,Q) = ∏i∑n􏰄∏jπ(qj)A(qi,qj)B(Oi|qj)
* P(O,0) = π0*B(0)^1 * (B(1)|B(0))^0 *(B(2)|B(0),B(1))^0 *(B(3)|B(0),B(1),B(2))^0 *(B(4)|B(0),B(1),B(2),B(3))^0 
* P(O,1) = π1*(B(1)^1 * (B(2)|B(1))^0 *(B(3)|B(1),B(2))^0 *(B(4)|B(1),B(2),B(3))^0 )/(B(0)^1 * (B(1)|B(0))^0 *(B(2)|B(0),B(1))^0 *(B(3)|B(0),B(1),B(2))^0 *(B(4)|B(0),B(1),B(2),B(3))^0 ) 
* P(O,2) = π2*(B(2)^1 * (B(3)|B(2))^0 *(B(4)|B(2),B(3))^0)/(B(0)^1 * (B(1)|B(0))^0 *(B(2)|B(0),B(1))^0 *(B(3)|B(0),B(1),B(2))^0 *(B(4)|B(0),B(1),B(2),B(3))^0 ) 
* P(O,3) = π3*(B(3)^1 * (B(4)|B(3))^0)/(B(0)^1 * (B(1)|B(0))^0 *(B(2)|B(0),B(1))^0 *(B(3)|B(0),B(1),B(2))^0 *(B(4)|B(0),B(1),B(2),B(3))^0 ) 

其中，B(i)为观测值Oi对应的状态Qi的观测概率。