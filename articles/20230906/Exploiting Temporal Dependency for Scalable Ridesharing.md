
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Ride-sharing apps are increasingly used to meet people’s transportation needs by providing a platform where users can request and offer transport services such as taxi or shared ride car pooling. However, these platforms suffer from scalability issues due to their large user base and high demand for real-time service availability. To address this problem, we propose a novel deep reinforcement learning approach that exploits temporal dependency among users’ requests and offers during the time of app deployment. Specifically, we use a Q-network based policy gradient algorithm to learn policies that optimize for both immediate rewards and long term cumulative rewards obtained through exploration of future actions. We evaluate our method on several datasets collected over a period of two years and show significant performance improvements compared to existing methods in terms of deployment cost reduction, waiting time reduction, and fairness across different types of users. Furthermore, we demonstrate the effectiveness of our method in reducing load and traffic congestion on Google Maps by improving the system’s reliability and reduce average travel times by up to 40%. Finally, we present insights into how users perceive and interact with the deployed ride-sharing app, which helps us identify opportunities for further improvement.

In this paper, we explore using deep reinforcement learning techniques to deploy efficient and reliable ride-sharing apps at scale. Our goal is to improve overall efficiency and revenue potential while ensuring user satisfaction and quality of service. By leveraging temporal dependencies among user requests and offers, we aim to address scalability issues caused by the massive amount of data involved in making transport decisions. We introduce a new model called TGAC-Net that uses multi-agent reinforcement learning to train agents simultaneously to maximize mutual cooperation. In particular, each agent learns independently but collaborates with others to select optimal actions under uncertainty. Using a combination of imitation learning, exploration strategies, and demonstrations, we achieve competitive results in solving complex transportation problems and deploying highly available and scalable ride-sharing apps at low cost. Overall, our work demonstrates the importance of exploring temporal dependency between users' requests and offers when designing effective solutions for scalable ride-sharing app deployment, and provides valuable insights into how users interact with the resulting systems.


# 2.相关工作
To date, there has been limited research in developing scalable and efficient ride-sharing apps that provide real-time service availability to millions of users. Existing approaches typically rely on centralized routing algorithms to direct vehicles towards destinations based on static maps. While they enable high service availability, the increased computing complexity and operational costs make it challenging to maintain real-time updates and handle fluctuating traffic conditions. 

Recent advancements in artificial intelligence have made it possible to develop automated decision-making mechanisms that can effectively manage dynamic traffic situations. Although successful applications include self-driving cars and automatic taxi allocation systems, these technologies require specialized hardware and software infrastructure. Additionally, most of these systems lack the capability to adapt to rapid changes in user preferences and demands. 


Our proposed solution combines multiple deep reinforcement learning techniques with imitation learning to overcome these limitations. First, we employ multi-agent reinforcement learning to train multiple autonomous agents jointly to minimize congestion and fulfill all user requests. Second, we use an LSTM-based model to encode temporal information about individual users’ historical behavior, allowing us to capture patterns and trends in user preferences and anticipate upcoming events. Third, we combine imitation learning and a hierarchical reinforcement learning architecture to leverage expert knowledge and past experience to guide training. This allows us to solve more difficult tasks without requiring extensive amounts of labeled data.

We also experimented with using supervised learning models instead of RL algorithms. These models could be trained offline using historical data to predict the likelihood of a customer requesting or offering a specific vehicle type or route, enabling the app to allocate vehicles to customers according to predicted preferences. However, these models were unable to generate near-real-time predictions necessary for maintaining high service availability. Thus, we chose to focus on a hybrid approach combining deep reinforcement learning and imitation learning.



# 3.动机与目标
Ride-sharing apps are increasingly used to meet people's transportation needs by providing a platform where users can request and offer transport services such as taxi or shared ride car pooling. However, these platforms suffer from scalability issues due to their large user base and high demand for real-time service availability. To address this problem, we propose a novel deep reinforcement learning approach that exploits temporal dependency among users' requests and offers during the time of app deployment. Specifically, we use a Q-network based policy gradient algorithm to learn policies that optimize for both immediate rewards and long term cumulative rewards obtained through exploration of future actions. We evaluate our method on several datasets collected over a period of two years and show significant performance improvements compared to existing methods in terms of deployment cost reduction, waiting time reduction, and fairness across different types of users. Furthermore, we demonstrate the effectiveness of our method in reducing load and traffic congestion on Google Maps by improving the system's reliability and reduce average travel times by up to 40%. Finally, we present insights into how users perceive and interact with the deployed ride-sharing app, which helps us identify opportunities for further improvement.




# 4.设计与实现
## 4.1.目标与评价指标
We aim to deploy efficiently and reliably ride-sharing apps at scale. To do so, we first need to understand what factors influence the choice of vehicle, distance traveled, fare amount, and arrival time of riders? Based on empirical observations, we found that key factors including travel demand, personal preferences, current traffic conditions, driver ratings, and last mile connectivity play crucial roles in determining rider choices. Therefore, our main objective is to build an AI-powered decision support tool that takes into account these factors to deliver better transportation options to riders.

To measure the success of our solution, we will compare its effectiveness against four alternative approaches:

1. Baseline heuristic approach - This approach assigns one single vehicle to each rider, optimizing for the shortest travel time and highest average rating received from drivers.

2. Random assignment approach - This approach randomly allocates vehicles to riders until no unallocated riders remain. It assumes that riders follow similar tactics and makes the same decision every time.

3. Greedy matching approach - This approach assigns the next best match of driver and rider whenever possible. It only considers the closest pair of matches, ignoring any additional matches beyond the nearest neighbor.

4. Historical trip data approach - This approach builds upon historical records of trips performed by riders, predicting the likelihood of each rider choosing a certain vehicle type, distance traveled, fare amount, and arrival time. The probability estimates are then combined with the rider's personal preferences to assign vehicles.

We consider three evaluation metrics:

1. Average Travel Time Reduction - This metric measures the total gain in travel time achieved by the deployed app versus the baseline approach. 

2. Total Fairness Improvement - This metric measures the percentage change in the distribution of vehicle usage rates across the user population before and after deployment.

3. Effective Vehicle Usage Rate - This metric measures the percentage of times that a vehicle was actually assigned to at least one passenger, regardless of whether or not the passengers requested or offered the vehicle themselves.

## 4.2.系统模型
To implement our solution, we developed a multi-agent reinforcement learning framework that incorporates both deep neural networks and Monte Carlo Tree Search. Each agent represents a rider and consists of a policy network and a value network. The policy network outputs an action given a state, which specifies the chosen vehicle, distance traveled, fare amount, and arrival time. The value network evaluates the expected reward of the next state given the current state and action taken by the agent. Both networks are updated periodically using stochastic gradient descent to ensure convergence.

To train the agents, we use the Monte Carlo Tree Search (MCTS) algorithm to simulate various scenarios of individual user interactions with the deployed application. For example, we can assume that some riders accept requests to pool their vehicles, whereas others choose to wait out the waiting line if other riders are already in the pool. Given a set of observed states, actions, and rewards, MCTS searches the space of possible actions and generates rollouts that represent the potential outcomes of executing those actions. Agents take these rollouts as inputs and update their policy and value functions accordingly.

To enhance the computational efficiency of the framework, we implemented distributed processing on Amazon Web Services (AWS). Each instance runs the simulation code and stores intermediate results in a database. A separate client node accesses the database and aggregates the results to obtain the final output. This approach reduces the required computation time and enables us to run simulations faster than running locally.

Additionally, we implemented batch normalization layers and dropout regularization to prevent overfitting. We reduced the number of parameters in our neural networks to increase their expressivity and prevent overfitting. Moreover, we utilized the tensorboard package to visualize the progression of the learning process and track the performance of the agents.