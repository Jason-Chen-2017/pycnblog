
作者：禅与计算机程序设计艺术                    

# 1.简介
  

- 时代背景
  - 数据爆炸时代（数据处理、计算能力爆炸）
  - 互联网创业浪潮（市场竞争激烈）
  - AI革命（让机器学习应用到各个领域）
- 对工程师的要求
  - 没有头绪？不知道自己该做什么？
    - 不断积累经验、实践出真知
    - 不断发现问题、思考解决方案、改进
    - 接受新事物、开拓眼界
    - 持之以恒、不断突破、不断进步
  - 以问题为导向、视野开阔？
    - 从问题切入，清晰定义目标、路径规划
    - 拥抱变化，拥抱开源、云计算、无服务器
    - 多角度看问题，结合不同维度、工具、框架
    - 深耕细作，边做边学、边分享
  - 科研型工作？
    - 积极主动、持续投入，有所作为
    - 培养自我驱动力、好奇心
    - 有自由时间、生活技巧、健康生活
    - 有助于提升职场竞争力、个人能力

# 2.Core Concepts and Terms
## Algorithmic thinking 
- Divide and Conquer Strategy: divide the problem into smaller subproblems recursively until they are small enough to solve directly; then combine the solutions of those subproblems by applying a certain strategy such as merge sort or quicksort, which can have linear time complexity in the worst case scenario.

- Dynamic Programming Strategy: use memoization (caching) to store intermediate results so that we don't need to recalculate them again and again during later computations. This technique has exponential time complexity but provides very efficient solution for some problems with overlapping subproblems. It is particularly useful when solving optimization problems where many subproblems share common substructures. 

- Greedy Strategy: choose locally optimal choices at each step based on the current state, without considering the future consequences. The algorithm will often produce an approximate optimal solution within a reasonable amount of time. However, it may not always give the exact optimal solution and requires careful analysis to determine if its efficiency gains offset any potential losses from approximation errors.

- Branch and Bound Strategy: explore all possible branches of the search tree and select the one(s) that lead to the global optimum. The algorithm maintains two data structures: a priority queue and a set of visited nodes, which helps it avoid redundant calculations and prune unnecessary branches. The branching factor of the search space grows exponentially with the size of the problem instance. This approach is suitable for problems with complex constraints or nonconvex objectives, such as the traveling salesman problem or job shop scheduling.

## Data Structures
### Arrays
An array is a collection of elements of same type, typically integers or floating point numbers, referenced by contiguous memory locations. An element can be accessed using its index, which gives its position in the array. In Python, arrays are implemented as lists.

The time complexity of accessing an element in an array is O(1), i.e., constant time. However, inserting or deleting elements at the end of the array takes O(n) time due to the requirement to shift all subsequent elements downward. There are several ways to improve performance when working with large arrays, including:

- Using linked list instead of an array when dealing with sequences of unsorted items. Linked lists allow efficient insertion and deletion operations at both ends of the sequence.

- Sorting the array before searching or modifying it to achieve logarithmic time complexity. Merge sort or quicksort algorithms provide this capability efficiently.

- Implementing dynamic resizing to reduce wasted space and increase capacity as needed. A commonly used scheme involves doubling the array capacity whenever it becomes full and halving it back to original size when empty.

### Hash Tables
A hash table (also known as map or dictionary) is a data structure that maps keys to values. Each key is associated with a value via a unique index generated by hashing function applied to the key. The mapping between keys and indices is managed by a hash function and collision resolution method. If multiple keys generate the same index, there is a conflict and the hash table must resolve it using various techniques such as chaining or open addressing.

In general, lookup, insert, and delete operations in a hash table take average O(1) expected time on average, assuming that the number of buckets is sufficiently large compared to the number of entries. When the load factor exceeds a certain threshold, the hash table needs to be resized, which causes the execution time to become proportional to the number of entries in the new bucket array. To minimize the probability of resizing, it's important to choose a good hash function and proper collision handling schemes.

Hash tables are ideal for fast lookups, even when the number of collisions is high, since the likelihood of finding an entry by traversing the chain of linked records is low. On the other hand, hash tables require extra storage for managing collisions, increasing the overhead compared to simpler data structures like trees or arrays. Moreover, modifications to existing entries also cause entire chains of records to be searched and copied, making hash tables less suitable for frequent updates. Therefore, hash tables should only be used for scenarios where frequent retrieval dominates the cost of modification.

### Heaps / Priority Queues
A heap is a binary tree where each node has a value greater than or equal to its children. The largest element in the heap is called the root. Heap sort uses heaps to sort a given list in ascending order. Unlike sorting algorithms that rely on comparisons to exchange elements, heaps operate by swapping elements around the heap to maintain their relative positions. Popping the smallest (or largest) element off the top of the heap takes O(log n) time. Inserting an element into the heap takes O(log n) time too. Thus, heap sort is an efficient alternative to sorting algorithms like bubble sort or selection sort that perform comparisions and exchanges between elements.

Heaps are especially well suited for implementing priority queues, where the highest-priority task is dequeued first. Applications include network routing protocols, resource allocation, game play simulation, etc.