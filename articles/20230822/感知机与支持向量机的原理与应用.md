
作者：禅与计算机程序设计艺术                    

# 1.简介
  

感知机（Perceptron）是一种二类分类器，输入空间(feature space)上定义的超平面将数据划分为两类，基本假设就是输入特征之间存在某种线性关系。它的学习方式是基于错误的数据样本进行修正。其基本形式是一个线性函数$f(\mathbf{x})=\sum_{i=1}^n w_ix_i+b$,其中$\mathbf{w}=[w_1,\cdots,w_n]^T$是权重参数，$\mathbf{x}=[x_1,\cdots,x_n]$是输入向量，$b$是偏置项。通过误分类的数据点$(\mathbf{x},y)$对权重进行更新，使得$f(\mathbf{x})>0 \Leftrightarrow y=1$, $f(\mathbf{x})<0 \Leftrightarrow y=-1$. 在监督学习中，训练数据由输入向量组成$\{\mathbf{x}_1,\cdots,\mathbf{x}_N\}$及对应的输出值$y_1,\cdots,y_N$,感知机的目标是在给定的输入空间上找到一个能够将正负实例分开的超平面，即$y_i f(\mathbf{x}_i)>0$, 而$y_j f(\mathbf{x}_j)<0$, 如果可以找到这样的超平面，则称该感知机是非欠约束的（non-parametric）。

支持向量机（Support Vector Machine, SVM）是一种二类分类器，它利用核函数将原始空间中的数据映射到高维特征空间，从而实现非线性分类，并达到更好的分类效果。SVM的学习方法主要基于优化目标函数，即求解最大间隔区分超平面的问题。具体地，首先选择线性不可分的数据集，在此基础上构造一个高维的特征空间，然后使用核函数将原始空间数据映射到特征空间。用损失函数对原始空间、特征空间和映射后的特征空间中的点进行评估，选取具有最小损失的映射作为最优解。具体来说，SVM首先确定基函数的参数，使得在特征空间内的所有点都成为支持向量，即对每个训练样本$\mathbf{x}_i$，满足条件：
$$
y_i (\sum_{j=1}^{m}\alpha_jy_j\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)+b)=1, i = 1,2,\cdots,N; j = 1,2,\cdots,m
$$
其中$\alpha_i$是拉格朗日乘子，$\phi(\mathbf{x})$表示对输入向量$\mathbf{x}$进行的特征变换。损失函数是Hinge损失函数：
$$
L(\alpha, b) = \frac{1}{N_s}\sum_{i=1}^{N_s}(\max(0, 1-y_i(\sum_{j=1}^{m}\alpha_jy_j\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)+b))) + \lambda ||\alpha||^2
$$
其中$N_s$表示支持向量的个数，$\lambda > 0$是正则化参数。

SVM的学习过程是极大化最小化这个损失函数的过程。对于某个训练实例$\mathbf{x}$,若其不属于任何支持向量，那么它的拉格朗日乘子$\alpha_i$就会等于零。如果$\alpha_i$不等于零，那么就意味着该训练实例是支持向量，且它的方向和距离超平面的距离是确定的，所以当它与其他支持向量的距离缩小时，损失就会增加，直到所有支持向量都满足条件$\sum_{j=1}^{m}\alpha_jy_j\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)+b=0$. 从而获得了支持向量机模型。

# 2.算法流程图示
下图展示了SVM的算法流程图示：

1. 输入：训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$,其中$x_i \in R^{n},y_i \in {-1,1}$.
2. 初始化：
   - 设置阈值参数$C>0$,用于控制软间隔最大化准则或硬间隔最大化准则。
   - 随机初始化权重向量$\theta=(\theta^{(1)},\theta^{(2)},..., \theta^{(p)})$
   - 将$\theta$拆分为两部分$u$,$v$:
      * $u=(u^{(1)},u^{(2)},..., u^{(p)})$,其中$u^{(j)}\in R^{n}$
      * $v \in R$,用于标志位移
3. 对第$t$轮迭代:
   1. 对第$i$个实例$x_i$，计算输出值$\hat{y}=sign(\sum_{j=1}^{p}\theta_ju^{(j)}^Tx_i+\theta_{p+1})\equiv h_{\theta}(x_i)$.
   2. 根据训练数据的实际标签$y_i$，计算实际输出值$\tilde{y}=\pm 1$.
   3. 更新$\theta$：
       * $\theta_k:= \theta_k-\eta(y_i(\sum_{j=1}^{p}\theta_ju^{(j)}^Tx_i+\theta_{p+1}-1))_+\mu v$
       * 其中$\eta$是步长因子,$(+)_+$是符号函数。
   4. 计算$L(\alpha,b)$的值并判断是否收敛，若收敛则结束训练。
   5. 当遇到新的数据点$(x_{new},y_{new})$时：
       1. 计算输出值$h_{\theta_{new}}(x_{new})=\sum_{j=1}^{p}\theta_{new,j}u^{(j)}^Tx_{new}+\theta_{new,p+1}$.
       2. 判断其是否满足支持向量的定义式，若满足，则添加；否则删除当前$\theta$的一个元素，再重新执行第4步。

# 3. 其他概念说明
## 支持向量
支持向量是定义在边界上的点，即满足$\sum_{j=1}^{m}\alpha_jy_j\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)+b=0$的点。这些点既能最大化间隔也能保证可行性，因此它们对后续学习至关重要。由于其位置在边缘处，所以它们对预测结果影响较小。通常情况下，通过对训练数据的局部扰动，可以得到几何最优解，即形成一个凸壳。SVM算法通过硬间隔最大化策略寻找合适的超平面，因此只有支持向量才可能构成凸壳。如果某个样本点没有落入支持向量所定义的半径内，说明该点在超平面之外，不能作为支持向量，容易发生过拟合现象。另一方面，对于异常值点，只要设置合理的惩罚参数$C>0$，就可以提升其影响力，限制其对分类的贡献。
## 多核技巧
SVM利用核函数将输入空间映射到特征空间，进而使用核函数的变换矩阵来处理非线性分类问题。核函数的变换矩阵可以通过多项式核或径向基函数等多种方法来生成，这些核函数是线性不可分的，但是可以用内积的方式来表示。通过这种方式，可以将线性不可分的数据集映射到线性可分的数据集，从而实现非线性分类。核技巧虽然能够提升分类性能，但同时需要更多的计算资源来生成核函数矩阵，导致SVM在训练时的时间复杂度高于传统方法。