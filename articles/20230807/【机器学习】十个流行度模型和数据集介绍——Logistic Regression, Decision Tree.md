
作者：禅与计算机程序设计艺术                    

# 1.简介
         
1959年，罗恩·罗兰提出了著名的Logistic回归模型，它用来解决分类问题。至今仍然是一种常用的统计方法。其基本思想是：给定一个样本，通过计算样本属于某一类的概率，预测该样本所属的类别，即判定模型(discriminant model)。比如，对于手写数字识别任务来说，通过计算手写数字图片中的像素值与阈值的差距，可以预测该图像中是否有明显的边缘、字符、线条等特征，从而实现不同类的分割，并赋予合适的标签。 Logistic回归是最简单的判定模型之一，但实际应用中，一些更复杂的模型会得到更好的结果。
         1985年，苏力生等人提出的决策树模型(decision tree)则被广泛使用。它的基本思想是基于若干特征划分子空间，通过找到最优的切分方式，将输入空间划分成互不相交的区域，使得各区域内部的分布尽量一致，不同区域之间的分布也尽可能不同。决策树模型广泛用于分类、回归、聚类、关联分析等领域，在实际应用中得到了很好的效果。
         1993年，雷德罗·布鲁克斯提出了随机森林(random forest)算法，它是多棵树的集成学习方法。与单个决策树算法相比，随机森林在训练阶段生成了一组可靠的决策树，然后再用多数表决的方法对这些决策树进行综合。因此，随机森林能够降低模型方差，防止过拟合。在很多机器学习竞赛中，随机森林往往取得了不俗的成绩。
         2001年，梯度提升(gradient boosting)被提出作为集成学习的代表模型，它主要由决策树组成，每一颗树都有一定的权重，先学习一颗粗糙的树，然后利用残差(residual error)拟合后面的树。因此，GBM在一定程度上克服了传统决策树容易产生过拟合的问题。
         2006年，周志华教授提出了K近邻(k-Nearest Neighbors, KNN)算法，它是一种简单而有效的非参数化的分类方法。KNN算法采用测地距离(Euclidean distance)或其他距离函数，根据输入向量附近的K个邻居点的类别，对输入向量进行分类。KNN算法可以融入到各种监督学习任务中，如分类、回归、推荐系统等。
         1974年，贝叶斯学派的朴素贝叶斯算法被提出，它是一种计算概率分布的通用方法，具有广泛的应用前景。朴素贝叶斯算法的基本思路是在所有已知的样本中，计算每个样本所属的类别的先验概率，然后基于这些信息来计算后验概率，最后根据后验概率选择最大的那个类别作为当前样本的预测类别。
         20世纪90年代末期，SVM(support vector machine, 支持向量机)逐渐成为一种流行的机器学习算法。它是一种二类分类模型，通过求解一个最佳的超平面来定义输入空间的边界，从而对输入实例进行分类。SVM模型可以有效处理高维数据，并且在训练过程中，可以自动选取核函数(kernel function)，使得优化问题变成凸优化问题。
         2014年，Hochreiter及其同事提出了长短时记忆网络(Long Short Term Memory Network, LSTM)，它是一种递归神经网络，其特色在于能够学习时序依赖关系。LSTM可以学习到数据的长期历史信息，并且通过隐藏层的输出控制信息的流动方向。
         2012年，神经网络ConvNets[4]横空出世。CNN采用卷积(convolutional)运算替代全连接层，通过局部感受野的提取建立特征图，再通过池化(pooling)层进一步提取全局特征。它可以有效地抓取到局部的模式信息，且参数共享使得计算复杂度大幅减少。CNN在图像分类、目标检测、语义分割等任务上均有着不错的效果。
         2014年，Facebook AI Research开发出以GAN为代表的生成对抗网络，这是一种基于神经网络的无监督学习方法。在训练阶段，它生成新的样本，在测试阶段则判断新生成的样本是真实还是伪造。GAN模型能够实现图片、文本、音频等复杂数据的高质量生成，极大地推动了深度学习的研究和创新。