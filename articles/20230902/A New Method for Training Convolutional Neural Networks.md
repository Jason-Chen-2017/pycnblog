
作者：禅与计算机程序设计艺术                    

# 1.简介
  

随着摄像头、智能手机等新型设备的普及，高速图像处理和图像识别技术已经成为当下最热门的研究方向之一。近年来，卷积神经网络(CNN)作为当今最具代表性的深度学习模型，在图像识别领域取得了极大的成功。但是，如何高效地训练大规模的CNN模型，仍然是一个重要的问题。
最近，李飞飞团队提出了一套新的训练CNN模型的方法——跳级链接，即将一个CNN分层训练，将不同层之间的连接关系通过参数共享的方式进行优化。该方法有效地减少计算量，同时提升准确率。本文中，作者总结并分析了现有的几种训练CNN模型的方法，并分析跳级链接方法的优点和局限性，最后给出了一种新的训练方式，即纵向跳级链接（vertical skip connections）。
# 2.相关工作介绍
目前，已有许多方法可以用来训练CNN模型。如AlexNet、VGG、GoogLeNet、ResNet等等。这些方法都具有良好的效果，但它们往往需要较多的计算资源。而且，一些方法对训练过程的控制能力也不够强，不能适应复杂的任务。比如，一些方法仅仅调整网络结构或数据增广方式，而忽视了超参数调节、正则化项、损失函数设计、学习率衰减策略、梯度裁剪等其他一些关键因素的影响。因此，如何有效地训练大规模的CNN模型，仍然是一个重要的研究课题。
# 3.主要贡献与创新点
目前，CNN模型的训练存在两个瓶颈：一方面，训练计算代价大；另一方面，容易过拟合。为了解决这两个问题，一种新的训练方法——纵向跳级链接（vertical skip connections）被提出来，这是一种将不同层之间的连接关系通过参数共享的方式进行优化的训练方法。该方法使用跳跃链接把相邻两层之间的特征图直接联系起来，并将它们连接在一起，实现信息共享。这种方法可以在一定程度上缓解过拟合问题。此外，它还能够降低计算负担。通过增加跳级链接，作者提升了CNN的深度、宽度、感受野等参数，并且能够有效地训练到更大的规模上。作者也分析了当前存在的训练CNN模型的方法，并总结其局限性，如性能损失、网络稀疏等，并提出了一个新的训练方式——纵向跳级链接（vertical skip connections），能够有效地提升性能和效率。
# 4.实验设置与结果
作者在ImageNet ILSVRC 2017数据集上评估了两种不同的训练方式。第一种是采用标准方法（无跳级连接），第二种是采用纵向跳级连接的方法。实验结果表明，两种方法均能够有效地训练CNN模型。从分类性能上看，采用纵向跳级连接的方法能够达到更好的效果。由于使用了纵向跳级连接的方法，训练速度要快于无跳级连接的方法。另外，使用纵向跳级连接的方法还能够有效地降低过拟合问题，并且能够加速收敛。
# 5.讨论与结论
本文首次提出了一种新的训练CNN模型的方法——纵向跳级连接（vertical skip connections），这种方法能够有效地解决过拟合问题，提升模型的深度、宽度、感受野等参数，并且训练速度快。作者发现，现有的训练CNN模型的方法虽然能够达到很好的效果，但它们都需要较多的计算资源，耗费时间长，对于大规模的数据集的训练效果并不是特别好。因此，本文提出的纵向跳级连接方法能够有效地缓解过拟合问题，提升模型性能。同时，该方法还能有效地降低计算负担，减小训练时间，适用于大规模的图像识别任务。最后，作者总结了现有的几种训练CNN模型的方法，并分析了跳级链接方法的优点和局限性，并且给出了一个新的训练方式——纵向跳级连接（vertical skip connections）。基于这个新的训练方式，作者还在ImageNet ILSVRC 2017数据集上评估了两种训练方式，验证了其有效性，并取得了良好的效果。