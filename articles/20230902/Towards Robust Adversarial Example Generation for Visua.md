
作者：禅与计算机程序设计艺术                    

# 1.简介
  

图像识别系统的性能受到广泛关注。但现有的方法仍然存在脆弱性，攻击者可以轻松地通过对抗样本进行错误分类、漏洞等安全隐患。为了进一步提高图像识别系统的鲁棒性，作者们开发了一种新的对抗示例生成方法。他们证明，这种方法能够在分类模型中生成具有鲁棒性的对抗样本，从而对抗机器学习模型中固有的对抗攻击。作者还展示了其有效性和实用性。
# 2.相关工作
图像识别系统通常依赖于深度神经网络(DNN)进行分类。训练好的模型用于预测输入图像所属的类别或标签。黑盒攻击通过给出对抗样本集合，研究人员可以评估这些模型的防御能力。因此，已有很多研究工作试图设计对抗样本来对抗现有的模型。一些研究已经提出了对抗样本的生成技术，如FGSM、PGD、CWL2等，它们都是基于梯度的对抗样本方法。除此之外，还有研究论文将目标函数替换成交叉熵损失，并针对不同数据集进行调整，提升模型的鲁棒性。但是，这些方法仍然无法完全抵御对抗样本攻击。另一种方式是利用正则化项来限制模型的复杂度，通过限制模型参数范围来减少对抗攻击的空间。然而，这种方法只能缓解部分对抗攻击，并且容易受到欠拟合或过拟合的问题。因此，作者们采用了一个更加创新的方法，即去构造一个平滑的分类器，并利用该分类器生成对抗样本。
# 3.问题定义
攻击者对计算机视觉系统进行恶意攻击，包括对抗样本攻击和非对称模型攻击等。对抗样本攻击指的是黑客在不知情的情况下篡改正常的样本的标签，目的是让被分类的模型产生误判。非对称模型攻击则是攻击者根据正常样本和对应分类结果，构造一种假冒合法的模型，使得正常样本被判定为目标类别，而实际上却是随机猜测的类别。由于对抗样本攻击往往比非对称模型攻击更加容易制造，因此现有的防御方法不足以应对对抗样本攻击。为了提高系统的鲁棒性，作者希望开发一种新的对抗样本生成方法，能够在分类模型中生成具有鲁棒性的对抗样本，并探索如何改进该方法提升鲁棒性。
# 4.方法论
作者设计了一种新的基于优化的对抗样本生成方法，即去噪攻击。这种方法通过最小化分类器的误差来构造对抗样本。首先，作者设计了去噪攻击算法，它是一个迭代过程，每一步都从原始图片构造一组对抗样本，然后利用目标模型判断这些样本是否是合法的（即正确类别）。对于每个新构造的对抗样本，算法会更新模型的参数，使得它在下一次迭代中偏向于正确的类别，并降低其他类的置信度。算法运行多次后，便会收敛到最佳参数值。
为了生成对抗样本，算法需要两步。第一步是利用噪声扰动初始化原始图片。第二步是执行一步迭代过程，产生一个新的图片。具体算法如下：

1. 初始化原始图片x0。
2. 对原始图片进行添加噪声（有利于使得模型难以判别），得到x1。
3. 将x1输入分类器得到输出y。
4. 如果输出y为原始类别l，则跳过第五步；否则执行第五步。
5. 对当前类别l的概率分布P[i]进行蒙特卡洛采样。其中，i=1,...,c，表示分类标签，c为模型预测类别的个数。
6. 根据蒙特卡洛采样的结果，计算下一个类别的标签k。
7. 通过图像的像素变化的方式，将x1中的l类转换为k类，并赋值给x2。
8. x2送入分类器得到输出y。
9. 如果输出y为k类，则重复第3-8步，直至找到一个合法的对抗样本。
10. 重复1-9步，直至找到n个合法的对抗样本。

如图1所示，该算法生成了多个对抗样本，其中一些具有较大的改变，且对原始模型的分类准确率无影响。
# 5.实验结果及分析
作者在ImageNet数据集上进行了实验，实验结果表明，该方法能够有效地生成对抗样本。该方法能够发现多种类型的对抗样本，且对抗样本的效果很好。作者还比较了不同的噪声水平下的对抗样本生成效果，噪声水平越低，生成的对抗样本越容易被分类器分错。最后，作者利用Defense Against Adversarial Attacks (DAA)基线测试了作者的方法，结果显示作者的方法比传统方法更加有效。
# 6.代码实现