
作者：禅与计算机程序设计艺术                    

# 1.简介
  

支持向量机（support vector machine）是一种监督学习方法，属于分类模型。其由<NAME>提出，在机器学习界十分受欢迎，它具有以下特点：

1、非线性数据集上表现优异；

2、训练速度快；

3、有很好的处理高维度数据的能力；

4、模型参数可以解释；

5、对异常值不敏感。

感知机（perceptron）是一种无监督学习方法，属于分类模型。其由Rosenblatt提出，在神经网络和深度学习等领域也十分流行。

支持向量机和感知机都可以解决二类分类的问题，但它们之间又存在着本质上的区别。

基于这个原因，笔者认为需要先对两者进行基本的概念和术语的阐述，之后再进行更进一步的详细阐述。

首先，给读者们提供一些学习知识的必要准备工作：

1、了解监督学习与非监督学习的概念；

2、了解线性代数、概率论和信息论相关理论知识。

本文将围绕以上两个主题进行展开介绍。
# 2.基本概念与术语介绍
## 2.1 支持向量机（Support Vector Machine）
支持向量机（SVM）是一种监督学习方法，主要用来解决分类问题。它利用正负样本间的最长分离超平面来划分类别，其中最长分离超平面就是支持向量机的核心概念。

假设输入空间X中有n个点构成的数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X为输入变量，yi∈Y为输出变量，取值为+1或-1表示类别，例如，若yi=+1则表示实例xi属于正类（ positive class），否则表示实例xi属于负类（ negative class）。一般来说，支持向量机需要能够处理多维特征，因此，每个样本点xi可以是一个向量。支持向量机学习的目标是在保证较高的分类精度的前提下，使得决策边界（ decision boundary ）尽可能宽松，即能够将样本点完全正确地分隔开。

支持向量机由定义在数据空间中的一个最大间隔的分离超平面所组成，最大间隔意味着支持向量机把不同的类别用较大的间隔分开，这样的超平面能够最大化地将训练样本分隔开。而且，在最大间隔确定的情况下，超平面的方程是唯一的，这就保证了支持向量机的简单性和鲁棒性。

直观地说，支持向量机的优化问题可以理解为寻找一个超平面（分割超平面），能最大化地将正类样本和负类样本分隔开。

对于一个训练数据集D={x(i),y(i)}，其中xi∈X为输入变量，yi∈{-1,1}，表示实例xi是否属于正类（positive class），则问题可转换为求解如下约束最优化问题：

min ∥w∥

s.t. y(i)(w·x(i)+b) ≥ 1, i = 1 to m (1)

w·x(i)+(b)=y(i), i = 1 to m (2)

其中，w=(w1,w2,...,wn)^T为超平面的法向量，|w|=1为超平面长度，b为超平面的截距项。为了方便起见，引入拉格朗日乘子α(i)，且令α(i)≥0。那么，带入约束条件(1)到(2)中得到的优化问题变为:

min L(w,b,α)

s.t. y(i)(w·x(i)+b)-α(i) ≥ 1, i = 1 to m 

y(i)(w·x(i)+b) + α(i) - b = 0, i = 1 to m 

i = 1 to m

βj(w) = max{0,1-y(i)(w·x(i)+b)} for i in Mij and j = 1 to K-1

where βj is a slack variable that represents the degree of violation of the constraint on the δ(Mij)th example of class Cj if w were not the optimal solution. In this case we want to minimize the largest possible value of β over all examples from different classes. The objective function now consists of two terms, one corresponding to the loss term which measures the error made by the hyperplane model, and another corresponding to the regularization term which prevents overfitting. To optimize these two terms separately, we need to add more constraints to the problem formulation as follows: 

0 ≤ α(i) ≤ c(m), i = 1 to m 

where c(m) is a user specified upper bound on the sum of the alphas across all training data points. This allows us to control the complexity of the model by setting an upper bound on how much each training instance can contribute to the optimization process. Finally, we define ε > 0 as the margin parameter which controls the tradeoff between keeping the hyperplane as simple as possible while still achieving good generalization performance on unseen test instances. Given the above conditions, we then derive the final form of the optimization problem: 

max J(w,b) = sum(α(i))-ε/2||w||^2

such that 0 ≤ α(i) ≤ c(m), i = 1 to m;

y(i)(w·x(i)+b)-α(i) ≥ 1-ε, i = 1 to m ;

βj(w) = max{0,1-y(i)(w·x(i)+b)} for i in Mij and j = 1 to K-1;

where the set Mij contains the indices of the misclassified examples belonging to class j when w is used to predict their labels. This ensures that the optimization does not get stuck in local minima. Additionally, note that the bias term has been added into the equation of the decision surface so that it becomes non-degenerate even if there are no support vectors that lie exactly on the decision surface.