
作者：禅与计算机程序设计艺术                    

# 1.简介
  

## 概念定义
对抗样本（adversarial example）是一种在测试时很容易被误认为真实样本的扰动输入，通过对抗训练，可以使模型更难分辨出真实样本和其对应的对抗样本，从而提升模型的鲁棒性、健壮性及防御能力。

针对该问题，机器学习研究者提出了各种对抗样本生成的方法，其中比较成功的是FGSM(Fast Gradient Sign Method)方法，通过梯度下降法更新神经网络的参数，将图片像素点的梯度值设置为目标标签，生成对抗样本，使得模型很难分辨出原始样本和生成的对抗样�之间的差别。近年来，随着对抗攻击的不断演进，人们越来越认识到这种攻击方法存在一定的局限性和缺陷，也逐渐提出新的对抗样本生成方法，如PGD(Projected Gradient Descent)，DropOut等。近年来，越来越多的研究人员开始关注对抗样本的生成、检测、识别，并应用于图像领域、文本领域、序列领域。随着对抗样本生成方法的不断涌现、完善和部署，对抗样本成为当前热门研究课题。

## 相关工作概述
### 对抗样本的生成和检测
目前，已有的对抗样本生成方法主要基于梯度下降（FGM）或改进的梯度下降（IG-FGM）。这些方法基于噪声扰动的导向，通过生成对抗样本往往具有较好的准确率，但生成过程需要时间成本较高。根据所使用的优化算法，有无监督攻击方法，有监督攻击方法，半监督攻击方法，预训练方法等。

### 对抗样本的识别
一般来说，对抗样本的识别主要包括分类攻击和推理攻击两个方面。分类攻击旨在利用对抗样本对模型进行错误分类，使模型对手工制作的对抗样本分类错误，比如对手工制作的黑白化攻击样本。推理攻击旨在通过对抗样本构造对抗样本库，通过模型的决策边界判断其属于哪个类别。主要方法有基于黑盒攻击的黑箱模型检测和基于灰盒攻击的白盒模型检测。

### 对抗样本的应用
由于对抗样本具有高度敏感性，因此在传统的分类任务中存在对抗样本泛化性差的问题。为了缓解这一问题，Google团队提出了一种新型的对抗样本的生成方法AutoAugment，基于强化学习的算法，能够自动生成有效的对抗样本。该方法通过对训练数据的某些统计信息进行探索，将数据分布划分为多个子集，然后分别用不同的操作去对抗每个子集，生成不同的对抗样本，提高模型的泛化能力。近期，Facebook团队也提出了一种新的方法Understanding-Distortion in Adversarial Examples，它能够通过对抗样本的视觉特征，发现模型在处理对抗样本时的行为和能力上的一些偏差。


# 2.基本概念术语说明
## 扰动（perturbation）
扰动是指对原始样本的某个维度加上一个噪声，该噪声影响原始样本的预测结果。直观地说，如果噪声足够小，则会导致模型预测结果发生变化；如果噪声足够大，则会导致模型难以正确分类。对抗样本的生成即通过添加扰动的方式来干扰模型的预测，来达到对抗攻击的目的。

## 对抗训练（Adversarial Training）
在训练阶段，同时使用正常样本和对抗样本一起进行训练，可以提高模型的鲁棒性和健壮性，让模型更适应各种类型的扰动攻击。具体做法是在训练过程中，先使用正常样本对模型进行训练，然后使用对抗样本进行修正，提升模型的稳定性。

## 模型鲁棒性（Robustness）
模型的鲁棒性指模型对扰动容忍度的大小，当扰动幅度较小时，模型仍然可以良好工作，但当扰动幅度较大时，模型就会变得脆弱。模型的鲁棒性通常通过评估模型的鲁棒性程度来衡量，常用的衡量标准有模型平均分类误差（model averaged classification error），模型对抗样本鲁棒性（robustness against adversarial examples），对抗样本抗扰动能力（attack strength of the model）。

## 数据增强（Data Augmentation）
数据增强（data augmentation）是通过引入噪声来生成新的训练样本，目的是增加模型对于样本扰动的容忍度。

## 生成对抗网络GAN（Generative Adversarial Network）
生成对抗网络（generative adversarial network，GAN）由两部分组成：生成器（generator）和判别器（discriminator）。生成器是一个由生成网络生成输出的神经网络，其作用是将潜藏空间的数据转换为样本空间的数据。判别器是一个由判别网络评估生成样本和真实样本的神经网络，其作用是判断生成样本是否是真实的。生成器和判别器互相博弈，通过反复训练，最终使生成器的能力逼近判别器的能力，从而生成具有真实分布的数据。