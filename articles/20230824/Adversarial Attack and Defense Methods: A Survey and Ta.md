
作者：禅与计算机程序设计艺术                    

# 1.简介
  



Adversarial attack (AA) and defense methods (ADMs) are crucial for ensuring the security of machine learning systems from both adversaries with a limited access to the system and those who attempt to break into it. They play an essential role in achieving the goal of robustness against such attacks. In this survey paper, we provide a comprehensive overview on AA and ADMs along with their related applications. We also identify some common techniques that can be used for these tasks and categorize them according to specific methodologies, including black-box attacks, white-box attacks, grey-box attacks, transfer attacks, and defense strategies. Furthermore, we summarize recent research advances to understand the trade-offs among different AA and ADM approaches based on real-world constraints and practical considerations. Finally, we offer guidance towards future directions of study and research on these techniques. 

In summary, our work offers a systematic understanding of current state-of-the-art methods and trends in the field of ML-based adversarial attacks and defenses. It provides clear explanations and analyses of each technique's strengths and weaknesses. Moreover, it makes recommendations for selecting appropriate tools or methods to meet various application requirements while highlighting potential risks and challenges faced by practitioners. Our effort is intended to inspire developers, researchers, and industry leaders to advance the field of AI security through rigorous research in this important area. 


# 2.相关工作与背景介绍



Adversarial examples (AEs), i.e., input data modified intentionally to cause a neural network to misclassify its target label, have emerged as a critical vulnerability of deep neural networks in computer vision and natural language processing tasks [9]. Over the last several years, numerous studies have been conducted to develop new effective attacks against deep neural networks, ranging from one-step gradient descent based attacks to more sophisticated multi-step iterative attacks [10-12]. Recently, a number of works has focused on developing defensive algorithms against AE-based attacks, specifically targeted at preventing their use by malicious parties without knowledge of the underlying model architecture or training parameters [13,14]. Nevertheless, most existing reviews focus mainly on technical details of individual methods rather than covering an overview of relevant literature and identifying gaps and opportunities for further research. 



In addition, while there exists a general consensus that the development of efficient and accurate ML models requires increasing attention to security vulnerabilities, less attention has been paid to addressing exploitation and detection of such vulnerabilities, particularly from adversaries with limited resources and capability to exploit known vulnerabilities. This gap is especially acute when dealing with large scale industrial settings where access to infrastructure and devices is often limited, and malware targeting multiple enterprise assets at once may pose even greater threats. To address this challenge, several works have proposed adaptive defense mechanisms that adjust the behavior of the trained model over time based on observed patterns of normal behavior and suspicious activity [15-17], which require careful consideration of both accuracy and efficiency during inference time. Despite significant progress on these fronts, however, much of the previous work still focuses primarily on developing novel attacks against DL models and neglects broader issues surrounding adversarial attacks and defenses in general. As a result, there exist many open questions regarding the efficacy and effectiveness of existing defenses, reliability and interpretability of learned models under adversarial conditions, and potential privacy and security concerns related to deploying adversarial defense techniques in production environments.



To bridge this gap, we propose in this survey article to review the state-of-the-art methods in the context of adversarial attacks and defenses, drawing on the following perspectives: 

1. Comprehensiveness and clarity: We first provide an extensive discussion of the types of attacks and defenses and highlight the key differences between them. This enables readers to better understand how they relate to each other and choose suitable ones for different scenarios. For example, black-box attacks examine the internal structure of a model while white-box attacks modify the input or output directly. Grey-box attacks rely on either side information about the nature of the inputs or models, making them very useful in pruning out unnecessary features and improving computational efficiency. Transfer attacks aim to transfer attacker capabilities across domains or devices, whereas defense strategies operate at the level of a whole system and typically target intermediate layers or components of the model. Overall, this clarification helps ensure that readers can make an informed decision when applying different methods for solving the same problem.

2. Categorization and comparison: Second, we organize existing methods into eight categories according to their approach, design objective, and applicable domain. This allows us to compare and contrast similar approaches and formulate meaningful hypotheses for future research efforts. Additionally, it reduces redundancy and overlaps between different methods, reducing the chances of duplicating work and introducing biases.

3. Emphasis on performance: Third, we emphasize the importance of performance metrics and evaluations for evaluating the quality of ADM techniques. While traditional evaluation frameworks like FID and IS are popular in image synthesis and generation tasks, these do not always translate well to the complexities involved in adversarial examples and defenses. Instead, we suggest relying on standard benchmarks, such as ImageNet-A and CIFAR-10-C, and presenting results obtained using established evaluation protocols, such as L_infty and L_2 perturbation bounds. These comparisons help guide future research efforts toward finding promising techniques that achieve high levels of success across diverse benchmarks and hardware configurations.

4. Evidence-based suggestions: Fourth, we strongly encourage readers to carefully read papers and apply them to real-world problems to evaluate their effectiveness and limitations. Such experiments should involve a combination of both off-line and online evaluation techniques, including human annotations and quantitative analysis of failure modes. Additionally, we stress the importance of experimentation and iteration to improve the robustness and reliability of ADM methods, both in terms of sample size and complexity of the attack scenario. Finally, we recommend sharing best practices and insights learned from actual deployments to promote widespread acceptance of these methods and establish standards for measuring trustworthiness and utility.

5. Bridging theoretical and practical aspects: Fifth, we strive to draw connections between theory and practice, making sure that both fields are fully integrated together. Specifically, we discuss prospective directions for combining theory with practice in areas such as robust optimization, confidence estimation, and counterfactual reasoning. This will help inform future research directions in both fields.