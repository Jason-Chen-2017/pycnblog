
作者：禅与计算机程序设计艺术                    

# 1.简介
         

视觉transformers，又称为vit，是一个基于transformer模型的深度学习模型，它的架构类似于CNN，能够有效解决图像分类、目标检测等计算机视觉任务中所遇到的困难。然而，在模型训练时，往往需要较大的计算资源，同时也会消耗大量的时间，导致普通的GPU集群无法满足实时的需求。因此，作者提出了一种新的可压缩训练方法Low-Rank Compressible Training (LCT)，通过把参数矩阵分解成两个低秩矩阵，再把低秩矩阵投影到新的子空间上进行训练，从而可以减少参数矩阵大小、降低计算资源占用，同时保持模型精度不变。经过实验验证，作者的方法对各种任务（如图像分类、目标检测）都取得了很好的效果，并能在资源利用率和训练时间方面大幅度地提升，同时仍然保持模型的高性能。
# 2.相关工作与启发
Low-Rank Compressible Training(LCT) 是由Yan Zhang在他的论文 Low-Rank Tensor Approximation and Learning of Deep Neural Networks for Computer Vision 中提出的一种训练策略。其主要思想是在网络训练过程中对参数矩阵分解成两个低秩矩阵，再将低秩矩阵投影到新的子空间上进行训练。通过这种方式，可以极大地减少参数矩阵大小、降低计算资源占用，同时保持模型精度不变。 

最近，一种新的压缩感知训练方法ConvNet2SqueezeNet，被证明比其他压缩感知训练方法都要有效。该方法的基本思路是在训练过程中，先使用单层卷积核对输入特征图进行编码，然后再解码得到输出特征图。这个过程相当于对输入特征图进行降维，从而达到降低参数数量和计算量的目的。由于不需要反向传播求梯度，因此训练速度更快，且精度损失微乎其微。  

此外，有研究表明，VIT中使用的self-attention机制可以有效缓解梯度消失的问题。这种注意力模式可以帮助模型学习全局依赖关系并自我更新参数，而不是局部依赖关系。作者认为，这一点对于LCT方法同样重要，因为它能够使得子空间更加紧凑。  
# 3.主要贡献
本文的主要贡献如下：

1. 提出一种新颖的可压缩训练策略Low-Rank Compressible Training (LCT),通过把参数矩阵分解成两个低秩矩阵，再把低秩矩阵投影到新的子空间上进行训练。

2. 在图像分类、目标检测、多任务学习等多个视觉任务上进行了广泛的实验验证，发现LCT方法可以显著降低参数数量和计算量，并且保持模型精度不变。

3. 通过分析所得结果，发现LCT方法可以在资源利用率和训练时间方面取得显著的提升，并仍然保持模型的高性能。

# 4.方案概述
LCT方法的基本思路是，在模型训练过程中，先对参数矩阵分解成两个低秩矩阵$W^1 \in \mathbb{R}^{m\times n}$ 和 $W^2 \in \mathbb{R}^{n\times p}$,其中$m,n,p$分别表示参数矩阵$W \in \mathbb{R}^{k\times k}$ 的行列数目。假设原始参数矩阵$W$具有的秩为$\rho_W = rank(W)$，则存在一些系数$c_i$ 使得$\| W_{ij} - c_i (\sum_{l=1}^{\rho_W} |c_l|)^{-1}\|_{\infty} \leq \epsilon$,其中$\epsilon > 0$ 表示可接受的误差范围，$W_{ij}=0$ 表示$j$阶第$i$个元素无关，因此$W^{1}_{ij},W^{2}_{ij}$与$W_{ij}$近似。那么，我们可以按照下面的步骤进行训练：

1. 初始化两个低秩矩阵$W^1,W^2$，随机选择$n$个正态分布随机变量作为$W^1$的一半，剩下的部分初始化为零；

2. 使用梯度下降法迭代更新两个低秩矩阵$W^1$和$W^2$，即使用以下公式迭代更新：

$$
W^1 := W^1 - \alpha \frac{\partial L}{\partial W^1}(W^1+W^2\\
W^2 := W^2 - \alpha \frac{\partial L}{\partial W^2}((I-\beta^{T}H)W^2 + HW^1)\\
H:= \text{softmax}(\gamma^{T}W^1)
$$

其中，$I$ 为单位矩阵,$\beta \in \mathbb{R}^{n}$ 表示编码器的权重,$\gamma \in \mathbb{R}^{m}$ 表示解码器的权重,$\alpha>0$ 为学习速率,$H=\text{softmax}(\gamma^{T}W^1)$ 表示投影矩阵。



3. 最后，将两个低秩矩阵$W^1,W^2$投影回原来的参数空间$W$，得到最终的可训练参数矩阵$Z$:

$$
Z := P_W (I+\beta^{T}P_W^T)\begin{bmatrix} W^{1}\\ W^{2}\end{bmatrix}\\
P_W:=\text{svd}(W)\qquad \text{(Singular Value Decomposition)}\\
$$


4. 与传统训练方法相比，LCT方法在计算资源、内存占用以及稳定性方面都有明显的优势。首先，LCT方法仅需额外占用$np+mp$的存储空间和$np$的计算量，远小于传统方法需要的参数数量。其次，LCT方法采用了低秩分解的方式，可有效减少参数矩阵的稀疏性，并减少内存占用，进一步促进稳定性。最后，LCT方法允许用户控制隐含的超参数$\epsilon$，可根据实际情况调整，从而得到不同的模型性能。