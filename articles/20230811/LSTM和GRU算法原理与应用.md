
作者：禅与计算机程序设计艺术                    

# 1.简介
         

## 什么是LSTM？
Long Short-Term Memory（LSTM）是一种长短期记忆网络，它可以学习长时依赖关系。在传统的神经网络模型中，信息只能从输入层流动到输出层一次，因此对于时间序列数据缺乏长时记忆能力。而LSTM是为了解决这一问题而提出的，它可以解决上述问题，并在一定程度上弥补了传统神经网络模型中的缺陷。
## 什么是GRU？
Gated Recurrent Unit（GRU）是LSTM的变体，其结构和功能与LSTM类似，但它在计算门控单元的设计上做出了改进。GRU更加简单，训练速度也更快。
# 2.基本概念术语说明
## 1、时序信号处理
### 时域和频域
1、时域：指一段连续的时间或时间段。例如，我们每隔一秒钟采集的数据称为时域信号。

2、频域：指信号变化的频率范围。例如，语音信号的频域就是声音的变化频率。
### 2、 时延：时延是指接收端接收到信号与发送端发出信号之间的时间间隔。

3、 时移：指数据点之间的距离。例如，若采样率为Fs Hz，则一个采样周期为T=1/Fs s。则对同一个信号进行不同频率分辨的两种方式，它们对应的时移分别为Ts Ts / (2F) T=1/(2F*Fs) s,其中F表示采样频率。

## 2、正向传播和反向传播
在机器学习领域，正向传播（forward propagation）是指输入层向隐藏层传递信号；而反向传播（backward propagation）则是隐藏层向输出层传递误差信号。总的来说，正向传播是由外向内计算，逐层激活，最后得到输出值；而反向传播是由输出层向隐藏层传递误差信号，依据链式法则逐层修正权重，直至修正无误。
## 3、多项式拟合与回归分析
1、多项式拟合（Polynomial fitting）：多项式拟合是一种最简单的曲线拟合方法，它的目标是在给定的点集上找到一条最佳拟合多项式曲线。

2、回归分析（Regression analysis）：回归分析是一种统计分析方法，用于研究两个或多个变量间的相关性和因果关系。它主要用于预测和 forecasting，即用已知数据去推断未知数据。

## 4、DNN、CNN、RNN三种网络模型
### DNN（Deep Neural Network）：深层神经网络。

1、深度：指网络中隐藏层的数量。

2、非线性：指隐藏层中的节点使用非线性函数作为激活函数，使得网络能够学习复杂的模式。

3、多层：指网络具有多个隐藏层，每个隐藏层都可以学习复杂的特征。

4、全连接：指所有节点都是相互连接的，可以任意组合。

### CNN（Convolutional Neural Networks）：卷积神经网络。

1、卷积核：卷积核是一个小矩阵，通常具有高维空间上的局部联系性，通过滑动窗口进行扫描。

2、池化层：池化层是一种特殊的卷积层，用于降低网络参数个数、减少过拟合风险等。

3、滤波器：是卷积层的组成部分，用于提取图像中特定特征。

### RNN（Recurrent Neural Networks）：循环神经网络。

1、循环：指网络在每一时刻根据前一时刻的输出结果进行计算。

2、时序：指网络在处理序列数据，例如文本、语音、视频等。

3、双向循环神经网络：是RNN的变体，可以在不失真的情况下获取前向和后向的信息。

## 5、LSTM、GRU三种RNN结构
### LSTM（Long Short-Term Memory）：长短期记忆网络。

1、长期记忆：指网络可以保存较久远的状态信息，例如文字识别中的“忘记”、图片理解中的“跟随”。

2、短期记忆：指网络能够快速更新当前的状态，保持灵活性，例如视频中的运动检测。

3、遗忘门控制：指网络的遗忘门控制着是否遗忘之前状态信息，例如图片分类中的“忘记”。

4、输入门控制：指网络的输入门控制着新输入信息对当前状态信息的影响，例如文档摘要中的关键词提取。

5、输出门控制：指网络的输出门控制着新状态信息对输出结果的影响，例如语言模型中的生成概率。

### GRU（Gated Recurrent Unit）：门控循环单元。

1、门控：指网络中的门控单元能够有效地控制信息的流动，使得网络能够更好地学习长时依赖关系。

2、重置门控制：指网络的重置门控制着上一次的状态信息对当前状态信息的影响，帮助网络解决梯度消失和梯度爆炸的问题。

3、更新门控制：指网络的更新门控制着是否更新上一次的状态信息，帮助网络学习长期依赖关系。