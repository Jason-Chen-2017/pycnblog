## 1. 背景介绍

### 1.1 Transformer 架构概述

Transformer 模型自 2017 年由 Vaswani 等人提出以来，迅速成为自然语言处理领域的主流架构。其核心思想是利用自注意力机制来捕捉序列数据中的长距离依赖关系，并通过编码器-解码器结构实现各种 NLP 任务，例如机器翻译、文本摘要、问答系统等。

### 1.2 层归一化和残差连接的作用

在 Transformer 模型中，层归一化（Layer Normalization）和残差连接（Residual Connection）是两个重要的技术，它们有助于稳定训练过程、加速收敛并提高模型性能。

*   **层归一化** 通过对每一层的输入进行规范化，缓解了梯度消失和梯度爆炸问题，使得训练过程更加稳定。
*   **残差连接** 允许梯度直接流