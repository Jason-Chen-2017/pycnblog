
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


## 概述
Video reasoning about daily activities (VidVRD) is a new video understanding task that aims to predict human behaviors in realistic scenarios with various complex factors such as occlusion, background changes, dynamic interactions between people, motion blur, low resolution, etc., where the goal is to identify intentional actions of individuals towards specific objects or situations based on their visual inputs.
To create an effective benchmark dataset for this task, we collect videos from different sources including social media platforms, news reports, recorded surveillance footage, family videos, etc. The videos are annotated with object categories, actions performed by humans, locations visited, and temporal events occurring in the scene. In addition, our benchmark also includes several pre-trained models that can be used to evaluate the performance of algorithms trained using these datasets. We hope that the proposed benchmark will inspire researchers, developers, and industries to develop novel solutions towards solving this challenging yet important problem.
In this work, we present the first large-scale dataset for video reasoning tasks focused on daily activities and propose two benchmarks: i) VidOR (Vision + Observation Relevance) and ii) VidQA (Visual Question Answering). These benchmarks aim to answer questions related to reasoning about daily activities through natural language queries over multimodal information extracted from multiple modalities such as text, audio, and visual. To achieve high accuracy and cover diverse aspects of daily activity recognition, we collected hundreds of hours of raw videos from YouTube and Microsoft Video Intelligence Platform (MVI), which provides rich metadata for each clip, along with annotations and qualitative analysis of scenes and behavior.
## VidVRD任务目标
The primary objective of the VidVRD task is to recognize what action was taken by an individual towards a given object(s) in a video sequence when analyzing a particular situation. This involves processing all possible relevant frames within a short duration of time, tracking the movement of objects, inferring the intentions behind actions, locating and identifying objects, detecting spatial relationships, and comprehending contextual cues from other speakers or faces. It requires advanced video analysis techniques such as pose estimation, motion detection, semantic segmentation, object tracking, event detection, and concept identification to capture the full complexity of a typical daily life scenario. Despite its complexity, it is not impossible to understand how individuals make decisions and interact with the world around them while observing their environment and talking to others. Therefore, video understanding systems must consider all available data and use intelligent algorithms to enable user-centered decision making across different devices and environments.

Another significant challenge in developing automated video understanding systems is to handle a wide range of unstructured content from different sources, languages, and styles. As the demand for more realistic and detailed video content grows, so does the need for higher quality labelled training sets to improve system performance. However, building and maintaining robust benchmark datasets for complex tasks like VidVRD remains a daunting and resource-intensive process.

## 数据集特点
We built VidVRD using three main data sources - YouTube, MVI, and Facebook. We selected thousands of hourly clips containing visual observations, audio recordings, and text captions that were uploaded to online social media websites such as YouTube, Microsoft Video Intelligence Platform (MVI), and Facebook. These multi-modal data streams provide rich metadata for each clip that enables us to annotate videos with diverse labels, including actions performed by individuals and objects located at certain positions in space.

Our dataset contains millions of videos spanning fourteen days of continuous observation videos capturing everyday life scenarios. The videos are mostly shot with good lighting conditions, either from directly above or captured from different angles. There is typically one person per camera viewpoint, who moves relatively slowly and maintains stable poses. During daytime, most videos contain only one subject whose body is visible in almost all views. Occlusions and partial views are rare except under extreme circumstances such as rainy weather, sudden change of scenery, firefighter arrival, sneak attack, etc. The foreground objects may move smoothly within the frame or exhibit jittering motions due to shadow, snow, wind speed fluctuations, or object collisions. The dataset covers both day and night time scenarios, as well as different seasons and cultures throughout the year. Overall, our dataset has high diversity but limited size compared to standard computer vision datasets, making it suitable for evaluation of algorithmic performance in practical applications.