
作者：禅与计算机程序设计艺术                    

# 1.简介
  

时序数据指的是随着时间推移而发生的相关性很强的数据集合，包括但不限于金融数据、物流运输信息、股市交易价格等。对时序数据进行预测可以为决策提供参考信息，从而更好的管理资源，提高效率。

传统的机器学习方法主要使用回归或分类模型对连续变量进行预测，但是这种方式无法对时序数据进行准确的预测。所以，为了能够更好地处理时序数据，人们在近几年提出了基于深度学习的方法。

在本文中，我们将会介绍两种用于时序数据的预测方法——循环神经网络（RNN）和卷积神经网络（CNN），并比较它们的优缺点。


# 2. 基本概念及术语说明

## 2.1 时序数据

### 2.1.1 什么是时序数据

时序数据指的是随着时间推移而发生的相关性很强的数据集合，包括但不限于金融数据、物流运输信息、股市交易价格等。最简单的时序数据形式就是一个列表或者数组，其中每一行或每一列代表着一个时间步长上的观察值。比如，一个月的每天的气温、房价、销售额都可以构成一个月份的时序数据。当然，时序数据还可以包括多个维度的信息，例如，用房价作为目标变量，还有可能还有供求关系的信息。

<center>
</center>

图1 简单时序数据示例

### 2.1.2 时序数据类型

按照是否包含时间维度分，时序数据又可分为非时序数据（无时间维度，如文本、图像）和时序数据（具有时间维度，如股票市场）。

#### （1）非时序数据

非时序数据可以直接输入到神经网络模型中进行训练，因此不需要考虑时间维度的影响。如文本数据，文本的每个单词是一个基本单元，而非时序数据的分布式特性使得采用传统的RNN或CNN模型较难对其建模。

#### （2）时序数据

时序数据的特点是存在时间维度，即每个样本之间存在时间上的先后顺序关系。如股票市场、经济指标、传感器数据等。

时序数据一般包含以下四个要素：

1. 多变量，即数据有多个维度，除了自身的时间维度外，还可以添加其他的特征；
2. 序列，即数据是按时间先后顺序排列的一系列数据，如每日的收盘价、开盘价、最高价、最低价；
3. 可变长度，即不同样本的长度不同，有的样本短些，有的样本长些；
4. 时序依赖，即数据的前面一些样本对后面的某些样本影响较大，如某支股票的上涨动力；

## 2.2 RNN

### 2.2.1 为什么需要RNN

由于时序数据存在时间上的先后顺序关系，因此它不能被简单地输入到传统的神经网络结构中，否则的话，模型就会丢失时序信息。如下图所示，在传统的神经网络结构中，输入层只能接收一个样本，不能捕获到之前的样本。因此，就需要引入循环神经网络（Recurrent Neural Networks，RNN）来解决这个问题。

<center>
</center>

图2 RNN结构图

RNN通过让网络记住之前的输出信息，使得模型可以捕获到序列中存在的时间依赖性。具体来说，RNN通过固定大小的向量来保存历史信息，称之为隐藏状态。循环连接使得隐藏状态可以不断更新。每次计算时，RNN都会把当前的输入与之前的隐藏状态传入一个激活函数，并得到新的隐藏状态。在更新隐藏状态的过程中，RNN还会利用外部输入（如偏置项或外部控制信号）来改变更新方向。这样，RNN就可以捕获到序列中的时间依赖性。

### 2.2.2 基本RNN结构

<center>
</center>

图3 RNN基本结构图

一个典型的RNN结构如图3所示，它由输入门、遗忘门、输出门以及隐藏状态组成。

* 输入门：用来决定应该更新隐藏状态还是保持现状。它的计算公式为：

$$i_t = \sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)$$

其中$i_t$表示输入门的激活值，$\sigma(\cdot)$表示sigmoid函数，$h_{t-1}$表示上一次的隐藏状态，$b_i$表示偏置项。

* 遗忘门：用来决定应该遗忘哪些之前的历史信息。它的计算公式为：

$$f_t = \sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)$$

其中$f_t$表示遗忘门的激活值，$\sigma(\cdot)$表示sigmoid函数，$h_{t-1}$表示上一次的隐藏状态，$b_f$表示偏置项。

* 候选隐藏状态：用来计算新的隐藏状态。它的计算公式为：

$$\tilde{C}_t = tanh(W_{xc} x_t + W_{hc} (r_t * h_{t-1}) + b_c)$$

其中$\tilde{C}_t$表示候选隐藏状态的值，$tanh(\cdot)$表示tanh函数，$(r_t * h_{t-1})$表示输入门与遗忘门的乘积，$b_c$表示偏置项。

* 输出门：用来决定应该输出什么作为下一步的输出。它的计算公式为：

$$o_t = \sigma(W_{xo} x_t + W_{ho} (\tilde{C}_t * h_{t-1}) + b_o)$$

其中$o_t$表示输出门的激活值，$\sigma(\cdot)$表示sigmoid函数，$\tilde{C}_t$表示候选隐藏状态的值，$h_{t-1}$表示上一次的隐藏状态，$b_o$表示偏置项。

* 最终隐藏状态：用来决定下一个时间步的隐藏状态。它的计算公式为：

$$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$$

其中$C_t$表示最终隐藏状态的值，$f_t$表示遗忘门的激活值，$C_{t-1}$表示上一个时间步的隐藏状态。

* 输出：用来给定当前时间步的输出。它的计算公式为：

$$y_t = o_t * tanh(C_t)$$

其中$y_t$表示当前时间步的输出值。

### 2.2.3 RNN参数数量计算公式

假设输入序列长度为$T$，单个时间步的隐藏单元个数为$H$，则RNN的参数数量可以计算如下：

$$\begin{align*}
&\frac{1}{2}(N(W_{xh})^2 + N(W_{hh})^2), \\
&+N(W_{ci})^2 + N(W_{cf})^2 + N(W_{co})^2 + N(W_{ch})^2 + N(b)^2 
+\sum_{t=1}^T N(W_{xc})^2 + N(W_{hc})^2 + N(b)^2 + NT(N(W_{xh})^2 + N(W_{hh})^2),\\
&\qquad \qquad +NT(N(W_{ci})^2 + N(W_{cf})^2 + N(W_{co})^2 + N(W_{ch})^2 + N(b)^2),\\
&\qquad \qquad +N(W_{xo})^2 + N(W_{ho})^2 + N(b)^2 + T(N(W_{xo})^2 + N(W_{ho})^2)\\
&\qquad \qquad + T(N(W_{xh})^2 + N(W_{hh})^2).
\end{align*}$$

其中，$N(X)$表示$X$的元素个数。

## 2.3 CNN

### 2.3.1 为什么需要CNN

由于时序数据往往含有时空信息，因此它不能仅通过简单地堆叠普通的神经元层来进行建模，否则模型的表达能力就会受到限制。

对于时间序列数据，一个比较有效的方法就是利用CNN对时序数据进行建模。CNN的核心思想是将卷积运算和池化运算结合起来，从而提取出时序数据的全局特征。同时，CNN可以保留时序上的依赖关系，并有助于解决序列填充等问题。

<center>
</center>

图4 CNN结构图

### 2.3.2 基本CNN结构

<center>
</center>

图5 CNN基本结构图

一个典型的CNN结构如图5所示，它由卷积层、池化层、全连接层和激活层组成。

* 卷积层：通常有多个卷积核，对输入进行卷积运算，提取不同频率上的特征。在本文中，我们只讨论一个卷积核，因为时序数据往往具有周期性，因此一个卷积核可以捕获到整个时序的特征。

* 池化层：对卷积后的结果进行池化，缩小特征图尺寸。池化可以降低计算复杂度，提升模型的整体性能。

* 全连接层：用来分类或回归，根据特征进行预测。

* 激活层：用于防止过拟合，起到正则化作用。

### 2.3.3 CNN参数数量计算公式

假设输入序列长度为$T$，单个时间步的隐藏单元个数为$H$，卷积核的个数为$K$，则CNN的参数数量可以计算如下：

$$\begin{align*}
&NK(NHW + K^2), \\
&\qquad \qquad +HNkW + HNW, \\
&\qquad \qquad +HN + HW + N.
\end{align*}$$

其中，$N$, $H$, $W$ 分别表示输入特征图的深度、高度、宽度。