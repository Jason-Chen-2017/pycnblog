
作者：禅与计算机程序设计艺术                    

# 1.简介
  

模型评估是机器学习领域的一个重要环节。很多算法都需要用评估指标对结果进行评价。一般来说，模型评估指标分为两类：一类是监督学习中的性能评估指标，另一类是无监督学习中的聚类评估指标。本文将主要介绍监督学习中最常用的几种性能评估指标。

## 1.1 AUC - ROC曲线
ROC（Receiver Operating Characteristic）即接收者操作特性曲线。它由两个变量组成，横轴表示False Positive Rate（FPR），纵轴表示True Positive Rate（TPR）。横轴表示模型将正样本预测错误的比例；纵轴表示模型将所有负样本预测正确的比例。



AUC（Area Under the Curve）即曲线下的面积。它用来衡量模型在所有阈值上的预测能力。当模型的预测能力达到1时，AUC为1。当模型的预测能力等于随机预测时，AUC为0.5。

## 1.2 Accuracy （精确率）
准确率是分类问题中最常用的性能指标。它计算了测试集中被分类正确的样本数量与总样本数量之比。

$$accuracy=\frac{tp+tn}{tp+fp+fn+tn}$$ 

其中tp是真阳性，fp是假阳性，fn是假阴性，tn是真阴性。精确率是一个介于0到1之间的数，越接近1代表分类效果越好。但是准确率无法判断一个样本是否被正确分类，而只是判断了一个类的样本总体的正确率。因此，准确率并不是一种单一指标能够描述模型的预测效果的唯一方式。

## 1.3 Precision （查准率）
查准率也称PPV或positive predictive value，是二分类问题中特有的性能指标。它计算了测试集中被正确标记为正的样本数量与实际上为正的样本数量之比。

$$precision= \frac {tp} {tp + fp}$$ 

Precision 高意味着模型只返回了正确的结果，并且这些结果是有意义的。如果我们要找出预测为正的样本，那么只返回正确的结果就非常重要。 Precision与精确率不同的是，Precision关注的是每一个正样本的正确分类率。因此，当样本分布不均匀或者正负样本数量不一致时，Precision可能成为衡量模型好坏的不够客观的指标。

## 1.4 Recall （召回率）
召回率也称Sensitivity或True Positive Rate，是二分类问题中特有的性能指标。它计算了测试集中被正确识别为正的样本数量与实际上为正的样本数量之比。

$$recall=\frac{tp}{tp+fn}$$ 

Recall 高意味着模型找到所有的正样本。相比于 Precision ， Recall 更关心样本的全覆盖情况，更加注重模型的泛化能力。但是 Recall 不能反映出模型的特定正样本被成功检索到的程度。当样本分布不平衡时， Recall 的表现可能会比较差。因此，相比于精确率， Recall 可以更好的评估模型的分类性能。

## 1.5 F1 score （F-measure）
F1 score 通常用来综合考虑查准率和召回率。它是精确率和召回率的调和平均数。F1 score 值越高，代表模型分类性能越好。

$$F1\ score = 2*\frac{(precision*recall)}{(precision+recall)} $$

F1 score 也是一种常用的性能指标，但是其计算比较复杂。建议优先使用其他指标，比如AUC、ROC曲线。