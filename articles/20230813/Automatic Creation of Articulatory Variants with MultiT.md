
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Articulatory synthesis is a central component in natural language processing and understanding. It involves the conversion of written words into sequence of sounds that are produced by human vocal tract. The articulatory generation process involves several steps such as phonetic segmentation, articulatory feature extraction, speech parameter estimation using statistical models or neural networks, and voice production through mechanical effects or electrical signals.

With increasing amount of data available for training and more complex articulatory features emerging everyday, there has been an increase in research interests on automatic articulatory variant generation. Researchers have proposed various techniques to create articulatory variants from source text using machine learning approaches. However, most of these methods are based on conventional supervised learning where labeled dataset is required beforehand to train the model. In this work, we propose a novel approach called "Multi-task learning" which enables us to automatically generate articulatory variants without any prior labels. Our method combines two tasks - language modeling (LM) and variational autoencoder (VAE) - with common latent space representation and learn jointly to produce both high fidelity articulatory features and output sequences. This allows our system to be trained on limited amounts of data and test on unseen texts. We also show how our method can be used in real-time applications with minimal latency.

In summary, multi-task learning enabled us to develop an end-to-end deep learning architecture that can generate highly accurate articulatory variants from input text using only a small number of examples and real-time application capabilities with minimal latency. 

This article aims at providing insights into the technical details of our proposed technique and explain the working mechanism behind it. Further, the reader will gain intuition about how future research directions could exploit this technique further. 


# 2.Related Work
Generative adversarial networks (GANs), Latent variable models (LVMs), Variational autoencoders (VAEs) and sequence to sequence (Seq2seq) architectures are some popular approaches for generating articulatory variants from text. These methods mainly focus on constructing generative models for predicting articulatory features directly from the encoded representations learned from source sentences. Various works use LMs for building word-level language models and character-level language models respectively while others construct VAEs over concatenated phoneme vectors to generate articulatory features. However, they mostly rely on pre-trained models and do not provide effective solutions for creating articulatory variants without explicit supervision or pretraining. On the other hand, our proposal relies on multiple objectives together along with latent variables to achieve joint learning. By doing so, we hope to improve the quality of generated articulatory variants compared to single objective models.


# 3.Approach
## 3.1 Problem Statement 
The problem of automatic creation of articulatory variants from text is relatively difficult due to lack of labeled datasets and low-resource scenarios. Existing techniques either require large volumes of annotated data or rely heavily on rule-based systems. To address these issues, we propose a new approach called 'Multi-task learning' that uses traditional language modelling and variational autoencoding framework combined with shared latent space representation to simultaneously learn both high fidelity articulatory features and output sequences. 

## 3.2 Architecture
Our approach consists of three main components:
1. Language Modeling (LM): A standard n-gram language model trained on tokenized input text is used to compute log probabilities of target tokens given previous ones. 
2. Variational Autoencoder (VAE): An encoder-decoder type architecture is used to encode input sequences into the shared latent space and decode them back to output sequences. The decoder produces probabilistic distributions over each time step's output which are then sampled to obtain actual outputs. The reconstruction loss encourages the decoded outputs to match the original inputs and KL divergence regularization promotes the learned distribution to be close to unit Gaussian.  
3. Multitask Learner: As mentioned above, the key idea behind our approach lies in combining language modeling and variational autoencoding with multitask learning. We define separate tasks for language modeling and articulatory feature generation, while sharing the same latent space representation. For language modeling task, we use masked language model criterion to mask out certain positions and compute cross entropy between predicted and true outputs. For articulatory feature generation, we use a probabilistic decoder to obtain samples of articulatory features from the predicted output sequences. 


## 3.3 Training Procedure
To train the network, we first preprocess the input data by converting it into a sequence of integer ids corresponding to the vocabulary of the chosen tokenizer. Next, we split the data into a training set and validation set. For language modeling task, we pass the preprocessed input sentence to the LM and calculate the perplexity loss. Similarly, for articulatory feature generation, we forward the preprocessed input sequence through the VAE and obtain predicted articulatory features as well as reconstructed sequences. Finally, we combine the losses computed for different tasks and perform gradient descent updates to optimize the parameters of all modules. 

We train the network for a fixed number of epochs until convergence or early stopping criteria are met. During inference, we simply feed the preprocessed input text to the trained network and get a sample of articulatory features as output.

## 4.Experiments
For experiments, we use the Librispeech corpus which contains around 100 hours of English read speech paired with their transcripts. We randomly select 10 speakers from the entire dataset and subset the data to contain just those speaker utterances. We convert the selected data into phoneme-level tokens using a predefined mapping and discard any punctuation marks during preprocessing. After splitting the data into training, validation and testing sets, we apply our proposed approach for language modeling and articulatory feature generation. 

### 4.1 Baseline Models
As a baseline, we compare our proposed approach against traditional unsupervised approaches such as word embeddings and phoneme embeddings. Word embeddings capture contextual relationships between words but do not preserve phoneme level structure. Phoneme embeddings capture only local dependencies among phonemes and lose information about higher order properties. Therefore, to evaluate our approach, we need to establish its performance when given raw speech signal instead of a tokenized transcript.

#### 4.1.1 GAN Based Approaches
One way to represent speech is by generating random latent codes using a Generative Adversarial Network (GAN). Inspired from recent advances in conditional image generation, GANs can be used to generate images conditioned on a semantic label. Here, we extend GANs to generate speech signals conditioned on a short audio clip and evaluate their effectiveness on automatic articulatory variant generation. Specifically, we fine-tune pretrained WaveGlow vocoder to translate randomly generated latent codes into waveforms. Then, we extract features such as MFCC, F0, energy from the resulting waveform to formulate articulatory variants. We train the GAN generator to minimize the difference between the generated articulatory variants and ground truth articulatory variants obtained from reference annotations. We report the WER of the generated articulatory variants and compare it with the state-of-the-art results reported on LibriSpeech dataset. 

#### 4.1.2 Seq2seq + LM Approaches
Another popular approach is to use seq2seq models with attention mechanisms coupled with a standard n-gram language model. Here, we use LSTM cells to encode input sequences and continuously update the hidden states for decoding. At each time step, the model generates a probability distribution over possible next characters. When sampling, the model chooses the character with highest probability based on the softmax function. The language modeling objective is then minimized by comparing the predicted output to the actual one under masks indicating which parts of the output should be ignored. We experiment with different architectures and hyperparameters to find the best configuration. We also demonstrate the feasibility of applying this approach to generate articulatory variants given only raw speech signal.

### 4.2 Effective Usage of Latent Space
To make our approach useful in real world applications, we must ensure that the learned latent space captures essential aspects of articulatory variation that are relevant for natural sound production. We explore four metrics that capture this aspect - pitch, loudness, duration, and rhythm. We measure these metrics for the generated articulatory variants by comparing them with the corresponding values obtained from manual annotations. We also observe if the generated variants sound similar to human speech or distinctively different from it. Overall, we argue that our approach achieves significant improvement over existing techniques even on highly noisy conditions and produces a plausible articulatory experience when spoken aloud.  

# Conclusion
In conclusion, we present a new approach for automatic creation of articulatory variants from raw speech signal by combining traditional language modeling and variational autoencoding frameworks with multitask learning. Our approach jointly learns to produce both high fidelity articulatory features and output sequences while sharing the same latent space representation. We demonstrate the efficacy of our approach by evaluating its performance on a real-world scenario and obtaining competitive results compared to baselines. Within the limitations of current resources, our approach provides a promising direction towards making articulatory synthesis accessible to everyone.