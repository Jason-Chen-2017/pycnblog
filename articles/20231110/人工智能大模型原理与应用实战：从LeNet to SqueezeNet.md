                 

# 1.背景介绍


人工智能（AI）技术主要分为两大类，机器学习（ML）和深度学习（DL）。近年来，越来越多的人开始关注并借鉴深度学习的最新进展，在模型结构上采用更复杂、规模化的大模型。然而，由于深度神经网络的复杂性和参数量过大导致训练耗时长，其性能通常不如传统的机器学习算法高效。为了提高深度神经网络的性能和效率，研究人员开发出了各种缩减版的神经网络，称之为“小型神经网络”或“轻量级模型”。这些模型往往可以与传统机器学习算法相媲美甚至超越。例如，Google团队提出的MobileNets模型就可以在极低成本下达到较好的识别准确率。本文将介绍目前最流行的三种常用的缩减版神经网络——LeNet、GoogLeNet 和 SqueezeNet。它们都基于卷积神经网络（CNN），并对传统的 LeNet 模型进行了简化，并加入了一些创新性的组件。在随后的实践中，我们将展示如何利用 TensorFlow 实现这些模型，以及使用开源框架 Keras 的 API 进行快速构建。最后，我们还将讨论这些模型的优缺点及其适用场景。
# 2.核心概念与联系
## （1）LeNet
LeNet 是一种很古老的深度学习模型，它由LeCun在1998年提出。它的基本结构包括两个卷积层，分别是卷积层1和池化层1，第二个卷积层，再加上池化层2，两个全连接层。如下图所示：

LeNet 一共包含三个卷积层，第一个卷积层有6个卷积核，尺寸大小为5*5，每个卷积层后面跟着一个步长为2的池化层，池化层的大小为2*2；第二个卷积层有16个卷积核，尺寸大小为5*5，每个卷积层后面跟着一个步长为2的池化层；第三个卷积层有120个卷积核，尺寸大小为7*7，第二个全连接层有84个神经元，最后一个全连接层输出数字分类结果。

## （2）GoogLeNet
GoogLeNet是2014年ImageNet图像识别挑战赛的冠军，由Szegedy等人提出。它的基本结构与LeNet类似，但增加了多个卷积层和池化层，而且多了一个Inception模块。如下图所示：

其中，Inception模块用于将输入划分成不同尺寸的子特征图，然后在每个子特征图上运行不同的卷积操作，再合并回输出。具体的操作包括三个不同尺寸的卷积核，池化层，激活函数ReLU，及拼接操作。

## （3）SqueezeNet
SqueezeNet是一个轻量级的CNN，于2016年ICLR提出，以AlexNet的计算速度提升为目标，被认为是下一代的AlexNet。其核心思想是通过端到端的方式压缩模型，即每一层通道的数目降低到原来的一半，同时减少通道之间的联系。最终输出的特征图尺寸缩小到原来的1/16。如下图所示：

它主要由三个模块组成，首先是Fire模块，它是一种通过重复卷积的简单架构，可以让信息在通道维度上交叉传递。然后是混合网络模块，它将不同通道之间的特征图拼接，并做归一化处理；最后是全局平均池化模块，它将输出的特征图变成一维向量，进行最后的分类。

## （4）总结
表格中对比了以上三种模型的特点和不同之处，如下：

| 模型 | 特点 |
|---|---|
| LeNet | 参数量少，训练速度快，效果一般 |
| GoogLeNet | 使用Inception模块提取多种尺度的特征，参数量和计算量都很大，但效果很好 |
| SqueezeNet | 通过降低通道数目，参数量和计算量减小了一半，但是准确率相对于GoogLeNet要差些 |

下面我们就着眼于各个模型的具体内容，依次看一下它们的各个方面。