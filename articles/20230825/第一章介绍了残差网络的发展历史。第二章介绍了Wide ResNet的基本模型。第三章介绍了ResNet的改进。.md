
作者：禅与计算机程序设计艺术                    

# 1.简介
  

## 概述
残差网络（Residual Network）是深度学习中很重要的一个分支。2015年，何凯明等人在CVPR上发表论文《Deep Residual Learning for Image Recognition》提出了残差网络，并取得了很大的成功。至今已成为深度学习领域里极具影响力的模型之一。其主要原因是它能够解决深层神经网络训练困难的问题，使得深层神经网络不再受限于局部感受野和梯度消失的问题。
## 发展历史
残差网络的发展可以分成三个阶段:
### (1)2015 年
- 何凯明等人在 CVPR 上的论文《Deep Residual Learning for Image Recognition》
- 作者在 VGG 和 GoogLeNet 的基础上设计了一个新的网络结构——ResNet。
### (2)2016 年
- <NAME>、<NAME>、<NAME> 及其他同事利用 ResNet 发现其训练误差存在退化问题，因此提出了一种改进策略——残差块（Residual Block）。
- 作者提出了两种不同尺寸的残差块：小残差块(ResNeXt Block)和 Wide ResNet 。
### (3)2017 年
- 作者观察到 ResNet 中的两个关键问题: 一是梯度消失，二是网络退化。为了缓解这两个问题，作者提出了一些方案：残差边界回归（Residual Boundary Regression）、丢弃网络（Dropout Network）、权重衰减（Weight Decay）、批量归一化（Batch Normalization）。
- 此外，作者还将 ResNet 的超参数进行了调整，得到了较好的结果。如添加快捷连接、更高的学习率、标签平滑、集成学习方法等。
## 框架图示

图1：ResNet 网络框架图示
# 2.Wide ResNet
## 概述
ResNet 的缺点是深度不够深。因此研究者提出了 Wide ResNet ，即增加网络宽度而不是深度。本节将介绍该网络的基本模型，并探索其扩展性。
## 结构
Wide ResNet 包含多个串联的残差块，每一个残差块都由若干卷积层构成，第一个卷积层输出通道数相同，后续卷积层输出通道数是输入通道数的两倍。这样做的目的是为了扩充网络宽度，使得每个残差块能够提取更多特征。另外，在 ResNet 中，每一个残差块输出的特征图大小都是相同的。但是，如果输入图像的尺寸较小，则某些残差块需要对特征图进行池化操作，以使得特征图的大小相近。但池化操作会损失掉一些空间信息，因此，作者设计了一种新的模块，即宽高比为 $1\times 1$ 的瓶颈层，用以替代池化操作。

图2：Wide ResNet 模型结构示意图
## 扩充计算能力
在传统的 ResNet 中，每一个残差块后面紧跟一个池化层，池化层将下采样后的特征图缩小到同一尺寸。由于池化层引入了额外的参数，导致模型的计算量变多。因此，作者提出了通过跳连接（Shortcut Connections）的方式代替池化层来增大模型的计算容量。

图3：使用 Shortcut Connections 替换池化层的 Wide ResNet 模型结构示意图
## 实现细节
Wide ResNet 的实现仍然沿袭 ResNet 的基本模式，只是把简单残差块替换成了复杂残差块。其中，复杂残差块由多个卷积层堆叠而成，并且采用宽高比为 $1 \times 1$ 的瓶颈层来降低维度。通过这种方式，Wide ResNet 可以有效地扩充网络宽度，从而提升性能。