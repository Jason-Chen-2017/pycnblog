
作者：禅与计算机程序设计艺术                    

# 1.简介
  

深度学习中最基本的就是激活函数。激活函数是用来确定神经元输出值的非线性映射关系。在机器学习领域中，我们通常会使用sigmoid函数或者tanh函数作为激活函数。但是最近几年随着神经网络的普及，一些新的激活函数的发明和提出使得研究者们对激活函数的选择和理解变得更加复杂和深入。本文主要讨论下三种经典的激活函数Sigmoid、Tanh、ReLU。由于篇幅原因，其他激活函数暂不进行详细阐述。
# 2.基本概念与术语
## 激活函数（Activation Function）
激活函数是神经网络的核心组成部分之一。它是一个非线性函数，它会将输入信号转换成输出信号。其作用是为了解决输入信息传递过程中信息丢失的问题。其定义为：
$$\hat{y}=f(x)$$
其中$\hat{y}$为神经元输出值，$x$为神经元输入值。$f$代表激活函数，它将输入值$x$转换为输出值$\hat{y}$。激活函数的选择是为了防止信息的丢失和欠拟合。
## Sigmoid函数
sigmoid函数是一种S型曲线激活函数，它的值域是$(0,1)$区间，可以将其看作概率值，用于分类任务。它的表达式为：
$$f(x)=\frac{1}{1+e^{-x}}$$
图：Sigmoid函数图像。
## Tanh函数
tanh函数也称双曲正切函数或双曲线激活函数，它的表达式为：
$$f(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{(e^x - e^{-x})/2}{(e^x + e^{-x})/2}$$
tanh函数和sigmoid函数都是属于激活函数中的常用函数。他们都具有不同优点和局限性。当输入数据接近饱和时，tanh函数能够提供较为平滑的输出，而sigmoid函数则具有更多的区分能力。另外，当输入数据取值为负值时，tanh函数的导数存在“折返”现象，而sigmoid函数的导数不出现该现象。因此，在实际应用中，不同的激活函数可能会表现出不同的行为。
## ReLU函数
ReLU函数（Rectified Linear Unit，修正线性单元）是神经网络的常用激活函数。它的表达式为：
$$f(x)=max(0,x)$$
ReLU函数也被称为修正线性单元（rectifier）。它是一个简单的非线性函数，一般只用于隐藏层，因为它对负输入值很敏感，容易造成梯度消失或者爆炸。另一方面，它不需要加入太多参数，计算量也比较小，所以它可以在一定程度上缓解过拟合。
# 3.深度学习中的激活函数原理及实现
## Sigmoid函数
Sigmoid函数是S型曲线激活函数，适用于分类任务。如图所示，它将输入信号压缩到$(0,1)$区间内。其定义为：
$$f(x)=\frac{1}{1+e^{-x}}$$
### Sigmoid函数的特点
1. S型曲线: Sigmoid函数的形状类似于钟摆（钟形曲线），因此有时候也叫做S型曲线激活函数。S型曲线的两个极端分别对应于0和1。
2. 避免了死亡梯度问题: Sigmoid函数的导数在输入信号较大的情况下会变得非常小。这就保证了梯度不会因输入信号过大而发生爆炸现象。
3. 可微性: Sigmoid函数可导数，其表达式为：
   $$f'(x)=f(x)(1-f(x))$$
### 使用Python语言实现Sigmoid函数
```python
import numpy as np
def sigmoid(z):
    """
    Compute the sigmoid of z

    Arguments:
    z -- A scalar or numpy array of any size

    Return:
    s -- sigmoid(z)
    """
    
    # Compute sigmoid(x)
    s = 1 / (1 + np.exp(-z))
    
    return s
```
### 反向传播推导过程
为了对训练结果进行评估，需要采用损失函数（Loss function）对模型预测值和真实值之间的差异进行衡量。比如，交叉熵损失函数常用于多类别分类问题。Sigmoid函数在二分类问题中使用的话，可以用损失函数对模型输出的概率值进行评估。假设当前训练样本的真实标签为y，模型预测结果的概率值为p，则损失函数可以表示如下：
$$L(\hat{y}, y)=-[ylog\hat{y}+(1-y)log(1-\hat{y})]$$
若使用Sigmoid函数作为激活函数，则还需要通过求导求出其导数。下面我们证明一下Sigmoid函数的导数公式：
$$f'(z)=\frac{df}{dz}=\frac{d}{dx}\left[\frac{1}{1+e^{-x}}\right]=\frac{-e^{-x}}{(1+e^{-x})^2}=\frac{1+e^{-x}-1}{(1+e^{-x})^2}=f(z)(1-f(z))$$
因此，Sigmoid函数的导数表达式为：
$$f'(z)=f(z)\left(1-f(z)\right)$$
## Tanh函数
Tanh函数是激活函数中的常用函数。它也是一种非线性函数，但其范围是$(-1,1)$。它的表达式为：
$$f(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{(e^x - e^{-x})/2}{(e^x + e^{-x})/2}$$
它可以看作是Sigmoid函数的平滑版本。但是Tanh函数的输出范围比Sigmoid函数更窄，所以它的输入信号应该缩放到一个适合的值域再使用。图示如下：
Tanh函数可以看做是Sigmoid函数的平滑版本，但是Tanh函数的输出范围比Sigmoid函数更窄，所以它的输入信号应该缩放到一个适合的值域再使用。Tanh函数在深度学习领域很常见，尤其是在卷积神经网络（CNN）中。它也是深度神经网络中不可或缺的部分。
### 使用Python语言实现Tanh函数
```python
import numpy as np
def tanh(z):
    """
    Compute the hyperbolic tangent of z

    Arguments:
    z -- A scalar or numpy array of any size

    Return:
    a -- tanh(z)
    """
    
    # Compute tanh(x)
    a = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))
    
    return a
```
### 反向传播推导过程
Tanh函数也可以利用链式法则进行反向传播。首先根据激活函数的导数公式进行导数计算：
$$f'(z)=f(z)(1-f(z))$$
然后依据链式法则进行梯度计算：
$$\frac{\partial L}{\partial x}=\frac{\partial L}{\partial \hat{y}}\times\frac{\partial \hat{y}}{\partial h}\times\frac{\partial h}{\partial o}\times\frac{\partial o}{\partial i}\times\frac{\partial i}{\partial w}$$
注意到：
$$\hat{y}=f(o)=tanh(h)=f(i)=relu(w)=max(0,w)$$
因此：
$$\frac{\partial L}{\partial i}=\frac{\partial L}{\partial \hat{y}}\times f'(h)\times\frac{\partial h}{\partial i}=\frac{\partial L}{\partial \hat{y}}\times\left[1-h^2\right]\times 1$$
即：
$$\frac{\partial L}{\partial b_i}+\sum_{j}^{} w_{ij}\frac{\partial L}{\partial z_j}$$