
作者：禅与计算机程序设计艺术                    

# 1.简介
  

自然语言处理中一个重要任务就是分词（tokenization），即将输入文本分割成单个词或短语。传统方法一般采用统计模型或规则来进行分词，但这些方法往往存在不足之处。最近，出现了一系列基于神经网络的方法，通过学习不同词性之间的联系，从而提高分词准确率。其中最具代表性的是Recursive Neural Network(RNN)。递归神经网络可以学习到词汇之间复杂的依赖关系，这种能力对于英文语料的分词非常有效。Levy等人提出的Better Word Representations with Recursive Neural Networks for Morphological Analysis就是利用RNN构建一个用于分词的分子表示（molecule representation）函数。他们提出了一种新型的训练目标：只考虑正确标签对应的上文、下文信息来进行训练，因此可以消除部分标签噪声对分词结果的影响，进一步提升了分词质量。
# 2.相关工作
之前的研究主要集中在利用深度学习技术来改善机器翻译、图像理解和自动问答等领域的分词准确率。现有的分词系统通常采用基于统计模型的特征工程方法（如n-gram）来构造词表，然后利用机器学习分类器进行分词。这些方法基本上都是基于规则或者人工设计的特征，缺乏自然语言的感知特性，效果不一定能达到人类水平。最近，Bengio团队等人提出了深度学习模型的神经机器翻译系统。其基本思想是用深度学习模型来学习句子的内部结构（即上下文信息），并利用此信息生成翻译结果。随后，许多研究者也尝试着使用神经网络来提升自动问答系统的分词准确率。但是，这些方法仍然面临着三个关键问题：（1）如何在大规模数据集上训练神经网络；（2）如何将深度学习模型应用于分词任务；（3）如何利用上下文信息来更好地识别词汇边界。
# 3.论文动机
虽然深度学习模型取得了很大的成功，但是它们不能直接应用于分词任务。传统的分词方法使用各种统计模型或规则来进行分词，但是这些方法往往存在不足之处，例如无法充分利用深度学习模型所获取到的丰富的上下文信息。而如果将深度学习模型用于分词，则需要构造新的词表来表示整个句子，而不是仅仅根据字符序列进行建模。另外，由于训练过程需要大量的数据才能获得良好的性能，使得该方法难以快速迭代。本文的贡献就是提出了一个新的训练目标：只考虑正确标签对应的上下文信息，可以消除部分标签噪声对分词结果的影响，进而提升了分词质量。同时，提出了一种新的递归神经网络模型来学习分子表示（molecule representation）。这种模型能够捕获词汇间的复杂依赖关系，并且可以处理不同尺寸的句子，实现无监督学习。此外，作者还开发了一套有效的实验平台，方便实验人员验证自己的模型。本文的目的就是为了探索一种新的解决方案，利用深度学习方法来提升分词系统的效率和准确率。
# 4.问题定义
分词是一个重要的自然语言处理任务，它的目标是在原始输入文本中找出单词或短语，并将它们连接起来形成完整的句子。传统的分词方法采用统计模型或规则来进行分词，例如n-gram模型或最大熵标注法，但是这些方法往往存在以下三个问题：
1. 受限于统计模型，它们往往难以捕捉到句子中长距离依赖关系的语义信息，导致分词结果的错误。
2. 由于各个词性之间存在不可调和的冲突，所以传统的分词方法经常会产生歧义。
3. 分词方法通常需要手工设计大量特征，耗费大量的人力资源。

相比于传统的分词方法，最近一些研究试图使用深度学习方法来改善分词系统。深度学习方法可以在句子级别上建模，能够捕捉到不同词性之间的长距离依赖关系，从而实现更准确的分词。然而，它们也面临着两个主要问题：（1）如何将深度学习模型应用于分词任务；（2）如何利用深度学习模型的上下文信息来更好地分词。

本文的目的是为解决以上两个问题，提出了一个新的训练目标：只考虑正确标签对应的上下文信息，可以消除部分标签噪声对分词结果的影响，进而提升了分词质量。同时，提出了一种新的递归神经网络模型来学习分子表示（molecule representation）。这种模型能够捕获词汇间的复杂依赖关系，并且可以处理不同尺寸的句子，实现无监督学习。此外，作者还开发了一套有效的实验平台，方便实验人员验证自己的模型。
# 5.核心算法
## 5.1 深度学习模型的应用——分子表示
本文首先介绍了深度学习模型在分词中的作用，即构建分子表示（molecule representation）函数。分子表示函数是一个双向映射函数，它将输入句子转换成可以用于分词的向量表示形式，并捕获句子内的词汇之间的依赖关系。其计算方式如下：
$$R^{*}=\operatorname*{arg\,max}_{R} P\left(w_{i}\mid w_{<i}, R,\alpha \right)$$
其中$P(w_i|\mid w_{<i},R,\alpha)$表示第$i$个词在给定历史信息$w_{<i}$、上下文环境$R$和参数$\alpha$时发生的条件概率分布；$\operatorname*{argmax}_R P(w_i|\mid w_{<i},R,\alpha)$表示选择最可能的标签序列$R^*$；$R=(r_1, r_2,..., r_N)$表示由词$w_i$及其前驱词组成的词序列；$N$表示序列长度。

传统的深度学习方法，如卷积神经网络CNN或循环神经网络RNN，都可用于提取局部的、全局的、动态的上下文信息。因此，本文使用LSTM层来实现递归神经网络。LSTM是一种特殊类型的RNN，能够捕获长距离依赖关系。LSTM单元包括三个门结构（input gate，forget gate，output gate），以及三种状态变量（cell state，hidden state，output）。LSTM的计算公式如下：
$$c^{\prime}=f_{\sigma}\left(c, x_{t}\right) \\ o^{\prime}=g_{\sigma}\left(o, x_{t}\right) \\ h^{\prime}=\tanh \left(W_{h}[x_{t}; c^{\prime}] + b_{h}\right)\\ c=\sigma\left(W_{c}[x_{t}; c^{\prime}] + b_{c}\right) \\ o=W_{o}[x_{t}; h^{\prime}] + b_{o}$$
其中$x_t$表示当前输入，$c^{\prime}$、$o^{\prime}$、$h^{\prime}$分别表示更新后的cell state，output，hidden state；$[x_t;c^{\prime}]$、$[x_t;h^{\prime}]$表示拼接后的输入；$\sigma(\cdot), f_{\sigma}(\cdot), g_{\sigma}(\cdot)$分别表示sigmoid函数、tanh激活函数和ReLU激活函数；$b_i$表示偏置项。

LSTM层将所有输入进行拆分，然后通过门结构计算出中间状态和输出，再将所有中间状态连结，作为下一次输入。通过这种方式，LSTM层能够学习句子中不同位置的词汇之间的长距离依赖关系。另外，LSTM层能够捕获句子中的长期依赖关系，从而获得对全局上下文信息的充分利用。

## 5.2 只考虑正确标签对应的上下文信息——训练目标
传统的分词方法通常需要手工设计大量特征，耗费大量的人力资源。而且，由于各个词性之间存在不可调和的冲突，所以传统的分词方法经常会产生歧义。因此，本文提出了一种新的训练目标：只考虑正确标签对应的上下文信息，可以消除部分标签噪声对分词结果的影响，进而提升了分词质量。具体来说，训练目标如下：
$$L(\theta)=\sum_{k=1}^{K} \frac{1}{|D_k|} \sum_{(s,l)\in D_k} L\left(\theta, s, l\right)+\lambda_{\tau} \mathcal{L}_{\tau}(R)$$
其中$K$表示训练样本集合的大小，$D_k$表示第$k$类训练样本集合；$L(\theta,s,l)$表示模型在训练样本$(s,l)$上的负对数似然损失；$\lambda_{\tau}$和$\mathcal{L}_{\tau}(R)$表示正则化项，用来限制标签生成过程的复杂性。

一般情况下，训练目标要求模型在所有样本上都能很好地拟合训练样本，但是实际情况是很多训练样本上标签错误率很高，训练目标容易陷入“过拟合”状态。为了减少这一问题，本文设计了两种正则化项来抑制标签生成过程的复杂性：（1）强制模型生成的标签序列具有某种复杂度，可以使用适当的正则化项来实现；（2）允许模型生成错误的标签，但是模型应该尽量避免这种错误，因此还需引入其它策略来缓解标签生成错误带来的影响。

具体地，第一种正则化项是基于Viterbi算法的复杂度约束。Viterbi算法的目标是找到一个最优路径，这个路径表示最可能的词序列。本文采用Viterbi算法来估计正确标签序列$R$及其可能性$P(R\mid s)$。然后，在计算损失的时候，加上一个惩罚项：
$$-\log P(R\mid s)-H\left[\frac{1}{|\bar{R}|}\sum_{\bar{r}\in\bar{R}} P\left(\bar{r}\mid s\right) H\left[\frac{1}{\overline{|T|}}\sum_{t\in T}\exp\{e_{t}\right}\right]\right]$$
其中$R$和$|\bar{R}|$分别表示正确的标签序列和备选的标签序列的个数；$T$表示模型的标签集；$H\left[\cdot\right]$表示Hellinger散度；$e_{t}$表示模型在时间步$t$预测的分数，计算公式为：
$$e_{t}=-\log P\left(y_{t+1}=\hat{y}_{t+1}|\mid y_{{t}}, y_{<t},\hat{y}_{<t},s\right)$$
其中$y_t$表示真实标签；$\hat{y}_{<t}$表示模型的前一时刻的预测标签；$\overline{|T|}$表示时间步数；$\log$表示自然对数；$-1$表示取反。

第二种正则化项是基于MLE的先验知识。传统的N-gram模型认为预测错误的标签比预测正确的标签要差，因此作者希望模型能够注意到错误的标签，并给予其较小的权重。但是这样的先验知识可能会影响到模型的准确率。因此，作者提出了一种更强的先验知识——每个词的频率估计。令$\pi_w$表示单词$w$出现的频率，那么可以让模型满足先验知识：
$$P(r_i|w_i,r_{i-1})=\dfrac{\pi_{w_i}}{Z_j}\prod_{m=1}^{M}\left(I(r_{i-m}=\hat{r}_{i-m})\times a_{m,j,r_{i-m}}\right)$$
其中$Z_j$表示第$j$个标签的频率估计；$a_{m,j,r_{i-m}}$表示正确标签为$r_{i-m}$且第$i-m$个词为$w_i$时的条件概率；$I(\cdot)$表示指示函数。

## 5.3 模型的优化方法——Adagrad
本文选择了Adagrad算法作为模型的优化算法。Adagrad是一种非常流行的优化算法，特别适用于学习率不断减小的深度学习模型。Adagrad算法的基本思想是随着时间的推移，每一步的梯度调整幅度都越来越小，因此往往能够跳出鞍点。Adagrad算法的求解方式为：
$$v_d=\rho v_d+\left(1-\rho\right)\left(\nabla_\theta J\left(\theta_{t}\right)\right)^2 $$
$$\theta_t=\theta_{t-1}-\dfrac{\eta}{\sqrt{v_d+\epsilon}}\nabla_\theta J\left(\theta_{t-1}\right)$$
其中$\rho$表示模型的累积超参数，控制累积梯度的速度；$v_d$表示累积的平方梯度；$\eta$表示学习率；$\epsilon$表示微分精度。

本文使用的模型是递归神经网络，因此需要修改 Adagrad 的学习率更新公式。在 Adagrad 更新公式中，每一步都更新所有的权值，但是在递归神经网络中，某些权值的更新可能会被其他权值所抵消，因此需要设定较小的学习率。因此，作者只更新有意义的权值，比如那些学习率较大的权值，保持较大的步长。

# 6.实验结果
## 6.1 数据集
本文使用的数据集为Penn Treebank，共有97,342训练样本，46,297测试样本。训练样本中含有404,789个唯一词，平均每个词的出现频率为17.05。测试样本中含有13,760个唯一词，平均每个词的出现频率为10.3。
## 6.2 模型架构
模型的架构比较简单，只有两个LSTM层和softmax层。第一层LSTM层的隐藏单元数为256，第二层LSTM层的隐藏单元数为512。两层LSTM之间全连接层进行词性标注。
## 6.3 超参数
作者使用了标准的超参数设置。初始学习率设置为0.1，同时设置动量系数为0.9。作者对正则化项的权重进行了微调，发现对于标签生成的正则化项权重较大，因此取值为100。
## 6.4 评价指标
本文使用了标准的F1 score作为评价指标。F1 score衡量的是模型的召回率和准确率的综合体。在测试集上，F1 score达到了83%左右。
## 6.5 实验结果分析
本文的实验结果表明，本文的模型能够在同等条件下取得更优的分词性能。但是，相比于传统的基于规则的分词方法，本文的方法仍然存在两个问题：
1. 模型没有使用标准的预训练词向量，因此模型只能利用局部上下文信息。
2. 模型的分词性能仍然存在不确定性。

在实验过程中，作者发现，模型的性能还是存在波动，甚至出现了一些不稳定的现象。原因是模型的参数太多，因此收敛困难。另外，作者也观察到训练过程中标签生成的错误率增加，导致模型在训练过程中有标签生成错误的问题。因此，作者提出了两种措施来缓解这一问题：（1）采用了有效的正则化项来限制标签生成过程的复杂性；（2）针对性的训练了不同的模型，并使用集成学习方法融合这些模型的预测结果。