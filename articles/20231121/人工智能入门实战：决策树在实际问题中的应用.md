                 

# 1.背景介绍


## 一、什么是决策树
决策树(decision tree)是一种常用的机器学习方法，它用于分类或回归问题，可以认为是一个 if-then 规则集合，也可以用来进行预测分析。它的基本想法是从根节点到叶子节点依据一些特征划分数据，每一个中间节点根据某种评判标准将样本集划分成若干子集，并在每个子集上递归地构建一个子树。最后，把所有子树的结论综合起来决定当前样本所属的类别或者值。
## 二、决策树适用场景及优缺点
### 适用场景：
- 对离散或标称的数据进行分类；
- 有多个输入变量时，能够找到最佳的划分方式；
- 需要处理多维数据时，能够较好地描述数据的内在含义，不容易出现“过拟合”现象。
### 优点：
- 简单直观：决策树模型非常容易理解和解释，便于理解和控制。
- 模型具有可解释性：决策树模型中的各种条件概率可以帮助用户对决策过程有一个直观的认识。
- 不容易发生过拟合:决策树模型相对于其他算法更加保守，一般不会发生过拟合现象。
- 计算速度快：决策树模型训练速度快，可以在短时间内生成模型，并且运行效率高。
- 可处理多分类问题:决策树模型可以很好地解决多分类问题。
### 缺点：
- 模型可能过于复杂:决策树模型可能会产生过于复杂的树结构，导致无法很好的应付复杂的数据集。
- 模型可能欠拟合:决策树模型可能由于在训练过程中遇到噪声数据而欠拟合。
- 没有考虑到不同特征之间的相关性:决策树模型不考虑各个特征之间的关联关系，因此可能导致过于偏向于选择单一的特征。
## 三、决策树应用案例——西瓜数据集的购买预测
### 数据简介
为了方便理解，我们用西瓜数据集作为决策树的应用案例。西瓜数据集包括8个特征和1个目标变量，其中有些特征已经被归一化处理（均值为零，标准差为1），目标变量是是否购买。数据集共有159个样本，被分为两组：59个训练集样本和90个测试集样本。
### 目标变量分布情况
目标变量的取值分布如下图所示：
数据中只包含两种取值，分别表示没有购买和购买，数据集中正负样本比例平衡。
### 构建决策树
首先，我们要定义决策树的终止条件，即什么时候停止继续划分。这里，我们设置划分节点的最大深度为5，也就是说，决策树不允许超过5层的节点。
然后，我们按照信息增益准则选取第一个特征作为划分特征。通过计算每个特征的信息熵，我们得到特征"色泽"的信息增益为0.25，其次为"根蒂"和"敲声"的信息增益分别为0.26和0.15，所以我们选择"色泽"作为划分特征。
接着，我们按照信息增益的方式，再次计算每个特征的信息熵，得到特征"根蒂"的信息增益为0.22，其次为"密度"和"脐部"的信息增益分别为0.25和0.14，所以我们选择"根蒂"作为第二个特征。
第三次计算信息增益时，我们得到特征"敲声"的信息增益为0.02，其次为"纹理"和"触感"的信息增益分别为0.0 and 0.0，所以我们选择"纹理"作为第三个特征。
最后，我们得到决策树如图所示：
### 决策树的局限性
- 决策树容易受到样本不均衡的问题影响，比如某个类的样本数量少于另一类。
- 决策树只能处理连续型变量，不能处理离散型变量，比如手写数字识别问题。