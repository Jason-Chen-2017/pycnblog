                 

# 1.背景介绍


　　强化学习(Reinforcement Learning, RL)是机器学习领域的一个重要研究方向，它试图通过“引导”行为者从奖赏反馈中学会长期规划，使得决策者能够更好地预测未来的行为并进行相应调整，从而使智能体适应环境变化、达到最优策略。RL可以用于解决许多复杂的问题，如机器人控制、图像识别、自动驾驶、游戏AI等。目前，RL已经被广泛应用于各种领域，包括计算机视觉、自然语言处理、工程建模等。

　　对于游戏AI来说，由于游戏具有复杂的环境和限制，让人无法仅凭直观感受到游戏世界带来的各种状态和奖励，因此需要借助于强化学习的方法来做出更加智能的决策。强化学习主要有两个组成部分：（1）状态空间和动作空间；（2）Reward函数和Value函数。状态空间表示游戏当前的状况，动作空间表示游戏可以执行的行为。 Reward函数是一个映射关系，将当前状态映射为一个奖励值，其中奖励越高，代表当前局面对玩家的价值越大；Value函数则给出了不同状态下可能获得的最大的收益或最小的损失。

　　实际上，RL算法可以分为两类，一种是基于值函数的算法，另一种是基于策略梯度的方法。基于值函数的算法包括Q-learning、Sarsa、Expected Sarsa等，这些方法利用已知的状态和动作及其对应奖励，训练得到最优的Action-value函数或State-value函数。基于策略梯度的方法包括Policy Gradient、Actor Critic等，它们利用策略梯度来更新参数，使得策略能够更好地选择动作。本文将主要讨论基于策略梯度的AlphaZero算法，这是国际象棋世界顶尖水平的AI算法之一。

# 2.核心概念与联系
　　首先要明确AlphaGo Zero的组成元素。AlphaGo Zero由五个部分组成：蒙特卡洛树搜索MCTS、神经网络NNet、变异策略以及参数复制。蒙特卡洛树搜索MCTS用来探索各种可能的下一步移动，也就是模拟游戏。神经网络NNet是一个黑盒模型，接受当前的棋盘状态作为输入，输出对手的下一步所有可能的落子位置及对应的概率。变异策略用来在神经网络学习过程中引入随机噪声，减少过拟合。参数复制则用来把训练好的模型参数复制到下一轮进行进一步训练。下面将详细介绍这些组成元素的功能。

　　1.蒙特卡洛树搜索MCTS：蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS），是一种有效的、基于遍历策略的蒙特卡洛模拟方法。它的基本思想是用树结构来存储和管理可能的游戏状态，并通过树搜索算法来评估不同状态的胜负价值。每一次迭代中，MCTS都会生成若干子树，然后在这些子树上运行下一步的模拟，同时还会收集后续反馈数据。最后，根据反馈信息来计算不同节点的得分，并选取其中最佳的节点作为下一步的决策。MCTS的好处是能够有效的探索游戏的各个可能情况，并找寻最优的策略。

　　2.神经网络NNet：神经网络（Neural Network，NNet）用于执行蒙特卡洛树搜索算法中模拟游戏，输入是棋盘状态，输出是每个落子位置的概率。它可以是一个深度神经网络，也可以是一个广义线性模型（Generalized Linear Model）。与传统的强化学习方法不同的是，NNet不需要事先知道游戏规则，只需要提供足够量的数据并经过训练，即可有效地完成对手落子位置的预测。虽然NNet看起来像黑盒子，但是通过黑盒分析可以发现它的一些关键特性。第一，NNet学习能力强，随着数据的增加，模型的准确率不断提升。第二，NNet具备自我纠错能力，可以通过蒙特卡洛模拟误差反向传播的方式修正错误的决策。第三，NNet可以在不同情况下表现出很大的灵活性，比如采用不同的神经网络结构、调整超参数等。

　　3.变异策略：变异策略（Mutation Strategy）是一种控制NNet的参数更新方式，目的是减少过拟合。一般来说，NNet在学习过程中会遇到过拟合的困境，即模型过度依赖训练数据而导致欠拟合。为了防止这种情况发生，就可以在每次迭代时引入一定的随机噪声，使得NNet在更新参数的时候不一定严格遵守原始目标函数，从而产生一定程度的自主学习能力。变异策略的具体做法是在NNet的权重矩阵中添加一个正态分布的噪声，并乘以一个较小的正数，以减轻模型的依赖。

　　4.参数复制：参数复制（Parameter Sharing）是指把训练好的模型参数复制到下一轮进行进一步训练。在AlphaGo Zero中，每一个训练好的模型都是相互独立的，没有共享参数。如果直接用同一个模型的话，就会造成前面的训练结果对后面的影响。因此，在训练过程中，模型的参数不是直接更新，而是按照一定比例复制到目标模型中。这样做的目的就是降低模型之间的耦合度，防止出现单个模型权重太大而导致其他模型无法学习的问题。

　　5.整体流程：AlphaGo Zero的整体流程如下：输入棋盘状态->蒙特卡洛树搜索MCTS生成不同子树->执行蒙特卡洛模拟->形成蒙特卡洛树->评估不同节点的价值->选择最佳节点->回溯到根节点->形成新子树->执行蒙特卡洛模拟->形成蒙特卡洛树->更新模型参数->重复以上过程直到满足结束条件。
# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
　　接下来，将详细介绍AlphaGo Zero算法的关键步骤及其数学模型公式。

　　AlphaGo Zero的蒙特卡洛树搜索MCTS算法包括两步：
　　　　1.selection：从根结点开始，通过一系列的比例因子来决定每一步的走法。具体的说，每一步随机地选择一个节点，并根据UCT（Upper Confidence Bounds for Trees）算法计算其累计访问次数、平均访问奖励和平均访问步数等信息。然后，根据这个评价标准来选择该节点的子节点，继续递归地进行下去。

　　　　2.Expansion：当某个叶节点的访问次数达到了阈值之后，即认为它是可扩展的（expandable），就需要进行扩展。具体地，我们从可行动作集中随机选取一个动作，然后根据当前棋盘的状态和该动作进行游戏模拟，得到下一步棋盘的状态、该动作的奖励以及是否获胜的信息。如果模拟结果显示出了这个动作是可行的，就把这个子节点加入到树中，并继续执行上述两个步骤，直到达到最终的结束状态。

　　UCB算法（Upper Confidence Bound for Trees，UCT）用来评估每一个结点的价值。UCB算法根据采样数和结点的平均价值来确定结点的置信度。具体的算法步骤如下：

　　　　1.初始置信度c_p=1/根节点的访问次数

　　　　2.每走一步，增加根节点的访问次数t

　　　　3.对每个子结点n，计算其UCB值：UCB = Q + c * sqrt[ln(父结点的访问次数)/n的访问次数]

　　　　4.选出当前结点的子结点，按照UCB值大小排序，并以此遍历各子结点

　　　　5.在每个结点中，根据累积访问次数和平均奖励更新该结点的值。

　　AlphaGo Zero的神经网络NNet是一个带有多层隐含层的深度神经网络，通过BP（Backpropagation）算法进行训练。BP算法利用Bellman方程，通过计算目标函数的一阶偏导数，逐渐逼近真实的目标函数。具体的训练步骤如下：

　　　　1.在初始时刻，神经网络的权重设置为随机值

　　　　2.利用蒙特卡洛树搜索算法生成一个完整的蒙特卡洛树

　　　　3.对该树中的每个叶结点，利用蒙特卡洛模拟计算其对应神经网络输出

　　　　4.对所有训练样本进行误差反向传播，计算出每个神经元的权重和偏置更新值

　　　　5.更新神经网络的权重，使得误差最小化

　　 AlphaGo Zero的变异策略主要是通过添加正态分布的噪声来引入随机性，以减少模型的过拟合。具体的算法步骤如下：

　　　　1.在每次迭代开始之前，先设置一个较小的学习速率

　　　　2.对神经网络的权重矩阵W进行随机扰动，根据指定的幅度和标准差来生成新的矩阵Z

　　　　3.计算新的神经网络的输出，并计算新的误差

　　　　4.根据新旧误差的比值，比较新旧误差，若小于一定值，则终止迭代，否则，更新学习速率并进入下一轮迭代

　　AlphaGo Zero的参数复制策略也比较简单，就是把训练好的模型参数复制到目标模型中，以减少模型之间的耦合度。具体的算法步骤如下：

　　　　1.在每一轮迭代开始时，生成一个新的神经网络，把旧模型的参数复制到新模型中

　　　　2.在每一步迭代中，修改新模型的参数，再把新模型的参数复制到目标模型中

　　　　3.重复上面两步，直到满足结束条件

　　 本文将专门对蒙特卡洛树搜索MCTS、神经网络NNet、变异策略和参数复制四个部分进行详细阐述。
　　首先介绍蒙特卡洛树搜索MCTS。蒙特卡洛树搜索算法（Monte Carlo Tree Search，MCTS）是一种基于蒙特卡洛树（Monte Carlo Tree，MT）的策略生成方法，它利用蒙特卡洛随机游走（Monte Carlo Exploring Starts，MCEE）方法来构建和评估可能的游戏状态。MCTS将游戏称为一个搜索问题，用树结构来表示所有可能的路径，并通过一次伪随机的游戏往返时间（Game-Playing Time）来估计不同节点的价值。在每一步迭代中，算法从根节点开始，以多种方式从各个子节点选择下一步的动作。每一步都用一定的概率（P）从可行动作集合中随机选取一个动作，并通过一次模拟来生成新的子节点。然后，根据子节点的访问次数、平均奖励和平均访问步数等信息，以UCT（Upper Confidence Bound for Trees）算法计算子节点的评估值，以便在下一步迭代时根据子节点进行决策。经过多次迭代后，算法终将收敛至最优的决策策略。

　　　　蒙特卡洛树搜索MCTS算法的核心思想是以多次模拟游戏来收集各节点的访问次数、平均奖励和平均访问步数等信息，并根据这些信息来选择最佳的下一步动作。蒙特卡洛树搜索算法的流程如下：

　　　　1.初始化根结点（或称为状态），作为搜索树的起始点

　　　　2.迭代搜索：根据根结点开始多轮蒙特卡罗模拟，依据访问次数、平均奖励、平均访问步数等信息，进行决策

　　　　3.回溯：根据决策路径，回溯到根结点，沿着路径上的每一条边，更新其访问次数、平均奖励、平均访问步数等信息

　　　　蒙特卡洛树搜索算法的关键点在于如何用一系列的比例因子来决定每一步的走法。蒙特卡洛树搜索算法用了UCB（Upper Confidence Bound）算法来衡量不同节点的价值，并基于此进行决策。UCB算法的思路是将平均奖励和置信度的系数结合起来作为节点的排序依据。置信度的定义为该节点的访问次数，系数c通常取1/√n，n为父结点的访问次数。UCB值表示了一个节点值的加权平均值，其计算方法为：V+ = V / N + c*sqrt(log N / n)，V是节点的平均奖励，N是父结点的访问次数，c是置信度的系数。这样，UCB算法能更精确地估算不同节点的价值，以便在搜索时选出最佳的下一步动作。

　　　　蒙特卡洛树搜索算法的效率非常高，它不需要事先知道游戏的规则，只需根据一组游戏数据的统计规律进行多轮蒙特卡罗模拟，从而找到全局最优的决策策略。这种快速、有效的方法使得蒙特卡洛树搜索成为许多AI领域中的常用方法。另外，蒙特卡洛树搜索算法还能有效处理“指导性”任务——不受限制地探索游戏空间以寻求最佳策略，这对于一些特定任务非常有用。例如，AlphaGo Zero就使用蒙特卡洛树搜索算法来探索游戏棋盘空间，找到对手的下一步落子位置。

　　接下来介绍神经网络NNet。神经网络（Neural Network，NN）是由多个节点组成的计算系统，通过非线性激活函数来模拟人脑的神经信号传递机制。神经网络是人工智能领域的一个基础概念，是模仿生物神经系统行为的理想模型。AlphaGo Zero的神经网络是多层隐含层的深度神经网络，由全连接层（fully connected layer）和卷积层（convolutional layer）构成。

　　　　AlphaGo Zero的神经网络由五个部分构成，分别是输入层（input layer）、神经层（neural layer）、输出层（output layer）、损失层（loss layer）和辅助层（supporting layer）。输入层接收外部输入，为棋盘状态编码得到特征表示。输入层输出是一个二维张量，其中每一个元素代表当前棋盘上某个位置的属性，如是否有子将、对手是否下子等。神经层接收输入特征，通过非线性激活函数来实现信息处理和计算。输出层接收神经层的输出，并通过softmax函数计算每个落子位置的概率。损失层接收输出的概率分布与标签的真实分布，计算交叉熵损失，以便训练神经网络。辅助层包括平滑项（smoothed item）、稀疏项（sparsity term）、L2正则项（L2 regularization）以及BN层（Batch Normalization Layer）等。

　　　　神经网络的学习过程可以分为两步，即预训练阶段和微调阶段。预训练阶段用于训练网络的基本功能，包括输出的概率分布与真实分布之间的差距，使得神经网络逐渐学会了识别棋盘状态、预测对手下子的位置以及下一个落子的概率分布。微调阶段用于优化网络的性能，通过反向传播算法来调整模型参数，使得神经网络在新的数据集上的表现更好。微调阶段的目标是最小化预训练阶段的损失，使得网络对于游戏的新情况有更强的鲁棒性。

　　　　AlphaGo Zero使用的神经网络结构是LeakyReLU（leaky ReLU）激活函数的多层密集连接神经网络，其中隐藏层有192、192、192个神经元。输入层的神经元个数等于棋盘宽度乘棋盘高度，对应着棋盘状态的特征数量。输出层有128个神经元，对应着128种落子位置的概率。损失函数是均方误差（Mean Squared Error，MSE），在训练过程中最小化预测的均方误差。

# 4.具体代码实例和详细解释说明
　　最后，将介绍AlphaGo Zero的Python实现，并用示例代码展示它的使用方法。

　　以下是AlphaGo Zero的Python实现的代码：

```python
import numpy as np
from copy import deepcopy
import time


class GoBang:
    def __init__(self):
        self.board = None   # 棋盘状态
        self.turn = None    # 当前走棋方
        self.prev_move = () # 上一步走的位置
        self.num_moves = 0  # 下一步还剩多少步
        self.actions = []   # 可行动作列表
        self.probs = []     # 每个动作的概率分布

    def reset(self):
        """
        初始化棋盘、当前走棋方、剩余步数
        :return: 
        """
        self.__init__()

    def set_board(self, board):
        """
        设置棋盘状态
        :param board: 棋盘状态
        :return: 
        """
        self.board = board

    def set_turn(self, turn):
        """
        设置当前走棋方
        :param turn: 当前走棋方
        :return: 
        """
        self.turn = turn

    def set_prev_move(self, prev_move):
        """
        设置上一步走的位置
        :param prev_move: 上一步走的位置
        :return: 
        """
        self.prev_move = prev_move

    def set_num_moves(self, num_moves):
        """
        设置下一步还剩多少步
        :param num_moves: 下一步还剩多少步
        :return: 
        """
        self.num_moves = num_moves

    def set_action_probabilities(self, actions, probs):
        """
        设置可行动作列表和每个动作的概率分布
        :param actions: 可行动作列表
        :param probs: 每个动作的概率分布
        :return: 
        """
        self.actions = actions
        self.probs = probs

    def get_possible_moves(self):
        """
        获取当前棋盘的所有可行动作
        :return: 可行动作列表
        """
        moves = [(i // self.board.width, i % self.board.height)
                 for i in range(len(self.board.state)) if self.board.state[i] == 0]
        return moves

    def execute_move(self, action):
        """
        执行动作并返回下一步棋盘状态、奖励和是否获胜信息
        :param action: 下一步动作
        :return: 字典对象{next_board, reward, done}
        """
        x, y = action
        move = (x * self.board.width + y)

        next_state = deepcopy(self.board.state)
        next_state[move] = self.turn
        reward = self._compute_reward()
        done = False

        info = {'value': -np.inf, 'policy': [], 'pv': ''}

        if self.is_game_over():
            done = True

        if not done and len(self.get_possible_moves()) == 0:
            print('warning! no possible moves left')
            done = True

        return {'next_board': GoBangBoard(next_state),
               'reward': reward,
                'done': done,
                'info': info}

    def is_game_over(self):
        """
        判断游戏是否结束
        :return: bool类型
        """
        return any([self.has_four_lines(),
                    self.has_three_connects()])

    def has_four_lines(self):
        """
        检查是否有四子连珠
        :return: bool类型
        """
        width = self.board.width
        height = self.board.height

        state = self.board.state.reshape((width, height))

        for player in [1, 2]:
            pieces = np.argwhere(state == player).tolist()

            lines = [[], [], [], []]
            for piece in pieces:
                ptx, pty = piece

                lines[0].append(pty)
                lines[1].append(-ptx)
                lines[2].append(-pty)
                lines[3].append(ptx)

                for line in lines:
                    counter = {}

                    for pt in line:
                        if abs(pt) < width//2:
                            continue

                        value = min(abs(pt)-1, max(counter.values()))

                        while value > 0:
                            for k in sorted(counter.keys()):
                                v = counter[k]

                                if v <= value:
                                    del counter[k]
                                else:
                                    counter[k] -= value
                                    break

                            value -= 1

                    keys = list(counter.keys())

                    for key in keys:
                        pos = (-key[0], -key[1])

                        if self.check_position(*pos):
                            return True

        return False

    def check_position(self, x, y):
        """
        检查指定位置是否有子
        :param x: 横坐标
        :param y: 纵坐标
        :return: bool类型
        """
        return 0 <= x < self.board.width and 0 <= y < self.board.height \
               and self.board.state[(x * self.board.width) + y]!= 0

    def has_three_connects(self):
        """
        检查是否有三个子连成线
        :return: bool类型
        """
        width = self.board.width
        height = self.board.height

        state = self.board.state.reshape((width, height))

        directions = [(-1, -1), (-1, 0), (-1, 1),
                      (0, -1),           (0, 1),
                      (1, -1),  (1, 0),  (1, 1)]

        player = self.turn

        count = 0

        for dx, dy in directions:
            for x in range(max(dx, -dx), width-min(dx, -dx)):
                for y in range(max(dy, -dy), height-min(dy, -dy)):
                    tx = x + dx
                    ty = y + dy

                    if tx >= 0 and tx < width and ty >= 0 and ty < height:
                        s = state[tx][ty]
                        if s == player or s == -player:
                            cx, cy = x - dx, y - dy
                            ex, ey = x + dx, y + dy

                            if self.check_position(cx, cy) and self.check_position(ex, ey):
                                return True

        return False

    @staticmethod
    def _compute_reward():
        """
        根据双方的连珠情况计算奖励
        :return: float类型奖励
        """
        pass


class GoBangBoard:
    def __init__(self, state):
        self.state = state   # 棋盘状态
        self.width = int(len(state)**0.5)   # 棋盘宽
        self.height = self.width      # 棋盘高

    def legal_moves(self):
        """
        获取当前棋盘的所有可行动作
        :return: 可行动作列表
        """
        go_bang = GoBang()
        go_bang.set_board(self)
        return go_bang.get_possible_moves()


class MCTSNode:
    def __init__(self, parent, prior_probability, position):
        self.parent = parent             # 父节点
        self.children = {}               # 孩子节点
        self.visited_times = 0           # 访问次数
        self.total_reward = 0            # 总奖励
        self.prior_probability = prior_probability    # 先验概率
        self.position = position         # 位置

    def expand(self, game):
        """
        在当前节点扩展子节点
        :param game: GoBang实例
        :return: 
        """
        valid_positions = game.legal_moves()
        probabilities = [self.prior_probability / len(valid_positions)
                          for _ in range(len(valid_positions))]

        for i in range(len(valid_positions)):
            child = MCTSNode(self,
                             prior_probability=probabilities[i],
                             position=valid_positions[i])
            self.children[tuple(valid_positions[i])] = child

    def update(self, result):
        """
        更新节点信息
        :param result: 节点结果，是一个字典对象{reward, visit_count, leaf}
        :return: 
        """
        self.visited_times += 1
        self.total_reward += result['reward']

    def best_child(self, temperature=1.0):
        """
        返回最佳子节点
        :param temperature: 探索度，越高，代表着更多的探索
        :return: 最佳子节点
        """
        total_visit_counts = sum(child.visited_times for child in self.children.values())
        children_value = {child: ((child.total_reward / child.visited_times)
                                  if child.visited_times > 0 else 0)
                          + temperature * math.sqrt(math.log(total_visit_counts) / child.visited_times)
                          for child in self.children.values()}
        best_child = max(children_value, key=lambda c: children_value[c])
        return best_child

    def is_leaf(self):
        """
        是否是叶子节点
        :return: bool类型
        """
        return len(self.children) == 0


class MonteCarloTreeSearch:
    def __init__(self, policy_network, exploration_param=1.41, rollout_limit=1000):
        self.root = None                    # 根节点
        self.policy_network = policy_network        # 策略网络
        self.exploration_param = exploration_param    # 探索参数
        self.rollout_limit = rollout_limit          # 模拟次数限制

    def search(self, root_position, root_player, time_budget):
        """
        搜索
        :param root_position: 根位置
        :param root_player: 根玩家
        :param time_budget: 时间预算
        :return: 最佳动作
        """
        start_time = time.time()
        game = GoBang()
        game.reset()
        game.set_board(GoBangBoard(np.zeros((game.width**2))))
        game.set_turn(root_player)
        node = MCTSNode(None, 1.0, root_position)
        self.root = node

        while time.time() - start_time < time_budget:
            leaf = self._select_leaf()
            self._evaluate(leaf, game, rollout=True)

        path = self._backtrack()
        current_node = self.root
        policy_probs = []
        for step in reversed(path[:-1]):
            current_node = current_node.children[step]
            prob = (current_node.visited_times
                     if current_node.visited_times > 0 else 0)
            policy_probs.insert(0, prob)

        pi = dict(zip(self.root.children.keys(), policy_probs))

        best_action = max(pi, key=lambda a: pi[a])
        value = self._value(best_action)

        return {'best_action': tuple(best_action), 'value': value}, pi

    def _select_leaf(self):
        """
        选择叶子节点
        :return: 叶子节点
        """
        current_node = self.root

        while not current_node.is_leaf():
            current_node = current_node.best_child(temperature=self.exploration_param)

        return current_node

    def _evaluate(self, leaf, game, rollout=False):
        """
        评估叶子节点的价值
        :param leaf: 叶子节点
        :param game: GoBang实例
        :param rollout: 是否进行模拟
        :return: 
        """
        if rollout:
            results = self._roll_out(game, limit=self.rollout_limit)
        else:
            game.execute_move(leaf.position)
            results = [{'reward': game.execute_move(child)[1]['value'],
                        'visit_count': child.visited_times}
                       for child in leaf.children.values()]
            game.undo_move()

        leaf.update(results)

    def _roll_out(self, game, limit=1000):
        """
        进行一次模拟
        :param game: GoBang实例
        :param limit: 模拟次数限制
        :return: 模拟结果
        """
        moves = game.legal_moves()
        random.shuffle(moves)
        rewards = []
        game_copy = deepcopy(game)

        for i in range(min(len(moves), limit)):
            game_copy.execute_move(moves[i])
            values = [self._value(action) for action in game_copy.legal_moves()]
            reward = values[random.randint(0, len(values)-1)]
            rewards.append({'reward': reward})
            game_copy.undo_move()

        mean_rewards = sum(r['reward'] for r in rewards) / len(rewards)
        visit_count = len(rewards)

        return {'reward': mean_rewards, 'visit_count': visit_count, 'leaf': True}

    def _backtrack(self):
        """
        从根节点一直回溯到叶子节点
        :return: 节点路径
        """
        path = []
        current_node = self.root

        while not current_node.is_leaf():
            path.append(random.choice(list(current_node.children.keys())))
            current_node = current_node.children[path[-1]]

        path.append(random.choice(list(current_node.children.keys())))
        path.reverse()

        return path

    def _value(self, action):
        """
        获取指定动作的价值
        :param action: 指定动作
        :return: 动作的价值
        """
        input_tensor = torch.tensor([[float(x)] for x in action]).unsqueeze(dim=0).to("cuda")
        with torch.no_grad():
            output = self.policy_network(input_tensor)[:, :, :-1]
            value = output.squeeze().data.cpu().numpy()[int(action[0]), int(action[1])]

        return value


if __name__ == '__main__':
    from model import PolicyValueNetwork

    device = "cuda" if torch.cuda.is_available() else "cpu"
    net = PolicyValueNetwork(height=19, width=19, output_dim=7 ** 2, device="cuda").to(device)

    mcts = MonteCarloTreeSearch(net, exploration_param=1.41, rollout_limit=1000)

    positions = [(3, 3), (4, 4), (5, 5)]
    players = [-1, 1, -1]

    history = []

    for i in range(len(players)):
        result, policy = mcts.search(positions[i], players[i], time_budget=60)
        best_action = result['best_action']
        history.append((best_action, policy))

        game = GoBang()
        game.reset()
        game.set_board(GoBangBoard(np.zeros((game.width ** 2))))
        game.set_turn(players[i])

        _, reward, done, info = game.execute_move(best_action)
        print('Player {}, Move {}'.format(players[i], best_action))

    winners = [1 if h[0] == (3, 3) or h[0] == (4, 4) or h[0] == (5, 5) else -1 for h in history]
    print('Result:', sum(winners))