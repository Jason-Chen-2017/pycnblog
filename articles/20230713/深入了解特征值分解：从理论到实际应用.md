
作者：禅与计算机程序设计艺术                    
                
                
## 特征值分解的由来
在许多应用中，都需要对矩阵进行分解。特别是在图像处理、信号处理等领域，矩阵往往十分复杂，因此需要用一些方法将其简化或分析出更加有意义的信息。其中最常用的方法就是特征值分解（SVD），它将矩阵分解成三个不同的矩阵相乘而得。

举个例子，比如有一个$n    imes m$维矩阵A，如果希望找到这个矩阵的秩为k的最小奇异值分解（singular value decomposition）$\underline{\mathbf{U}}\hat{\Sigma}\overline{\mathbf{V}}^T=\mathbf{A}$，那么就可采用特征值分解的方法。特征值分解可以将矩阵A分解为两个向量组成的矩阵和一个实对角矩阵：
$$\hat{\Sigma}=\begin{pmatrix}\sigma_1&\cdots&0\\ \vdots&\ddots&\vdots\\ 0&\cdots&\sigma_m\end{pmatrix}, \quad \mathbf{U}= \begin{pmatrix}\mathbf{u}_1&\cdots&\mathbf{u}_{n-r}\\ \hline \mathbf{0}&\cdots&\mathbf{0}\end{pmatrix}, \quad \overline{\mathbf{V}}= \begin{pmatrix}\mathbf{v}_1&\cdots&\mathbf{v}_r\\\end{pmatrix}$$
其中$\sigma_i>0$为奇异值（singular values），$\mathbf{u}_i,\mathbf{v}_j$为左奇异向量（left singular vectors）、右奇异向量（right singular vectors）。由于$\overline{\mathbf{V}}$只有$r$列，所以我们取$\mathbf{A}\approx\hat{\Sigma}\cdot \underbrace{\left(\frac{1}{\sqrt{\sigma_1}}\mathbf{u}_1\ldots\left(\frac{1}{\sqrt{\sigma_{r-1}}\mathbf{u}_{r-1}}\right)}_{    ext{$r$个奇异值对应的左奇异向量}}}^    ext{$\underline{\mathbf{U}}$矩阵}+\left(\frac{1}{\sqrt{\sigma_r}}\mathbf{v}_r\right)^    ext{$\overline{\mathbf{V}}$矩阵} \right.$

通过奇异值分解，我们就可以对原始矩阵进行有意义地分析了。例如，对于像图像这样的高纬数据，我们可以求出奇异值分解之后的前几项奇异值，并据此选取合适的特征个数，然后再利用这些奇异值对应的左奇异向量的图像特征进行分类、识别等。当然，还可以将奇异值分解用于机器学习中的数据降维、数据压缩、数据表示等。

## SVD的数学基础及其性质
### 对称正定矩阵
对于任意一个矩阵$    extbf{A} \in R^{m     imes n}$, 如果存在非负实数$x$,使得$    extbf{Ax}^T=xx^T    extbf{A}^T$成立，那么就称该矩阵$    extbf{A}$是对称正定的。也就是说，方阵$    extbf{A}$的每个主轴（eigenvector）之间是正交关系。

### 矩阵的奇异值分解
设$    extbf{A} \in R^{m     imes n}$, $    extbf{A} =     extbf{U} \mathbf{\Sigma} \overline{    extbf{V}}^T$, $\mathbf{\Sigma}$是一个$m     imes n$对角矩阵，对角线上的元素分别称为奇异值（singular values），且满足$\lambda_1 > \lambda_2 > \cdots > \lambda_n$。即：
$$    extbf{A} =     extbf{U} \mathbf{\Sigma} \overline{    extbf{V}}^T \Rightarrow     extbf{A}^\dagger = (\overline{    extbf{V}})^\dagger \mathbf{\Sigma}^{-1}     extbf{U}^\dagger = \mathbf{\Sigma}^{-1}     extbf{U}^\dagger \overline{    extbf{V}} \Rightarrow     extbf{A}^\dagger = \mathbf{\Sigma}^{-1}     extbf{U}^\dagger \overline{    extbf{V}} $$
这里$\overline{    extbf{V}}$是$    extbf{A}$的右奇异矩阵，而$    extbf{U}$则是$    extbf{A}$的左奇异矩阵。


### 矩阵的最大奇异值和最小奇异值的下标
当$\mathbf{\Sigma}$是对角矩阵时，我们可以得到矩阵的最大奇异值和对应下标，即$\lambda_\max = \max\{|\lambda_1|, |\lambda_2|, \cdots, |\lambda_n|\}$, $j_\max = \arg\max\{|\lambda_1|, |\lambda_2|, \cdots, |\lambda_n|\}$. 当$\mathbf{\Sigma}$不是对角矩阵时，我们无法直接求出矩阵的最大奇异值和对应下标。但是，可以取所有奇异值绝对值的最大值作为近似的值，并且记下对应下标，这一步被称为矩阵的谱范数分解（spectral factorization），即：
$$\hat{    extbf{A}} = \hat{\boldsymbol{u}}_1 \hat{\boldsymbol{\sigma}}_1 \hat{\boldsymbol{v}}_1^    op +...+ \hat{\boldsymbol{u}}_n \hat{\boldsymbol{\sigma}}_n \hat{\boldsymbol{v}}_n^    op $$
其中$\hat{\boldsymbol{\sigma}}_i = \mathrm{sgn}(\lambda_i)\sqrt{|(\lambda_i)|}$。则$\lambda_{\max} = \max_{i=1,\cdots,n} |(\lambda_i)|$,$j_{\max} = \arg\max_{i=1,\cdots,n} |(\lambda_i)|$. 

### 矩阵的谱范数分解
矩阵的谱范数分解（spectral factorization）是指对任意给定的矩阵，找出其最大奇异值和对应的奇异向量构成的矩阵。矩阵的谱范数分解形式如下：
$$\hat{    extbf{A}} = \hat{\boldsymbol{u}}_1 \hat{\boldsymbol{\sigma}}_1 \hat{\boldsymbol{v}}_1^    op +...+ \hat{\boldsymbol{u}}_n \hat{\boldsymbol{\sigma}}_n \hat{\boldsymbol{v}}_n^    op, \quad {\hat{\boldsymbol{\sigma}}}_i \ge 0$$
其中$\hat{\boldsymbol{\sigma}}_i = \sqrt{\lambda_i}$是矩阵$\hat{    extbf{A}}$的第$i$个奇异值，$({\hat{\boldsymbol{u}}}_i, {\hat{\boldsymbol{v}}}_i)$是矩阵$\hat{    extbf{A}}$的第$i$个奇异向量，$i=1,\cdots, n$. $\hat{    extbf{A}}$可以理解为由奇异向量的乘积生成的一个新矩阵。上述结论的证明过程比较复杂，读者可以参考以下两篇经典的文章进行了解：
* 莱昂哈德·戴明斯：《奇异值分解（singular value decomposition）》；
* 梅尔文：《数值分析》第四版，第九章，《奇异值分解》。

## SVD的作用
### 数据降维
当原始的数据维度过高时，可以使用SVD对其进行降维。举个例子，比如一张彩色图像矩阵$\mathbf{X} \in R^{h     imes w     imes c}$，其中$c$代表颜色通道数目。我们可以先计算各个通道的平均值，得到$m$维平均值向量$\bar{\mathbf{x}}$，然后减去这个平均值，得到$    ilde{\mathbf{X}}$。由于PCA只能找到线性相关的方向，而忽略了非线性的影响，因此SVD可以让我们找到更多有价值的信息。如：
$$    ilde{\mathbf{X}} =     ilde{\mathbf{X}} - (m_1\bar{\mathbf{x}}_1 + m_2\bar{\mathbf{x}}_2 + \cdots + m_c\bar{\mathbf{x}}_c)^{    op} \otimes \mathbb{I}_w$$
其中$m_i$为图像$    ilde{\mathbf{X}}$的第$i$个通道的平均值，而$\otimes$是Kronecker积运算符。

### 数据压缩
当原始的数据过多或者无关信息太多时，可以使用SVD进行数据压缩。举个例子，比如一张图片，可能有数千万个像素点。由于只需要几百个特征描述，因此可以用SVD进行数据压缩。如：
$$\hat{    ilde{\mathbf{X}}} = \hat{\mathbf{U}}_r \hat{\mathbf{\Sigma}}_r \hat{\mathbf{V}}_r^    op$$
其中$\hat{\mathbf{U}}_r \hat{\mathbf{\Sigma}}_r \hat{\mathbf{V}}_r^    op$是$    ilde{\mathbf{X}}$的最大奇异值分解。

### 数据表示
当对矩阵进行奇异值分解后，得到的几个矩阵的乘积能够反映矩阵本身的内容。尤其是在奇异值分解的过程中，得到的奇异向量能够反映数据的主要结构。例如：
* 奇异值：当原始矩阵有多个相同奇异值时，这些奇异值对应的奇异向量之间的投影也可能会变得很相似。因此，不同奇异值对应的奇异向量是有区别的；
* 奇异向量：这些奇异向量能够代表数据的某种重要方面，其长度决定着模型的复杂度。

