
作者：禅与计算机程序设计艺术                    
                
                
## 一、数据集简介

图卷积神经网络(Graph Convolutional Neural Network, GCN)是一种用于处理图结构数据的深度学习模型，能够有效地提取出图中节点间复杂的依赖关系并对不同子图之间的信息进行编码，从而可以用于图数据的分类、链接预测等任务。GCN在图数据分析领域已经成为主流方法之一，它的应用也越来越广泛。目前，图卷积神经网络主要分为两类:一种是基于卷积核的方法，另外一种则是基于注意力机制的方法。本文所论述的算法属于基于卷积核的方法。

随着近几年计算机视觉、自然语言处理等领域的发展，越来越多的图数据集被提出来作为研究的对象，例如图信号处理（Signal Processing on Graph）和推荐系统（Recommender Systems）。这些图数据集具有丰富的特征，如网络、文本、序列等，它们在图的表示、节点连接、节点聚类等方面都表现出了独特的特性。

其中一个重要的任务就是如何将图数据转换成图结构化输入，然后输入到图卷积神经网络中进行训练。为了解决这个问题，许多工作尝试着设计新的预处理方法，将图数据变换为适合图卷积神经网络的输入形式。但是，无论是用什么方式，都是需要一定代价的，因为图卷积神局网络模型要求其输入的数据要满足某些条件。因此，为了更好地理解和改进图卷积神经网络的预处理方法，作者希望通过研究方法和算法的原理，向读者介绍一种新的图数据预处理方法——结点归一化和邻接矩阵正则化。

# 2.基本概念术语说明
## （1）定义
### 图的邻接矩阵
图$G=(V,E)$是一个由顶点集合$V$和边集合$E$组成的集合，记作$\left<V, E\right>$，其中，$|V|=n$, $|E|=m$. 如果$(u,v)\in E$,$\left\{ u_{i}, v_{j}\right\}$则称$u$和$v$相邻；如果$(u,v)
ot \in E$，则称$u$和$v$不相邻。设$\mathcal{A}=\left(\begin{array}{ccc}{a_{1}} & {\cdots} & {a_{n}}\end{array}\right)$为二阶对角矩阵，它表示节点的特征向量；若$\left(\begin{array}{cc}{x}_{u}\\{y}_{u}\\{\vdots\\}\\{z}_{u}\end{array}\right)$和$\left(\begin{array}{cc}{x}_{v}\\{y}_{v}\\{\vdots\\}\\{z}_{v}\end{array}\right)$分别表示$u$和$v$的坐标，那么，图$G$的邻接矩阵可以定义如下：
$$A=D^{-1/2}AD^{-1/2}$$
其中，$D_{    ext{ii}}$表示节点$i$的度数，即边的条数。当$D_{    ext{ii}}$不存在或为零时，令$D_{    ext{ii}}=1$；然后，可以把$A$视为节点$i$的邻接矩阵。对于$k$-hop的邻居，只需把矩阵$A^{k-1}$乘以邻接矩阵$A$得到。具体计算方法见第四章。

### 图的Laplacian矩阵
图$G=(V,E)$的拉普拉斯矩阵（Laplace matrix）$L$是由节点特征向量$X$的函数：
$$L_{ii}=-d_i+w_i$$
组成的矩阵，其中，$d_i$表示节点$i$的度数，$w_i$表示节点$i$的权重，即$w_i=|\{ (u,v):(u,v)\in E,(u,i)\in E \}|$。拉普拉斯矩阵是图的一阶微分算子。对于无向图来说，$L$等于$I-D^{-1}W$。

## （2）定义
### 结点归一化（Normalization of Vertexes）
结点归一化（Normalization of Vertexes）又叫做节点中心化（Node Centric），是指将图中的每个结点的特征进行规范化，使得结点的特征向量在单位长度方向上满足向量空间的标准范数约束（即范数等于1）。结点归一化的方法一般包括以下几种：
#### 1.MinMax归一化
MinMax归一化是最简单的归一化方法，该方法将每个结点的特征归一化到[0,1]范围内，具体实现方法为：
$$
\hat{X}_{i}=
\frac{X_{i}-\min X_i}{\max X_i-\min X_i}
$$
#### 2.Z-Score归一化
Z-Score归一化也称为规范化，是另一种常用的归一化方法。该方法将每个结点的特征归一化到均值为0，方差为1的分布下，具体实现方法为：
$$
\hat{X}_{i}=
\frac{X_{i}-\mu_i}{\sigma_i}
$$
其中，$\mu_i$表示所有结点的平均值，$\sigma^2_i$表示所有结点的方差。

#### 3.Laplacian Eigenmaps方法
Laplacian Eigenmaps方法是对拉普拉斯矩阵进行特征分解，将拉普拉斯矩阵分解为对角矩阵和奇异矩阵的乘积，并利用奇异矩阵的特征值求出映射后的点位置，具体实现方法为：
$$
Y = X \Sigma V^T
$$
其中，$X$为原始特征矩阵，$Y$为映射后的特征矩阵，$\Sigma$为拉普拉斯矩阵的特征值矩阵，$V$为拉普拉斯矩阵的特征向量矩阵。


## （3）定义
### 邻接矩阵正则化（Regularization for Adjacency Matrix）
邻接矩阵正则化（Regularization for Adjacency Matrix）是指将邻接矩阵的元素添加正则项，目的是使得邻接矩阵具有稀疏性并且对估计的影响减小。常用的邻接矩阵正则化方法有：
#### 1.Symmetric Normalization
Symmetric Normalization 是一种常见的邻接矩阵正则化方法。该方法假设对角线上的元素$a_{ii}=1$，其他元素均为0，则可以通过将对角线上元素除以与该元素对应的行列式绝对值的倒数，并将除去绝对值之外的元素除以$s$，其中$s$为邻接矩阵元素的总和。这样就可以使得对角线上的元素的值较大，其他元素较小。
$$
A_{reg}=\frac{A}{D^{-1/2}W D^{-1/2}}
$$
#### 2.Degrees Centrality Normalization
Degrees Centrality Normalization 是另一种常见的邻接矩阵正则化方法。该方法通过将对角线上的元素除以与该元素对应的度数$d_i$的倒数，并将除去对角线之外的元素除以$s$，其中$s$为邻接矩阵元素的总和。这样就可以使得对角线上的元素的值较大，其他元素较小。
$$
A_{reg}=\frac{A}{D^{-1}+I}
$$
#### 3.Adding Small Factors to the Diagonal
Adding Small Factors to the Diagonal 是一种特殊的邻接矩阵正则化方法。该方法通过给对角线元素加上一个很小的因子，将邻接矩阵的精确值保留下来。一般地，我们将该因子设置为某个范围内的随机数。

