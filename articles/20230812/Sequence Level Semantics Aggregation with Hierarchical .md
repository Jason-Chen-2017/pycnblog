
作者：禅与计算机程序设计艺术                    

# 1.简介
  

序列级语义聚合是NLP领域中一个重要方向，它通过利用上下文信息提升模型的理解能力。目前主要由三种方法进行实现：Global-Level Semantics Aggregation (GSA)，Tree-Structured Recursive Neural Network (TSRNN) 和 Hierarchical Recurrent Neural Network (HRNN)。本文将会分别介绍这几种方法及其优缺点，并详细阐述一种新的基于树结构的双向递归神经网络(BDRNN)——Hierachical BiDirectional RNN(HRBiRNN)。

# 2.背景介绍
序列级语义聚合主要是基于上下文信息的文本表示学习，通过从词、短语、语句到整个句子级别的建模，得到整体的语义表示，而后通过聚合这些表示来完成任务。它的目的是增强模型的理解能力，使得模型能够更准确地理解输入数据中的含义。由于不同层次的语义层次越往下，表示越抽象，因此更具全局性。例如，在文章分类任务中，如果直接将句子作为输入，则模型只能看到单个句子的所有内容，无法获取句子之间的关联，容易导致模型无法捕获长距离关系。相反，如果采用词、短语或者子句作为输入，则能够更好地捕获局部或全局的语义信息，但会丢失句子内相关性。

不同的方法之间存在着诸多差异，如图1所示。



GSA属于全局级语义聚合方法，它通过将整句话作为一个整体来处理，将所有的词、短语、甚至整个句子都看作同样的一组信号进行处理。GSA最大的问题就是不能捕获到长范围的依赖关系，因此无法很好的处理文本生成任务。

TSRNN是一个采用树形结构的递归神经网络，每一个节点代表了一个词、短语或者句子，内部节点代表子树，边代表关系。它通过建立树形结构，并且在每一层上都做语义表示的学习。TSRNN的优点是可以很好的捕获到长距离的依赖关系，但不足之处是需要定义树形结构，且计算量较大。

HRNN的基本想法是在每一层间引入递归连接，将不同层次之间的表示融合起来，使得不同层次的表示能够互相影响，同时也可有效降低模型的复杂度。HRNN的优点是不需要定义树型结构，可以适用于各种类型的序列模型，且能够对长距离依赖关系进行建模，但是它可能存在梯度消失或梯度爆炸等问题。

本文介绍一种新的基于树结构的双向递归神经网络——HRBiRNN。HRBiRNN采用了树结构，其中每个结点代表一个词、短语或者句子，内部结点代表子树，边代表关系，不同层次的结点之间通过递归连接来融合上下文信息。HRBiRNN还可以处理长距离依赖关系，而且通过双向递归神经网络可以克服梯度消失和梯度爆炸问题，并且可以在多个层级上学习到特征表示，最终达到比其他方法更好的性能。HRBiRNN的基本原理如下：

给定一个序列 $X=\{x_i\}_{i=1}^n$ ，首先将其分割成不同层级的词、短语和句子，树状结构如下：

$$T = \left\{ \begin{array}{ll} S & : x_1 \\ D_{j-1}(S) & : x_j \\ D_{k-1}(D_{j-1}(S)) & : x_k \\ D_{m-1}(D_{k-1}(D_{j-1}(S))) & : x_m \\ \cdots\\ D_l(D_{m-1}(\cdots D_{k-1}(D_{j-1}(S)))) : x_n \end{array}\right.$$ 

其中 $S$ 是根结点，$D_j(\cdot)$ 表示第 $j$ 个叶子结点到根结点的路径。如此构成的树结构使得HRBiRNN可以从不同层次获取语义信息，并且在各层间具有可传递性，因此可以捕获长距离的依赖关系。

假设层 $i$ 的结点个数为 $|V_i|$ ，那么每个结点 $v_i^j$ 在该层的表示由一个隐藏层 $h_i^{j, t}$ 生成，其中 $t$ 为时间步，并且参数化为 $W^{ji}, U^{ij}, b_i^j$ 。假设上下文窗口大小为 $\delta$ ，则 $v_i^{j+1}$ 的生成可以由以下的双向递归神经网络公式表达：

$$v_i^{j+1} = [\overrightarrow{h}_i^{j, t+1}; \overleftarrow{h}_i^{j+1, t}] = \sigma(\overrightarrow{h}_i^{j, t})[\overrightarrow{h}_i^{j, t+1}; \overleftarrow{h}_i^{j+1, t}] + v_i^j \odot \tanh(c_i^j[\overrightarrow{h}_i^{j, t+1}; \overleftarrow{h}_i^{j+1, t}]) $$

其中，$\sigma(\cdot)$ 是激活函数，$\odot$ 表示对应元素乘积，$\overrightarrow{h}_i^{j, t+1}$ 和 $\overleftarrow{h}_i^{j+1, t}$ 分别是向右和向左两个方向的隐藏状态，$c_i^j$ 是权重矩阵。

最后，通过将所有层级上的隐藏状态串联起来得到最终的表示：

$$r_j = [r_1^j; r_2^j; \ldots ; r_l^j]$$

其中 $r_i^j$ 表示层 $i$ 的第 $j$ 个叶子结点的表示。本文将HRBiRNN与传统的自然语言模型RNN、Transformer等结合使用，取得了比传统方法更高的效果。