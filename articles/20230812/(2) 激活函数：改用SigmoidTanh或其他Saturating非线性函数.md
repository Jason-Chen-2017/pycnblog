
作者：禅与计算机程序设计艺术                    

# 1.简介
  

机器学习中，激活函数（Activation Function）是一个神经网络模型中非常重要的组成部分。不同的激活函数对神经网络的训练方式影响很大。本文从广义上理解激活函数并讨论其作用及不同激活函数之间的异同。
# 2.基本概念和术语
## 激活函数

在人工神经网络（Artificial Neural Network，ANN）中，激活函数起着最重要的作用之一。它是指用来对输出值进行非线性变换的函数。激活函数的引入主要是为了解决非线性组合函数的不易优化的问题，使得神经网络能够拟合复杂的非线性函数关系，提升学习能力。下面，让我们看一下什么是激活函数。

激活函数（activation function）是一个神经元中的一个处理单元，接收输入信号，通过某种计算，将输入信号转换为输出信号。激活函数通常会限制或者改变输入信号的取值范围，输出信号的值域通常在[0, 1]或[-1, +1]之间。这些特性对于不同层的神经网络节点、边界、权重等都起到重要的作用。

目前来说，激活函数分为两大类：

1. Sigmoid 函数
2. Tanh 函数

还有一些其他激活函数如 ReLU、LeakyReLU、ELU 等，但它们往往只用于某些特定领域。

## Sigmoid 函数

Sigmoid函数，也称Logistic函数，是一个S型曲线形状，属于概率联系函数，又叫做“逻辑斯谛函数”。

Sigmoid函数由以下公式定义:

$$
f(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{e^{x}+1}
$$

当$x \rightarrow -\infty$时，$f(x) \rightarrow 0$；而当$x \rightarrow \infty$时，$f(x) \rightarrow 1$。因此，Sigmoid函数常被用作神经网络的激活函数。

### 优点

1. S形曲线回归平滑，易于导数求导，梯度下降收敛快。
2. 输出结果位于(0,1)区间，便于输出值的映射。
3. 可微，可以直接求导得到每个隐含层神经元的导数，实现梯度下降法更新权值。

### 缺点

1. 在一定程度上造成了饱和现象，sigmoid函数容易把梯度弄到最大值或最小值，导致模型无法继续优化。
2. 对输入的敏感度较强，如果输入值较小，那么输出可能很小，导致网络无法学习长期依赖关系。

## Tanh 函数

Tanh 函数也称双曲正切函数（Hyperbolic Tangent），是Sigmoid 函数的一种修正版本，它的输出值域为(-1,+1)。它的表达式如下：

$$
f(x)=tanh(x)=\frac{\sinh(x)}{\cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

Tanh 函数是 Sigmoid 函数的平滑版本，即它的曲线比Sigmoid函数光滑很多，而且输出值域为(-1,+1)，因此它被广泛地使用作为激活函数。与Sigmoid函数相比，Tanh函数在激活后有更大的范围，适用于需要输出范围比较广的场合。但是，它存在梯度消失、梯度爆炸的问题，并且在不同的初始化条件下，Tanh函数的表现也是不一致的。所以，Sigmoid函数和Tanh函数各有特点。一般情况下，选择Sigmoid函数作为激活函数。

## 选择合适的激活函数

无论选择哪个激活函数，关键都是要找到一个好的函数，能够抑制模型的过拟合现象，并保持较好的性能。