
作者：禅与计算机程序设计艺术                    

# 1.简介
  


Locally Linear Embedding (LLE)是一种非线性降维方法，它可以用来分析、理解、可视化高维数据集中的全局结构信息，并从中发现数据的内在规律，是一种无监督学习的方法。它最早由<NAME>和<NAME>于2003年提出。它的主要思想是通过一个局部空间模型来学习数据的低维表示，使得样本之间的距离相似，但样本点到局部空间的映射不相似。LLE算法通过对原始数据进行采样，将每个样本投影到一个嵌入空间中的一组连续值上，同时保持了局部几何结构的信息。这种空间嵌入的目的是能够保留原始数据中的几何结构信息，以及其局部邻域的关系。

LLE算法属于Manifold Learning (流形学习) 的一种，属于一种线性无约束优化问题，也可以说是一种形式的特征映射方法。它通过一个局部线性嵌入矩阵C来定义，C是一个低维空间中的坐标系，每一个样本都可以在这个坐标系下得到一个坐标向量。对于每个样本i，我们希望找到一个最小均方误差(MMD)的映射：

$$\min_{c_i} \sum_{j=1}^N [||x_i-c_ix_j||^2+\beta||x_i-y_i+c_i(\mu-\frac{1}{2})^Tx_i||^2]$$

其中$x_i$, $x_j$分别是样本$i$, $j$的特征向量；$\beta$ 是正则化参数，控制约束条件强弱；$\mu$ 为均值向量，控制投影方向；$y_i$是投影目标向量，通常是输入空间中平均值的映射。

# 2.基本概念术语
## 2.1 低维空间
假设原始数据集是$X=\{x_1,\cdots, x_n\}$，其中$x_i\in R^{m}$。该数据集可以看作是一个二维或三维图像，或者是具有高度维度的样本集合。如果我们希望从原始数据中提取出有意义的信息，并建立起具有全局联系的有效模型，那么首先需要考虑是否可以将其转换到一个低维空间去研究。通常情况下，较低的维度可以提供比高维更有价值的线索。

LLE算法是在欧式空间中寻找一个低维嵌入空间$Z$，满足以下性质：

1. 具有相同的测度
2. 直观可见地表示复杂的高维分布

基于这样的性质，可以通过一些算法（如主成分分析PCA）来找到$Z$。然而，由于高维空间存在着各种噪声影响，LLE算法还有一个优点就是能够利用局部结构信息来降低噪声。

## 2.2 局部线性嵌入矩阵C
$C$是一个$d\times k$矩阵，代表了一个低维嵌入空间。$k$为嵌入的维度，$d$为原始数据特征的维度。每一个样本$x_i$都对应了一个$k$维向量$z_i=(z_{i1},\cdots, z_{ik})\in Z=\mathbb{R}^{k}$。为了计算方便，可以用$c_i=(c_{i1},\cdots, c_{id})\in C=\mathbb{R}^{d}$来表示每个样本的嵌入坐标。

$C$可以被看做是一个低维空间中每一个样本的坐标系。它不是绝对不变的，而是受到重构误差的影响。因此，嵌入后的样本点与原来的样本点之间距离仍然可能存在差异。

## 2.3 概率分布族
LLE算法假设输入样本是由一个概率分布族生成的，该分布族是由一系列具有不同参数的连续型分布组合而成的，这些分布可以看作是局部空间的结构。

具体来说，概率分布族可以由多个具有不同参数的多元正态分布族($\mathcal{N}(x;\mu, \Sigma)$)的组合而成。一般来说，某个样本$x_i$的$k$维表示可以表示为：

$$p_{\theta}(x_i)=\frac{1}{Z(\theta)} \prod_{l=1}^{k}\pi_{l}(x_i; \theta_{l}) \mathcal{N}(x_i;\mu_{l},\Sigma_{l}), l=1,\cdots, k$$

这里，$Z(\theta)$表示所有概率密度函数的积分；$\pi_{l}(x_i; \theta_{l})$表示第$l$个多元正态分布的参数，即$\pi_{l}(x_i; \theta_{l})=\frac{1}{\sqrt{(2\pi)^d|\Sigma_{l}|}}exp(-\frac{1}{2}(x_i-\mu_{l})^{\mathrm{T}}\Sigma^{-1}_{l}(x_i-\mu_{l}))$；$\mu_{l}$和$\Sigma_{l}$分别表示第$l$个多元正态分布的均值向量和协方差矩阵；$\theta=[\theta_{1},\cdots, \theta_{k}]$表示分布族的参数向量。

由此，概率分布族可以表示为：

$$P_{\theta}(x):=\frac{1}{Z(\theta)}\prod_{l=1}^{k}\pi_{l}(x; \theta_{l}) \mathcal{N}(x;\mu_{l},\Sigma_{l}).$$

其中，$P_{\theta}$表示分布族，$\theta$表示参数向量，$Z(\theta)$表示归一化因子。

## 2.4 目标函数

LLE算法的目标函数是一个正则化的最小均方差估计，即：

$$\min_{C}\sum_{i=1}^N \sum_{j=1}^N ||x_i-c_ix_j||^2+\beta||x_i-y_i+c_i(\mu-\frac{1}{2})^Tx_i||^2,$$

其中，$x_i$ 和 $x_j$ 分别表示两个样本点，$c_i$ 表示样本$i$的嵌入向量，$\beta$ 是正则化参数，控制约束条件强弱；$\mu$ 为均值向量，控制投影方向；$y_i$是投影目标向量，通常是输入空间中平均值的映射。

注意：$C$ 是待求解的变量，也就是所要学习的嵌入矩阵。

## 2.5 拟牛顿法
拟牛顿法是一种近似解高维空间的优化问题的方法，它通过牛顿法迭代求解海森矩阵的逆矩阵来得到近似解。海森矩阵是一个矩阵，其元素为关于变量变化的雅各比矩阵的积分，是一个普遍存在的问题。LLE算法使用拟牛顿法来解决这一优化问题。

首先，给定一个初始的猜测值$C^{(0)}$，利用梯度下降法更新该值：

$$C^{(t+1)}=C^{(t)}+\alpha \Delta C^{(t)},$$

其中$\Delta C^{(t)}=-\nabla f(C^{(t)})$，即目标函数$f(C)$的负梯度；$\alpha$ 是步长。

然后，利用拟牛顿法对猜测值$C^{(t)}$进行修正：

$$\begin{aligned}
\delta C^{(t)}&\approx H^{-1}\nabla f(C^{(t)}) \\
&=-H^{-1}\left(\sum_{i=1}^N \sum_{j=1}^N 2(x_i-c_ix_j)(c_{ji}-c_{ij})+(b+\beta y_i+c_i(\mu-\frac{1}{2})^Tx_i)\right),\\
&=-A+B(h_C)+B(h_\beta h_\mu). 
\end{aligned}$$

其中，$H$是海森矩阵，它可以看做是梯度的二阶矩；$A$表示平凡项，即$f(C)$的一阶项；$B$表示二阶项；$h_C$表示沿着$C$轴的梯度；$h_\beta$表示沿着$\beta$轴的梯度；$h_\mu$表示沿着$\mu$轴的梯度。

接着，利用修正值$\delta C^{(t)}$更新猜测值：

$$C^{(t+1)}=C^{(t)}+\delta C^{(t)}.$$

重复以上过程，直到收敛。

## 3.数学原理
## 3.1 流形学习的概念
流形学习（Manifold Learning）是指利用样本数据构建回归模型时，通过学习数据的局部结构，将原始数据投影到一个合适的低维空间（通常是欧式空间），从而实现数据可视化、分类、聚类等目的。

流形学习的特点有：

1. 提供一种更简洁、易于理解的模型表示，同时能够在高维空间发现隐藏的模式；
2. 通过投影到低维空间，获得更多有用的信息，在某些情况下可以替代传统的核技巧，如SVM和PCA；
3. 可以用于数据集中含有噪声的数据。

流形学习方法可以分为基于样本的流形学习、基于内核的流形学习和基于图的流形学习三种类型。本文只讨论基于样本的流形学习方法，即利用样本数据构造流形模型，然后应用全局方法来学习低维空间的嵌入表示。

## 3.2 局部线性嵌入模型
LLE算法利用局部线性嵌入矩阵$C$来定义，$C$是一个低维空间中的坐标系，每一个样本都可以在这个坐标系下得到一个坐标向量。LLE算法的主要思想是通过一个局部线性嵌入矩阵$C$来定义，$C$是一个低维空间中的坐标系，每一个样本都可以在这个坐标系下得到一个坐标向量。

假设原始数据集是$X=\{x_1,\cdots, x_n\}$，其中$x_i\in R^{m}$。在LLE算法中，首先随机地从原始数据中抽取若干个点作为子集$S$。然后，我们选择一个映射$\phi:S\rightarrow R^d$，把原始空间中的样本投影到$\phi(S)$上，记为$\hat{x}_i=M(x_i)-Q$。这里，$M$是一个映射，它将样本从原始空间映射到高维空间；$Q$是一个偏移向量，它保证每个样本都落在同一个集合$S$中；$-Q$是$M^{-1}(Q)=\text{argmax}_q M(q)$，即$Q$使得$M$最大。

记$F$为子集$S$上的核函数矩阵，$F_{ij}=K(x_i,x_j)$，这里，$K$是核函数，$K(x_i,x_j)$表示$x_i$和$x_j$的核值。令$\tilde{\phi}(\cdot)$表示对$\phi$的限制，且仅依赖于子集$S$：

$$\tilde{\phi}(u)=M(\sum_{i\in S}w_ix_i)$$

此处，$w_i$为$x_i$在子集$S$上的权重。

那么，$\hat{x}_i$就可以被解释为在$\tilde{\phi}(S)$下的坐标，等于：

$$\hat{x}_i=\tilde{\phi}(x_i)$$

再对权重$w_i$进行规范化，得到：

$$\bar w_i = \frac{w_i}{\sum_{j\in S}w_j}$$

此时，$\hat{x}_i$就表示在$\tilde{\phi}(S)$上的坐标，而且对权重进行了规范化。

那么，为什么要采用这种方法呢？因为在学习嵌入矩阵$C$时，有两条限制条件：

1. 对称性：$\forall i, j,\quad C(x_i,x_j)=C(x_j,x_i)$
2. 可微性：$C(x_i,x_j)=\int_x K(x',x_j)C(x',x_i)dx'$

因此，我们必须构造一个满足这两条限制条件的矩阵$C$.

## 3.3 目标函数及约束条件
LLE算法的目标函数是：

$$\min_{C}\sum_{i=1}^N\sum_{j=1}^Nc(x_i,x_j) - (\lambda/2)\|C\|_{F}^{2}.$$

其中，$c(x_i,x_j)$表示样本$x_i$和$x_j$之间的核函数值；$\lambda$是正则化参数，控制约束条件强弱；$\|C\|_{F}$表示$C$的范数，这里，范数是指矩阵元素绝对值的总和。

对应的约束条件是：

$$\forall i,\quad \|C_{:,i}\|=1,$$

$$\forall i,j,\quad c(x_i,x_j)=c(x_j,x_i),$$

$$\forall i,\quad \langle C_{:,i},M(x_i)-Q\rangle=\langle \tilde{\phi}(S),w_i\rangle.$$