
作者：禅与计算机程序设计艺术                    

# 1.简介
  

“神经元”是神经网络中的基本计算单元。在输入信号到达后，会经过一个非线性函数处理，即“激活函数”。这个非线性函数决定了网络的复杂程度和拟合能力。激活函数可以使得输出值在一定范围内波动，从而能够更好地适应输入数据分布。不同的激活函数对模型的效果会有较大的影响。常用的激活函数包括Sigmoid、tanh、ReLu和Leaky ReLU等。本文主要讨论两种常用的激活函数：ReLU（Rectified Linear Unit）和Leaky ReLU。
## 2.1 激活函数简介
### (1) Sigmoid 函数
 sigmoid 函数最早由 <NAME> 和 <NAME> 在 1958 年提出。sigmoid 函数也叫做 logistic 函数，定义域为 $(-\infty,+\infty)$ ，值域为 $[0,1]$ 。其函数形状类似于正态曲线，上下起伏不定跳跃，因此被称作 sigmoid 函数。如下图所示：
 
 
 上图中，$x$ 为输入信号，$y$ 为 sigmoid 函数输出值。
 ### (2) tanh 函数
tanh 函数也是一种 sigmoid 函数，但是它的曲线变得平滑一些，因此被称作 tanh 函数。函数表达式如下：
$$
f(x)=\frac{\sinh{(x)}}{\cosh{(x)}}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$
其中 $\cosh x$ 是双曲余弦函数，$\sinh x$ 是双曲正弦函数。如下图所示：


上图中，$x$ 为输入信号，$y$ 为 tanh 函数输出值。

### (3) ReLU 函数
ReLU 函数，即 Rectified Linear Unit 函数，是非常流行的激活函数之一。它也叫做修正线性单元函数或规范化线性单元函数。它的函数形式为:
$$
f(x)=max(0,x), \quad for \,\, x<0; f(x)=x, \quad for \,\, x\geqslant 0
$$
ReLU 函数是所有激活函数当中最简单的一种，因为它只保留输入信号 x 当 x>=0 时的值，并将其他值全部截取为 0。如下图所示：


上图中，$x$ 为输入信号，$y$ 为 ReLU 函数输出值。

### (4) Leaky ReLU 函数
Leaky ReLU 函数又称为 Parametric ReLU 函数，它的优点是在负区间下，神经元的输出不是 0，而是具有一定的斜率，避免了死亡神经元的问题。其函数形式为：
$$
f(x)=\left\{
    \begin{array}{ll}
      ax & : x < 0 \\
      x & : x \geqslant 0
    \end{array}
  \right.
$$
其中 a 表示负区间时的斜率。如图所示：


上图中，$x$ 为输入信号，$a$ 为斜率参数，$y$ 为 Leaky ReLU 函数输出值。

## 2.2 使用 ReLU 函数的原因
ReLU 函数的好处很简单，就是能够减少梯度消失或者爆炸现象。解决这一问题的方法之一就是让神经元的输出值有一定的惩罚作用，即在负区间的输出值要小于 0，这样的话，即便出现了一个大的梯度误差，但是如果没有采用 ReLU 函数作为激活函数，那么很可能会直接消失掉。另一方面，ReLU 函数具有良好的抗阻击性能，能够有效防止网络的过拟合。

## 2.3 使用 Leaky ReLU 函数的原因
Leaky ReLU 函数相对于普通的 ReLU 函数来说，有一个缺点就是负半区间的输出值太小了，导致网络训练时容易发生梯度消失或爆炸现象。为了解决这一问题，Leaky ReLU 函数引入了一个恒定的斜率项 a，使得负半区间的输出值不至于太小。也就是说，对于负半区间的输入信号，其输出值将比普通的 ReLU 函数稍微大一些，从而抵消了负半区间的输出值的折衷。因此，Leaky ReLU 函数能够缓解梯度消失或者爆炸现象的发生，从而使得网络学习过程更加顺利。