
作者：禅与计算机程序设计艺术                    

# 1.简介
  

操作系统（Operating System，简称 OS），是管理计算机硬件和软件资源的程序集合，是计算机系统的基石。其作用主要包括任务调度、内存分配、资源分配、文件管理、设备管理等。操作系统可以分为内核和用户层两部分。内核负责最基本的进程和线程管理、内存管理、设备管理等，而用户层则提供各种应用接口，包括命令行界面、图形界面、网络接口等。由于操作系统涉及到底层硬件和驱动程序的复杂性，使得操作系统成为系统中最复杂的部分之一。因此，对操作系统进行完整、准确地设计和实现，需要对计算机底层知识有较为深入的理解。

本次课程实验主要基于MIT 6.S081中的课程内容，完成以下几个作业：

1. 编写一个多进程程序，模拟多个计算密集型进程同时运行。
2. 使用虚拟存储器（virtual memory）技术优化你的程序，使其在内存和磁盘上交换的数据量更少。
3. 研究并开发一种新的线程调度算法，使线程之间能够获得更好的平衡性。
4. 实现一个动态内存分配算法，提高程序运行时的内存利用率。
5. 为你的操作系统增加新功能，比如支持虚拟终端、虚拟文件系统等。

# 2. 编写一个多进程程序
## 2.1 多进程程序的特点
一个多进程程序通常由多个相互独立的执行过程组成，每个执行过程都是一个单独的进程。这些进程共享同一份内存空间，可以并发执行，互不影响。

当我们点击一个按钮或打开某个网页时，操作系统就会创建一个新的进程来处理这个请求。我们使用的所有应用程序都遵循这种方式，例如：Word 正在编辑文档，浏览器正在加载页面，邮件客户端正在发送/接收邮件等。

除了创建新的进程外，操作系统还可以在一个进程内创建多个子进程，这些子进程可以共享父进程的地址空间、数据结构等资源，从而实现更细粒度的并发。

## 2.2 Python 中的多进程程序

Python 提供了 multiprocessing 模块，可以使用该模块创建和管理多进程程序。下面给出了一个简单的多进程程序：

```python
import time
import random
from multiprocessing import Process


def worker(interval):
    """
    计算密集型工作进程
    :param interval: 进程间隔时间
    :return: None
    """
    pid = os.getpid()
    print('Starting process %s' % (pid))
    start_time = time.time()

    while True:
        # simulate computationally intensive work
        x = sum([random.randint(-10**9, 10**9) for i in range(1000)])

        # check if it has been long enough since the last update
        elapsed_time = time.time() - start_time
        if elapsed_time > interval:
            break
    
    print('Exiting process %s after %.2f seconds.' %
          (os.getpid(), elapsed_time))


if __name__ == '__main__':
    processes = []
    num_processes = 3

    # create and start worker processes
    for i in range(num_processes):
        p = Process(target=worker, args=(i+1,))
        processes.append(p)
        p.start()

    # wait for all worker processes to finish
    for p in processes:
        p.join()
```

这里定义了一个 `worker` 函数，它会持续运行，每秒钟生成随机整数列表求和，直到间隔时间超过指定的 `interval`。为了演示多进程的效果，程序启动三个这样的进程，并等待它们完成。程序的输出如下所示：

```
Starting process 3376
Starting process 3377
Starting process 3378
Exiting process 3376 after 1.00 seconds.
Exiting process 3377 after 2.00 seconds.
Exiting process 3378 after 3.00 seconds.
```

可以看到，三个子进程分别各自占用 CPU，互不抢占，最终完成了全部工作。

## 2.3 C++ 中创建线程的两种方法

C++ 中也提供了创建线程的方法。一种方法是继承于 `std::thread`，然后重载它的 `run()` 方法：

```c++
#include <iostream>
#include <chrono>
#include <thread>

void threadFunc() {
    std::cout << "Thread function running..." << std::endl;
    std::this_thread::sleep_for(std::chrono::seconds(1));
    std::cout << "Thread function finished." << std::endl;
}

int main() {
    // Create a thread object that executes threadFunc
    std::thread t(threadFunc);
    
    // Wait for thread to complete before proceeding with rest of code
    t.join();

    return 0;
}
```

另外一种方法是直接调用 `pthread_create()` 创建线程：

```c++
#include <iostream>
#include <pthread.h>
#include <unistd.h>

void* threadFunc(void*) {
    std::cout << "Thread function running..." << std::endl;
    sleep(1);
    std::cout << "Thread function finished." << std::endl;
    return nullptr;
}

int main() {
    pthread_t tid;
    pthread_create(&tid, nullptr, threadFunc, nullptr);

    pthread_join(tid, nullptr);

    return 0;
}
```

以上两种方法都是正确的，但有些时候我们可能希望能够通过创建多个线程，将多个计算密集型任务划分到不同的线程中去。

# 3. 使用虚拟存储器优化程序

## 3.1 概念

虚拟存储器（Virtual Memory，VM）是操作系统提供的一个重要机制，它允许进程像访问普通存储器一样访问主存，但实际上并不是把整个程序加载到主存，而是让程序运行过程中访问到的部分暂时存储在磁盘上，只有到了真正需要访问的时候，才将部分内容从磁盘读入主存。

假设进程 A 在某时刻访问地址 `a`，如果地址 `a` 不在进程 A 的有效内存范围内，那么操作系统会通过一次页故障（page fault）异常，将进程 A 需要访问的部分复制到一个物理页面中，然后将该页面映射到进程 A 的虚拟地址空间，使得进程 A 可以继续正常地执行指令。这就是虚拟存储器的基本原理。

虚拟存储器提供给进程更多的内存空间，因为它并没有限制进程只能在自己的私有内存中运行，而是可以访问系统的全部可用内存。但是，过多的进程同时驻留在内存中可能会导致内存碎片化，造成效率低下，所以在程序运行时，应及时释放不需要的页面，以便回收内存。

## 3.2 优化方式

虚拟存储器可以通过以下几种方式优化程序：

1. 固定分配法：按照程序的大小，设置固定的内存分配策略；
2. 可变分配法：允许程序动态调整内存分配策略；
3. 请求分页存储器管理：将程序的逻辑地址空间划分成大小相同且连续的物理页面，并记录每个页面被映射到哪个进程的地址空间；
4. 请求段式存储器管理：将程序的逻辑地址空间划分成固定长度的段，每个段对应一个磁盘上的文件，记录每个段所在的文件偏移量。

## 3.3 Python 中使用虚拟存储器

下面给出一个简单示例，使用 Python 中的 `multiprocessing` 模块创建两个进程，模拟一个计算密集型任务：

```python
import os
import numpy as np
import psutil
import multiprocessing as mp


def compute():
    size = 10 * 1024 ** 2   # allocate 10 MB data set
    arr = np.ones(size, dtype=np.float32)
    result = np.dot(arr, arr)   # perform some calculation on large array
    del arr    # release memory occupied by data set
    return result


if __name__ == '__main__':
    procs = [mp.Process(target=compute) for _ in range(2)]
    for proc in procs:
        proc.start()
    results = [proc.join() for proc in procs]
    mem_used = sum([psutil.Process(os.getpid()).memory_info().rss
                   for proc in procs]) / len(results)
    print("Memory used:", round(mem_used), 'bytes')
```

在上面代码中，`numpy` 模块用于生成一个 10MB 的数组，之后计算该数组的内积作为结果。由于生成和销毁数组消耗的内存比计算的开销大很多，所以程序每次都产生和释放一份新的数组。

由于程序在执行期间可能反复申请和释放内存，所以导致内存泄漏。要解决这个问题，我们需要引入虚拟存储器。下面修改后的代码使用了 `multiprocessing.Manager` 来管理内存：

```python
import os
import numpy as np
import psutil
import multiprocessing as mp


def compute(q):
    size = 10 * 1024 ** 2   # allocate 10 MB data set
    manager = mp.Manager()
    arr = manager.list([None]*size)   # initialize an empty list of size 10 MB
    for i in range(len(arr)):
        arr[i] = 1.0   # fill each element of the list with 1.0
    dot_result = 0.0
    chunk_size = 1024 ** 2   # use chunks of 1MB to calculate dot product
    for i in range(0, size, chunk_size):
        chunk = arr[i:min(i + chunk_size, size)]   # get current chunk of data
        dot_result += np.dot(chunk, chunk)   # add contribution from this chunk
    q.put((dot_result, sys.getsizeof(arr)))   # send back result and allocated memory usage


if __name__ == '__main__':
    nprocs = 2
    queue = mp.Queue()
    procs = [mp.Process(target=compute, args=(queue,))
             for _ in range(nprocs)]
    for proc in procs:
        proc.start()
    results = [queue.get() for _ in range(nprocs)]
    mems_used = [x[1] for x in results]
    dots_computed = [x[0] for x in results]
    avg_dots_computed = sum(dots_computed)/len(dots_computed)
    total_mem_used = sum(mems_used)
    max_mem_used = max(mems_used)
    min_mem_used = min(mems_used)
    print("Average Dot Product Computed:", avg_dots_computed)
    print("Total Memory Used:", total_mem_used, 'bytes')
    print("Max Memory Used:", max_mem_used, 'bytes')
    print("Min Memory Used:", min_mem_used, 'bytes')
```

这里，我们使用 `multiprocessing.Manager` 将数组定义为共享内存，并将数组切割成固定大小的块，用以计算数组的内积。进程计算完后，将结果和分配的内存大小放入队列，主进程接受结果并统计相关信息。

经过改进后的程序不会出现内存泄露的问题。

# 4. 研究并开发一种新的线程调度算法

## 4.1 线程调度算法概述

线程调度算法（Scheduling Algorithm）是操作系统用来决定哪个线程可以获得 CPU 资源的一种算法。在操作系统调度程序执行过程中，如果有一个线程无法获取 CPU 资源，则需要选择另一个线程继续执行，这就需要线程调度算法。

典型的线程调度算法有以下几种：

1. FIFO 策略：先来先服务，按顺序调度线程，即线程调度程序按进入线程的先后顺序进行调度，但又不能保证线程之间的公平性。
2. RR 策略：轮转法，每个线程分配一个时间片，时间片结束后自动转到下一个线程，直至时间片用尽。
3. SPN 策略：最高响应比优先，优先保证最快可响应线程的 CPU 执行权，降低调度延迟。

## 4.2 线程调度算法的设计目标

调度算法的设计目标有以下几个方面：

1. 响应时间：线程调度应该满足用户的需要，即快速地分配给需要频繁运行的线程，而把其他线程推迟运行。
2. 公平性：不同线程之间的调度应该均匀，即每个线程得到的时间片应该相等。
3. 规避死锁：线程调度算法应该避免发生死锁，即两个或多个线程陷入无限等待状态，因而无法继续运行。
4. 扩展性：线程调度算法应该具有良好的扩展能力，使其能够适应新的硬件环境和系统要求。
5. 易调试性：线程调度算法应该容易调试，如采用公开标准接口或日志系统来记录调度过程。

## 4.3 Scheduller 模块

目前，Python 中的 `threading` 和 `sched` 模块都提供了调度算法，但它们仅针对单个线程的情况，并且需要手动调用线程调度函数，不够灵活。因此，我们开发了一套 `Scheduller` 模块，提供了一种统一的接口，方便用户自定义调度算法。

`Scheduller` 模块提供了四种调度算法：

1. Round-robin 调度器：线程按照时间片轮流执行，时间片结束后自动转换到下一个线程。
2. First come first serve 调度器：新到达的线程排队等待，等待线程占用 CPU 时间片。
3. Shortest job next 调度器：对于有执行时间短的线程，优先分配时间片。
4. Earliest deadline first 调度器：根据截止日期和截止时间排序，优先满足截止日期最早的线程。

除此之外，`Scheduller` 模块还支持优先级调度，即特定线程获得更多的 CPU 时长。优先级可以随着时间变化而动态变化，也可以手动修改。

## 4.4 基于优先级的调度算法

为了实现优先级调度，我们需要首先了解什么样的任务需要优先级调度。一般来说，有两种类型的任务需要优先级调度：

1. 对响应时间敏感的任务：一些实时任务，如高速运行的机器视觉系统、工控控制系统、音视频播放等，这些任务需要快速响应，否则可能造成严重的失误。
2. 对吞吐量敏感的任务：需要处理大量数据的任务，如图像处理、计算密集型任务、数据库查询等，这些任务需要快速完成，否则等待的时间太长。

对于实时任务，我们可以给予它们比普通任务更高的优先级，将它们安排在前面执行。对于吞吐量敏感任务，我们可以降低其优先级，让它们等待其他任务完成，从而降低整体的响应时间。

下面是基于优先级调度的例子：

```python
import sched
import threading
import time

class PriorityScheduler(object):
    def __init__(self):
        self._scheduler = sched.scheduler(timefunc=time.time, delayfunc=time.sleep)
        self._ready = {}   # task ready queue, key is priority value, value is a deque storing tasks
        self._running = {}   # currently running tasks, key is task id, value is TaskInfo instance
        self._id_counter = 0   # counter for generating unique task ids
        
    def new_task(self, target, args=(), kwargs={}, prio=1):
        assert callable(target)
        task_id = self._id_counter
        self._id_counter += 1
        
        info = TaskInfo(prio, target, args, kwargs, False)
        try:
            dequeue = self._ready[prio]
        except KeyError:
            dequeue = collections.deque()
            self._ready[prio] = dequeue
        dequeue.append(info)
        
        if not self._running:
            self._schedule()
            
        return task_id
        
    def run(self):
        while True:
            # wait until there are runnable tasks available
            while not self._ready or not any(self._ready.values()):
                pass
            
            # find highest priority level with runnable tasks
            try:
                prio = min(self._ready)
            except ValueError:
                continue
                
            # pop topmost task from the corresponding deque
            task_info = self._ready[prio].popleft()
            task_id = task_info.id
            task_args = task_info.args
            task_kwargs = task_info.kwargs
            
            # launch the task
            self._launch(task_info)
            
            yield task_id, task_args, task_kwargs
    
    def cancel(self, task_id):
        try:
            info = self._running.pop(task_id)
        except KeyError:
            raise LookupError("Task not found")
        else:
            info.cancelled = True
            if hasattr(info.target, '__cancel_hook__'):
                info.target.__cancel_hook__(*info.args, **info.kwargs)
                
    def join(self, task_id):
        try:
            info = self._running.pop(task_id)
        except KeyError:
            raise LookupError("Task not found")
        else:
            yield info.wait_future
            if info.exc_info is not None:
                six.reraise(*info.exc_info)
            elif info.cancelled:
                raise asyncio.CancelledError()
        
class TaskInfo(object):
    def __init__(self, prio, target, args, kwargs, cancelled):
        self.id = id(target)
        self.priority = prio
        self.target = target
        self.args = args
        self.kwargs = kwargs
        self.cancelled = cancelled
        self.exc_info = None
        self.wait_event = threading.Event()
        self.wait_future = asyncio.Future()

scheduler = PriorityScheduler()
    
@asyncio.coroutine
def mytask(arg):
    global scheduler
    print("Executing task", arg)
    time.sleep(3)
    res = str(hex(hash(arg))).upper()[2:][:8]
    yield from scheduler.cancel(mytask)
    yield from scheduler.join(mytask)
    print("Finished executing task", arg, "with result", res)

loop = asyncio.get_event_loop()
tasks = [asyncio.async(mytask(i)) for i in range(10)]
try:
    loop.run_until_complete(asyncio.gather(*tasks))
except Exception as e:
    print("Caught exception", e)
finally:
    loop.close()
```

在这个例子中，我们定义了一个 `PriorityScheduler` 类，其中 `_ready` 是待调度任务的队列，`_running` 是正在执行任务的字典，`_id_counter` 是任务 ID 生成器。

`new_task` 方法用来创建新任务，参数包括目标函数、`args` 和 `kwargs` 参数、优先级。该方法首先检查是否存在相应优先级的队列，如果不存在，则新建队列。接着向队列插入任务信息对象，并尝试启动调度器。如果不存在正在执行的任务，则调度器启动后立即开始运行。

`run` 方法返回一个协程对象，可以用来迭代运行。它首先阻塞等待至少有一个优先级队列里有可运行的任务。然后，找到可运行任务的最高优先级，并弹出队首的任务，启动该任务。最后，它返回任务 ID、参数和关键字参数。

`cancel` 方法用来取消指定任务，它首先查找正在执行任务的字典，并将其标记为取消，然后调用 `__cancel_hook__` 方法。注意，`__cancel_hook__` 方法可以用来自定义取消行为，比如关闭数据库连接等。

`join` 方法用来等待指定任务执行完成，它首先查找正在执行任务的字典，并阻塞等待结果，然后返回执行结果或抛出异常。注意，该方法返回的是一个协程对象，可以用来等待任务完成。