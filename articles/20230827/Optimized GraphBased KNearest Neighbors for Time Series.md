
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Time series classification (TSC) is a challenging task that involves classifying a sequence of observations into different categories based on their intrinsic characteristics or behaviors. It has many applications in fields such as finance, healthcare, and remote sensing. One popular TSC technique called k-nearest neighbors (KNN) has become one of the most widely used methods in practice. However, there are still many challenges to be solved for applying this technique in real-world scenarios. 

In recent years, graph-based approaches have been shown to effectively handle complex relationships between time series data due to its high dimensional structure. In this paper, we propose an optimized version of graph-based KNN algorithm called optimized GBKNN (OG-GBKNN), which improves over existing ones by incorporating several key techniques including matrix decomposition, nonparametric regression model, and semi-supervised learning. OG-GBKNN achieves significant improvements in terms of accuracy and efficiency compared to conventional GBKNN algorithms in various benchmark datasets. Furthermore, it also shows promising results in banknote authentication scenario where the goal is to detect fraudulent transactions using two-dimensional images of handwritten digits captured from customers’ wallets. We hope that this work can inspire other researchers and developers to explore novel ideas to enhance the performance of time series classification in real-world scenarios.  

# 2.相关工作
## 2.1图神经网络Graph Neural Networks (GNNs)
Graph neural networks (GNNs) are recently gaining more attention in the field of natural language processing (NLP). They have achieved state-of-the-art performances in many tasks related to text representation, sentiment analysis, and node/link prediction. Similarly, graph-based representations can be learned directly from time series data without any pre-processing steps like feature extraction, which makes them suitable for TSC tasks. The idea behind these models is similar to convolutional neural networks (CNNs) in image recognition: they use graphs as input instead of traditional tabular data, and learn spatial features and contextual dependencies through graph convolution operations. Despite their importance, however, no efficient implementation of graph-based KNN exists yet. Therefore, in this paper, we design and implement an optimized version of graph-based KNN algorithm based on graph neural networks. 


## 2.2非参数模型Nonparametric Regression Model
The core component of our proposed GBKNN algorithm is a nonparametric regression model, commonly known as Local Linear Embedding (LLE). LLE maps a d-dimensional dataset onto a lower-dimensional space while preserving local neighborhood structures. This method has proven to be effective at capturing global patterns and making accurate predictions about new instances in high dimensions. Nonetheless, it may not always work well when dealing with highly irregular datasets, where the distribution of points could be very dispersed or cluttered. To address this problem, we further employ a semi-supervised learning approach in conjunction with LLE, which enables us to focus on identifying patterns within clusters rather than trying to capture all the underlying pattern information globally.


# 3.优化目标Problem Formulation
We aim to solve the following optimization problem: given a set of multivariate time series, find the optimal partition of the dataset into groups so that each group exhibits distinct behavior and interactions. Specifically, for a given query point q and a predefined value of k, we need to identify the k nearest neighbors of q among all the training points, and predict the label of q based on the majority voting scheme across all the identified neighboring points. Here, the dimensionality of the data is n x m, where n is the number of samples and m is the length of each sample. The predicted labels are binary values indicating whether each group contains anomaly or normal behavior.

To optimize the solution, we first define an appropriate distance metric to measure the similarity between pairs of time series vectors. Then, we apply matrix decomposition techniques such as SVD or NMF to transform the original dataset into a low-rank form. Next, we fit a nonlinear regression model to approximate the mapping function from the low-rank subspace back to the original space. Finally, we combine these components with semi-supervised learning techniques to automatically generate labeled clusters, which act as constraints during the inference phase. Overall, our objective is to develop a scalable, efficient, and effective framework for solving TSC problems by leveraging the power of graph neural networks and modern machine learning tools.