
作者：禅与计算机程序设计艺术                    

# 1.简介
  

远程感知图像分类（Remote Sensing Image Classification）是计算机视觉领域的一个热门研究方向。传统上，基于深度学习的方法，如卷积神经网络（CNN），循环神经网络（RNN），或者自注意力机制（Self-Attention Mechanism），都可以用来解决这一问题。然而，最近几年来，基于多尺度的视觉Transformer模型（Vision Transformers）取得了非常好的效果。本文将结合remote sensing scene classification(RSC)任务对多尺度的视觉Transformer进行介绍，并设计了一个新的多尺度视觉Transformer模型——Mutli-scale Vision Transformer network (MViT)来解决RSC任务。
RSC任务是指给定一个待分类的RGB图像序列，判定其所属的特定类别（如建筑、河流等）。传统方法中通常采用卷积神经网络或循环神经网络来处理图像序列数据。近年来，卷积神经网络在图像分类任务中的成功引起了广泛关注。但是，对于像远程感知图像分类这样的复杂场景，这些CNN模型往往需要大量的时间和计算资源才能训练，很难适应不同的环境和应用。另一方面，基于自注意力机制的模型也能取得不错的结果。然而，由于自注意力机制依赖于输入特征之间的相互联系，因此不能够有效地捕获不同尺度上的空间信息。因此，本文提出了一种多尺度的视觉Transformer模型，这种模型能够显著降低计算负担，同时仍然能够捕获到不同尺度上的空间信息。
本文贡献如下：
1. 提出了一个多尺度视觉Transformer网络，即MViT，它通过利用尺度信息来进一步提升模型性能。MViT能够同时融合不同尺度的全局和局部信息，从而达到比传统方法更好的结果。
2. 在RSC任务上，基于多尺度的视觉Transformer模型，取得了最先进的成果。在PASCAL VOC 2012数据集上，MViT的准确率达到了78.19%，排名第一。此外，作者还在ISPRS CVLab数据集上，也取得了不错的结果。
3. 作者提出的MViT模型在各项指标上都优于现有的相关方法，并且可以在不同的应用场景下应用。
# 2.相关工作
卷积神经网络已经在图像分类领域取得了相当好的成绩。但是，它们无法捕捉到图像的空间结构及变化模式。一些研究者试图将CNN应用到其他领域，如遥感图像分类、对象检测等。2018年底，微软研究院提出了一种新型的卷积神经网络，称为可分离卷积网络（Separable Convolutional Networks），该网络通过分离卷积层和点卷积层实现了有效的特征抽取和处理。该论文提出了一个自注意力机制，使得CNN模型能够捕捉到不同尺度的空间信息。自注意力机制在机器翻译、文本摘要生成、图像合成等领域都有着重要的作用。Transformer模型则是为了解决自注意力机制的计算效率问题提出的模型，它的主要特点是并行计算，并支持序列建模。与自注意力机制相比，Transformer更加关注整个序列的信息，而不是单个元素的关联性。2020年，ECA-Net也被提出来作为CNN模块的替代方案。其通过扩展的卷积核进行自适应滤波，消除了深度和宽度之间固定的权衡。
针对多尺度视觉Transformer模型，有几种方法已经被提出。一种是将输入图像划分为多个子图像，然后分别送入不同的Transformer模型进行处理。第二种是将输入图像通过池化、采样等方式得到不同尺度的特征图，再送入同一个Transformer模型进行处理。第三种是将多种大小的Transformer堆叠起来，最后通过融合的方式得到最终的输出结果。目前，比较知名的多尺度视觉Transformer网络有ViT、DeiT、Swin-Transformer等。除此之外，还有一些研究人员通过修改网络架构来提升模型性能。如用更大的模型结构来代替ResNet、将ViT应用到视频数据上等。
# 3.多尺度视觉Transformer网络
## （1）多尺度的视觉Transformer
多尺度视觉Transformer，也称为MViT，是一种具有多尺度特征提取能力的视觉Transformer模型。与传统的视觉Transformer模型一样，MViT也由编码器和解码器组成。不同的是，MViT使用了不同的尺度的输入图像，来获得不同尺度下的全局和局部信息。下图展示了MViT的主要结构。
**Fig1.** MViT模型的结构示意图
MViT模型使用标准的Transformer结构，包括多个编码器和解码器模块。编码器模块使用卷积层、位置嵌入层和多头自注意力层来获取不同尺度的全局特征；解码器模块使用反卷积层、位置嵌入层和多头自注意力层来逐步恢复输入尺度的全局和局部特征。其中，位置嵌入层用于将位置信息编码到输入特征中，便于Transformer模块捕获不同尺度的空间关系。
MViT模型使用了两种尺度的输入图像，分别是低分辨率（LR）图像和高分辨率（HR）图像。从结构上看，MViT的编码器模块接收两个尺寸的输入图像，分别是低分辨率的$l \times l$的图像块，和同样大小的高分辨率图像块。每一个编码器模块会产生两个张量，一个代表全局特征，另一个代表局部特征。解码器模块根据全局特征和局部特征来恢复原始输入的尺度。
## （2）模型细节
### （2.1）数据增强策略
为了获得更好的性能，MViT对训练数据做了数据增强。首先，对低分辨率的图像进行随机剪裁、缩放、翻转、以及色彩抖动；然后，对高分辨率的图像进行中心裁剪、缩放、以及色彩抖动。最后，随机裁剪一定比例的图像块，并组合到低分辨率图像中。这样就可以减少图像的中心区域可能带来的干扰，避免模型将中心区域的信息误读成周围区域的信息。另外，还可以对低分辨率的图像和高分辨率的图像进行不同的数据增强操作。例如，可以对低分辨率的图像进行随机旋转、随机翻转、以及随机放缩；也可以对高分辨率的图像进行仿射变换、模糊操作等。
### （2.2）不同尺度的嵌入
MViT使用不同尺度的嵌入，来捕捉到不同尺度的全局和局部信息。下表展示了四种不同尺度的嵌入维度。
| Dimension | Embedding Size |
|:---------:|:--------------:|
|    $d_{model}$     |     512       |
|    $d_{\text{FF}}$   |     2048      |
|    $d_{\text{kv}}$   |      64       |
|    $d_{\text{q}}^k$  |      64       |
其中，$d_{model}$表示输入张量的通道数量，也是Transformer模型中隐藏状态的维度。$d_{\text{FF}}$表示前馈网络的中间层输出维度。$d_{\text{kv}}$表示键、值矩阵的维度。$d_{\text{q}}^k$表示每个查询向量的维度。MViT的Encoder-Decoder模块中，使用了不同的嵌入来捕捉不同尺度的全局和局部信息。
### （2.3）正则化方法
为了防止过拟合，MViT使用了残差连接和裁剪梯度的方法。残差连接是指将输入直接添加到输出上，从而保证模型的收敛性。裁剪梯度方法是在反向传播过程中，将梯度的绝对值限制在一个范围内。
### （2.4）相邻特征融合方法
为了获得更好的全局和局部特征，MViT采用了相邻特征融合的方法。相邻特征融合是指使用特征图中的相邻特征作为输入，来增强模型的性能。相邻特征融合的方法在多个位置上使用了相同的卷积核。因此，相邻特征融合仅会影响模型的一部分参数，不会增加模型的参数量。
## （3）实验结果
### （3.1）数据集
本文对RSC任务进行了实验。RSC任务就是给定一个RGB图像序列，判定其所属的特定类别（如建筑、河流等）。目前，比较流行的RSC数据集有Baidu Dataset、UCI-HAR Dataset等。
### （3.2）评估指标
评估RSC任务的主要指标是准确率（Accuracy）。其计算方式为：正确预测的图像个数除以总的预测图像个数。
### （3.3）超参数设置
在训练MViT模型时，作者使用Adam优化器、初始学习率为0.0001、权重衰减系数为0.0001、学习率更新步长为20、批量大小为128、dropout rate为0.1、学习率衰减率为0.5、起始层数为12、中间层数为12、最终层数为3、注意力头数为8、embedding size为512、feedforward size为2048、patch size为8、输入图像大小为$2^{n} \times 2^{n}$。
### （3.4）实验结果
作者对三个不同数据集（Baidu Dataset、UCI-HAR Dataset和ISPRS CVLab）上的MViT模型进行了训练和测试。实验结果如下：

Dataset|Model|Input Resolution|Pretrained Weights|Backbone|Top-1 Accuracy (%)|GPU Utilization (%)<|im_sep|>