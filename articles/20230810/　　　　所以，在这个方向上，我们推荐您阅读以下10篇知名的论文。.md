
作者：禅与计算机程序设计艺术                    

# 1.简介
         
## 一、ConvNets[1]
### （1）简介
卷积神经网络（ConvNets）是一种用于处理图片、视频等高维数据的机器学习模型。由于其简单、有效、高度参数化等特点，它被广泛应用于计算机视觉、自然语言处理、生物信息学等领域。目前的计算机视觉任务中，图像分类、目标检测、实例分割、人脸识别等都可以看做是基于ConvNets进行的。
图1:通过卷积核提取特征的过程示意图。

### （2）特点
#### a.全连接层的缺失
卷积神经网络(CNN)采用全连接网络设计结构,相比之下,全连接层在一些任务如图像分类时,会造成模型过于复杂、过拟合,因此,CNN不使用全连接层,而是采用卷积层代替。
#### b.局部感受野
为了能够捕捉不同位置的上下文信息,CNN中的卷积层一般具有局部感受野,即只能观察到输入的一小块区域而不是整张图片的所有像素信息。这样便可以避免模型陷入训练误差从而达到更好的效果。
#### c.权重共享
相同输入通道的权重会在同一层中共享,因此减少了参数量并提升了性能。
#### d.多种池化策略
不同池化策略可调整池化窗口大小和步长,获得不同粒度的特征。常用的池化方式包括最大池化、平均池化、L2池化。
#### e.Dropout
正则化方法Dropout可缓解过拟合现象。随机删除一定比例的神经元输出,防止神经元之间产生共适应性。
#### f.数据增强
图像数据集有限,往往需要通过数据增强的方式增强数据数量,以弥补过拟合和提升模型性能。常用的数据增强方法包括水平翻转、垂直翻转、剪切扩充、裁剪、旋转、滤波、噪声添加。
#### g.模型正则化
L2正则化或其他正则化方法对模型权重进行约束,以提升模型鲁棒性和泛化能力。
#### h.残差网络ResNets
残差网络(ResNets)是卷积神经网络(CNN)的最新进展。ResNets引入了跳跃连接,使得网络可以从某个深层网络学习到的知识直接应用于另一个浅层网络，并能够提升准确率和速度。
#### i.迁移学习
迁移学习(transfer learning)利用已经训练好的模型作为基础模型,然后将该模型的部分权重固定住,再加上新的分类器层,最终得到新的模型。迁移学习有助于快速解决新任务,降低训练难度。

## 二、Transformer[2]
### （1）简介
自注意力机制（self attention）是最重要的模块之一。Transformer由Self-Attention，Feed Forward Neural Network，以及Decoder堆叠而成，并有一定的改进，如BERT模型。
图2:Transformer模型架构示意图。

### （2）特点
#### a.缩短编码器-解码器循环
Transformer采用编码器-解码器结构,它把编码器和解码器分别作为两个子模型，编码器处理输入序列，生成固定长度的向量表示，解码器根据这个向量表示生成相应的输出序列。这样，可以允许解码器不用依赖于整个源序列的信息，可以使得模型端到端地学到序列表示和输出序列的映射关系，因而速度快。
#### b.相对位置编码
相对位置编码使得Transformer可以更好地关注相关位置的信息。在原始的Transformer模型中，编码器和解码器都会生成绝对位置编码，但是相对位置编码只会影响到解码器的自注意力模块。相对位置编码指的是编码器生成的位置嵌入向量不是严格的绝对值坐标，而是基于输入序列元素之间的距离编码的。
#### c.多头注意力机制
相对于传统的单头注意力机制来说，Transformer提出了多头注意力机制。多头注意力机制可以允许模型同时关注不同输入子空间上的信息，其计算公式为:
$ Attention = SoftMax({Q}^{T} {K}) {V} $,其中 Q, K, V 为三个矩阵, Q 和 K 的维度都是 d_model x nHead 。
#### d.位置-编码的探索
Transformer采用位置编码探索更精细化的位置关系。在Transformer中，位置编码主要用来增加非线性变换后的位置信息的丰富度。
#### e.训练技巧
在训练Transformer模型的时候，有如下技巧可以提升性能。
- Masking: 训练过程中要随机遮盖掉一些信息，从而反映模型所处理的数据含有随机噪声。
- Label Smoothing: 使用标签平滑的方法使得模型能够学会更好的适配测试数据，降低模型的过拟合风险。
- 梯度累计: 在Transformer模型中，梯度消失和爆炸的问题一直是一个研究热点。而梯度累计是训练Transformer模型的一个有效技巧。
- Scheduled Sampling: 可以让模型逐渐地从较小的权重更新变换到较大的权重更新，进而逼近实际场景下的效果。