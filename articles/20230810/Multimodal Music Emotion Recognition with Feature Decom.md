
作者：禅与计算机程序设计艺术                    

# 1.简介
         

随着多媒体时代的到来，音乐作为最具代表性的多媒体形式在人们的生活中越来越受到重视。在人们不断寻找感动、满足、享受的同时，它也成为娱乐产业的热点话题。因此，对于音乐情绪识别系统的需求也越来越大。现有的基于特征的音乐情绪识别方法存在着很大的缺陷，主要原因在于无法处理多模态信息，即音频信号与文本信号之间缺乏统一的特征表示。

为了解决这一问题，本文提出了一种新的多模态音乐情绪识别模型，该模型能够将音频信号和文本信号进行分离并分别进行特征抽取，然后通过堆叠多个任务学习器对不同类型的特征进行学习，最后结合两个特征的融合结果对音乐情绪进行预测。实验表明，该模型在多个数据集上取得了较好的性能，并获得了更高的准确率。

# 2.相关工作
首先回顾一下音频信号与文本信号的多模态分类方法。由于音频信号与文本信号之间的差异，传统的多模态分类方法一般会采用不同类型的特征进行表示。例如，TonGCN [2] 将语音信号经过时间频率分析得到不同频率下的时间向量；LyricsCNN [3] 通过卷积神经网络模型提取音频信号中的歌词信息；WaveBERT [4] 和 C-TCN [5] 对音频信号进行特征提取后，再和文本信号一起输入到BERT等预训练模型中进行分类。这些方法均采用了不同的特征表示。

而随着多模态机器学习领域的兴起，有一些方法试图将不同模态的信息整合起来形成有效的特征表示。例如，GLAD [6] 提出了一个多模态框架，使得不同模态的特征可以共同参与到一个统一的表征学习阶段中。这个方案成功地整合了图像和语音的特征，并产生了比单独使用一个模态的特征表现更好的结果。其中，MMSA-SOTA [7] 使用联合学习方法来融合不同模态的特征，并取得了最先进的结果。

然而，这些方法仍然面临着以下三个主要困难：

1. 模型的复杂度：目前的方法需要针对每个模态独立设计特定的特征表示，并且还需要兼顾不同模态间的关系。这导致模型的复杂度很高，且难以部署到生产环境。
2. 数据分布不平衡：现有的多模态数据集往往存在极端不平衡的数据分布，比如某个类别的数据数量远远小于其他类别。这会影响到模型的性能。
3. 效率低下：当使用大规模数据集时，这些方法通常具有计算开销高昂、耗时长的问题。

因此，如何将多模态音频信号和文本信号的特征表示有效地整合起来，并通过学习器完成不同任务的融合，是一个重要的课题。

# 3.核心算法
## 3.1 多模态特征表示
### 3.1.1 分离声谱图
不同模态的特征需要通过一个特征抽取层（FE）进行统一的特征表示。由于音频信号包含的频率范围和调制方式都有所不同，所以我们首先需要将声谱图（STFT）进行分离，使得声谱图只包含单个模态的特征。这样，我们就可以为每个模态提取特定的特征，并对它们进行特征融合。

假设我们有音频信号$x=\{x_a, x_b\}$，其对应的声谱图分别为$\hat{\Theta}_a$和$\hat{\Theta}_b$。将它们分离后的声谱图可分别表示为$\hat{\Theta}^{audio}= \{ \theta_a^a, \theta_a^b, \theta_b^a, \theta_b^b \}$和$\hat{\Theta}^{text}=\{\theta^{text}_a,\theta^{text}_b\}$。

### 3.1.2 特征抽取层
不同的模态各自具有独特的特征，所以需要分别提取特征。为此，我们可以使用不同模型或特征选择方法对每个模态的特征进行抽取。在这篇论文中，我们使用神经网络模型去提取特征。

在声谱图$\hat{\Theta}^{audio}$中，我们对每种特征进行抽取，得到相应的特征$f^{audio}(t)$。具体来说，对$k$阶MFCC特征，我们可以通过卷积神经网络（CNNs）实现特征抽取，具体流程如下：

$f^{audio}(t)=\underset{i=1}{\overset{K}{\sum}}w_{ik}\hat{\Theta}_a(t-n_i)$.

其中，$w_{ik}$是第$i$个滤波器核的参数，$\hat{\Theta}_a(t)$是声谱图$\hat{\Theta}_a$的第$k$个频率分量。$K$代表滤波器的数量，$n_i$代表第$i$个滤波器中心距当前频率分量中心的偏移距离。

在文字序列$\theta^{text}_a$中，我们使用双向LSTM网络对字符级别的特征进行编码。这里，我们定义字符级特征向量$v^{text}_c$和上下文级特征向量$v^{text}_{ctx}$，分别对应于序列中当前时间步的字符和上下文信息。具体流程如下：

$v^{text}_c = LSTM(\theta^{text}_a)$

$v^{text}_{ctx} = LSTM([\theta^{text}_{p}, \theta^{text}_{n}]$，其中$\theta^{text}_{p}$和$\theta^{text}_{n}$分别代表序列中前一时刻和后一时刻的字符)。

在两个特征向量$v^{audio}_c$和$v^{text}_c$、$v^{audio}_{ctx}$和$v^{text}_{ctx}$之后，我们可以通过连接操作将它们连接成一个特征向量$v=(v^{audio}_c, v^{audio}_{ctx}, v^{text}_c, v^{text}_{ctx})$。

### 3.1.3 特征融合层
得到不同模态的特征后，需要通过特征融合层（FM）进行特征的融合。FM由三个部分组成：特征权重矩阵W、特征投影矩阵P、特征组合层C。

首先，使用特征权重矩阵W将特征向量$v$转换为另一种更加紧凑的特征表示$F=\Omega v$。具体地，令$\alpha_m$和$\beta_n$分别表示模态$m$和模态$n$的权重系数。那么，$F_m$就是特征权重矩阵$W$左边第一列的$m$号元素乘以$v_m$，而$F_n$就是特征权归矩阵右边第一行的$n$号元素乘以$v_n$。因此，$F=\left[\begin{matrix} F_1 \\ F_2 \end{matrix}\right]=\Omega v=\begin{bmatrix} \alpha_1 & \alpha_2 &... \\ \beta_1 & \beta_2 &... \end{bmatrix}\cdot\begin{bmatrix} v_1\\ v_2\\... \end{bmatrix}$.

然后，使用特征投影矩阵P将特征向量$F$投影到一个合适的维度空间，从而减少噪声并消除冗余信息。具体地，令$P_m$和$P_n$分别表示模态$m$和模态$n$的投影矩阵。那么，$f^{output}_m=\operatorname{diag}(\lambda_1, \lambda_2,..., \lambda_d)\circ P_m^{-1}\cdot F_m$和$f^{output}_n=\operatorname{diag}(\mu_1, \mu_2,..., \mu_d)\circ P_n^{-1}\cdot F_n$是模态$m$和$n$对应的输出特征向量，$\lambda_j$和$\mu_j$则是对应模态的权重系数。

最后，使用特征组合层C将不同模态的特征向量进行融合，从而完成最终的预测结果。具体地，令$C_{\tilde{x}}$表示最终的预测输出层，则输出$\tilde{y}$的表达式为$\tilde{y}=\sigma(C_{\tilde{x}}\cdot f^{output})$, $\sigma$函数是一个非线性激活函数。

## 3.2 任务学习器
本文提出的模型利用不同任务学习器来学习不同类型的特征。具体地，在本文中，我们使用分类器、多任务学习器、循环神经网络（RNN）和变压器（Transformer）来完成特征的学习。

### 3.2.1 分类器
为了分类音乐情绪，我们使用监督学习的方式。首先，我们用声谱图特征$f^{audio}$和文本特征$f^{text}$作为输入，送入分类器进行训练。分类器的结构通常包括卷积层、池化层、全连接层，并应用了dropout、batch normalization等技巧来减轻过拟合问题。

### 3.2.2 多任务学习器
为了学习多种模态特征，并进行特征融合，我们使用多个任务学习器。具体来说，我们使用CNNs和LSTMs来学习声谱图特征$f^{audio}$和文本特征$f^{text}$。

CNNs用于声谱图特征的学习。具体地，对于声谱图$X=\{x_a, x_b\}$, 我们提取一系列不同的卷积核$\omega^{(i)}$，得到卷积特征$H=(h_a^{(i)}, h_b^{(i)})$。其中，$h_a^{(i)}, h_b^{(i)}$是卷积核$\omega^{(i)}$在频谱图$\hat{\Theta}_a, \hat{\Theta}_b$上的响应，$H$是一个$N\times M$的矩阵，其中$N$和$M$分别是频谱图的宽度和高度。

LSTM用于文本特征的学习。具体地，对于文本序列$\tau=[\tau_1,...,\tau_l]$，我们用双向LSTM网络来得到特征向量$V=\{v_a, v_b\}$。其中，$v_a$和$v_b$是分别对应于文本序列$\tau_a$和$\tau_b$的特征向量。

### 3.2.3 循环神经网络
为了能够同时学习声谱图和文本特征，我们引入循环神经网络（RNN）。具体地，我们使用两层GRU来学习音频信号$x$和文本序列$\tau$的特征。首先，用一层GRU对输入序列$X=[x_1, x_2,..., x_T]$进行编码，得到隐藏状态$H_X$。第二层GRU根据$H_X$和文本序列$\tau$的初始状态$z_0$和隐藏状态$H_0$生成输出序列$Y=[y_1, y_2,..., y_T]$。

### 3.2.4 变压器
为了降低计算复杂度，我们考虑对模型进行微调。具体地，我们使用变压器（Transformer）来对模型进行微调，其训练目标是在不损失准确度的情况下，减少模型参数的个数。

具体地，我们对不同模态的特征使用不同的预训练模型，然后通过连接操作来进行特征的融合。然后，我们仅对融合后的特征进行微调，而不需要重新训练整个模型。这样，我们就实现了对模型的压缩。

# 4.实验评估
## 4.1 数据集设置
### 4.1.1 播放列表情感数据集（MADEPEA）
播放列表情感数据集（MADEPEA）是第一个真正的多模态音乐情绪识别数据集。它包含129个YouTube视频，来自9种不同的情绪类型：Happy、Angry、Neutral、Sad、Disgust、Fear、Surprise和Boredom。该数据集提供的音频流和对应的文本注释都是真实的，可以用来进行训练和测试。

### 4.1.2 UrbanSound8K
UrbanSound8K数据集包含8000首嘈杂环境噪音声音片段，来自8个类别：Air Conditioner、Car Horn、Children Playing、Dog bark、Drilling、Engine Idling、Gun Shot、Jackhammer。该数据集提供了音频流和对应的标签信息，但没有提供文本注释。

为了方便比较，我们对MADEPEA和UrbanSound8K数据集进行合并，构造了一个更加通用的多模态情绪识别数据集，即MDRECA。在该数据集上，共包含114个样本，4种情绪类型、2种模态（音频信号和文本信号）、7种数据源（MADEPEA和UrbanSound8K）。

### 4.1.3 实验设置
为了验证模型的效果，我们选取了两种模型——CNN和Multi-task。这两种模型使用的训练数据集为MDRECA，包含7种不同模态（音频信号和文本信号），8种情绪类型。另外，为了评估特征组合的有效性，我们还使用了两种特征融合策略——特征交叉和特征平均，以及一种无融合策略。实验结果如下。

## 4.2 模型性能
在本节中，我们讨论不同模型在MDRECA数据集上的性能。为了比较各个模型的效果，我们计算了AUC ROC曲线。

### 4.2.1 CNN
CNN模型的架构如下图所示。


CNN模型使用两个卷积层和两个最大池化层来学习音频信号和文本信号的特征。两个卷积层具有32个卷积核，每层的kernel size为$(4,4), (4,4)$，步长为$(2,2), (2,2)$，使用ReLU激活函数。两个最大池化层具有kernel size为$(2,2), (2,2)$，步长为$(2,2), (2,2)$。最终，输出层是一个softmax函数，输出类别概率分布。

训练优化器为Adam，学习率为0.001，损失函数为二元交叉熵。训练过程中，学习率每10个epoch衰减一次，并在验证集上选择最优模型。

实验结果如下：

|      | AUC-ROC   | Accuracy     | Precision    | Recall       | F1-score     | 
|:----:|:---------:|:------------:|:------------:|:------------:|:------------:|
| CNN  | $0.8432$  | $0.6479$     | $0.6509$     | $0.6433$     | $0.6456$     | 

### 4.2.2 Multi-task
Multi-task模型的架构如下图所示。


Multi-task模型首先使用两个CNNs分别学习音频信号和文本信号的特征。两个CNNs分别使用32和16个卷积核，每层的kernel size为$(4,4)$，步长为$(2,2)$，使用ReLU激活函数。两个最大池化层具有kernel size为$(2,2)$，步长为$(2,2)$。最终，输出层是一个softmax函数，输出类别概率分布。

然后，Multi-task模型在两个特征上都使用LSTM来学习特征的上下文信息。LSTM的输出维度为128。

然后，将两个特征的上下文信息进行拼接，送入一个双向LSTM，得到最后的特征表示。输出层是一个softmax函数，输出类别概率分布。

训练优化器为Adam，学习率为0.001，损失函数为二元交叉熵。训练过程中，学习率每10个epoch衰减一次，并在验证集上选择最优模型。

实验结果如下：

|        | AUC-ROC    | Accuracy    | Precision   | Recall      | F1-score    |  
|:------:|:----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Multi | $0.8453$   | $0.6351$    | $0.6329$    | $0.6374$    | $0.6347$    |  

### 4.2.3 比较模型
本节比较了三种模型的性能。首先，我们观察相同类别的不同模型的AUC-ROC曲线。如下图所示，AUC-ROC曲线呈现出锐利的上升趋势，说明模型的鲁棒性较好。


其次，我们通过计算各种指标来观察模型的性能。如表格所示，所有模型的AUC-ROC、精度、查准率、召回率、F1值均达到了很高的水平。但是，Multi-task模型的AUC-ROC曲线略高于CNN模型的曲线，说明特征融合的有效性。

最后，我们通过分析模型的训练过程观察模型是否收敛。如图所示，Multi-task模型的训练过程收敛的很快，验证集上的性能较CNN模型要好很多。这说明特征的表示能力和学习效率对模型的训练非常重要。



## 4.3 特征融合策略
为了验证不同特征融合策略的有效性，我们训练了Multi-task模型，并使用两种特征融合策略——特征交叉和特征平均。实验结果如下。

### 4.3.1 特征交叉
特征交叉（Feature Cross）是特征工程的一个基础方法，即将两者的特征相乘或求和。具体地，我们构造了一个模型，其输入是一个音频信号和一个文本序列，输出是一个类别。然后，把两者的特征分别做特征交叉后再输入到模型中。

#### 4.3.1.1 模型架构
模型的架构如下图所示。


输入是一个音频信号$x_a$，一个文本序列$\tau$，一个类别$y$。首先，对于音频信号$x_a$，我们通过一个CNN提取特征，得到特征向量$v_a$。对于文本序列$\tau$，我们用一个双向LSTM来学习特征的上下文信息，得到特征向量$v_t$。

然后，我们构造了一个特征交叉的特征向量$v=(v_a, v_t, v_at)$，其中$v_at$是将$v_a$和$v_t$相乘得到的特征。然后，我们送入一个双向LSTM，得到输出$y'$。输出层是一个softmax函数，输出类别概率分布。

#### 4.3.1.2 训练和评估
训练优化器为Adam，学习率为0.001，损失函数为二元交叉熵。训练过程中，学习率每10个epoch衰减一次，并在验证集上选择最优模型。

#### 4.3.1.3 实验结果
实验结果如下表所示：

|         | AUC-ROC    | Accuracy    | Precision   | Recall      | F1-score    |
|:-------:|:----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Feature cross | $0.8366$   | $0.6358$    | $0.6341$    | $0.6365$    | $0.6348$    |  

### 4.3.2 特征平均
特征平均（Feature Average）是一种简单但有效的特征融合方法。具体地，我们构造了一个模型，其输入是一个音频信号和一个文本序列，输出是一个类别。然后，把两者的特征求平均后再输入到模型中。

#### 4.3.2.1 模型架构
模型的架构如下图所示。


输入是一个音频信号$x_a$，一个文本序列$\tau$，一个类别$y$。首先，对于音频信号$x_a$，我们通过一个CNN提取特征，得到特征向量$v_a$。对于文本序列$\tau$，我们用一个双向LSTM来学习特征的上下文信息，得到特征向量$v_t$。

然后，我们构造了一个特征平均的特征向量$v'=\frac{1}{2}(v_a+v_t)$，然后送入一个双向LSTM，得到输出$y''$。输出层是一个softmax函数，输出类别概率分布。

#### 4.3.2.2 训练和评估
训练优化器为Adam，学习率为0.001，损失函数为二元交叉熵。训练过程中，学习率每10个epoch衰减一次，并在验证集上选择最优模型。

#### 4.3.2.3 实验结果
实验结果如下表所示：

|          | AUC-ROC    | Accuracy    | Precision   | Recall      | F1-score    |
|:--------:|:----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Feature average | $0.8432$   | $0.6385$    | $0.6374$    | $0.6390$    | $0.6380$    |  

### 4.3.3 无融合
无融合（No Fusion）是一种特殊的情况，即没有任何特征融合。具体地，我们构造了一个模型，其输入是一个音频信号和一个文本序列，输出是一个类别。然后，把两者的原始特征直接输入到模型中。

#### 4.3.3.1 模型架构
模型的架构如下图所示。


输入是一个音频信号$x_a$，一个文本序列$\tau$，一个类别$y$。首先，对于音频信号$x_a$，我们通过一个CNN提取特征，得到特征向量$v_a$。对于文本序列$\tau$，我们用一个双向LSTM来学习特征的上下文信息，得到特征向量$v_t$。

然后，我们构造了一个没有特征融合的特征向量$v'=(v_a, v_t)$，然后送入一个双向LSTM，得到输出$y'''$。输出层是一个softmax函数，输出类别概率分布。

#### 4.3.3.2 训练和评估
训练优化器为Adam，学习率为0.001，损失函数为二元交叉熵。训练过程中，学习率每10个epoch衰减一次，并在验证集上选择最优模型。

#### 4.3.3.3 实验结果
实验结果如下表所示：

|          | AUC-ROC    | Accuracy    | Precision   | Recall      | F1-score    |
|:--------:|:----------:|:-----------:|:-----------:|:-----------:|:-----------:|
| No fusion | $0.8307$   | $0.6304$    | $0.6271$    | $0.6336$    | $0.6295$    |