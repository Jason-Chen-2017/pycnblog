
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


This article presents a novel approach to teach artificial intelligence (AI) mathematical concepts and operations through the use of falsified input data generated by machine learning models that do not accurately predict correct output values for given inputs in training sets. The goal is to build an accurate predictor model that can generalize well to unseen testing scenarios where inputs are falsified. This work explores how we can train AI systems with incorrect but sufficiently informative input data to learn various mathematical expressions such as arithmetic operations, geometric transformations, and fractions without explicitly providing ground truth labels for each example during training or supervision. Specifically, this paper proposes two algorithms - Teacher-Student Learning and Maximum Mean Discrepancy Learning (MMD-Learning), which are used to generate falsified expression examples and train AI systems to recognize them correctly. Experiments conducted using real world datasets show that these methods improve the accuracy of predictors trained with falsified input data while still being able to generalize well to unseen test cases.

The main contributions of this work include:

1. We present new techniques for generating falsified expression examples using teacher-student learning and MMD-learning algorithms based on kernel embeddings learned from labeled training data. These algorithms enable us to generate highly informative and diverse falsified expression examples by taking into account both expressive properties of the math concept and statistical distribution of its input domain.
2. Based on these falsified examples, we develop a framework called TEMPIRE (Teacher-Empowerment Model for Intelligent Pretreatment REcognition). The TEMPIRE framework combines pretraining with self-supervised learning to achieve robustness against adversarial attacks and transferability across different tasks. During pretraining, TEMPIRE fine-tunes a deep neural network pretrained on large amounts of labelled data, enabling it to learn complex features such as semantic relationships between expressions. Self-supervised learning ensures that the resulting model does not require explicit human annotation or manual feature engineering to learn meaningful representations. 
3. Finally, experiments demonstrated that TEMPIRE outperforms state-of-the-art methods like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) when evaluated on recognizing mathematics expressions task. For instance, TEMPIRE achieves an average recognition accuracy of 97% on the real world dataset considered in this work. 

Overall, our work demonstrates the feasibility of leveraging machine learning technologies to tackle challenging problems related to artificial intelligence in education. By incorporating prior knowledge about specific domains, such as math expressions, into an AI system, we can effectively train it to perform complex and sophisticated tasks, such as solving algebraic equations, geometry calculations, and trigonometry tasks. Using falsified input data generated by machine learning models that do not accurately predict correct output values for given inputs in training sets, we can also ensure the generality and robustness of the learned models against adversarial attacks and reduce their dependence on expensive manual annotations. With further research, we can leverage insights gained from this work to create more powerful, efficient, and effective mathematical AI systems for educational purposes.

# 2.核心概念与联系
## 2.1 Math Concept
Mathematics refers to the study of numbers, quantities, relations among these objects, patterns, and structures. Mathemeticians have long been interested in developing models, tools, and algorithms for understanding mathematical concepts and operations. However, building mathematical machines requires expertise in many areas including computer science, statistics, linear algebra, calculus, and physics. Thus, most modern mathematical models rely heavily on specialized hardware components and software libraries, making it difficult to scale up to handle large volumes of data or high complexity tasks. Therefore, there has been an increasing interest in developing mathematical models that can be easily deployed within applications and websites. One popular solution for computational modeling is Artificial Intelligence (AI). In recent years, significant progress has been made in applying AI techniques to solve various types of mathematical problems, ranging from basic arithmetic computations to advanced topics like scientific computing, graph theory, and optimization. 

In order to fully understand what AI can provide for mathematical problems, let's first look at some of the common mathematical operations. Here are several commonly used arithmetic operations that could benefit from AI technology:

1. Arithmetic addition: An AI model could take multiple similar examples of arithmetic additions and identify patterns in the input space and output space that allow it to make accurate predictions on unseen testing examples. 

2. Arithmetic subtraction: Similarly, an AI model could take multiple similar examples of arithmetic subtractions and identify patterns in the input space and output space that allow it to make accurate predictions on unseen testing examples. 

3. Geometric transformation: Another type of mathematical operation that benefits from AI technology is the application of geometric transformations like rotation, scaling, translation etc. A good AI model should be capable of identifying patterns in the input space and output space that allow it to transform geometric objects accurately from one form to another. 

4. Fractions: Fraction representation is useful for many practical applications, especially in education and entertainment settings. It allows students to communicate and reason about fractions better than if they were only presented with the numerator and denominator separately. A good AI model could recognize patterns in the input space and output space that allow it to represent fraction terms accurately. 

5. Comparison operators: Numerous comparison operators are often used in arithmetic and logical calculations. A good AI model should be capable of identifying patterns in the input space and output space that allow it to evaluate the results of comparisons accurately. 

6. Propositional logic inference: When dealing with propositional logic statements, a good AI model should be capable of inferring conclusions from premises accurately. Knowledge bases or databases can be augmented with additional information such as symbolic logic constraints, definitions, axiomatic rules, and other assumptions to improve the accuracy of the inferences.  

Therefore, we can see that AI technology can significantly accelerate the development of mathematical models. As AI becomes more widespread and widely available, we can expect AI models to become even more powerful and efficient in handling numerous types of mathematical operations. 

## 2.2 Approach
To address the challenges mentioned above, this article proposes a novel methodology that employs two key ideas: teacher-student learning and maximum mean discrepancy (MMD)-based learning algorithm. 

### 2.2.1 Teacher-Student Learning 
One way to understand teacher-student learning is to consider it as a special case of supervised learning, where the learner uses feedback from the teacher to adjust its behavior. Instead of having the teacher provide accurate labels for all the examples beforehand, we can generate synthetically designed input examples from scratch using a teacher-student setup. Let’s define the following terminology:

- **Input:** A set of variables x, y, z.... representing the operands involved in a mathematical operation. They could be numerical values or symbols representing variables. Each variable could be associated with a dimension d (e.g., a vector or matrix in R^d, or tensor in R^{d_1 \times d_2 \times...} ). 

- **Output:** A scalar value representing the result of the mathematical operation obtained by plugging in appropriate values for the variables. 

We can use standard regression-based ML models to fit a mapping function from X to Y, i.e., f(X)=Y. But instead of relying on a predefined target function, we can design the student’s model as a black box that takes in synthetic inputs rather than actual ones. To generate the synthetic inputs, we can use an algorithm known as teacher-student learning, where the teacher produces a pool of fake data points that resemble the desired pattern of outputs while the student attempts to reconstruct the underlying structure of the input space from the noisy data points provided by the teacher. The key idea behind teacher-student learning is that the teacher can guide the student towards a natural decision boundary in the input space that separates the true data points from the synthetic data points created by the teacher. The student then applies a convex loss function such as least squares error to minimize the distance between the predicted outputs and the true outputs. Once the student converges, the synthetic inputs can be replaced with the real ones and the teacher/student pair can produce the final model.

### 2.2.2 MMD-Based Learning
Another important aspect of AI-based mathematical problem-solving is the ability to distinguish between fraudulent and authentic input data samples. To accomplish this, we need to measure the similarity between the distributions of input vectors coming from two different sources (i.e., fraudulent versus authentic data sources). There exist several approaches for measuring the similarity between probability density functions, including KL-divergence and earth mover's distance. However, none of these measures directly optimize over the entire joint distribution of input vectors, which makes them impractical for the task of generating fraudulent input examples. Other approaches involve calculating distances between pairs of individual vectors, which can be computationally expensive and sensitive to noise and missing values. Moreover, these approaches may not capture all the relevant correlations between input vectors that occur naturally due to the contextual nature of the corresponding mathematical operation. Thus, in this article, we propose a new metric called maximum mean discrepancy (MMD) that captures the distribution difference between the true and synthetic distributions of input vectors efficiently. 

Maximum mean discrepancy (MMD) was proposed by Arora et al. [1] as a non-parametric measure of the distance between two probability distributions. MMD relies on a kernel trick, allowing us to compute the MMD between two distributions very efficiently, even when they have millions of samples. Formally, given two distributions p(x) and q(x), the MMD between them is defined as follows: 

$$\text{MMD}(p(x),q(x)) = ||f(x)||_{H}^2 + ||g(x)||_{H}^2 - 2 E_{x \sim p(x)}[f(x)]E_{y \sim q(y)}[k(x,y)],$$

where $||\cdot||_{H}$ denotes the Hilbert space norm, $f$ and $g$ are feature mappings applied to the inputs x, and $k(\cdot,\cdot)$ is a positive semidefinite kernel function that computes the inner product of the transformed inputs. The square bracket notation inside the expectation indicates that we are averaging over all possible couples of samples $(x,y)$. The first term on the right hand side represents the sample complexity penalty, where the second term measures the alignment of the two distributions. Ideally, the expected values of $f(x)$ and $g(x)$ should be zero, indicating that both distributions match perfectly. If either term is greater than zero, then the distributions are likely to differ slightly, which suggests that we might want to focus on improving the matching of the two distributions rather than trying to change them arbitrarily. 

By using kernel embeddings of the input vectors, we can approximate the distributions with respect to a low dimensional latent space that can be compared quickly using MMD. We can thus combine the strengths of teacher-student learning and MMD-based learning to generate fraudulent input examples that closely match the underlying distribution of authentic inputs. 

# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
Let's now discuss the details of the proposed algorithm. 

## 3.1 Synthetic Data Generation Process
Here is the detailed process for generating synthetic data using the Teacher-Student learning technique:

1. Define the mathematical operation: We start by defining the mathematical operation that needs to be performed. We assume that we already know what the correct answer would be for any given input, so we simply select a subset of those inputs whose answers are correct and generate randomized versions of them.  

2. Generate Pool of Inputs: Next, we need to generate a pool of inputs from which the student will choose the correct ones. Since there is typically a lot of variability in the range of acceptable values for each input variable, we randomly perturb each input independently according to its normal distribution. Then, we add Gaussian noise to the perturbed inputs to simulate errors introduced by sensors and processing artifacts. The size of the pool depends on the number of times we want to repeat this process and the degree of variation required. For example, if we want to find examples for multiplication of two matrices, we can try different combinations of sizes and scales of the matrices, and increase the difficulty gradually until we find enough representative examples. 

3. Train Student Model: Now, we can train the student model on the synthetic data generated by the teacher. The simplest approach is to use a simple regression-based model such as linear regression or ridge regression. After training, the student model generates predictions on the unperturbed inputs. We compare the student’s predictions with the true output values and calculate the mean squared error (MSE) or cross entropy loss (CE) depending on whether the problem involves classification or regression respectively. 

4. Determine Uncertainty Threshold: Depending on the desired level of uncertainty, we can set a threshold on the student model’s confidence score. Any input whose student prediction falls below this threshold can be flagged as potentially fraudulent. We continue selecting potential candidates for fraud and repeating steps 2-4 until we obtain enough examples that cover a wide range of contexts. We also update the pool of candidate inputs periodically to avoid overfitting and keep the diversity of the examples. 

5. Label Candidates: Once we have a pool of candidate inputs that seem potentially fraudulent, we ask a human expert to manually review them to determine whether each one indeed contains fraudulent material. If a candidate passes the review, we mark it as “authentic” and exclude it from future iterations of the algorithm. Otherwise, we move it back to the pool of candidate inputs for the next iteration. 

6. Repeat the Process: Finally, we repeat step 5 until we obtain a small enough subset of authentic inputs that cover the full range of contexts necessary to detect fraudulent activity. We can then use this subset to train a reliable classifier that can separate fraudulent activities from legitimate ones.

## 3.2 Mathematical Models and Representations
Next, we will describe the mathematical models and representations employed in this work. 

### 3.2.1 Kernel Embeddings for Input Distributions
A central component of the teacher-student learning scheme is the use of kernel embedding spaces to represent the input distributions. A kernel embedding is a mapping from a high-dimensional input space to a lower-dimensional space where similar elements tend to map close together. It encodes the dependencies between the input variables, making it easier to extract global patterns that characterize the relationship between them. In this work, we use radial basis functions (RBF) kernel embeddings to encode the input distributions. The RBF kernel parameter $\sigma$ controls the smoothness of the learned feature mappings and is chosen automatically to trade off between flexibility and efficiency. 

Given a dataset D of N input instances, the kernel matrix $K=\frac{1}{N}\mathbf{X}^\top\mathbf{X}$ defines the similarity between the input instances. We use the median heuristic to estimate the hyperparameter $\sigma$, which is the bandwidth parameter of the RBF kernel: $$\sigma = \sqrt{\frac{median(|\mathbf{x}_i-\mathbf{x}_{j}|^2)}{\log N}},$$ where $\mathbf{x}_i$ and $\mathbf{x}_{j}$ are the $i$-th and $j$-th instances in D, respectively. We initialize $\sigma=0.1$ for the initial exploration phase and refine it later using a grid search or Bayesian optimization strategy. 

After computing the kernel matrix, we apply a nonlinear transformation $\phi:\mathbb{R}^{n_\text{input}} \rightarrow \mathbb{R}^{n_{\text{embedding}}}$ to the normalized input instances $\mathbf{x}_i$ to obtain the embedded feature vectors $\hat{\mathbf{x}}_i$: $$\hat{\mathbf{x}}_i = \phi(\frac{1}{\sigma}\mathbf{x}_i),$$ where $\sigma$ is estimated based on the median heuristic described earlier. The choice of the number of dimensions $n_{\text{embedding}}$ determines the degree of compression achieved during the embedding process, trading off between interpretability and efficiency. 

Finally, we normalize the embedded feature vectors to have unit length to control the magnitude of the contribution of each input variable to the similarity measure: $$||\hat{\mathbf{x}}_i||_2 = 1,$$ where the normalization is performed elementwise along the embedding axis. 

### 3.2.2 Deep Neural Network Predictor 
The predictor architecture consists of several layers of neurons that transform the embedded feature vectors into a scalar output. In this work, we use a convolutional neural network (CNN) to classify input images into categories such as digits or letters. We chose CNN because of its ability to capture spatial relationships between pixels and their surrounding neighbors, which are crucial for image recognition tasks such as object detection. In contrast to traditional classifiers such as logistic regression and SVMs, CNNs can adapt dynamically to changes in the input distribution, improving the performance under slight perturbations. 

For simplicity, we can assume that the pixel intensity values lie between 0 and 1, leading to binary activation units. Alternatively, we can use sigmoid activations to obtain continuous outputs. The primary challenge of this part of the pipeline is to ensure that the predictor is robust to variations in the input distributions. Hence, we introduce regularization techniques such as dropout and early stopping to prevent overfitting. 

The overall architecture of the predictor is shown in Figure 1. 


Figure 1: Overall Architecture of Predictor Model

### 3.2.3 Empirical Risk Minimization Objective 
Once we have a trained predictor model, we need to construct an objective function that guides the construction of the synthetic data pool. We use the cross entropy loss function as the empirical risk minimization objective: $$\min_{\theta} L_{\text{CE}}\left(\theta;\mathcal{D},\mathcal{D}_{syn}\right)=-\frac{1}{|\mathcal{D}|}\sum_{(\mathbf{x}_i,y_i)\in \mathcal{D}} \log p(y_i|F_{\theta}(\mathbf{x}_i)),$$ where $\mathcal{D}$ is the original labeled dataset consisting of $N$ labeled examples and $\mathcal{D}_{syn}$ is the synthetic pool of fraudulent data. We hope that the synthetic pool contains both authentic and fraudulent data, leading to an optimal balance between data quality and quantity. 

Formally, the cross entropy loss maximizes the likelihood of the observed data under the parameterized model, where $F_{\theta}(\mathbf{x}_i)$ represents the predicted class probabilities for the $i$-th instance in the dataset. The parameters $\theta$ consist of weights and biases for the hidden layer of the predictor model, as well as hyperparameters such as learning rate, batch size, and regularization coefficients. 

### 3.2.4 Adversarial Attacks
Since the ultimate goal of this project is to generate fraudulent examples that confuse the detector model, we must ensure that the synthetic data cannot be too easy to spot. One way to accomplish this is to implement strong defense mechanisms, such as adversarial training and adversarial attacks. In this section, we briefly discuss how adversarial training works and how we can modify it to suit the requirements of the fraudulent generation setting. 

Adversarial training is a popular technique for increasing the robustness of deep neural networks. It adds adversarial examples constructed to deceive the discriminator model, which aims to differentiate between real and synthetic inputs. At training time, the generator model tries to generate inputs that fool the discriminator, while the discriminator remains calibrated to maximize the softmax cross-entropy loss on clean data. Adversarial attacks are modifications to the input data that are intended to mislead the learning process. In this work, we experiment with three attack strategies: Gradient Sign Attack (GSA), Jacobian Saliency Map Attack (JSMA), and Feature Squeezing Attack (FSA). 

#### GSA
Gradient sign attack (GSA) is a fast and simple attack that assigns a small constant step size to the gradient of the loss with respect to the input. Given an input point $\mathbf{x}$, GSA selects a small set of directions in the directional derivative of the loss with respect to the input and updates the input iteratively by adding a scaled version of the selected gradients: $$\delta^{\text{(GSA)}}_{i+1}=argmin_{\delta}\frac{1}{2}\lVert\nabla_{x}\ell(\mathbf{x})+\delta-\mathbf{g}\rVert^2,$$ where $\ell(\mathbf{x})$ is the loss function and $\mathbf{g}$ is the gradient of the loss with respect to the input, computed using backpropagation. Here, we fix $\delta=e^{-st}$ and vary the value of $s$ during training to explore a range of step sizes. 

During validation and testing, we use $\delta^{\text{(GSA)}}_{k+1}$ to substitute the actual gradient $\nabla_{x}\ell(\mathbf{x})$ in place of the gradient computed using the current value of the input. We train the predictor model on both clean and adversarial examples simultaneously to protect against both targeted and non-targeted attacks. We found that this variant of GSA was less effective than using the same step size throughout training. 

#### JSMA
Jacobian saliency map attack (JSMA) extends GSA to perform more sophisticated attacks that exploit the salient features of the input. Starting from a base point $\mathbf{x}_0$, JSMA perturbs the input point by replacing a single coordinate with a small adversarial perturbation that maximizes the response of the model to that coordinate. The approach is summarized in the pseudocode below: 

1. Set a counter $t$ to zero and set $\mathbf{x}^{(0)}=\mathbf{x}_0$. 
2. While $t<T$ and $\|\delta^{(t)}\|=1$, increment $t$ and select a subset of coordinates $S$ to perturb. 
3. Compute the jacobian matrix of the loss function $\nabla_{\mathbf{x}}\ell(\mathbf{x}^{(t)})$ and pick a reference point $\mathbf{r}$. 
4. For each coordinate $i$ in $S$, compute the saliency map of the loss function with respect to the $i$-th coordinate: $$\eta_i^{(t)}=\argmax_{\eta}\|\nabla_{\mathbf{x}}\ell(\mathbf{x}^{(t)})_{ij}-\nabla_{\mathbf{x}}\ell(\mathbf{x}^{(t)})_{i}\|.$$ 
5. Select the coordinate $i$ with largest absolute value of the saliency map, $\eta_{jsma}$: $$i=\arg\max_{i\in S}\eta_i^{(t)},\quad \eta_{jsma}=|\eta_i^{(t)}|.$$ 
6. Choose a small adversarial perturbation $\delta_i^{(t)}$ that maximizes the response of the model to the selection $i$: $$\delta_i^{(t)}=\argmin_{\delta}\frac{1}{2}\lVert\nabla_{x}\ell(\mathbf{x}^{(t)})_{i}+\delta-\eta_{jsma}(-\nabla_{\mathbf{x}}\ell(\mathbf{x}^{(t)})_{i})\rVert^2.$$ 
7. Update the input point $\mathbf{x}^{(t+1)}=(1-\epsilon) \mathbf{x}^{(t)}+\epsilon \mathbf{x}_i+\delta_i^{(t)}.$ Stop if $\|\delta^{(t+1)}\|>1$. Return $\mathbf{x}^{(t+1)}$. 

Here, we set $T=10$ and $\epsilon=0.01$ as the default values for the number of iterations and perturbation size, respectively. During validation and testing, we use $\delta_{jsma}$ to replace the actual gradient $\nabla_{x}\ell(\mathbf{x})$ in place of the gradient computed using the current value of the input. We train the predictor model on both clean and adversarial examples simultaneously to protect against both targeted and non-targeted attacks. Despite its appealing simplicity, JSMA had limited success in practice. 

#### FSA
Feature squeezing attack (FSA) constructs adversarial examples by linearly interpolating between nearby samples and corrupting certain features of the interpolated samples. The approach is summarized in the pseudocode below: 

1. Construct a binary mask $B$ that specifies which features to squeeze and which to leave unchanged. 
2. Pick a base point $\mathbf{x}_0$ and a direction $\tilde{\mathbf{v}}$ in the directional derivative space. 
3. Initialize an empty list of advantages $\Delta_k$ and iterate k steps: 
    * Compute the interpolation coefficient $\alpha_k$ based on the distance to the nearest neighbor and radius of influence. 
    * Interpolate the input point $\mathbf{x}^{(k)}=\alpha_k \mathbf{x}_0+(1-\alpha_k) \mathbf{x}_k$ and perturb it based on the selected features: 
        * For each active feature index $i$ in $B$, perturb it by sampling a uniform random number $\epsilon_i$ between [-1,1], and updating the feature as $\tilde{\mathbf{x}}^{(k+1)}_{i}=B_{ii}\mathbf{x}^{(k)}_{i}+\epsilon_iB_{ii}/2$. 
        * For each inactive feature index $j$ in $B$, update the feature as $\tilde{\mathbf{x}}^{(k+1)}_{j}=B_{jj}\mathbf{x}^{(k)}_{j}.$ 
    * Evaluate the loss of the adversarial example $\tilde{\mathbf{x}}^{(k+1)}$ and record the advantage $\Delta_k=\ell(\tilde{\mathbf{x}}^{(k+1)})-\ell(\mathbf{x}^{(k)})$. 
4. Select the best adversarial example $\tilde{\mathbf{x}}^{\star}=\underset{\tilde{\mathbf{x}}}max\Delta_k$, where $\tilde{\mathbf{x}}$ is the input point closest to $\mathbf{x}_0$ in the direction $\tilde{\mathbf{v}}$. Normalize the adversarial example to have unit length to preserve its direction. 
5. Train the predictor model on both clean and adversarial examples simultaneously to protect against both targeted and non-targeted attacks. 

Despite its appearance, FSA seems promising for generating fraudulent examples that confuse the detector model, but it did not perform well in practice. Nonetheless, we found it worth exploring since it is simpler than existing attacks and provides a solid baseline against which we can compare future improvements.