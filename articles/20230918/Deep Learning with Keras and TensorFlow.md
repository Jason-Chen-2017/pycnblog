
作者：禅与计算机程序设计艺术                    

# 1.简介
  

深度学习（deep learning）近年来在计算机视觉、自然语言处理等领域中得到了广泛应用。它的主要优点在于可以自动提取高级特征并进行有效分类和识别，取得了非常不错的效果。然而，要想真正掌握深度学习技术并加以应用，还是需要有扎实的理论基础和较强的编程能力。本文将以Keras和TensorFlow为主流工具介绍一些关于深度学习的基础知识和常用算法，旨在帮助读者更好地理解深度学习，并有针对性地使用深度学习技术解决实际问题。
# 2.基本概念及术语
## 概念
- **神经网络（neural network）**：一种基于计算的模拟器，由若干相互连接的神经元组成。它对输入数据进行分析、运算、输出结果。
- **深度学习（deep learning）**：使用多层次的神经网络作为学习模型，并根据大量训练样本对这些神经网络进行迭代改进，最终达到学习数据的复杂模式。
- **激活函数（activation function）**：神经元中用来衡量神经元输出是否激活的函数。一般来说，激活函数的选择会影响神经网络的性能。
- **权重（weight）**：指神经网络的连接强度。
- **偏置（bias）**：指神经元在某些情况下可能出现的激活值偏离正常值的程度。
- **损失函数（loss function）**：用来衡量模型预测值与真实值的差距大小，即目标函数。
- **优化器（optimizer）**：用于更新神经网络参数的算法。
- **输入层（input layer）**：网络的输入，通常是一个向量或矩阵。
- **隐藏层（hidden layer）**：由多个神经元组成的神经网络层，每个神经元与下一层中的所有神经元相连。
- **输出层（output layer）**：输出层又称“全连接层”或“输出层”，它与隐藏层的神经元相连，是整个神经网络的最后一层，也是网络的输出。
- **损失（loss）**：神经网络模型训练过程中，损失函数的值表示模型预测值与真实值之间的差距大小。
- **梯度下降法（gradient descent）**：一种最基本的优化算法，通过最小化目标函数来寻找最佳的参数值。
- **反向传播（backpropagation）**：是一种计算神经网络误差的方法，它通过调整网络参数，使得误差逐渐减小。
- **批归一化（batch normalization）**：一种对神经网络的中间层进行归一化的技术。
- **超参数（hyperparameter）**：网络结构中固定不变的参数，例如学习率、迭代次数等。
## 术语
- **张量（tensor）**：多维数组对象，它具有不同维度的索引。
- **张量的维度（rank）**：张量的秩，也叫阶数，表示张量的秩。
- **切片（slice）**：张量的一部分，它是一个新的张量，其数据来源于原始张量的一个子集。
- **形状（shape）**：张量的维度尺寸构成的元组。
- **坐标轴（axis）**：张量上某个方向的索引。
- **均值（mean）**：张量元素求平均值的结果。
- **标准差（standard deviation）**：张量元素与其均值差的平方根的算术平方根。
- **张量积（tensor product）**：两个张量的乘积。
- **特征缩放（feature scaling）**：把输入数据转换为合适的比例，比如[0,1]或[-1,1]。
- **标签（label）**：用来标记训练样本的类别。
- **样本（sample）**：由输入数据和相应的标签组成的数据项。
- **样本点（sample point）**：样本中的输入数据。
- **样本标签（sample label）**：样本对应的标签。
- **样本集（sample set）**：包含样本的集合。
- **特征（feature）**：样本点的属性，代表样本的外部信息。
- **特征空间（feature space）**：输入数据的低维空间，通过特征映射的方式得到。
- **样本空间（sample space）**：输出的可能范围，等于特征空间的非空子集。
- **类别（class）**：分类任务的判定标准。
- **样例（example）**：语义分类中，样本是句子，类别是情感类型。
- **类内距离（intra-class distance）**：同一类的样本之间距离的均值。
- **类间距离（inter-class distance）**：不同类的样本之间距离的最大值。
- **ROC曲线（receiver operating characteristic curve）**：是生物信息学中常用的一种绘制方法，描述了不同阈值下的TPR和FPR之间的关系。