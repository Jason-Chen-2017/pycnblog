                 

# 1.背景介绍



> 什么是强化学习？它是机器学习的一个子领域，它利用智能体与环境互动的方式来提升自身能力。而强化学习最重要的就是能够有效地学习到规律性、模式性、长期的智能行为与决策。它的关键在于如何让智能体学习到长远的奖赏。一般来说，强化学习可以分为基于模型的强化学习和基于策略的强化学习。基于模型的方法比较常用，主要包括Q-learning、DDPG等；基于策略的方法则较复杂一些，主要包括REINFORCE、PPO、A2C等。本文只讨论基于模型的强化学习——Q-learning。

强化学习的特点有三个：

* 模型学习：通过对环境的建模，智能体可以学习到环境中的状态转移函数，从而更好地做出决策。
* 学习奖励：智能体会不断获得环境反馈（即奖励）来指导其行为。
* 基于真实经验：强化学习通常依赖于真实的经验来训练智能体。也就是说，智能体要根据实际情况去学习。

# 2.核心概念与联系

## Q-learning

Q-learning，全称是Quality（质量）-Learning（学习），是一个基于状态的价值函数学习方法。它采用了一种被称为Q-table的表格来存储状态动作价值，并将当前状态作为输入，选择具有最大价值的动作作为输出，然后通过学习更新Q表格中的值，使得目标状态的预期回报尽可能高。

假设智能体处于某个状态s，采取动作a，收获reward r，那么下一个状态s'及动作a'可以表示如下：

$$Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha(r+\gamma max_{a'}Q(s',a'))$$

其中α是一个介于0和1之间的参数，用来控制学习率，γ是一个介于0和1之间的折扣因子，用于降低非优质路径上的探索效率。上式表示，Q函数中的值由旧值更新，新值=旧值(1−α)+α(reward+γmaxQ)。

Q-learning的算法流程如下：

1. 初始化一个Q-table，存放不同状态动作的价值，或者也可以叫做动作价值函数Q(s,a)。
2. 在每个episode内，智能体在当前状态s采取动作a，得到奖励r，并且得到环境的下一个状态s'。
3. 根据Q-table计算下一个状态的动作价值最大值maxQ(s')。
4. 更新Q-table中的对应状态动作价值Q(s,a)，使之等于Q(s,a)+α(r+γmaxQ(s')-Q(s,a))。这里的α，γ，以及(r+γmaxQ(s')-Q(s,a))都是超参数。
5. 重复以上过程，直至智能体达到终止状态。

## Sarsa与Q-learning

Sarsa与Q-learning有些类似，也是一种基于状态的价值函数学习方法。它们之间也有着一些区别，主要是Q-learning采用基于TD的方法进行更新，而Sarsa采用了Q-learning中的函数迭代法。Sarsa的目标是在每一步都执行一次实验，即执行动作a后获得奖励r和新的状态s'，再根据Q-table计算下一个状态动作的价值函数值，再执行动作a'，直到结束episode。

Sarsa的算法流程如下：

1. 初始化一个Q-table。
2. 在每个episode内，智能体在当前状态s采取动作a，得到奖励r和新的状态s'，然后根据Q-table计算下一个状态动作的价值函数值。
3. 根据Q-table选择动作a'，然后执行这个动作，得到奖励r'和新的状态s''。
4. 使用TD误差修正更新Q-table。
5. 重复以上过程，直至智能体达到终止状态。

## 行为空间与状态空间

在强化学习中，所有状态都被定义成离散或连续的，不同的环境可以有不同的状态空间和行为空间。通常情况下，状态空间由智能体能够感知到的所有状态组成，例如可以看到一个黑洞里的所有房间，但不会观测到其他位置的物品。行为空间则是指智能体能够做出的动作集合。当环境中有状态时，通常还存在一个奖励函数，它给予智能体在每个状态下所获得的奖励。