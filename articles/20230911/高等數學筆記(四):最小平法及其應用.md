
作者：禅与计算机程序设计艺术                    

# 1.简介
  

> 在機器學習领域中，使用最佳的模型來描述資料，是一個非常重要的問題。很多時候，我們會將資料做一些變換、組合或抽象化，然後透過計算機建立模型去描述或預測這些資料。而在模型訓練的過程中，往往需要衡量各種不同的方法，比如說最小二乘法、演算法搜索法、遗传演算法等，其中又以最小平法(Least Square Method)屢見不鮮。為了理解這個方法背后的原理與實現方式，並適當地應用於不同場景下，本文就會簡單介紹最小平法。  

## Least Squares Method是什麼?
最小平法是機器學習中的一項技術，它是一個搜尋最佳化問題的方法，也就是說，在某個函式空間中找出一個函式，使得該函式所對應的測資最小。舉例來說，如果我們想要求一個直線的斜率和截距，要怎麼找？因為直線方程式通常只有2個變數，所以可以選擇任意2點(x1,y1),(x2,y2)，如此即可求出斜率k和截距b。

\begin{equation} y = kx + b \end{equation} 

但是一般情況下，我們無法只使用2點來決定斜率與截距，而且也可能有更多的點或變數影響了函式的值，這樣子的題目叫做多變數問題。但是求解這些問題仍然可以通過最小平法來完成。

假設已知一組數據$(x_i,y_i)$，其中$i=1,2,\cdots,n$,並且希望找到一個$m$次多項式函式$f(\mathbf{x})=\sum_{j=0}^m a_jx^j$，使得該函式的擬合值$\hat{y}_i=f([x_i,x'_i,\cdots,x_m])$越接近真值$y_i$。也就是說，我們希望找到一個函式$a=(a_0,a_1,\cdots,a_m)$，使得下面的負載最小:

\begin{equation} J(\mathbf{a})=\frac{1}{2}\sum_{i=1}^n(y_i-\hat{y}_i)^2+\lambda R(\mathbf{a}),\end{equation}

其中$R(\mathbf{a})$為正則化項，$\lambda >0$是控制正則化項的強度。定義殘差平方和$RSS=\sum_{i=1}^n (y_i-\hat{y}_i)^2$，則$J(\mathbf{a})$就是:

\begin{equation} J(\mathbf{a})=\frac{1}{2} RSS + \lambda ||\mathbf{a}||_{\infty},\end{equation}

其中$||\cdot||_{\infty}$為向量的$\ell_\infty$范數，表示所有元素取最大值的總和。此時，對$a_j$求偏導，可得:

\begin{equation} \frac{\partial}{\partial a_j} J(\mathbf{a})=\frac{1}{2}(y_j-\hat{y}_j)(-2x_jy_j)+\lambda sign(a_j),\forall j.\end{equation}

由於求偏導並不是容易的事情，因此在求解時，可以先設定一個初始值$a^{(0)}=[a_0^{(0)},a_1^{(0)},\cdots,a_m^{(0)}]$，再用梯度下降法更新這些參數。梯度下降法的迭代公式如下:

\begin{equation} a^{(t+1)}=a^{(t)} - \alpha\nabla J(\mathbf{a}^{(t)})\end{equation}

其中，$t$表示第$t$次迭代，$\alpha$為步長，並且$\nabla J(\mathbf{a}^{(t)})$為$J$在$\mathbf{a}^{(t)}$所達到的局部最小值處的梯度向量。在最小平法的框架下，可以把$RSS$看作模型的殘差平方和，故可重寫成:

\begin{equation} J(\mathbf{a})=\frac{1}{2}\sum_{i=1}^n(y_i-f([x_i,x'_i,\cdots,x_m]))^2+\lambda R(\mathbf{a}).\end{equation}

## 時間序列資料的最小平法
時間序列資料是一種連續型態，具有時間性質，且具備一定的季節性，例如每年都會出現一些熱門的商品，或者是股票市場上的投資報酬率。對於時間序列資料，我們希望找到一個函式來描述它，其次，希望該函式的擬合值能夠在發生變化時，更加準確。這就是時間序列資料的最小平法研究。

例如，假設有一個時間序列資料$Z_t=(z_1,z_2,\ldots,z_p)$，代表著時間點$t$時刻的一種資產的價格變化，其中$z_i$代表第$i$只股票的股價。希望對這個時間序列進行建模，找出一個函式$g(t)=a_0+a_1 t+a_2 t^2+\cdots+a_q t^q$，其擬合值$\hat{Z}_t$越接近真值$Z_t$。最小平方法可以表述如下:

\begin{equation} J(a_0,a_1,\ldots,a_q)=\frac{1}{T}\sum_{t=1}^T (\log Z_t-\log \hat{Z}_t)^2+\lambda||[a_0,a_1,\ldots,a_q]||_{\infty}.\end{equation}

其中，$T$為時間序列的長度，$[\cdot]_{\infty}$為向量的$\ell_\infty$范數。該式的求解可以通過遞迴方法進行。另外，還存在其他種時間序列最小平方法，例如微分係數法(derivative coefficient method)。