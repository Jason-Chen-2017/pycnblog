
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在机器学习、深度学习领域中，许多复杂的模型都是由多个不同层次的神经网络层组合而成的。这种复杂的结构往往会带来极高的时间、空间上的计算开销，因此需要对其进行有效地优化。其中一个重要的方法就是求导。

由于我们一般无法直接对神经网络的每一层参数进行求导，只能求导某些特殊点的函数值。因此，对复合函数的求导方法就显得尤为重要了。本文将从复合函数的定义、求导法则和应用三个方面，详细介绍复合函数求导的基本知识。

# 2.复合函数
## 2.1 函数的复合定义
函数的复合(composite function)指的是由两个或更多的简单函数组合而成的复杂函数。通俗地说，就是将多个简单函数按照一定顺序结合起来，构成了一个更加复杂的函数。

举个例子，如图所示：


 在这个例子里，我们可以把$y=f(x)$和$z=g(y)$看做两个简单函数，复合函数$h(x)=\frac{1}{2}(z+f(x))$可以表示成：
 
 $$
 h(x)=\frac{1}{2}\left[g(\frac{1}{2}f(x))+f(x)\right]
 $$

这是一个复合函数，因为它由两个简单函数组成——$f$和$g$——并且按照一定的规则(称作结合律，即$\frac{d}{dx}[fg]=f'g+fg'$)，组合得到一个新的函数——$h$。

## 2.2 复合函数的求导
### 2.2.1 链式法则
前面已经提到，复合函数的求导方法是通过求各个函数单独的导数再相乘或者链式法则求出最终的导数。链式法则可以认为是对复合函数的微积分运算的一种推广，它适用于所有可导函数，包括凸函数、非凸函数甚至是整体不可导的函数。

例如，如果$z=\sin x$和$y=\cos^2 x$,那么$(y \cdot z)'=z'y+\tan x y'.$

链式法则还可以用来求解隐函数的偏导，利用链式法则，我们可以求解线性回归模型的最优解：

$$
h_\theta(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+\cdots+\theta_{p-1} x_{p-1} \\ 
J(\theta)=\frac{1}{m}\sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2
$$

令$g(x;w,b)=\frac{1}{1+e^{-wx-b}}$为激活函数，目标函数为：

$$
L(\hat{\beta},X,\hat{Y})=-[\prod_{i=1}^{N} y_i^{1-\phi_i}(\hat{y}_i)^{\phi_i}]_{+\infty}^{0} \\
\text{s.t.} \quad g(X;\hat{\beta})\geqslant 0
$$

对于此二类分类问题，由于存在不等式约束，因此模型参数$\beta$的解析解可能不存在，但可以通过迭代方式求解。假设$\beta_j$是第$j$个特征的参数，则有：

$$
\begin{cases}
h_{\beta}(x^{(i)}) &= \frac{1}{1+e^{-\beta^\top x}} \\ 
\nabla_{\beta} J(\beta) &= -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\frac{1}{1+e^{-\beta^\top x}}\log\frac{(1+e^{-\beta^\top x})}{y_i}+(1-y^{(i)})\frac{1}{1-e^{-\beta^\top x}}\log\frac{(1-e^{-\beta^\top x})}{1-y_i}]+\lambda||\beta||^2
\end{cases}
$$

其中$\nabla_{\beta}$表示梯度，$||\beta||^2$表示$\beta$的范数。上述方程可以通过链式法则求解：

$$
\begin{aligned}
&\nabla_{\beta} L(\hat{\beta},X,\hat{Y})=\sum_{i=1}^m [\frac{(1-\phi_i)(e^{\beta^\top x_i}-1)}{\hat{y}_i}+\frac{\phi_i(e^{\beta^\top x_i}/\hat{y}_i-1)}{\hat{y}_i}]\\&+2\lambda\beta
\end{aligned}
$$

其中$\hat{y}_i=\frac{1}{1+e^{-\beta^\top x_i}}$是预测的输出，$\phi_i$是真实标签。

除此之外，还有一些其他的求导技巧，如在神经网络的反向传播过程中应用链式法则求导，等等。

### 2.2.2 活化函数的导数
在深度学习中，许多函数都是基于激活函数的，这些激活函数本身的导数也是难以求得的。但是，很多时候，我们可以使用比较常用的sigmoid、tanh、ReLU等函数作为激活函数，他们的导数都很容易求出来。

例如，对于sigmoid函数$\sigma(x)=\frac{1}{1+e^{-x}}$及其导数：

$$
\begin{align*}
\sigma(x)&=\frac{1}{1+e^{-x}}\\
\frac{d\sigma}{dx}&=-\frac{e^{-x}}{(1+e^{-x})^2}\\
&=\frac{e^{-x}}{1+e^{-x}}(1-\frac{e^{-x}}{1+e^{-x}})\\
&=\sigma(x)(1-\sigma(x))
\end{align*}
$$

同样地，对于tanh函数$\mathrm{tanh}(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$及其导数：

$$
\begin{align*}
\mathrm{tanh}(x)&=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
\frac{d\mathrm{tanh}}{dx}&=\frac{e^x+e^{-x}}{(e^x+e^{-x})^2}\\
&=\frac{e^x}{e^x+e^{-x}}-\frac{e^{-x}}{e^x+e^{-x}}\\
&=\frac{\frac{e^x}{e^x+e^{-x}}}{1-\frac{e^x}{e^x+e^{-x}}}=\sigma(2x)-\sigma(-2x)\\
&=\frac{1-e^{-2x}}{1+e^{-2x}}=\frac{2}{1+e^{-2x}}-\frac{2}{1+e^{-2x}}=2\sigma(2x)
\end{align*}
$$

对于ReLU函数，它的导数也是比较简单的：

$$
\frac{d ReLU}{dx} =
    \begin{cases}
      1,&\text{$x > 0$} \\
      0,&\text{$x \leq 0$} 
    \end{cases}
$$