
作者：禅与计算机程序设计艺术                    
                
                
## 智能建筑概述
“智能建筑”作为一个较新的产业，近几年已经成为建筑行业的热门词汇。相对于传统建筑，智能建筑更注重于将“智慧”引入到建筑设计中，通过科技、信息化手段以及AI等手段解决建筑领域各类问题。智能建筑在不断地借鉴和更新，因此也越来越多的人开始关注这个新兴的产业。但是，由于智能建筑所面临的复杂性、缺乏规范的标准和规矩等方面的问题，以及建筑业本身面临的发展变化带来的挑战，导致了“智能建筑”领域仍然存在很大的不确定性。因此，如何利用大数据对智能建筑进行数据挖掘并找到建筑数据的价值的研究工作仍然具有重要意义。
## 数据定义及分类
为了有效地挖掘建筑数据价值，首先需要对数据的定义、分类以及特点有一个清晰的认识。通常情况下，大数据通常包含四种类型：结构数据、测量数据、文本数据、图像数据。其中，结构数据（包括电路设计、建筑模型、三维物体等）、测量数据（包括电压、流量、温度、湿度等）、文本数据（包括文字材料、文档等）以及图像数据（如摄像头拍摄的照片、矢量地图、卫星遥感影像等）。
## 建筑数据价值挖掘方法论
通常来说，建筑数据价值的挖掘方法可以分为以下五个阶段：收集数据、特征提取、数据预处理、数据挖掘、结果评估。其中，数据收集包括各种类型的数据采集，如结构数据、测量数据、文本数据和图像数据；特征提取指的是对不同类型数据进行抽象化、转换，得到统一的表达形式，这样才能将数据转化为可用于建模学习的形式；数据预处理是指对原始数据进行清洗、整合、过滤，使其能够更好地适应建模训练过程；数据挖掘则是在已有数据上进行统计分析和机器学习等技术，从中发现建筑数据中潜藏的信息；最后，结果评估则是根据挖掘结果判断其真实价值以及给予商业应用。
# 2.基本概念术语说明
## 大数据简介
大数据，也叫做海量数据，指的是一个数量巨大、类型多样、结构复杂的、非结构化、不可获取的数据集合。它既可以来源于互联网、企业内部系统、第三方服务等数据采集方式，也可以从实体经济活动产生的数据中产生。随着互联网、云计算、物联网等技术的普及，大数据应用越来越广泛，其膨胀速度超过了传统数据库。目前，大数据主要涉及数据的采集、存储、管理、分析、可视化、挖掘等多个环节。
## 数据挖掘相关概念
### 关键变量（Key Variables）
通常来说，数据挖掘的目标就是找出影响某些关键变量的因素，这些关键变量往往是指业务、产品或服务的质量属性、用户满意度、市场占有率等。关键变量通常由测量数据或者建模的输出决定，并且只能从数据中才能得到。
### 模型（Model）
模型是数据挖掘的重要工具之一，它是用来描述某一种现象出现的原因的理论依据。数据挖掘通常是用模型来进行预测、分析和决策。模型是基于经验、直觉和逻辑推导而得出的一个假设或者判断，用于解释某事物在某个特定条件下的现象。模型可用于预测、分类、聚类、降维等任务。
### 数据集（Dataset）
数据集是一个集合，里面包含的数据由数据项组成，每个数据项通常由属性和值组成。数据集包含两种形式：原始数据集和预处理后的数据集。
### 属性（Attribute）
属性是一个数据项的特征，它描述了一个对象或事物的一部分，比如人的年龄、职业、教育程度、婚姻情况等。属性可以是连续的、离散的或标称的。
### 类别（Category）
类别就是数据的分类标签。数据可以按照类别划分，不同的类别之间可以包含相同的属性。例如，可以按照城市、电影、食品等属性划分商品购买行为数据集。
### 距离（Distance）
距离是指两个向量之间的距离，通常采用欧氏距离（Euclidean Distance）或曼哈顿距离（Manhattan Distance）等。距离越小表示两个向量越相似。
# 3.核心算法原理和具体操作步骤以及数学公式讲解
## 特征提取
特征提取是指对不同类型数据进行抽象化、转换，得到统一的表达形式，这样才能将数据转化为可用于建模学习的形式。常用的特征抽取的方法有Bag of Words(词袋模型)、TF-IDF、LDA主题模型等。
### Bag of Words (词袋模型)
词袋模型是最简单的文本数据特征提取方法。其思想是将文本数据看作一个词序列，然后统计每个词的频率，将这些词频统计结果作为特征。
#### 操作步骤：
1. 分词：首先将文本数据按照字词进行切割，以便形成单词列表。
2. 词频统计：将切割后的单词列表统计各单词出现的次数，并排序。
3. 提取特征：选取一定比例的高频词作为特征，然后将其余词汇丢弃。
#### 优点：简单、不需要训练模型
#### 缺点：忽略了上下文信息、无法反映词序信息、不能处理长文本信息
### TF-IDF (Term Frequency-Inverse Document Frequency)
TF-IDF是一种经典的文本数据特征提取方法。其思想是统计每一个词的重要性，给予权重。重要的词会有更高的权重。TF-IDF是一种反映词频和逆向文档频率的词频加权方法，是一种统计学方法。
#### 操作步骤：
1. 分词：同Bag of Words模型。
2. 词频统计：统计切割后的词频。
3. 逆向文档频率统计：统计每个词的文档频率，即出现该词的文档数量。
4. TF-IDF计算：将词频除以文档频率，再乘以log(总文档数/文档频率)。
5. 提取特征：选取某一阈值下，TF-IDF的值较大的词作为特征，然后丢弃其他词。
#### 优点：考虑到了词频和逆向文档频率的权衡，可以获得更多有效信息。
#### 缺点：参数选择困难、算法效率低、无监督学习方法。
### LDA (Latent Dirichlet Allocation)主题模型
LDA主题模型是一种简化版的词袋模型，能够捕获文档的主题分布。其核心思想是先对文档进行主题建模，然后将文档表示为隐含主题的概率分布。
#### 操作步骤：
1. 对文本数据进行词库建模：建立一个词库，里面包含所有词和文档中的词。
2. 生成话题数：设置话题数目，一般设置为2～5。
3. 初始化主题向量：随机生成一个或几个主题向量。
4. 迭代生成文档主题：重复如下步骤：
    - 在每个文档中抽取K个话题。
    - 把每篇文档对应到各个话题上。
    - 更新主题向量，使得文档对应到各个话题上的概率最大。
5. 提取特征：选取一定比例的高频话题作为特征，然后丢弃其他话题。
#### 优点：简洁、自动提取文档的主题、无需训练模型。
#### 缺点：无法捕获长文本信息、不稳定性。
## 数据预处理
数据预处理是指对原始数据进行清洗、整合、过滤，使其能够更好地适应建模训练过程。数据预处理的目的有三个：去噪、聚类、降维。数据预处理的过程通常包括数据清洗、数据集成、特征选择、数据切分、数据标准化等步骤。
### 数据清洗
数据清洗是指将原始数据中异常值和缺失值删除掉，使数据变得更加干净、完整。数据清洗的方法包括去除空值、去除重复值、归一化等。
#### 方法1：去除空值
去除空值通常是最容易处理的，因为空值往往都是异常值或者缺失值。方法是把数据表格中的所有空值都删掉。
#### 方法2：去除重复值
如果数据中有重复值，那么它们就代表着数据本身的噪声。去除重复值的方法包括合并相同的数据条目、按照唯一标识符进行去重等。
#### 方法3：填充缺失值
在很多数据集中，有些值可能是缺失的。这种情况下，可以通过插值或使用其他方式填充这些缺失值。常见的填充方法有平均值、中位数、众数等。
### 数据集成
数据集成是指将多个不同来源的数据融合到一起。集成多个数据集是为了得到更加全面的信息。常见的数据集成技术有多视图回归、投票机制、混合模型、堆叠模型等。
#### 方法1：多视图回归
多视图回归将多份数据分别输入到相同的模型中，输出结果的平均值，来消除不同数据之间的差异。
#### 方法2：投票机制
投票机制是一种集成多个模型的思想。假设有n个模型，把它们的预测结果输入到一个投票器里，最后根据投票结果决定最终的预测结果。
#### 方法3：混合模型
混合模型是一种集成多个模型的思想。假设有m个模型，第i个模型的输出结果被作为权重w_i赋给第i个子模型。然后将所有子模型的预测结果输入到权重模型中，输出最终的预测结果。
#### 方法4：堆叠模型
堆叠模型是一种集成多个模型的思想。将不同的数据输入到不同的模型中，然后将所有的模型的预测结果叠加起来得到最终的预测结果。
### 特征选择
特征选择是指对数据集中冗余的、无关的、没啥用的特征进行剔除，只保留有效的、有用的特征。特征选择的目的是减少数据的维数，以提升模型的准确性和鲁棒性。特征选择的方法包括过滤、包装和嵌入等。
#### 方法1：过滤法
过滤法是指先对数据进行初步分析，然后筛选出重要的、有用的特征。常见的方法是Pearson系数法。
#### 方法2：包装法
包装法是指先对数据进行初步分析，然后将一些特征合并成一个特征。常见的方法是PCA降维法。
#### 方法3：嵌入法
嵌入法是指在机器学习中，将数据的特征直接映射到低维空间中。常见的方法是核方法。
### 数据切分
数据切分是指将数据集按一定规则划分成若干个子集，以便于交叉验证和模型调参。通常，训练集、验证集、测试集是最常用的划分方法。数据切分的标准是数据集尽可能保持相同的规模。
### 数据标准化
数据标准化是指将数据变换到同一量纲下，使得不同的特征之间更容易比较。常见的标准化方法包括Z-score标准化、MinMax标准化、L2标准化等。
#### Z-score标准化
Z-score标准化是指将数据按平均值和标准差标准化，即将每个特征的每个值都减去均值，然后除以标准差。
#### MinMax标准化
MinMax标准化是指将数据缩放到区间[0,1]，即将最小值变为0，最大值变为1。
#### L2标准化
L2标准化是指将数据按它的范数进行标准化，即除以它的平方根。
## 数据挖掘算法
数据挖掘算法有很多，包括线性回归、朴素贝叶斯、决策树、支持向量机、关联分析等。下面我们将详细介绍一些常用的数据挖掘算法。
### 线性回归
线性回归是一种简单但常用的线性模型。模型假设输入变量和输出变量之间存在线性关系。线性回归有很多种实现方式，这里介绍一种数学公式的形式。
#### 数学公式
y = a + bx
其中，y是输出变量，a是截距，b是斜率，x是输入变量。
#### 操作步骤
1. 拟合参数：拟合参数a和b，使得拟合误差最小。
2. 测试模型：使用测试数据集测试模型的准确性。
#### 优点
易于理解和解释，参数易于估计。
#### 缺点
容易受到异常值的影响。
### 朴素贝叶斯
朴素贝叶斯是一种简单的分类算法，属于判别模型。朴素贝叶斯模型认为各个特征之间相互独立，因此，朴素贝叶斯分类器实际上是一个独立假设的集合，每个假设对应着一个类的条件概率分布。
#### 数学公式
P(Ci|X) = P(Xi1=xi1, Xi2=xi2,..., Xin=xin|Ci)*P(Ci)/P(X)，其中，
* Ci：类别，C1、C2、...分别对应着类1、类2、...。
* xi：实例，xi1、xi2、...分别对应着实例的特征1、特征2、...。
* Xi：取值，Xi1、Xi2、...分别对应着特征的取值为xi1、xi2、...。
* P(Xi1=xi1, Xi2=xi2,..., Xin=xin|Ci)：条件概率，表示实例的特征条件下类别的发生概率。
* P(Ci):先验概率，表示实例属于类别Ci的先验概率。
* P(X):联合概率，表示实例的特征的联合概率。
#### 操作步骤
1. 准备数据：数据准备阶段需要对数据进行预处理，包括特征工程和数据预处理等。
2. 训练模型：朴素贝叶斯分类器是通过训练数据构建的，包括计算先验概率P(Ci), 条件概率P(Xi1=xi1, Xi2=xi2,..., Xin=xin|Ci)。
3. 测试模型：测试模型时，使用测试数据对分类器进行预测，同时计算正确率，即预测正确的概率和总样本数的比值。
#### 优点
简单快速，精度高。
#### 缺点
假设各个特征之间相互独立，可能会受到“协同偏置”影响。
### 决策树
决策树是一种基本的分类和回归方法。它采用树状结构，每个节点表示一个条件，其下的子节点表示不同的分支，从而将实例划分成不同的类。
#### 数学公式
Gini指数：
gini = 1 - Σ(pi^2)，其中，Σ表示求和，pi表示类别i所占比例。
信息增益：
gain = info(D) - info(D|A)
info(D)：训练数据集D的信息熵，
info(D|A)：特征A对训练数据集D的信息增益。
#### 操作步骤
1. 准备数据：数据准备阶段需要对数据进行预处理，包括特征工程和数据预处理等。
2. 构建决策树：决策树是递归构造的，每次对实例进行测试，根据测试结果选择最佳属性分裂。
3. 剪枝：剪枝是指当决策树过于复杂时，对它进行局部剪枝，使它变得简单。
4. 测试模型：测试模型时，使用测试数据对决策树进行预测，同时计算正确率，即预测正确的概率和总样本数的比值。
#### 优点
可解释性强，对异常值不敏感，容易处理连续数据。
#### 缺点
容易过拟合，偏向于局部最优解。
### 支持向量机
支持向量机（Support Vector Machine，SVM）是一种二分类模型，也叫做“软间隔支持向量机”。SVM通过求解超平面上的最大间隔，来将数据划分为两类。
#### 数学公式
目标函数：
min||w|| s.t y_i(wx_i+b)-1>=1, i=1,...,N
其中，w：超平面的法向量; b：超平面的截距; wx:样本向量和超平面的内积。
约束条件：
y_i=1 or -1
超平面：
w^T x + b = 0
y = w^T x + b
#### 操作步骤
1. 拟合参数：求解最优化问题，求得最优解w和b。
2. 测试模型：使用测试数据集测试模型的准确性。
#### 优点
理论基础性强，可以解决线性可分的问题。
#### 缺点
对非线性数据有严重依赖，且容易陷入局部最优解。
### 关联分析
关联分析是一种基本的分析工具，能够发现变量之间的关联关系。关联分析有基于规则、基于统计模型、基于图的方法。
#### 基于规则的关联分析
基于规则的关联分析是指利用一些规则，如“如果X出现，Y必定出现”，“如果A同时出现，B也同时出现”，“如果X出现，Y与C同时出现”，等等，来分析变量之间的关联关系。
#### 基于统计模型的关联分析
基于统计模型的关联分析是指利用统计学的方法，如卡方检验、皮尔逊相关系数、斯皮尔曼相关系数等，来分析变量之间的关联关系。
#### 基于图的方法的关联分析
基于图的方法的关联分析是指利用图论的方法，如强连通分量、最大团、最小割等，来分析变量之间的关联关系。
# 4.具体代码实例和解释说明
## 基于Python的数据挖掘案例——互联网金融数据分析
这个案例展示了如何利用大数据挖掘工具进行互联网金融数据分析。我们使用开源的Python库pandas、numpy、matplotlib、scikit-learn来加载、预处理、探索、分析和可视化互联网金融数据。这个案例涉及数据收集、数据清洗、特征提取、模型训练、模型评估、模型改进等多个步骤，希望能够帮助读者了解如何进行数据挖掘。
### 环境准备
1. 安装Anaconda Distribution或者Miniconda Distribution。
2. 创建一个名为finance的虚拟环境。
3. 使用pip安装pandas、numpy、matplotlib、seaborn、sklearn。
```python
conda create -n finance python=3.7
conda activate finance
pip install pandas numpy matplotlib seaborn sklearn
```
4. 在命令提示符中启动Jupyter Notebook。
```python
jupyter notebook
```

### 数据准备
金融数据是非常庞大复杂的，而且往往存在许多噪音和缺失值。因此，在进行数据分析前，我们需要进行数据的清洗、规范化、预处理等工作。在这一步，我们将收集的数据导入到pandas的DataFrame中，并进行数据清洗。我们假设数据集中包含以下字段：客户ID、交易日期、交易金额、币种、类型等。
1. 从互联网收集金融数据。
2. 将数据保存至本地文件。
3. 通过pandas读取数据并检查数据格式。
```python
import pandas as pd

# Read the data from local file.
df = pd.read_csv('data.csv')

# Check the format and head of the DataFrame.
print(df.head())
```

![image](https://user-images.githubusercontent.com/9861896/87773130-b5befb80-c84d-11ea-9f6e-faaaffcebc39.png)

4. 检查数据是否存在缺失值。
```python
print("Number of missing values:", df.isnull().sum().sum())
```

![image](https://user-images.githubusercontent.com/9861896/87773172-cbccbb00-c84d-11ea-9da2-ebca1276f8ec.png)

5. 清洗数据。
```python
# Drop rows with missing values.
df.dropna(inplace=True)

# Fill in missing values with median value for numeric columns and mode value for categorical column.
numeric_columns = ['transaction_amount']
categorical_column = 'type'
for col in numeric_columns:
    df[col].fillna(df[col].median(), inplace=True)
df[categorical_column].fillna(df[categorical_column].mode()[0], inplace=True)
```

6. 数据转换。
```python
# Convert currency to USD and transaction date to datetime format.
currency_to_usd = {'USD': 1.,
                   'EUR': 0.86}
df['transaction_amount'] *= currency_to_usd[df['currency']]
df['transaction_date'] = pd.to_datetime(df['transaction_date'])
```

7. 新增字段。
```python
# Add new field called weekday using the dayofweek function on transaction_date field.
df['weekday'] = df['transaction_date'].dt.dayofweek

# Add new field called month using the month function on transaction_date field.
df['month'] = df['transaction_date'].dt.month
```

![image](https://user-images.githubusercontent.com/9861896/87773339-1bcfaf80-c84e-11ea-9fa1-b6fd0117dc30.png)

### 数据探索
在数据探索阶段，我们希望了解数据集中的基本信息。
1. 查看数据集的基本信息。
```python
print("Data shape:", df.shape)
print("
")
print("Data types:")
print(df.dtypes)
print("
")
print("Missing values summary:")
print(df.isnull().sum())
print("
")
print("Descriptive statistics of each column:
", df.describe())
```

2. 绘制数据分布。
```python
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))
sns.distplot(df['transaction_amount'], ax=axes[0][0])
sns.countplot(df['customer_id'], ax=axes[0][1])
sns.boxplot(x='month', y='transaction_amount', hue='type', data=df, ax=axes[0][2])
sns.scatterplot(x='customer_id', y='transaction_amount', hue='type', style='weekday', data=df, alpha=0.5, ax=axes[1][0])
sns.lineplot(x='customer_id', y='transaction_amount', hue='type', data=df, ax=axes[1][1])
sns.barplot(x='customer_id', y='transaction_amount', hue='type', ci='sd', data=df, ax=axes[1][2]);
```

![image](https://user-images.githubusercontent.com/9861896/87773531-7db75700-c84e-11ea-997f-5a6fa2f7a34e.png)

### 数据预处理
在数据预处理阶段，我们将数据进行规范化和编码。
1. 数据规范化。
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df[['transaction_amount']] = scaler.fit_transform(df[['transaction_amount']])
```

2. 标签编码。
```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['type'] = encoder.fit_transform(df['type'])
```

3. 特征选择。
```python
from sklearn.feature_selection import SelectKBest, chi2

kbest = SelectKBest(chi2, k=5) # Select top 5 features by mutual information score
kbest.fit(df.drop(['customer_id', 'transaction_date'], axis=1).astype(float), df['type'])
features = df.drop(['customer_id', 'transaction_date', 'type'], axis=1).columns[kbest.get_support()]
print(features)
```

![image](https://user-images.githubusercontent.com/9861896/87773709-cd961e00-c84e-11ea-82a2-7c11c4b787de.png)

4. 划分训练集、测试集、验证集。
```python
from sklearn.model_selection import train_test_split

X = df[features]
y = df['type']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
```

### 模型训练
在模型训练阶段，我们将使用不同的机器学习算法来训练模型，并评估模型的效果。
1. 使用LogisticRegression训练模型。
```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train, y_train)
```

2. 使用DecisionTreeClassifier训练模型。
```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
```

3. 使用RandomForestClassifier训练模型。
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
rf.fit(X_train, y_train)
```

4. 使用SVM训练模型。
```python
from sklearn.svm import SVC

svc = SVC(kernel='rbf', C=1, gamma=0.1)
svc.fit(X_train, y_train)
```

5. 使用XGboost训练模型。
```python
!pip install xgboost==1.2
import xgboost as xgb

xg = xgb.XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, objective="multi:softprob", num_class=3,
                      booster='gbtree', reg_lambda=1, scale_pos_weight=1)
xg.fit(X_train, y_train)
```

6. 使用MLPClassifier训练模型。
```python
from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=[100, 50, 25], activation='relu', solver='adam', alpha=0.001,
                    batch_size='auto', learning_rate='adaptive', learning_rate_init=0.01, power_t=0.5, max_iter=200)
mlp.fit(X_train, y_train)
```

### 模型评估
在模型评估阶段，我们将使用不同的性能指标来评估模型的效果。
1. 使用accuracy_score评估模型效果。
```python
from sklearn.metrics import accuracy_score

y_pred = lr.predict(X_val)
acc = accuracy_score(y_val, y_pred) * 100
print("Accuracy of logistic regression model is {:.2f}%".format(acc))
```

2. 使用confusion_matrix评估模型效果。
```python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_val, y_pred)
print("Confusion matrix of logistic regression model:
{}".format(cm))
```

3. 使用classification_report评估模型效果。
```python
from sklearn.metrics import classification_report

cr = classification_report(y_val, y_pred)
print("Classification report of logistic regression model:
{}".format(cr))
```

4. 使用其他性能指标。
```python
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')
roc_auc = roc_auc_score(y_val, y_prob[:, 1])
print("Precision of logistic regression model is {:.2f}".format(precision))
print("Recall of logistic regression model is {:.2f}".format(recall))
print("F1-score of logistic regression model is {:.2f}".format(f1))
print("ROC AUC of logistic regression model is {:.2f}".format(roc_auc))
```

### 模型改进
在模型改进阶段，我们将尝试改善模型的效果。
1. 使用GridSearchCV寻找最优模型参数。
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['linear', 'poly', 'rbf']
}

svr = SVC()
grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

print("Best parameters after grid search are:", grid_search.best_params_)
```

2. 使用交叉验证调整模型参数。
```python
from sklearn.model_selection import cross_validate

cv_results = cross_validate(estimator=lr, X=X_train, y=y_train, scoring=['accuracy', 'precision','recall', 'f1'],
                            cv=5, return_train_score=False)

scores = {}
for metric in ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']:
    scores[metric.replace('test_', '')] = round(np.mean(cv_results[metric]), 2)

print("Cross validation results of logistic regression model:
{}".format(scores))
```

3. 使用BaggingClassifier训练模型。
```python
from sklearn.ensemble import BaggingClassifier

bagging = BaggingClassifier(base_estimator=None, n_estimators=100, max_samples=1.0, max_features=1.0,
                           bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False,
                           n_jobs=None, random_state=None, verbose=0)
bagging.fit(X_train, y_train)
```

