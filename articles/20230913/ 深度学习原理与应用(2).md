
作者：禅与计算机程序设计艺术                    

# 1.简介
  


# 2.神经网络中的激活函数
在深度学习的过程中，神经网络的输出结果是通过计算每一个输入特征和权重之间的乘积得到的。但是，这种简单直接的计算方式可能会导致某些神经元的输出“饱和”，从而失去神经元的激活功能，使得整个网络的性能下降。因此，需要引入非线性的激活函数来解决这一问题。下面我们介绍神经网络中最常用的激活函数——Sigmoid函数。
## Sigmoid函数
$$\sigma (x)= \frac{1}{1+e^{-x}}=\frac{e^x}{e^x + 1}$$
这个Sigmoid函数是一个S形曲线，即y随着x的增大或减小变化较大，并且在x=0处达到最大值（取值为0.5），在x无限大时趋于0。

我们通常用Sigmoid函数作为激活函数，其特点是输出范围是(0,1)，具有温度响应的特性，能够抑制节点输出值的梯度消失或爆炸现象，可以避免Vanishing Gradient的问题，能够有效处理不同数量级的数据。sigmoid函数表达式如下所示：

上面公式表示Sigmoid函数，其中e指自然对数的底，即$\ln(e)$；$x_i$表示第i个神经元的输入信号；$w_j$表示第j个神经元的权重参数；$\beta$表示偏置项。

Sigmoid函数的导数为：
$$\sigma'(x)= \sigma(x)(1-\sigma(x))$$

当sigmoid函数的输入信号越大时，输出越接近0或1；当输入信号越小时，输出越接近线性。

Sigmoid函数虽然简单易懂，但它存在两个问题：

1. 在横轴或者纵轴上变化较快，输出响应变化的速度过慢，容易发生Vanishing Gradient；
2. 当输入数据为负数时，sigmoid函数的输出为0，这在实际问题中可能造成困难。

为了缓解以上两个问题，出现了新的激活函数ReLU，目前效果最好的激活函数之一。

## ReLU函数
Rectified Linear Unit函数，也叫做线性整流函数，是一种非常简单的激活函数，其定义为：
$$f(x)=\max(0, x)$$
它的作用是将负值转变为0，使得神经元只能输出非负值。它具备各向同性的特点，能够在一定程度上缓解梯度消失或爆炸的问题。ReLU函数的导数为：
$$ReLU'(x)=\begin{cases}1,\quad x > 0\\0,\quad x < 0\end{cases}$$

ReLU函数对于正值几乎是线性的，但对于负值有阶跃迹，因此其收敛速度比Sigmoid函数快很多，而且由于缺少了sigmoid函数中第二个问题，因此在实际问题中往往效果更佳。

综上，Sigmoid函数在处理输入信号较大的情况下，会引起Vanishing Gradient的问题；ReLU函数在一定程度上缓解了这一问题，但由于缺少了sigmoid函数中第二个问题，因此在实际问题中往往效果更佳。

最后，我们总结一下：

1. Sigmoid函数：$y=\frac{1}{1+e^{-x}}$，优点是输出范围在(0,1)，能够避免Vanishing Gradient；缺点是有两个问题：1）在横轴或者纵轴上变化较快，输出响应变化的速度过慢；2）当输入数据为负数时，输出为0。
2. ReLU函数：$y=\max(0,x)$，优点是速度快，缺点是负值处仍然是阶跃迹，不够平滑。