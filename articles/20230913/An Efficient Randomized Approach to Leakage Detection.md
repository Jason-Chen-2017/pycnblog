
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Leakage detection (LD) is an essential technique for ensuring the security and reliability of critical systems such as communication networks, transportation systems, medical devices, etc. In this article, we will introduce a randomized approach based on thresholding that can achieve high LD performance with low false positive rates under both synthetic and real-world scenarios. The key idea behind our method is to sample uniformly at random from all possible pairs of input signals and measure their cross-correlation function R<|x_i x_j|> between them to detect any significant correlation patterns among them. 

Randomization has been shown to be effective in improving leakage detection performance by reducing the effect of interference due to overlapping signals or side channels. We use two independent methods to address this issue: oversampling and cross-validation techniques. Oversampling involves duplicating input signal samples so that each sample receives equal weight during processing. Cross-validation involves splitting the data into training and testing sets randomly and selecting a small subset of features or input dimensions for model learning. Both techniques help reduce statistical dependencies within and across the samples, which helps improve LD performance when there are multiple similar signals close together.

In addition to these technical improvements, we also propose a new metric called FDR (false discovery rate), which measures the proportion of incorrect positives (hypothesized correlated signals that do not exist in reality). This metric provides insight into how well LD works and can be used to tune hyperparameters or choose the appropriate level of significance required for practical applications. Our experiments show that the proposed algorithm can outperform existing approaches by achieving higher LD accuracy with lower false positive rates for both synthetic and real-world scenarios. Moreover, we demonstrate its efficacy using several benchmarks, including a large dataset generated using the NIST Statistical Testing Procedure Simulator (STPSim). Overall, our work demonstrates the potential benefits of using LD algorithms with additional computational efficiency and ease of implementation. 

2.概述
在通信网络、运输系统、医疗设备等系统中，对LD的需求日益增加。但目前还存在着很多问题：LD的准确性(accuracies)低下；缺乏有效的方法来处理interference、依赖于实际信噪比的环境噪声、FPR高等性能指标。因此，需要开发一种新的基于随机化技术的LD方法来改善LD性能，并提出一个新的FDR度量指标来衡量检测到的错误信号占总检测到的信号比例。

在本文中，我们将展示一种基于阈值化的随机化算法来实现LD，这种方法能够在合成和真实世界场景下，达到高度LD性能且误报率较低。关键点在于，我们采用随机取样的方式从所有可能的输入信号对中采样，并测量它们之间的交叉相关函数R<|x_i x_j|>, 发现其中的任何显著关联模式。

为了减少重叠信号或侧通道干扰对LD性能的影响，我们使用了两种独立的方法：过采样和交叉验证技术。过采样通过重复输入信号样本来增加每个样本的权重。交叉验证将数据划分为训练集和测试集，并选择模型学习所需的较小子集或特征维度。这两个技术可以帮助降低样本内部和跨样本间的统计依赖性，从而提高LD性能。

除此之外，我们还提出了一个新颖的度量标准——FDR（误报率），该指标衡量假设的相关信号在现实中是否存在。通过对多种基准测试的实验研究，我们证明了所提出的算法能够超过现有的LD方法，取得更高的LD精度和更低的误报率，并在合成和真实世界场景下进行评估。总而言之，我们展示了如何结合计算效率和简单易用性，应用LD算法来提升性能。

3.背景介绍
Leakage detection (LD) refers to the process of identifying and removing unwanted signals present in a given signal stream. It is crucial for ensuring the safety, security, and reliability of numerous critical systems such as communications networks, transportation systems, healthcare devices, etc. Currently, many researchers have developed various LD algorithms, but they typically employ handcrafted feature extraction and machine learning techniques, making it difficult to scale up and apply them to more complex systems. Furthermore, most studies only consider LD for binary input/output systems and ignore other important types of noise sources, such as multipath effects caused by interference or imperfect environmental conditions. Therefore, a better understanding of LD algorithms is essential to develop robust and efficient LD techniques for widespread deployment.

The goal of LD is to identify and isolate spurious signals that may interfere with desired information transfer in time-critical systems such as wireless communication networks, urban traffic management, medical devices, etc. While different implementations of LD algorithms vary in terms of theoretical guarantees, they share common components such as modeling the signal-to-noise ratio (SNR) distributions, performing inference on learned models, and dynamically adapting the detection thresholds based on measurement feedback. For instance, one popular LD algorithm is Wiener filter bank peak detector (WFBD) [3], which employs Gaussian mixture models (GMMs) to estimate SNR distributions and performs maximum likelihood estimation of hidden variables. Another popular algorithm is dual-threshold detector (DTD) [7], which uses median filtering and dynamic threshold adjustment to adaptively identify spurious signals while maintaining constant complexity. However, these methods often suffer from slow convergence and high false alarm rates, limiting their applicability in practice. To make progress towards faster convergence and improved detection performance, recent research has focused on developing novel LD algorithms based on deep neural networks (DNNs). These DNN-based approaches offer powerful capacity to learn non-linear representations of the input signal without relying on explicit feature engineering, enabling efficient computation and accurate prediction tasks.

However, even though DNN-based methods provide impressive performance on a variety of benchmark datasets, they still face fundamental challenges like lack of scalability, memory requirements, and sensitivity to hardware variations. Additionally, they require specialized hardware platforms like graphics processing units (GPUs) or field-programmable gate arrays (FPGAs), leading to costly deployments. Nonetheless, they promise to significantly advance the state-of-the-art in LD technology.

In this paper, we introduce a randomized approach to LD based on thresholding. Our main contributions are threefold. First, we derive a formula for measuring the cross-correlation function R<|x_i x_j|>, which can efficiently handle arbitrary dimensional inputs and produces more meaningful results than conventional window functions. Second, we implement a new LD algorithm called RDMDLE (randomized decoupled multiple detection of latent events) that combines oversampling and cross-validation techniques with DNN-based predictions to achieve high LD performance under both synthetic and real-world scenarios. Finally, we develop an alternative metric called FDR, which estimates the proportion of errors made by the LD algorithm and provides insights into the tradeoff between correctness and false positives. By combining these techniques, we show that RDMDLE outperforms existing DNN-based approaches by achieving higher accuracies with lower false positive rates, demonstrating the importance of addressing technical issues in LD algorithms development.