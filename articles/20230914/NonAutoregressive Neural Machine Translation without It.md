
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在Neural Machine Translation(NMT)任务中，传统的基于序列到序列学习(seq2seq learning)的方法往往存在困难收敛、过慢训练速度等问题。为了解决这个问题，最近出现了一些改进模型，如Transformer、Conformer等。这些模型采用注意力机制来处理输入序列的表示，能够更好地捕获序列间的关系。但是，这些方法都依赖于前面的输出进行后续的推断，而这种固有的模式对生成质量的影响很大。因此，作者提出了一个新颖的方案——Non-Autoregressive Neural Machine Translation(NAUT).通过在训练过程中不直接对输出结果进行回头修正，可以使模型获得更好的性能。其主要工作如下：

1）生成器（Generator）不接受解码器输出的单词，而是从一个均匀分布中采样生成。

2）生成器训练时不依赖于解码器的预测结果，而是在每个时间步随机选择输入。

3）生成器使用自回归（Self-Recurrent）网络来保证循环一致性。

4）生成器仅用于生成，而实际使用的是标准NMT模型中的编码器-解码器结构。

5）训练过程不需要迭代推理，直接优化生成器即可。

本文将会给出NAUT模型的原理及其实现，并介绍相关的实验结果。最后，本文还会讨论NAUT的优点、局限性以及可期待的发展方向。
# 2.背景介绍
在Neural Machine Translation任务中，给定输入序列x=(x1,...,xn)，翻译成目标语言的输出序列y=(y1,...,ym)。通常情况下，传统的方法需要通过解码器对序列进行推断，生成目标语言的句子。但是这种推断方式对模型的性能很重要，因为它涉及到两个独立的步骤，即输出序列的生成和推断。首先，生成器必须能够理解整个源语言的表达，并且尽可能地模仿它，生成接近原始语句的目标语言句子。第二，解码器的预测必须考虑当前已经生成的序列片段以及之前的输出。当生成器生成新片段时，模型就需要采取相应的措施来推断未来的输出，使得翻译质量更高。

传统的seq2seq模型通过解码器的输出来反向传播梯度，以更新模型的参数，但是由于解码器必须考虑历史输出，导致计算复杂度较高。因此，最先进的方法使用注意力机制来处理输入序列的表示，来捕获不同位置之间的关联关系。然而，这些模型又面临计算复杂度的限制。

针对这些问题，本文提出了一种新的非自动推理方法，称为Non-Autoregressive Neural Machine Translation(NAUT)。NAUT模型采用生成器代替解码器，同时在训练过程中不依赖于解码器的预测结果，而不是像RNN-based NMT模型一样依赖于前面的输出。这样做可以避免生成器陷入无法自我监督的模式，并有助于解决序列生成的问题。

此外，生成器由标准的RNN/LSTM等神经网络实现，可以自回归地产生序列，从而保证循环一致性。因此，它可以在多个时间步上生成相同的内容。这一点是与RNN-based NMT模型不同的地方。最后，生成器仅用于生成，而实际使用的是编码器-解码器结构，如RNN-based NMT模型。

据作者所述，NAUT模型能够比RNN-based NMT模型的性能更好，并且也不需要进行迭代推理，而直接优化生成器即可。

作者也提到了以下几点优点：

1）不需要进行迭代推理，能够快速且有效地训练生成器。

2）生成器可以自回归地产生序列，因此可以保证循环一致性。

3）训练过程不需要迭代推理，因此可以获得更加鲁棒、准确的翻译。

4）模型不需要编码器-解码器，因而可以节省算力资源。

5）模型可以利用从其他任务中学习到的知识，增强其性能。

然而，本文还要指出以下几点局限性：

1）训练的非随机性可能会影响模型的性能。

2）生成器依赖于之前的输出，所以只能生成序列；但是对于生成图像、音频或者文本等其他类型的序列，则不能采用该模型。

3）训练过程不依赖于解码器，因而可能会遇到某些困难情况，比如循环或遗忘等。

4）需要额外的优化，才能确保模型的性能。