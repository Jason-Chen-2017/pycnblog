
‰ΩúËÄÖÔºöÁ¶Ö‰∏éËÆ°ÁÆóÊú∫Á®ãÂ∫èËÆæËÆ°Ëâ∫ÊúØ                    

# 1.ÁÆÄ‰ªã
  

Scraping is a technique used to extract data from websites using automated scripts or programs. It involves the extraction of information from HTML web pages such as text, images, videos, etc., which can be useful for various purposes such as data collection, analysis, sentiment analysis, and many more. In this article, we will discuss about web scraping techniques in depth with an example on how to scrape data from Amazon product reviews website. We will also explain the basic concepts of web scraping including HTTP requests and responses, DOM manipulation, parsing and regular expressions, and finally wrap up by giving some tips and tricks on web scraping libraries and tools that are available today. If you have any doubts regarding this topic, feel free to ask them below! Also, I would love to hear your feedback on this article once it's published. Thank you for reading!üòäüëçüèºüåéüá®üá¶
# 2.Web Scraping Introduction
Web scraping refers to the process of extracting large amounts of data from websites automatically using computer programming. The data extracted can be analyzed further for insights, trends, and decision-making. Web scraping has become increasingly popular due to its versatility, speed, and flexibility. There are several ways to perform web scraping depending on the purpose and target audience of the data being collected. Popular web scraping techniques include:

1. Structured Data Extraction: This involves extracting only relevant and meaningful data points within predefined structures such as tables and forms. 

2. Semi-Structured Data Extraction: This involves extracting relevant data without prescribing specific formats, like blog posts or news articles. 

3. Unstructured Data Extraction: This involves extracting valuable data without prior knowledge of the structure of the webpage, such as emails, social media content, and comments posted online. 

In our example, we will use Amazon product review website to demonstrate different web scraping techniques such as structured, semi-structured, and unstructured data extraction. Specifically, we will focus on how to extract product ratings, helpfulness votes, user names, and reviews written by customers. By the end of this article, you should be able to understand web scraping techniques and implement them efficiently using Python libraries like BeautifulSoup and Requests. If not, please let me know in the comment section below!‚úåÔ∏èü§ùüèª
# 3.Basic Concepts of Web Scraping
Before diving into technical details, let‚Äôs first understand some fundamental terms and concepts related to web scraping.
## HTTP Request and Response
When a browser sends a request to a server to access a web page, it sends a message called an HTTP (Hypertext Transfer Protocol) request over the internet. The server responds back with an HTTP response message which contains all necessary files needed to display the requested web page. The status code in the HTTP response tells whether the request was successful or not. Here are some common status codes:
* **200 OK**: Indicates that the request was successfully processed by the server.
* **404 Not Found**: Indicates that the requested resource cannot be found on the server.
* **500 Internal Server Error**: Indicates that there is a problem on the server side.
HTTP protocol uses a client-server architecture where each device acts as either a client or a server. A client sends a request to the server to access resources and receives a response containing the required resources. When a browser makes a request to a website, it sends an HTTP GET request to retrieve web pages. An example of a typical HTTP request header looks like this:

```
GET / HTTP/1.1
Host: www.example.com
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
Accept: */*
Accept-Language: en-US,en;q=0.9
Connection: keep-alive
Cookie: sessionid=<session_key>
```

The above request asks the server to respond with the contents of the root directory of the site, along with additional metadata such as headers, cookies, etc. The `Host` field specifies the domain name of the server, while the `User-Agent`, `Accept`, and `Accept-Language` fields provide additional contextual information that helps the server identify the client making the request. Finally, the `Connection` field sets the connection type to be kept alive so that subsequent requests do not require a new TCP handshake.

Similarly, when the server processes the request, it returns an HTTP response message containing the requested resources or error messages if applicable. For example, here is a sample HTTP response message returned by the server:

```
HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 2157
Date: Tue, 10 Dec 2021 18:06:25 GMT
Server: Apache/2.4.41 (Ubuntu) PHP/7.3.31-1+ubuntu20.04.1+deb.sury.org+1 OpenSSL/1.1.1f
Last-Modified: Sat, 01 Sep 2021 13:07:44 GMT
ETag: "7d5c2e-5f3a-5b319f9c7f000"
Accept-Ranges: bytes
Vary: Accept-Encoding
Connection: close

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
   ...
  </head>
  <body>
   ...
  </body>
</html>
```

This response indicates that the server successfully retrieved the requested resource at `/`. Additional headers included in the response message tell us the length of the response body, date and time of retrieval, server software used, last modification timestamp, entity tag (an identifier representing the current version of the resource), vary header indicating variations between different representations of the same resource, and connection type set to `close`.

Both clients and servers follow certain communication protocols to exchange messages and ensure secure communication over the internet. Although web browsers usually handle these complex protocols behind the scenes, understanding their basic principles is essential for working with web scraping libraries and frameworks.
## Document Object Model (DOM)
A document object model (DOM) is a tree-based representation of a web page's elements. It represents the entire web page as a node hierarchy consisting of nodes such as elements, attributes, and text. Each element in the hierarchy corresponds to a distinct piece of content on the page, and they may contain other child nodes nested within them. Among other things, DOM allows JavaScript and other scripting languages to manipulate the content dynamically, enabling dynamic interactivity on web pages.

To illustrate how DOM works, consider the following HTML code:

```
<!DOCTYPE html>
<html>
  <head>
    <title>My Website</title>
  </head>
  <body>
    <h1>Welcome to my website!</h1>
    <p>
      Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vestibulum dictum ex vel purus imperdiet bibendum. Sed dapibus gravida leo, sed lacinia libero feugiat ut. Proin ut justo vel metus iaculis malesuada. Cras consequat, velit quis sollicitudin semper, mi lectus pretium est, eu pulvinar nunc sem vitae mauris. Integer cursus accumsan turpis eget blandit. Donec convallis ex nec ante semper, non tincidunt sapien eleifend.
    </p>
  </body>
</html>
```

We can represent this HTML code in DOM format as follows:


Here, the `<html>` element represents the entire web page, and the two children elements (`<head>` and `<body>`) correspond to the head section and the main content of the page respectively. Similarly, the `<h1>` and `<p>` elements represent headings and paragraphs respectively, and the text contained inside them is represented underneath them.

By manipulating the DOM, we can add, remove, or modify elements of the page programmatically, allowing us to build powerful interactive applications. Additionally, modern web frameworks like React, Angular, and Vue often use virtual DOM to simplify the management of the DOM and improve performance. However, understanding DOM basics is still crucial for working with web scraping libraries and frameworks.
## Parsing and Regular Expressions
Parsing is the process of converting raw data obtained through web scraping into a usable form. The most commonly used method of parsing is string manipulation via regular expressions. 

Regular expressions, also known as regex, are patterns that define search criteria that match character combinations in strings. They are widely used in text editing software, search engines, and database queries. With regex, we can specify what kind of data we want to extract from a given source, and then use pattern matching algorithms to locate and isolate the desired pieces of data. 

For instance, suppose we want to extract all links present in a web page. One way to accomplish this is by searching for all occurrences of the string `"href"` followed immediately by `"="` and a URL starting with `"http"`, like so:

```python
import re

html = "<html><body><a href='https://www.google.com'>Google</a></body></html>"

pattern = r'<a\shref=["\'](.*?)["\']>' # find all 'a' tags with attribute 'href'

matches = re.findall(pattern, html)

print(matches) # output: ['https://www.google.com']
```

Here, we import the `re` module, define a pattern string that matches all `'a'` tags with an `href` attribute, and call the `findall()` function to return a list of all matched URLs in the HTML string. Note that this approach assumes that the `href` attribute always appears after the corresponding opening `'a'` tag.

Other types of patterns that we can use with regex include finding email addresses, phone numbers, dates, times, and currency values in text documents. Regex provides a powerful tool for extracting data from web pages but requires careful attention to detail and consistency. Therefore, practicing with regex skills is essential for effectively scraping data from various sources.
# 4.Example Web Scraping Code
Now that we've learned the basic concepts of web scraping, let's write some practical examples of web scraping code using BeautifulSoup and Requests library. We will start with an Amazon product reviews website, which contains thousands of customer reviews ranging from positive to negative. Our goal is to extract product rating, number of helpful votes, username of the reviewers, and actual review text. Let's begin!