
作者：禅与计算机程序设计艺术                    

# 1.简介
  

深度玻尔兹曼机（Deep Boltzmann Machines，DBM）是近年来受到越来越多关注的一种无监督学习方法，由Hinton、Bengio、Leslie三个科学家于2006年提出。它是一种通过学习表示分布参数的方式，来对数据进行聚类、分类或预测的生成模型。它的特点是：

1. 使用连续可微分的Boltzmann分布作为激活函数；
2. 深层结构网络，能够捕获数据的非线性特性；
3. 通过隐变量之间的相互依存关系，学习到有效的表示。

在此基础上，深度玻尔兹曼机能够自动发现隐藏的特征模式、构造多模态数据表示，并对输入数据进行可靠的预测和分类。由于其天生的自编码特性，深度玻尔兹曼机可以有效地从数据中学习到有效的抽象表示，并在未知数据上进行泛化预测。
# 2.基本概念
## 2.1 DBN
深度玻尔兹曼机是基于马尔科夫网络（Markov Network）构建的，所以首先需要了解马尔科夫网络的一些基本概念及术语。

### 马尔科夫链
马尔科夫链（Markov Chain），又称为状态空间随机过程，是一个统计模型，描述的是在给定当前状态下，状态转移概率仅依赖于当前时刻的状态，与过去的历史无关，是由一个确定性递推的过程所产生。

### 马尔科夫网络
马尔科夫网络（Markov Network），也被称作马尔科夫随机场，是由观察者到各个状态的转换图形化的表示，是一种概率分布随机场。与马尔科夫链不同的是，马尔科夫网络中包括了潜在变量（Hidden Variables）。

### 半马尔科夫链
半马尔科夫链（Semi-Markov Chain），是指一个马尔科夫链，其状态转移矩阵只依赖于当前时刻的状态，不依赖于过去的历史状态。半马尔科夫链的转移概率矩阵也称为转移函数（Transition Function）。

### 负熵
对于给定的观察序列$X=\{x_1,\dots,x_t\}$，假设模型为$\pi(h_{t+1}|h_t)$，其中$h_t$为隐变量序列，那么极大似然估计即为：

$$L(\theta)=\prod_{t=1}^T p(x_t|h_t;\theta) \\ = \frac{1}{Z}\prod_{t=1}^T \sum_{h_t}p(x_t,h_t;\theta) $$

其中$Z$是归一化常数，$\theta$为模型的参数。为了简便起见，记：

$$D_{\theta}(X)=\ln Z+\sum_{t=1}^T\ln p(x_t,h_t;\theta)$$ 

该量代表观察序列$X$的对数边缘似然值，可以通过梯度下降法或拟牛顿法来优化。

我们可以使用正则化项（Regularization Term）来增加模型的复杂度。例如，可以使用KL散度（Kullback–Leibler Divergence）作为正则化项来惩罚模型的复杂度。

$$D_{\theta}(X)+R(\theta)$$ 

其中$R(\theta)$为正则化项。当模型越简单时，正则化项的值越小，反之亦然。因此，通过控制模型的复杂度，来选择合适的模型。

### 条件熵
对于隐变量序列$h_1,\dots,h_t$，定义条件熵为：

$$H(h_t|x_{1:t})=-\sum_{h_{t'}}p(h_{t'}|h_t)\log q(h_{t'}|h_t)$$ 

其中$q(h_{t'}|h_t)$表示已知当前状态$h_t$时，隐变量取值为$h_{t'}$的条件概率分布。条件熵越大，表明模型越难以正确的预测当前状态，因而对学习效率有更大的影响。

显然，通过最小化条件熵，就可以使得模型学习到一个合适的隐变量分布。