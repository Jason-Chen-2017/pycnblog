
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在这里，我将整理一些关于机器学习相关的最重要的数学公式、符号，希望大家能对机器学习有一个系统性的了解。虽然目前还没有太多的数据可以用来证明这些公式或者符号的正确性，但我觉得把这些公式放在一起，无论对于解决实际问题还是理解机器学习的原理都可能很有帮助。
# 2.一元线性回归模型
## 2.1 简单线性回归模型
$$
\begin{equation}
Y = \beta_0 + \beta_1 X + \epsilon \\[1ex]
\end{equation}
$$
- $Y$表示因变量
- $\beta_0$表示截距
- $\beta_1$表示系数
- $X$表示自变量
- $\epsilon$表示误差项或残差项
## 2.2 多元线性回归模型
$$
\begin{align*}
y_{i} &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}\\
&+ \epsilon_{i}, i=1,2,\dots,n
\end{align*}
$$
- $n$ 表示样本数量
- $x_{ij}$ 表示第j个自变量的值
- $\beta_0$, $\beta_1$,..., $\beta_p$ 为回归系数
- $\epsilon_{i}$ 表示第i个样本的误差项
## 2.3 最小二乘法估计参数
$$
\hat{\beta}=\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}\mathbf{X}^{\prime}\mathbf{Y}
$$
- $\mathbf{X}$ 为$n\times p$矩阵，其中每行为一个样本，$p$为自变量个数
- $\mathbf{Y}$ 为$n\times 1$列向量，每行对应一个样本的因变量值
- $\hat{\beta}$ 为回归系数估计结果
## 2.4 概率分布函数及其概率密度函数（PDF）
- **正态分布**
  - PDF：$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
  - CDF：$F(x)=\int_{-\infty}^{x}f(t)\mathrm{d}t$
  - 期望（均值）：$\mu$
  - 方差：$\sigma^2$
- **学生-$t$分布**
  - PDF：$f(x)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi\nu}\Gamma(\nu/2)}\Big[\frac{1+\frac{(x-\mu)^2}{\nu}}{\sqrt{\nu}(1+(x-\mu)^2/\nu)}\Big]^{-(\nu+1)/2}$
  - CDF：$F(x)=\int_{-\infty}^{x}f(t)\mathrm{d}t$
  - 形状参数：$\nu>0$
- **指数分布**
  - PDF：$f(x)=\lambda e^{-\lambda x}$
  - CDF：$F(x)=1-e^{-\lambda x}$
  - 平均等待时间：$\lambda$
- **贝塔分布**
  - PDF：$f(x|\theta)=\frac{\theta}{B(\alpha, \beta)}x^{\alpha-1}(1-x)^{\beta-1}, 0<x<1,$
  - CDF：$F(x|\theta)=I_{\theta}(x)$
  - 形状参数：$\theta\in [0,1]$；非负可加参数：$\alpha>0$；负可加参数：$\beta>0$
## 2.5 信息论中的熵和KL散度
- **互信息（mutual information）**：$I(X;Y)=D_\text{KL}\left(\frac{P(X, Y)}{P(X) P(Y)}\right), I(X;Y)>0$
- **熵（entropy）**：$H(X)=-\sum_{k=1}^K p_k \log p_k$
- **交叉熵（cross entropy）**：$H(p,q)=-\sum_{x\in\mathcal{X}}\left[p(x)\cdot\log q(x)\right]$
- **KL散度（Kullback-Leibler divergence）**：$D_{\text{KL}}(p||q):=\sum_{x\in\mathcal{X}}\left[p(x)\cdot\log\left(\frac{p(x)}{q(x)}\right)\right]$
- **条件熵（conditional entropy）**：$H(X|Y)=-\sum_{k=1}^K p_k(Y) \log p_k(Y)$
- **相对熵（relative entropy）**：$D_{\text{KL}}(p\|q)=\sum_{x\in\mathcal{X}}\left[p(x)\cdot\log\left(\frac{p(x)}{q(x)}\right)\right]-H(p)-H(q)+H(p,q)$