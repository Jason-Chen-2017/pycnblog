
作者：禅与计算机程序设计艺术                    

# 1.简介
  

Document retrieval is a critical task in information retrieval that aims to retrieve relevant documents from large collections based on query keywords or phrases. One common problem for this task is the sparsity of multilingual document collection, which means only a small fraction of documents are available in many languages. In order to improve performance and scalability of web-scale multilingual document retrieval system, we need to combine multiple training datasets into one corpus. In this paper, we propose a novel method called MultiCorpus Document Retriever (MCR) to handle such a scenario. MCR first divides all input corpora into several sub-corpora based on language pairs or other criteria. Then it trains an individual model for each sub-corpus using a pre-trained language model and combines them into a final combined model that can perform document retrieval efficiently across different languages. We evaluate our approach on two benchmark tasks: cross-language passage retrieval and Chinese word segmentation. Experimental results show that MCR achieves significant improvement over single-corpus models while maintaining high recall and precision levels. Moreover, we also demonstrate its efficiency in handling large volumes of data with millions of documents across various languages. The code implementation and trained models will be made publicly available for further research.

# 2.相关工作
There have been several approaches proposed in literature to handle the sparsity issue in multilingual document retrieval systems. Some of these methods include hybrid retrieval models that combine information from multiple sources or heterogeneous search engines that target specific languages individually. However, none of these methods consider combining multiple training datasets directly as the key factor for improving performance and scalability. 

Existing works focus either on optimizing language modeling for multilingual document retrieval or learning phrase representations to enhance cross-lingual retrieval performance. These approaches typically rely on carefully designed features or prior knowledge about how different languages share semantic similarities. However, they cannot capture long-range dependencies between words due to their local context. To address this limitation, some recent works use deep neural networks (DNNs) to learn distributed representation of text. Despite promising results, they still require tedious manual construction of feature vectors and complex architecture design. 

In this paper, we present a new algorithm called MultiCorpus Document Retriever (MCR), which addresses both issues of sparsity and complexity of representing texts in DNNs. First, we divide the input corpora into several sub-corpora according to language pairs or other criteria. Then, we train separate models for each sub-corpus by fine-tuning a pre-trained language model such as BERT or XLM-R. Finally, we combine these models into a unified combined model that makes predictions using weighted combinations of individual models' output. This approach allows us to capture both local and global contexts of words during inference time. Experiments conducted on two typical NLP tasks, cross-language passage retrieval and Chinese word segmentation, confirm the effectiveness and efficiency of our approach.

Overall, multi-corpus document retrieval has the potential to significantly improve performance and scalability of web-scale multilingual document retrieval systems. However, there remains challenges in terms of computational resources required to build multiple models, effective fusion strategies, and efficient indexing techniques for processing large amounts of data. Therefore, further research is needed to optimize these components for practical deployment. 


# 3.系统架构概览

 The figure above shows a schematic overview of the proposed MultiCorpus Document Retriever (MCR). It consists of four main modules: Corpora Division module, Model Selection module, Model Fusion module, and Inference module. 

 # 3.1 Corpora Division Module
 The Corpora Division module takes a set of input corpora as input, processes them to extract language pairs or any other criteria, and splits them into sub-corpora accordingly. For example, if the input corpora consist of parallel sentences in three different languages (e.g., English-Chinese, French-Spanish, Japanese-Korean), then the division module would split them into three separate sub-corpora - English-Chinese, French-Spanish, and Japanese-Korean. Each sub-corpus is represented by a pair of language codes (e.g., "en" and "zh", respectively).
 
 Note that in practice, dividing corpora into sub-corpora may not always be straightforward because different formats and languages could contain variations that affect the content of the resulting sub-corpora. A preprocessing step may be necessary to ensure consistency and reduce noise in the dataset before splitting. Additionally, some sub-corpora might end up containing very few documents compared to others, which can impact the overall quality of the final combined model.

 
 # 3.2 Model Selection Module
 Once the input corpora have been divided into sub-corpora, the next stage involves selecting appropriate models for each sub-corpus. As mentioned earlier, each sub-corpus requires its own specialized model. Hence, we leverage existing pre-trained language models like BERT or XLM-R and finetune them to optimize the downstream task of document retrieval. Specifically, we choose BERT base or large version and freeze its parameters except for the last layer. By doing so, we preserve the learned embeddings but discard the transformer layers responsible for generating the attention maps. These frozen layers act as fixed feature extractors that help the final combined model selectively exploit more informative features from the inputs. During training, we update the weights of remaining layers to adapt to the specific domain of the current sub-corpus.

 
 # 3.3 Model Fusion Module
 After selecting suitable models for each sub-corpus, the third stage entails fusing them together to form a unified combined model that can make accurate predictions across multiple languages. At a high level, this process involves merging the outputs of individual sub-models at multiple levels of granularity, including sentence-level, paragraph-level, or even document-level. While previous works have focused on integrating information from multiple sources or using heterogeneous search engines, here we combine the outputs of independent models in a manner that balances their complementary strengths. To achieve this goal, we define a loss function that encourages the predicted scores for the same entity to match across different sub-models. We minimize this loss iteratively until convergence, where each sub-model's contribution is adjusted based on its accuracy on validation sets. The resulting combined model thus learns to balance the errors of individual sub-models and effectively captures the shared features across different languages.

 
 # 3.4 Inference Module
 Finally, after building the unified combined model, the fourth and final stage is to apply it to predict document relevance given a user query. Given a user query and a list of candidate documents, the Inference module passes them through the selected combined model and returns the top k most relevant documents. Since the input corpora were divided into sub-corpora, the inferencing phase needs to accommodate the requirement of segmenting queries and documents into smaller chunks corresponding to each sub-corpus. Thus, the document ranking algorithm should take language-specific features into account when computing similarity scores or ranking candidates. Different methods for chunking queries and documents into smaller units may be applied depending on the nature of the underlying task and size of the input corpora.