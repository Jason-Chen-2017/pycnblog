
作者：禅与计算机程序设计艺术                    

# 1.简介
         
随着近年来人工智能领域的蓬勃发展，强化学习（Reinforcement Learning, RL）被越来越多的人认可并应用于人工智能领域。如今，RL已经可以处理许多复杂的问题，如自动驾驶、机器人控制等。在过去的一段时间里，我一直想和各位分享一下RL在人工智能中的未来发展方向，所以想把这一话题做成专业的技术博客文章。

本文将从以下几个方面对强化学习进行讨论：

① 大规模部署：如何通过GPU加速训练和应用RL；

② 小规模部署：如何快速开发、上线和部署RL模型；

③ 模型组合方法：如何通过RL模型设计生成更精确的预测模型？

④ 演化和进化：如何让RL模型更好地适应环境变化？

⑤ 多任务协作：如何用RL的方式实现多任务协同优化？

⑥ 在线学习：如何使RL模型可以实时学习新知识，而不依赖于离线训练？

# 2.基本概念术语说明
## （1）强化学习与监督学习的区别
强化学习（Reinforcement Learning，RL）是一种用来解决决策问题的方法。一般来说，RL算法会通过探索者（Agent）与环境互动，在一个环境中不断获得奖励和惩罚，目的是在一定的条件下寻找最优的决策策略。它与监督学习的最大区别就是RL的目标是找到一个能够使系统得到长期奖励的行为准则或规则，而不是由给定输入的数据直接学习出结果。

强化学习的核心组成包括环境（Environment），智能体（Agent），奖励函数（Reward Function），观察函数（Observation Function），转移概率函数（Transition Probability Function）。智能体通过与环境交互来获取奖励，并尝试利用这些奖励来改善其行为策略。与此同时，环境也会不断给智能体提供新的信息，比如初始状态、当前状态、动作、奖励等。

强化学习属于增强学习（Supervised Learning）的范畴，它的主要特点是通过学习样本数据，将原始数据的特征映射到有价值的表示上，然后利用表示来进行预测、分类或回归。其基本假设就是具有适当的先验知识（Prior Knowledge）。因此，监督学习（Supervised Learning）的目标是在给定输入 x 和输出 y 的情况下，学习一个映射函数 f(x) = y。而强化学习的目标则是学习一个策略来最大化长期奖励，也就是找到一个能够使得智能体在某个任务中持续产生正向奖励的行为准则或规则。

## （2）RL与传统机器学习的区别
RL与传统机器学习最大的不同之处是强调agent与environment之间的交互。传统机器学习的典型代表就是基于规则的逻辑推理和预测，如决策树、支持向量机、神经网络等。这些机器学习模型需要事先知道所有输入的情况，并且输出的信息只与输入相关。但是对于RL来说，它可以面临各种不同的情况，agent可能面临很多种不同的状况，它必须根据environment给出的反馈做出相应的反应，以达到环境目标。另外，由于RL模型可能会遇到一些非预期的事件，导致它的表现不会那么理想，所以也需要通过自我学习来逐步提升自己。

另一个不同之处是RL通常使用贝尔曼方程作为数学模型。传统的统计机器学习模型采用基于概率论的分布假设和参数估计，即根据已知数据估计未知数据的概率分布和参数。然而，这种参数估计方式往往会受到数据噪声影响很大。而在RL中，假设贝尔曼方程可以较好的描述agent与environment的交互过程。

## （3）OpenAI Gym（强化学习环境）
为了让大家了解RL模型的基本结构，以及RL在人工智能中的作用，这里我们需要提及的一个重要的工具就是OpenAI Gym（强化学习环境）。Gym是一个强化学习环境库，它提供了一系列的强化学习环境供用户测试自己的RL模型。我们可以在Gym上尝试各种不同的强化学习问题，比如使用随机策略玩游戏、使用Q-learning学习控制一个机器人的运动、或者解决谷歌翻译的机器翻译问题。

## （4）深度强化学习与宽度强化学习
深度强化学习（Deep Reinforcement Learning, DRL）和宽度强化学习（Wide Reinforcement Learning, WRL）是RL的两个分支。DRL使用深层次的神经网络来表示状态值函数、动作值函数等。它通过在多个尺度上抽象复杂的环境，来提高RL的效率。相比之下，WRL认为RL问题是一个统一的框架，并将其视为一个优化问题，需要所有的决策都在一个大的整体框架下考虑。WRL可以更好地解决复杂的问题，但效率较低。

# 3.核心算法原理和具体操作步骤以及数学公式讲解
## （1）DQN（深度Q-Networks）
DQN（Deep Q-Network）是深度强化学习中使用的一种模型。它的核心思路是将经验（Experience）存储在经验池（Replay Buffer）中，并使用神经网络拟合出动作值函数（Action Value Function）。DQN通过选择最优动作来控制agent。

DQN的具体流程如下图所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4204)
首先，智能体与环境进行交互，收集经验（S,A,R,S’）；
然后，将经验放入经验池中；
然后，从经验池中随机采样出一批样本数据，输入神经网络进行训练；
最后，在每一步操作之后，智能体都会接收到环境反馈，得到奖励R。DQN的目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对，并且能够在这过程中有效地收集奖励，从而训练出一个好的动作选择模型。

DQN的数学表达式如下所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4211)
其中，Q(s_t, a_t)表示智能体在时刻t时在状态s_t下的动作a_t的动作值函数，由神经网络拟合出来的；
r_{t+1}表示下一个时刻的奖励；
done_mask[t]表示第t个样本数据的结束信号（是否到了终止状态）。

DQN的训练过程如下：
首先，初始化神经网络参数w^i。然后，按顺序依次取出经验池中的样本数据B={(s_j, a_j, r_j, s'_j)}。对于每个样本数据，执行如下操作：

1. 从S_t选出当前状态的所有可能的动作集合A_t，计算每一个动作对应的动作值函数Q_t=(Q(s_t, a_t))；

2. 用概率π(a_t|s_t)选取动作a_t，根据Q_t与Q'(s', a')的差值，更新神经网络的参数w^i；

   更新方式如下：
  ![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4221)
   bellman_error表示贝尔曼误差，loss_q表示动作值函数损失，loss_p表示策略函数损失；

3. 更新经验池，每次取一定数量的经验数据用于训练。

DQN的优化目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对。所以，DQN的损失函数就是最大化连续序列的动作值函数的和，即：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4227)
Q_target(s_j, a_j)=r_j + gamma*max_{a'}Q'(s'_j, a'); gamma表示折扣因子。

## （2）DDPG（深度确定性策略梯度）
DDPG（Deep Deterministic Policy Gradient）是一种无偏估计的Actor-Critic算法，它的核心思路是使用双Q网络（Double Q-Network）来训练策略网络（Policy Network）和价值网络（Value Network），从而解决了DQN存在的诸多问题。

DDPG的具体流程如下图所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4232)
首先，智能体与环境进行交互，收集经验（S,A,R,S’）；
然后，将经验放入经验池中；
然后，使用策略网络选择动作，在当前状态下进行探索；
最后，在每一步操作之后，智能体都会接收到环境反馈，得到奖励R，并记入replay buffer中。DDPG的目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对，并且能够在这过程中有效地收集奖励，从而训练出一个好的动作选择模型。

DDPG的数学表达式如下所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4237)
其中，θ^l表示策略网络（Policy Network）的第l层参数，φ^l表示价值网络（Value Network）的第l层参数。θ'和φ'分别表示目标策略网络和目标价值网络的参数。

DDPG的训练过程如下：
首先，初始化策略网络参数θ^π和价值网络参数φ^v。然后，按顺序依次取出经验池中的样本数据B={(s_j, a_j, r_j, s'_j)}。对于每个样本数据，执行如下操作：

1. 使用当前策略网络θ^π选择动作a_t，计算Q值；
   Q_t=Q(s_t, a_t)；

2. 将当前的状态、动作、奖励、下一个状态送入目标策略网络θ'和价值网络φ'中进行预测；
   Q'_t=Q'(s'_t, π'(s'_t));
   
3. 根据Q'_t和Q_t计算actor loss和critic loss；
   actor loss=Q(s_t, π'(s_t)) - logπ(π'(s_t)|s_t);
   critic loss=(r_t+gamma*Q'_t)-Q(s_t, a_t).

4. 使用策略网络和价值网络优化参数θ^π和φ^v；
   θ^π=θ^π+α*grad(actor loss), φ^v=φ^v+α*grad(critic loss).

5. 使用polyak平均算法更新目标网络参数θ'和φ'；
   θ'=θ'^w+(1-w)*θ'; φ'=φ'^w+(1-w)*φ'.

DDPG的优化目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对。所以，DDPG的损失函数就是最大化连续序列的Q值函数的和，即：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4243)
Q'_target(s'_j, a'_j)=r_j + gamma*Q'(s'_j, argmax_{a''}(Q(s'_j, a''))). 

## （3）PPO（Proximal Policy Optimization）
PPO（Proximal Policy Optimization）是一种异步策略梯度算法，它的核心思路是借鉴TRPO（Trust Region Policy Optimization）的思想，直接最小化策略网络的KL散度，提升策略网络的稳定性。

PPO的具体流程如下图所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4249)
首先，智能体与环境进行交互，收集经验（S,A,R,S’）；
然后，将经验放入经验池中；
然后，使用策略网络选择动作，在当前状态下进行探索；
最后，在每一步操作之后，智能体都会接收到环境反馈，得到奖励R，并记入replay buffer中。PPO的目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对，并且能够在这过程中有效地收集奖励，从而训练出一个好的动作选择模型。

PPO的数学表达式如下所示：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4254)
其中，θ^l表示策略网络（Policy Network）的第l层参数，φ^l表示价值网络（Value Network）的第l层参数。α是学习率；K是迭代次数；ε表示稳定性系数；L表示KL散度。

PPO的训练过程如下：
首先，初始化策略网络参数θ^π和价值网络参数φ^v。然后，按顺序依次取出经验池中的样本数据B={(s_j, a_j, r_j, s'_j)}。对于每个样本数据，执行如下操作：

1. 从当前策略网络θ^π中随机采样动作a_t，计算Q值；
   Q_t=Q(s_t, a_t)；

2. 用当前的状态、动作、奖励、下一个状态送入目标策略网络θ'和价值网络φ'中进行预测；
   Q'_t=Q'(s'_t, π'(s'_t));
   
3. 使用当前策略网络θ^π生成动作a'_t，计算V值；
   V_t=V(s_t);

4. 计算KL散度，即衡量两个概率分布之间的差异，并用KL散度作为评估标准，判断是否需要进行策略网络的更新；
   KL_t=∫∇π(a|s)log((π'(a|s)/π(a|s))α)^−1da;

5. 如果KL_t大于设置的阈值ε，则跳至第8步；否则，继续迭代；

6. 更新策略网络参数θ^π；
   L_t=γ*λ*KL_t;
   A_t=min(β, (L_t)/(π(a'_t|s_t)))·r_t+γ*V(s'_t)+(1-γ)*V_t;
   ∇θ^π=-∇(logπ(a_t|s_t))*A_t.

7. 更新价值网络参数φ^v；
   ∇φ^v=∇(J_θφ(s_t,a_t))+∇(J_θφ(s_t,a'_t))-∇(J_θ^πφ(s_t,π'(s_t))).

8. 更新经验池，每次取一定数量的经验数据用于训练。

PPO的优化目标是使得智能体在游戏过程中，能够准确地预测接下来的状态动作对。所以，PPO的损失函数就是最大化连续序列的Q值函数的和，即：
![image](https://note.youdao.com/yws/public/resource/69e6a7c7e427cb7189e477beba78f3ee/xmlnote/WEBRESOURCEb1b5fc7d3dc1cf39c9cf2f0f8dc81465/4260)
V_target(s_j)=E[(Q(s_j, π'(s_j))+λV(s_j))].

# 4.具体代码实例和解释说明
为了更好地理解RL的基本原理，以及如何使用RL模型进行机器学习，我们以DQN算法为例，介绍DQN算法的具体实现。

## （1）DQN算法实现
DQN算法的关键在于如何利用神经网络拟合出动作值函数。DQN使用两个神经网络，一个是状态值函数Q，另一个是动作值函数A。状态值函数Q表示状态的价值大小，动作值函数A表示每个动作的价值大小。Q网络试图学习状态的价值，A网络试图学习状态下动作的价值。

### （1.1）状态空间与动作空间
首先，定义状态空间和动作空间。状态空间一般定义为环境提供的观测变量的取值范围，动作空间一般定义为智能体可以执行的行动范围。例如，状态空间可能包括距离当前位置的距离、速度、朝向等观测值；动作空间可能包括向左走、右走、前进、后退等行动指令。

### （1.2）算法流程
DQN算法的流程如下：

1. 初始化状态为s，初始化Q网络和Target Q网络；
2. 执行多步预测：在状态s下执行动作a，得到s',r；
3. 更新经验池：记住经验(s,a,r,s')；
4. 从经验池中采样数据集B；
5. 计算B中每个样本数据对应的目标动作值函数Q'_t；
6. 利用梯度下降法更新Q网络的参数；
7. 当回合结束时，进入下一个状态；

### （1.3）Q网络的设计
一般来说，DQN使用两层全连接网络构建Q网络，第一层是状态向量层，第二层是动作向量层。状态向量层的神经元个数等于状态空间的维度，动作向量层的神经元个数等于动作空间的维度。

```python
import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(Net, self).__init__()
        
        # 状态向量层
        self.state_layer = nn.Linear(state_dim, 64)
        self.relu1 = nn.ReLU()

        # 动作向量层
        self.action_layer = nn.Linear(action_dim, 64)
        self.relu2 = nn.ReLU()

        # 输出层
        self.output_layer = nn.Linear(64, 1)

    def forward(self, state):
        state_vector = self.relu1(self.state_layer(state))
        action_vector = self.relu2(self.action_layer())
        q_value = self.output_layer(torch.cat([state_vector, action_vector], dim=-1))

        return q_value
```

### （1.4）Q值的计算
Q值的计算使用Bellman方程。

$$Q_{    ext{target}}^{\pi}(s',a)=r+\gamma\max_{a'}Q_{    heta}'(s',a'),\quad r+\gamma\max_{a'}Q_{    heta}'(s',a')=\sum_{k=0}^{\infty}\gamma^kr+\gamma^{n+1}\max_{a'}\sum_{k=0}^{m}\gamma^{k}r+\cdots,$$

其中$\gamma$表示折扣因子，$r$表示奖励。

在DQN算法中，计算Q'_t时，仅仅考虑一阶收益，即只考虑下一个状态可能出现的最大Q值。目标Q网络（Target Q Network）的目标是跟踪Q网络的行为，目标网络的参数与主网络的参数保持一致。

### （1.5）经验池的设计
DQN算法使用经验池（Replay Pool）存储经验，即对每个状态、动作、奖励和下一个状态进行存储。经验池可以增强样本的多样性。

```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[-1] = (state, action, reward, next_state, done)
    
    def sample(self, batch_size):
        indices = np.random.choice(len(self.buffer), size=batch_size, replace=False)
        states, actions, rewards, next_states, dones = zip(*[self.buffer[idx] for idx in indices])
        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \
               np.array(next_states), np.array(dones, dtype=np.uint8)
    
    def __len__(self):
        return len(self.buffer)
```

### （1.6）训练算法
DQN算法的训练算法包括梯度下降法。

```python
def train():
    optimizer = optim.Adam(net.parameters(), lr=args.lr)
    
    for episode in range(args.episodes):
        ep_reward = 0
        state = env.reset()
        while True:
            action = net(torch.FloatTensor(state)).argmax().item()
            next_state, reward, done, _ = env.step(action)
            
            replay_buffer.push(state, action, reward, next_state, done)
            
            if len(replay_buffer) >= args.batch_size:
                bs = args.batch_size
                
                states, actions, rewards, next_states, dones = replay_buffer.sample(bs)
                
                target_Qs = net(torch.FloatTensor(next_states)).detach().numpy()
                max_actions = np.argmax(target_Qs, axis=1)
                targets = rewards + ((1 - dones) * args.gamma**args.n_steps * target_Qs[range(bs), max_actions])
                
                current_Qs = net(torch.FloatTensor(states)).gather(1, torch.LongTensor(actions).view(-1, 1)).squeeze()
                td_errors = targets - current_Qs.data.cpu().numpy()
                
                errors = np.abs(td_errors)
                
                mean_error = np.mean(errors)
                
                optimizer.zero_grad()
                loss = F.mse_loss(current_Qs, Variable(torch.FloatTensor(targets)))
                loss.backward()
                optimizer.step()
            
            state = next_state
            ep_reward += reward
            
            if done or t > args.max_steps:
                break
            
        writer.add_scalar('train/episode_reward', ep_reward, global_step=episode)
        print("Episode:", episode, "| Episode Reward:", round(ep_reward, 2))
        
if __name__ == '__main__':
    train()
```

## （2）DDPG算法实现
DDPG算法（Deep Deterministic Policy Gradient）是一种无偏估计的Actor-Critic算法。它的核心思路是使用两个神经网络，一个是策略网络，另一个是价值网络，来学习如何在环境中获得奖励。

### （2.1）算法流程
DDPG算法的流程如下：

1. 初始化状态为s，初始化策略网络、目标策略网络和价值网络、目标价值网络；
2. 执行多步预测：在状态s下执行动作a，得到s',r；
3. 更新经验池：记住经验(s,a,r,s')；
4. 从经验池中采样数据集B；
5. 计算B中每个样本数据对应的目标动作值函数Q'_t；
6. 利用梯度下降法更新策略网络和价值网络的参数；
7. 当回合结束时，进入下一个状态；

### （2.2）策略网络的设计
策略网络负责生成动作，可以使用连续空间的神经网络，也可以使用离散空间的神经网络。一般情况下，策略网络使用ReLU激活函数，输出一个在[-1, 1]之间的值，再乘以两个动作的限制范围。

```python
import torch
import torch.nn as nn

class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(Actor, self).__init__()

        self.linear1 = nn.Linear(state_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.relu1 = nn.LeakyReLU(negative_slope=0.2)

        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.relu2 = nn.LeakyReLU(negative_slope=0.2)

        self.mu_layer = nn.Linear(hidden_dim, action_dim)
        self.sigma_layer = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, state):
        x = self.relu1(self.bn1(self.linear1(state)))
        x = self.relu2(self.bn2(self.linear2(x)))

        mu = self.mu_layer(x)
        sigma = self.sigma_layer(x).exp()

        dist = Normal(mu, sigma)
        action = dist.rsample()
        log_prob = dist.log_prob(action).sum(-1, keepdim=True)

        return action, log_prob
    
```

### （2.3）价值网络的设计
价值网络负责预测一个状态的价值，可以使用的神经网络包括MLP、CNN。

```python
import torch
import torch.nn as nn

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(Critic, self).__init__()

        self.linear1 = nn.Linear(state_dim + action_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.relu1 = nn.LeakyReLU(negative_slope=0.2)

        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.relu2 = nn.LeakyReLU(negative_slope=0.2)

        self.output_layer = nn.Linear(hidden_dim, 1)
        
    def forward(self, state, action):
        x = self.relu1(self.bn1(self.linear1(torch.cat([state, action], dim=1))))
        x = self.relu2(self.bn2(self.linear2(x)))
        value = self.output_layer(x)

        return value
```

### （2.4）策略梯度的计算
策略梯度的计算与TD错误的计算类似。

$$J(\phi)=Q_{\phi}(s,\pi_\phi(s))+\alpha\cdot H[\pi_\phi(s)]-\frac{\beta}{d}
abla_    heta log\pi_\phi(s|a)\cdot Q^\pi_{    heta'}(s',\pi_    heta'(s')),$$

其中$\pi_    heta(a|s)$表示策略网络，$\pi_    heta'$表示目标策略网络，$Q^\pi_{    heta'}$表示目标价值网络，$H[\pi_\phi]$表示策略熵。

### （2.5）训练算法
DDPG算法的训练算法包括策略梯度下降（PG）算法。

```python
optimizer_policy = Adam(policy_net.parameters(), lr=args.lr)
optimizer_value = Adam(value_net.parameters(), lr=args.lr)

for i_episode in range(args.num_episode):
    obs = env.reset()
    score = 0
    sum_reward = 0
    for step in range(args.max_step):
        act, prob = policy_net(torch.from_numpy(obs).float())
        act = act.data.numpy()
        new_obs, rew, done, info = env.step(act)
        memory.store(obs, act, rew, new_obs, int(done))
        score += rew
        sum_reward += rew
        obs = new_obs
        
        if memory.mem_cntr > args.batch_size and step % args.update_freq == 0:
            transitions = memory.sample(args.batch_size)
            obses_t, actions, rewards, obses_tp1, dones = zip(*transitions)

            obses_t = torch.stack(obses_t)
            actions = torch.tensor(actions, dtype=torch.float32)
            rewards = torch.tensor(rewards, dtype=torch.float32)
            obses_tp1 = torch.stack(obses_tp1)
            dones = tensor(dones, dtype=torch.int8)

            with torch.no_grad():
                targets = rewards + args.gamma*(1.-dones)*target_value_net(obses_tp1).squeeze()
            predicted_values = value_net(obses_t).gather(1, actions.long()).squeeze()

            criterion = nn.MSELoss()
            value_loss = criterion(predicted_values, targets)

            optimizer_value.zero_grad()
            value_loss.backward()
            clip_grad_norm_(value_net.parameters(), args.clip_gradient)
            optimizer_value.step()

            gains = advantages.clone().detach()

            policy_loss = -(policy_net(obses_t)[1]*gains).mean()

            entropy_loss = (-policy_net(obses_t)[1]*policy_net(obses_t)[1]).mean()*0.01   # add entropy to the loss function

            optimizer_policy.zero_grad()
            (policy_loss - entropy_loss).backward()
            clip_grad_norm_(policy_net.parameters(), args.clip_gradient)
            optimizer_policy.step()

            
if __name__ == '__main__':
    train()
```

