# 学习率衰减Learning Rate Decay原理与代码实例讲解

## 1.背景介绍

在机器学习和深度学习的训练过程中,学习率(Learning Rate)是一个非常重要的超参数。学习率决定了权重在每次迭代时被更新的幅度。适当的学习率可以加快模型的收敛速度,但过高或过低的学习率都会导致模型无法收敛或收敛缓慢。

为了获得更好的模型性能,通常需要在训练过程中动态调整学习率。这种动态调整学习率的策略被称为学习率衰减(Learning Rate Decay)。学习率衰减可以在后期训练时减小更新步长,避免模型在最优解附近来回震荡,从而提高模型的收敛性和泛化能力。

### 1.1 为什么需要学习率衰减?

在训练的早期阶段,较大的学习率有助于模型快速逼近最优解。但随着训练的进行,如果学习率不衰减,会导致以下问题:

1. 模型在最优解附近来回震荡,无法收敛
2. 模型可能跳过最优解,无法找到全局最优
3. 模型的泛化能力下降

因此,在训练的后期需要减小学习率,使模型在最优解附近缓慢收敛,从而获得更好的性能。

### 1.2 常见的学习率衰减策略

常见的学习率衰减策略有以下几种:

- 阶梯衰减(Step Decay)
- 指数衰减(Exponential Decay) 
- 多项式衰减(Polynomial Decay)
- 余弦衰减(Cosine Annealing)

## 2.核心概念与联系

学习率衰减的核心思想是在训练过程中动态调整学习率,以平衡模型的收敛速度和收敛精度。不同的衰减策略反映了不同的调整方式。

### 2.1 阶梯衰减(Step Decay)

阶梯衰减是最简单的衰减策略,它在预先设定的几个阶段性地将学习率减小一个固定的比例。具体来说,每隔一定的训练步数(或epoch数),学习率就会乘以一个小于1的衰减系数。

$$
\begin{cases}
\eta = \eta_0, & \text{if } \lfloor\frac{t}{s}\rfloor \leq 0\\
\eta = \eta_0 \times \gamma^{\lfloor\frac{t}{s}\rfloor}, & \text{if } \lfloor\frac{t}{s}\rfloor > 0
\end{cases}
$$

其中:
- $\eta_0$是初始学习率
- $\gamma$是衰减系数(通常取0.1或0.5)
- $t$是当前训练步数
- $s$是衰减周期(如每隔100个step衰减一次)
- $\lfloor\frac{t}{s}\rfloor$表示对$\frac{t}{s}$向下取整

这种策略的优点是简单直观,缺点是在衰减点处学习率会突然下降,可能影响模型收敛。

### 2.2 指数衰减(Exponential Decay)

指数衰减策略让学习率按指数形式逐渐衰减,公式如下:

$$
\eta = \eta_0 \times \gamma^{\frac{t}{T}}
$$

其中:
- $\eta_0$是初始学习率
- $\gamma$是衰减率(取值范围(0,1))
- $t$是当前训练步数 
- $T$是总训练步数

这种策略可以使学习率平滑地衰减,避免了阶梯衰减中的突变。但需要预先设定总的训练步数$T$。

### 2.3 多项式衰减(Polynomial Decay)

多项式衰减策略让学习率按多项式的形式逐渐衰减,公式如下:

$$
\eta = \eta_0 \times (1 - \frac{t}{T})^\alpha
$$

其中:
- $\eta_0$是初始学习率
- $t$是当前训练步数
- $T$是总训练步数
- $\alpha$是一个大于0的指数参数,控制衰减速率

当$\alpha=1$时,就是线性衰减。$\alpha$越大,衰减越快。这种策略也需要预先设定总的训练步数$T$。

### 2.4 余弦衰减(Cosine Annealing)

余弦衰减策略让学习率先缓慢下降,后快速下降,并在训练结束时衰减到接近0,公式如下:

$$
\eta = \eta_0 \times \left(\frac{1}{2} \times \left(1 + \cos\left(\frac{t \pi}{T}\right)\right)\right)
$$

其中:
- $\eta_0$是初始学习率  
- $t$是当前训练步数
- $T$是总训练步数

这种策略可以在训练后期快速下降学习率,并在训练结束时将学习率衰减到接近0,有助于模型收敛。同样需要预先设定总的训练步数$T$。

### 2.5 核心概念总结

不同的学习率衰减策略反映了不同的调整思路,具有各自的优缺点。合理选择和设置衰减策略对模型的收敛性能至关重要。

## 3.核心算法原理具体操作步骤

不同的学习率衰减策略的核心算法原理和具体操作步骤如下:

### 3.1 阶梯衰减(Step Decay)

**算法原理:**
每隔一定的训练步数(或epoch数),将学习率乘以一个小于1的衰减系数。

**具体操作步骤:**
1. 设置初始学习率$\eta_0$、衰减系数$\gamma$和衰减周期$s$
2. 在训练过程中,每隔$s$个step(或epoch),计算当前step数$t$对$s$取整$\lfloor\frac{t}{s}\rfloor$
3. 根据公式$\eta = \eta_0 \times \gamma^{\lfloor\frac{t}{s}\rfloor}$更新当前学习率$\eta$
4. 使用更新后的$\eta$继续训练模型

### 3.2 指数衰减(Exponential Decay) 

**算法原理:**
学习率按指数形式逐渐衰减。

**具体操作步骤:**
1. 设置初始学习率$\eta_0$、衰减率$\gamma$和总训练步数$T$
2. 在训练过程中,根据当前step数$t$,计算当前学习率$\eta$:
   $\eta = \eta_0 \times \gamma^{\frac{t}{T}}$
3. 使用更新后的$\eta$继续训练模型

### 3.3 多项式衰减(Polynomial Decay)

**算法原理:** 
学习率按多项式的形式逐渐衰减。

**具体操作步骤:**
1. 设置初始学习率$\eta_0$、总训练步数$T$和指数参数$\alpha$  
2. 在训练过程中,根据当前step数$t$,计算当前学习率$\eta$:
   $\eta = \eta_0 \times (1 - \frac{t}{T})^\alpha$
3. 使用更新后的$\eta$继续训练模型

### 3.4 余弦衰减(Cosine Annealing)

**算法原理:**
学习率先缓慢下降,后快速下降,并在训练结束时衰减到接近0。

**具体操作步骤:** 
1. 设置初始学习率$\eta_0$和总训练步数$T$
2. 在训练过程中,根据当前step数$t$,计算当前学习率$\eta$:
   $\eta = \eta_0 \times \left(\frac{1}{2} \times \left(1 + \cos\left(\frac{t \pi}{T}\right)\right)\right)$  
3. 使用更新后的$\eta$继续训练模型

## 4.数学模型和公式详细讲解举例说明

### 4.1 阶梯衰减(Step Decay)公式详解

$$
\begin{cases}
\eta = \eta_0, & \text{if } \lfloor\frac{t}{s}\rfloor \leq 0\\
\eta = \eta_0 \times \gamma^{\lfloor\frac{t}{s}\rfloor}, & \text{if } \lfloor\frac{t}{s}\rfloor > 0
\end{cases}
$$

- $\eta_0$是初始学习率,通常设置为一个较大的值,如0.1或0.01
- $\gamma$是衰减系数,取值范围(0,1),通常取0.1或0.5
- $t$是当前训练步数
- $s$是衰减周期,如每隔100个step衰减一次
- $\lfloor\frac{t}{s}\rfloor$表示对$\frac{t}{s}$向下取整,确定当前处于第几个衰减周期

公式分两部分:
1. 当$\lfloor\frac{t}{s}\rfloor \leq 0$时,即还未到第一个衰减周期,学习率保持初始值$\eta_0$
2. 当$\lfloor\frac{t}{s}\rfloor > 0$时,即已经进入衰减周期,学习率按$\gamma$的$\lfloor\frac{t}{s}\rfloor$次方衰减

**例:** 设$\eta_0=0.1, \gamma=0.5, s=100$
- 当$t=50$时,$\lfloor\frac{50}{100}\rfloor=0$,则$\eta=0.1$
- 当$t=150$时,$\lfloor\frac{150}{100}\rfloor=1$,则$\eta=0.1 \times 0.5^1 = 0.05$ 
- 当$t=250$时,$\lfloor\frac{250}{100}\rfloor=2$,则$\eta=0.1 \times 0.5^2 = 0.025$

可见,每隔100步,学习率就衰减一半。

### 4.2 指数衰减(Exponential Decay)公式详解

$$
\eta = \eta_0 \times \gamma^{\frac{t}{T}}
$$

- $\eta_0$是初始学习率
- $\gamma$是衰减率,取值范围(0,1)
- $t$是当前训练步数
- $T$是总训练步数

指数衰减让学习率按$\gamma$的$\frac{t}{T}$次方逐渐衰减,可以使学习率平滑地下降。

**例:** 设$\eta_0=0.1, \gamma=0.9, T=1000$
- 当$t=250$时,$\eta=0.1 \times 0.9^{\frac{250}{1000}}=0.0787$
- 当$t=500$时,$\eta=0.1 \times 0.9^{\frac{500}{1000}}=0.0619$
- 当$t=750$时,$\eta=0.1 \times 0.9^{\frac{750}{1000}}=0.0487$

可见,学习率随着训练步数的增加而逐渐指数级下降。$\gamma$越小,下降越快。

### 4.3 多项式衰减(Polynomial Decay)公式详解

$$
\eta = \eta_0 \times (1 - \frac{t}{T})^\alpha
$$

- $\eta_0$是初始学习率
- $t$是当前训练步数
- $T$是总训练步数
- $\alpha$是一个大于0的指数参数,控制衰减速率

多项式衰减让学习率按$(1 - \frac{t}{T})^\alpha$的形式逐渐衰减。当$\alpha=1$时,就是线性衰减。$\alpha$越大,衰减越快。

**例:** 设$\eta_0=0.1, T=1000, \alpha=2$
- 当$t=250$时,$\eta=0.1 \times (1 - \frac{250}{1000})^2=0.0675$
- 当$t=500$时,$\eta=0.1 \times (1 - \frac{500}{1000})^2=0.0225$ 
- 当$t=750$时,$\eta=0.1 \times (1 - \frac{750}{1000})^2=0.0056$

可见,学习率随着训练步数的增加而逐渐多项式级下降。$\alpha$越大,下降越快。

### 4.4 余弦衰减(Cosine Annealing)公式详解

$$
\eta = \eta_0 \times \left(\frac{1}{2} \times \left(1 + \cos\left(\frac{t \pi}{T}\right)\right)\right)
$$

- $\eta_0$是初始学习率
- $t$是当前训练步数 
- $T$是总训练步数

余弦衰减利用了三角函数余弦的特性,让学习率先缓慢下降,后快速下降,并在训练结束时衰减到接近0。

**例:** 设$\eta_0=0.1, T=1000$
- 当$t=250$时,$\eta=0.1 \times \left(\frac{1}{2} \times \left(1 + \cos\left(\frac{250 \pi}{1000}\right)\right)\