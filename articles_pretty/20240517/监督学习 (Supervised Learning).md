# 监督学习 (Supervised Learning)

作者：禅与计算机程序设计艺术

## 1. 背景介绍
### 1.1 机器学习的分类
#### 1.1.1 监督学习
#### 1.1.2 无监督学习  
#### 1.1.3 强化学习
### 1.2 监督学习的定义与特点
#### 1.2.1 定义
#### 1.2.2 特点
### 1.3 监督学习的应用领域
#### 1.3.1 计算机视觉
#### 1.3.2 自然语言处理
#### 1.3.3 语音识别
#### 1.3.4 其他应用

## 2. 核心概念与联系
### 2.1 样本与标签
#### 2.1.1 样本的定义
#### 2.1.2 标签的定义
#### 2.1.3 样本与标签的关系
### 2.2 特征与特征工程
#### 2.2.1 特征的定义
#### 2.2.2 特征工程的重要性
#### 2.2.3 特征选择与特征提取
### 2.3 模型与参数
#### 2.3.1 模型的定义
#### 2.3.2 参数的定义
#### 2.3.3 模型与参数的关系
### 2.4 损失函数与优化算法
#### 2.4.1 损失函数的定义与作用
#### 2.4.2 常见的损失函数
#### 2.4.3 优化算法的定义与作用
#### 2.4.4 常见的优化算法

## 3. 核心算法原理具体操作步骤
### 3.1 线性回归
#### 3.1.1 线性回归的基本原理
#### 3.1.2 最小二乘法求解
#### 3.1.3 梯度下降法求解
### 3.2 逻辑回归
#### 3.2.1 逻辑回归的基本原理
#### 3.2.2 sigmoid函数与对数似然函数
#### 3.2.3 梯度下降法求解
### 3.3 支持向量机（SVM）
#### 3.3.1 SVM的基本原理
#### 3.3.2 硬间隔与软间隔
#### 3.3.3 核函数与核技巧
#### 3.3.4 SMO算法求解
### 3.4 决策树
#### 3.4.1 决策树的基本原理
#### 3.4.2 信息熵与信息增益
#### 3.4.3 ID3、C4.5与CART算法
### 3.5 随机森林
#### 3.5.1 随机森林的基本原理
#### 3.5.2 Bagging与随机子空间
#### 3.5.3 特征重要性评估
### 3.6 神经网络
#### 3.6.1 神经网络的基本原理
#### 3.6.2 前向传播与反向传播
#### 3.6.3 激活函数与损失函数
#### 3.6.4 优化算法与正则化

## 4. 数学模型和公式详细讲解举例说明
### 4.1 线性回归的数学模型
#### 4.1.1 一元线性回归模型
$$y = w x + b$$
其中，$y$为预测值，$x$为输入特征，$w$为权重，$b$为偏置。
#### 4.1.2 多元线性回归模型  
$$y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b$$
其中，$y$为预测值，$x_1, x_2, ..., x_n$为输入特征，$w_1, w_2, ..., w_n$为对应的权重，$b$为偏置。
#### 4.1.3 最小二乘法求解
目标是最小化预测值与真实值之间的均方误差（MSE）：
$$MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2$$
其中，$m$为样本数，$y_i$为第$i$个样本的真实值，$\hat{y}_i$为第$i$个样本的预测值。
对MSE求偏导，令偏导数为0，可得到最优解：
$$w = (X^T X)^{-1} X^T y$$
$$b = \bar{y} - w^T \bar{x}$$
其中，$X$为输入特征矩阵，$y$为真实值向量，$\bar{x}$为输入特征均值向量，$\bar{y}$为真实值均值。
#### 4.1.4 梯度下降法求解
目标函数为MSE，对$w$和$b$求偏导：
$$\frac{\partial MSE}{\partial w} = \frac{2}{m} \sum_{i=1}^{m} (w x_i + b - y_i) x_i$$
$$\frac{\partial MSE}{\partial b} = \frac{2}{m} \sum_{i=1}^{m} (w x_i + b - y_i)$$
更新$w$和$b$：
$$w := w - \alpha \frac{\partial MSE}{\partial w}$$  
$$b := b - \alpha \frac{\partial MSE}{\partial b}$$
其中，$\alpha$为学习率。重复迭代直到收敛。

### 4.2 逻辑回归的数学模型
#### 4.2.1 sigmoid函数
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
将线性函数的输出映射到(0, 1)区间，得到概率值。
#### 4.2.2 逻辑回归模型
$$p(y=1|x) = \sigma(w^T x + b)$$
$$p(y=0|x) = 1 - \sigma(w^T x + b)$$
其中，$p(y=1|x)$为样本$x$属于正类的概率，$p(y=0|x)$为样本$x$属于负类的概率。
#### 4.2.3 对数似然函数
$$L(w, b) = \sum_{i=1}^{m} [y_i \log p(y_i=1|x_i) + (1-y_i) \log p(y_i=0|x_i)]$$
目标是最大化对数似然函数，等价于最小化负对数似然函数：
$$J(w, b) = -\frac{1}{m} L(w, b)$$
#### 4.2.4 梯度下降法求解
对$w$和$b$求偏导：
$$\frac{\partial J}{\partial w} = \frac{1}{m} \sum_{i=1}^{m} (\sigma(w^T x_i + b) - y_i) x_i$$
$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (\sigma(w^T x_i + b) - y_i)$$
更新$w$和$b$：
$$w := w - \alpha \frac{\partial J}{\partial w}$$
$$b := b - \alpha \frac{\partial J}{\partial b}$$
其中，$\alpha$为学习率。重复迭代直到收敛。

### 4.3 支持向量机的数学模型
#### 4.3.1 硬间隔SVM
目标是最大化分类间隔：
$$\max_{w, b} \frac{2}{\|w\|}$$
$$s.t. \quad y_i (w^T x_i + b) \geq 1, \quad i=1,2,...,m$$
其中，$\|w\|$为$w$的L2范数，$y_i \in \{-1, +1\}$为样本$x_i$的标签。
引入拉格朗日乘子$\alpha_i \geq 0$，转化为对偶问题：
$$\max_{\alpha} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j$$
$$s.t. \quad \sum_{i=1}^{m} \alpha_i y_i = 0$$
$$\alpha_i \geq 0, \quad i=1,2,...,m$$
求解得到最优$\alpha^*$，再求出$w^*$和$b^*$：
$$w^* = \sum_{i=1}^{m} \alpha_i^* y_i x_i$$
$$b^* = y_j - \sum_{i=1}^{m} \alpha_i^* y_i x_i^T x_j$$
其中，$x_j$为任意一个满足$0 < \alpha_j^* < C$的支持向量。
#### 4.3.2 软间隔SVM
引入松弛变量$\xi_i \geq 0$，允许一些样本被错分：
$$\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{m} \xi_i$$
$$s.t. \quad y_i (w^T x_i + b) \geq 1 - \xi_i, \quad i=1,2,...,m$$
$$\xi_i \geq 0, \quad i=1,2,...,m$$
其中，$C > 0$为惩罚参数，控制误分类样本的惩罚程度。
转化为对偶问题：
$$\max_{\alpha} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j$$
$$s.t. \quad \sum_{i=1}^{m} \alpha_i y_i = 0$$  
$$0 \leq \alpha_i \leq C, \quad i=1,2,...,m$$
求解过程与硬间隔SVM类似。
#### 4.3.3 核函数与核技巧
对于线性不可分的数据，可以通过非线性变换将其映射到高维空间，使其线性可分。
核函数$K(x_i, x_j)$表示两个样本在高维空间的内积：
$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$
其中，$\phi(x)$为非线性映射函数。
常用的核函数有：
- 多项式核函数：$K(x_i, x_j) = (x_i^T x_j + c)^d$
- 高斯核函数（RBF）：$K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$
- Sigmoid核函数：$K(x_i, x_j) = \tanh(\gamma x_i^T x_j + c)$

在对偶问题中，用核函数$K(x_i, x_j)$替换内积$x_i^T x_j$，即可实现非线性SVM。这种技巧称为核技巧。

### 4.4 决策树的数学模型
#### 4.4.1 信息熵
样本集合$D$的信息熵：
$$H(D) = -\sum_{k=1}^{K} p_k \log_2 p_k$$
其中，$K$为类别数，$p_k$为第$k$类样本在$D$中的比例。
信息熵表示样本集合的不确定性，值越大表示不确定性越高。
#### 4.4.2 信息增益
特征$A$对样本集合$D$的信息增益：
$$g(D, A) = H(D) - H(D|A)$$
$$H(D|A) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} H(D^v)$$
其中，$V$为特征$A$的取值数，$D^v$为$A=a^v$的样本子集。
信息增益表示特征$A$使得样本集合的不确定性减少的程度，值越大表示特征$A$越有区分能力。
#### 4.4.3 ID3算法
1. 计算每个特征的信息增益，选择信息增益最大的特征作为当前节点的划分特征。
2. 对划分特征的每个取值，递归地构建子树。
3. 当样本集合中所有样本属于同一类别，或者没有更多特征可用于划分时，停止递归。
#### 4.4.4 C4.5算法
与ID3算法类似，但使用信息增益比来选择划分特征：
$$g_R(D, A) = \frac{g(D, A)}{H_A(D)}$$
$$H_A(D) = -\sum_{v=1}^{V} \frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}$$
其中，$H_A(D)$为特征$A$的熵。
信息增益比对特征取值数进行了惩罚，偏好取值数少的特征。
#### 4.4.5 CART算法
使用基尼指数来选择划分特征和划分点：
$$Gini(D) = 1 - \sum_{k=1}^{K} p_k^2$$
$$Gini(D, A) = \frac{|D_1|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)$$
其中，$D_1$和$D_2$为特征$A$按某个划分点划分得到的两个子集。
选择基尼指数最小的特征和划分点作为当前节点的划分依据。
CART算法生成的是二叉树，可以处理连续特征和缺