# GNN基础：图数据上的深度学习

作者：禅与计算机程序设计艺术

## 1. 背景介绍

### 1.1 图数据无处不在
在现实世界中，许多数据天然具有图结构，如社交网络、交通网络、分子结构、知识图谱等。图能够表达实体之间的复杂关联，蕴含丰富的结构信息和语义信息。

### 1.2 传统机器学习方法的局限
传统的机器学习方法如SVM、决策树等主要针对规整的结构化数据，很难直接建模图这种非规整数据。早期的图挖掘方法大多基于人工定义的图特征，泛化能力不足。

### 1.3 深度学习风靡 促进GNN发展
近年来，深度学习的盛行为图上机器学习带来了新的契机。一方面，深度学习模型能够自动学习数据的多层次表示；另一方面，卷积等算子的引入使得深度模型能够考虑数据的局部结构。这两点启发研究者去开发能够端到端建模图数据的深度学习方法，由此催生了图神经网络（GNN）的研究热潮。

## 2. 核心概念与联系

### 2.1 图的数学表示
图$G=(V,E)$由节点集$V$和边集$E$组成，可以表示为邻接矩阵$A$或特征矩阵$X$的形式。无向图的邻接矩阵是对称阵，有向图的邻接矩阵非对称。特征矩阵每行表示一个节点的特征向量。图神经网络通常同时使用邻接矩阵和特征矩阵作为输入。

### 2.2 图卷积的定义 
图卷积是图神经网络的核心操作，延续了CNN中局部连接、权重共享的思想，同时考虑了图的任意拓扑结构。与一维序列卷积沿时间轴滑动不同，图卷积的感受野沿各节点的邻域扩散。

设$h_v^{(l)}$表示第$l$层节点$v$的隐藏特征，图卷积定义为：
$$h_v^{(l+1)} = \sigma(b^{(l)} + \sum_{u\in\mathcal{N}(v)} w^{(l)}h_u^{(l)})$$

其中$w^{(l)}$和$b^{(l)}$分别是第$l$层的权重和偏置，$\mathcal{N}(v)$是节点$v$的邻居集，$\sigma$是激活函数。图卷积通过邻域信息聚合更新节点表示。

### 2.3 图卷积的矩阵形式
图卷积在矩阵形式下可以写成：
$$H^{(l+1)} = \sigma(\hat{A}H^{(l)}W^{(l)})$$

其中$H^{(l)}\in\mathbb{R}^{N\times d_l}$是第$l$层的隐藏特征矩阵，$W^{(l)} \in\mathbb{R}^{d_l\times d_{l+1}}$是第$l$层的权重矩阵。$\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$是对称归一化的邻接矩阵，$\tilde{A}=A+I$, $\tilde{D}=\text{diag}(\sum_j\tilde{A}_{ij})$是$\tilde{A}$每行之和构成的对角阵。

### 2.4 GNN的分类
按模型结构分，GNN可分为循环式（RecGNN）和卷积式（ConvGNN）两类。RecGNN从随机初始化开始，交替更新节点隐藏状态，直到收敛。ConvGNN使用层级结构，逐层进行图卷积，网络深度等同于感受野半径。

此外，按任务类型可将GNN分为节点级、边级、图级三种：
- 节点级任务如节点分类、社群发现等，关注单个节点的性质；
- 边级任务如链接预测，考察成对节点的关系；
- 图级任务如图分类，通过池化等操作将整图映射到向量表示。

## 3. 核心算法原理和具体操作步骤

### 3.1 GCN算法
GCN（Graph Convolutional Network）是最经典、应用最广泛的GNN模型之一。基本思想是对图做谱分解，在谱域定义卷积操作。牺牲部分表达能力以换取更高效的局部卷积。

GCN的前向传播过程为：
$$H^{(0)} = X$$
$$H^{(l+1)} = \sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$$
$$Z = \text{softmax}(H^{(L)})$$

其中$\hat{A}=A+I$是自连接的邻接矩阵，$\hat{D}=\text{diag}(\sum_j\hat{A}_{ij})$是$\hat{A}$对应的度矩阵，$W^{(l)}$是第$l$层的权重矩阵，$\sigma$是ReLU激活函数，$Z$是最终的预测概率。

GCN的训练采用带L2正则的交叉熵损失：
$$\mathcal{L} = -\sum_{v\in\mathcal{V}_L}\sum_{c=1}^C Y_{vc} \ln Z_{vc} + \lambda\sum_{l=1}^L \lVert W^{(l)}\rVert^2$$
其中$Y_{vc}$是节点$v$关于类别$c$的训练标签，$\mathcal{V}_L$是有标签的节点集。

GCN的具体操作步骤：
1. 创建归一化的邻接矩阵$\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}$
2. 初始化权重矩阵$W^{(l)}$，$l=1,2,\cdots,L$
3. 设置初始隐藏状态矩阵$H^{(0)}=X$
4. 前向传播计算每层隐藏状态$H^{(l+1)} = \sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})$
5. 对最后一层输出做softmax得到节点标签概率$Z$
6. 计算交叉熵损失$\mathcal{L}$
7. 反向传播计算梯度并更新模型参数$W^{(l)}$
8. 重复步骤4-7直到模型训练收敛

### 3.2 GraphSAGE算法
GraphSAGE（SAmple and aggreGatE）通过邻域采样和聚合的方式生成节点嵌入。相比GCN更适用于超大规模动态图。

GraphSAGE的隐藏状态更新规则为：

$$h_{\mathcal{N}(v)}^{(l)} =  \text{ AGGREGATE} (\{h_u^{(l-1)},\forall u\in\mathcal{N}(v)\})$$  
$$h_v^{(l)} = \sigma(W^{(l)}  \cdot \text{CONCAT}(h_v^{(l-1)}, h_{\mathcal{N}(v)}^{(l)}))$$

其中AGGREGATE是聚合函数，可以是求平均、池化等。CONCAT是拼接操作，将节点本身特征与邻居聚合特征拼接。

GraphSAGE的具体操作步骤：
1. 对每个节点采样固定大小$S^{(l)}$的邻居节点集合
2. 初始化权重矩阵$W^{(l)}$，$l=1,2,\cdots,L$  
3. 设置初始隐藏状态矩阵$H^{(0)}=X$
4. 计算每层邻居聚合特征$h_{\mathcal{N}(v)}^{(l)} =  \text{AGGREGATE}(\{h_u^{(l-1)},\forall u\in\mathcal{N}(v)\})$
5. 节点本身特征与邻居聚合拼接后非线性变换$h_v^{(l)} = \sigma(W^{(l)} \cdot \text{CONCAT}(h_v^{(l-1)}, h_{\mathcal{N}(v)}^{(l)}))$
6. 对最后一层输出$H^{(L)}$应用下游任务的损失函数
7. 反向传播计算梯度并更新模型参数$W^{(l)}$
8. 重复步骤4-7直到模型训练收敛

### 3.3 GAT算法
GAT（Graph Attention Network）通过注意力机制自适应地为邻居节点分配不同的权重。核心是注意力系数的计算。

首先，利用节点特征计算注意力系数：
$$e_{ij}^{(l)} = a^{(l)}(Wh_i^{(l)}, Wh_j^{(l)})$$
$$\alpha_{ij}^{(l)} =  \frac{\exp(e_{ij}^{(l)})}{\sum_{k\in N_i} \exp(e_{ik}^{(l)})}$$

其中$a^{(l)}$是共享的注意力权重向量，$e_{ij}^{(l)}$是节点$i$到$j$的注意力得分，$\alpha_{ij}^{(l)}$是归一化的注意力系数。

然后，利用注意力系数聚合邻居节点特征：

$$h_i^{(l+1)} = \sigma(\sum_{j\in N_i} \alpha_{ij}^{(l)} Wh_j^{(l)})$$

GAT的具体操作步骤：
1. 初始化权重矩阵$W^{(l)}$，注意力权重$a^{(l)}$，$l=1,2,\cdots,L$  
2. 设置初始隐藏状态矩阵$H^{(0)}=X$ 
3. 前向传播计算第$l$层每对节点的注意力得分$e_{ij}^{(l)} = a^{(l)}(Wh_i^{(l)}, Wh_j^{(l)})$
4. 对注意力得分做softmax得到注意力系数$\alpha_{ij}^{(l)}$
5. 用注意力系数聚合邻居节点特征$h_i^{(l+1)} = \sigma(\sum_{j\in N_i} \alpha_{ij}^{(l)} Wh_j^{(l)})$
6. 对最后一层输出$H^{(L)}$应用下游任务的损失函数  
7. 反向传播计算梯度并更新模型参数$W^{(l)}$和$a^{(l)}$
8. 重复步骤3-7直到模型训练收敛

## 4.数学模型和公式详细讲解举例说明

### 4.1 谱图卷积
GCN的理论基础是谱图卷积。谱图理论将图的拉普拉斯矩阵$L=D-A$进行特征分解：$L=U\Lambda U^T$。其中$U$是特征向量矩阵，$\Lambda$为特征值构成的对角阵。图信号$x$在谱域的傅里叶变换为$\hat{x} = U^Tx$。

定义谱卷积核为$g_\theta$，则谱图卷积的定义为：
$$x*_Gg_\theta = Ug_\theta U^Tx$$

可以看出谱图卷积本质上是将图信号映射到谱域，乘以卷积核（特征值的函数），再映射回顶点域。GCN用一阶切比雪夫多项式近似$g_\theta$，得到简化的图卷积公式：
$$g_\theta*x \approx \theta(I+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x$$

### 4.2 注意力机制
以两层GAT为例，说明其中的数学原理。首先将输入特征$X$右乘权重矩阵得到初始节点表示$H^{(0)}$：
$$H^{(0)} = XW^{(0)},W^{(0)}\in\mathbb{R}^{d_0\times d_1}$$

对于第一层，先计算注意力系数矩阵$A^{(1)}$：

$$ E^{(1)}_{ij} = \text{LeakyReLU}(a^T[Wh_i^{(0)} \Vert Wh_j^{(0)}])$$

$$ A^{(1)}_{ij} = \frac{\exp(E^{(1)}_{ij})}{\sum_{k\in\mathcal{N}(i)} \exp(E^{(1)}_{ik})}$$

注意力系数体现了节点$j$对$i$的重要性。将其用于邻居特征聚合：

$$h'^{(1)}_i = \sigma(\sum_{j\in\mathcal{N}(i)} A^{(1)}_{ij} Wh_j^{(0)})$$