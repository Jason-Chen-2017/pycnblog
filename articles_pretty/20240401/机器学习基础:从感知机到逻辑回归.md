# 机器学习基础:从感知机到逻辑回归

作者：禅与计算机程序设计艺术

## 1. 背景介绍

机器学习是当今人工智能领域中最重要的核心技术之一。它通过让计算机自动从数据中学习,并利用从数据中获取的知识做出预测或决策,在诸如计算机视觉、自然语言处理、语音识别等领域取得了巨大的成功。

在机器学习发展的历史上,感知机和逻辑回归是两个非常重要的基础模型。感知机是最早被提出的一种监督学习算法,它通过学习线性分类器来解决二分类问题。而逻辑回归则是一种广泛应用于分类问题的概率模型,它可以输出样本属于各类的概率,在很多实际应用中都有非常出色的表现。

本文将详细介绍感知机和逻辑回归的核心概念、原理和具体实现,并通过实际案例展示它们在机器学习中的应用。希望能够帮助读者深入理解这两种经典的机器学习模型,为进一步学习和应用机器学习打下坚实的基础。

## 2. 核心概念与联系

### 2.1 感知机

感知机是由美国心理学家弗兰克·罗森布拉特在1957年提出的一种二分类模型。它假设样本可以被一个超平面线性分割,通过不断调整超平面的参数,使得正负样本被正确分类。

感知机的核心思想是:

1. 定义一个线性判别函数 $f(x) = \mathbf{w} \cdot \mathbf{x} + b$, 其中 $\mathbf{w}$ 是权重向量, $b$ 是偏置项。

2. 对于训练样本 $(\mathbf{x}_i, y_i)$, 如果 $y_i f(\mathbf{x}_i) \leq 0$, 说明该样本被错分了,需要更新参数 $\mathbf{w}$ 和 $b$ 以纠正错误。

3. 通过不断迭代更新参数,直到所有训练样本被正确分类或达到最大迭代次数。

感知机算法非常简单高效,但它只能学习线性可分的数据,无法解决复杂的非线性问题。

### 2.2 逻辑回归

逻辑回归是一种广泛应用于分类问题的概率模型。它通过学习一个 logistic 函数来预测样本属于各类的概率:

$P(y=1|\mathbf{x}) = \frac{1}{1+e^{-\mathbf{w} \cdot \mathbf{x} - b}}$

其中 $\mathbf{w}$ 是权重向量, $b$ 是偏置项。

逻辑回归模型的训练过程是最大化对数似然函数,即找到使训练数据概率最大的参数 $\mathbf{w}$ 和 $b$。与感知机不同,逻辑回归可以处理非线性可分的数据,并输出概率值而不是简单的分类结果。

### 2.3 感知机和逻辑回归的联系

感知机和逻辑回归都属于线性分类模型,都试图学习一个线性判别函数来进行分类。但二者的核心目标函数和优化方法不同:

- 感知机是通过最小化分类错误来学习参数,而逻辑回归是通过最大化对数似然函数来学习参数。
- 感知机只能学习线性可分的数据,而逻辑回归可以处理非线性可分的数据。
- 感知机的输出是 +1 或 -1 的硬分类结果,而逻辑回归的输出是样本属于各类的概率。

总的来说,感知机和逻辑回归是机器学习中两个非常经典和重要的基础模型,它们在不同的应用场景下都有各自的优势。

## 3. 核心算法原理和具体操作步骤

### 3.1 感知机算法

感知机算法的具体步骤如下:

1. 初始化权重向量 $\mathbf{w}$ 和偏置项 $b$ 为 0。
2. 对于每个训练样本 $(\mathbf{x}_i, y_i)$:
   - 计算判别函数 $f(\mathbf{x}_i) = \mathbf{w} \cdot \mathbf{x}_i + b$
   - 如果 $y_i f(\mathbf{x}_i) \leq 0$, 说明该样本被错分了,需要更新参数:
     - $\mathbf{w} \leftarrow \mathbf{w} + \eta y_i \mathbf{x}_i$
     - $b \leftarrow b + \eta y_i$
   - 其中 $\eta$ 是学习率。
3. 重复步骤2,直到所有训练样本被正确分类或达到最大迭代次数。

感知机算法的收敛性已被理论证明,当训练数据线性可分时,该算法一定能找到一个能够正确分类所有样本的超平面。

### 3.2 逻辑回归算法

逻辑回归算法的具体步骤如下:

1. 初始化权重向量 $\mathbf{w}$ 和偏置项 $b$ 为 0。
2. 计算对数似然函数:
   $L(\mathbf{w}, b) = \sum_{i=1}^n [y_i \log P(y_i=1|\mathbf{x}_i) + (1-y_i) \log (1-P(y_i=1|\mathbf{x}_i))]$
   其中 $P(y_i=1|\mathbf{x}_i) = \frac{1}{1+e^{-\mathbf{w} \cdot \mathbf{x}_i - b}}$
3. 使用梯度下降法优化参数 $\mathbf{w}$ 和 $b$, 使对数似然函数最大化:
   - $\mathbf{w} \leftarrow \mathbf{w} + \eta \nabla_\mathbf{w} L$
   - $b \leftarrow b + \eta \nabla_b L$
   其中 $\eta$ 是学习率, $\nabla_\mathbf{w} L$ 和 $\nabla_b L$ 分别是对数似然函数关于 $\mathbf{w}$ 和 $b$ 的梯度。
4. 重复步骤3,直到收敛或达到最大迭代次数。

逻辑回归的优化问题是非凸的,但可以通过梯度下降法有效求解。收敛到局部最优解后,就可以使用学习得到的参数 $\mathbf{w}$ 和 $b$ 进行分类预测。

## 4. 项目实践：代码实例和详细解释说明

下面我们通过一个简单的二分类问题,来演示如何使用感知机和逻辑回归进行建模和预测。

### 4.1 数据准备

我们使用 scikit-learn 提供的 iris 数据集作为示例。该数据集包含 150 个样本,每个样本有 4 个特征(花萼长度、花萼宽度、花瓣长度、花瓣宽度),需要预测花卉的类别(山鸢尾、Virginia鸢尾、Setosa鸢尾)。

我们将数据集划分为训练集和测试集,并将类别标签转换为 +1 和 -1 进行二分类:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# 加载 iris 数据集
iris = load_iris()
X, y = iris.data[:, :2], (iris.target == 0).astype(int)  # 只使用前两个特征,二分类

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 将类别标签转换为 +1 和 -1
y_train[y_train == 0] = -1
y_test[y_test == 0] = -1
```

### 4.2 感知机模型

下面我们实现感知机算法,并在 iris 数据集上进行训练和测试:

```python
class Perceptron:
    def __init__(self, learning_rate=0.1, max_iter=100):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.w = None
        self.b = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0

        for _ in range(self.max_iter):
            for i in range(n_samples):
                if y[i] * (np.dot(X[i], self.w) + self.b) <= 0:
                    self.w += self.learning_rate * y[i] * X[i]
                    self.b += self.learning_rate * y[i]

    def predict(self, X):
        return np.sign(np.dot(X, self.w) + self.b)

# 训练感知机模型
perceptron = Perceptron()
perceptron.fit(X_train, y_train)

# 在测试集上评估模型
accuracy = np.mean(perceptron.predict(X_test) == y_test)
print(f'Perceptron accuracy: {accuracy:.2f}')
```

在这个示例中,我们定义了一个 `Perceptron` 类,包含了感知机算法的训练和预测过程。在训练阶段,我们根据感知机的更新规则不断调整权重向量 `w` 和偏置项 `b`,直到所有训练样本被正确分类或达到最大迭代次数。在预测阶段,我们使用学习得到的参数来计算判别函数的值,并输出 +1 或 -1 的分类结果。

### 4.3 逻辑回归模型

下面我们实现逻辑回归算法,并在 iris 数据集上进行训练和测试:

```python
from scipy.optimize import minimize

class LogisticRegression:
    def __init__(self, learning_rate=0.1, max_iter=100):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.w = None
        self.b = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def loss(self, params, X, y):
        self.w = params[:-1]
        self.b = params[-1]
        z = np.dot(X, self.w) + self.b
        return -np.mean(y * np.log(self.sigmoid(z)) + (1 - y) * np.log(1 - self.sigmoid(z)))

    def grad(self, params, X, y):
        self.w = params[:-1]
        self.b = params[-1]
        z = np.dot(X, self.w) + self.b
        dw = np.mean((self.sigmoid(z) - y) * X, axis=0)
        db = np.mean(self.sigmoid(z) - y)
        return np.concatenate([dw, [db]])

    def fit(self, X, y):
        n_samples, n_features = X.shape
        initial_params = np.zeros(n_features + 1)
        res = minimize(self.loss, initial_params, args=(X, y), method='BFGS', jac=self.grad, options={'maxiter': self.max_iter})
        self.w = res.x[:-1]
        self.b = res.x[-1]

    def predict(self, X):
        z = np.dot(X, self.w) + self.b
        return (self.sigmoid(z) > 0.5).astype(int)

# 训练逻辑回归模型
logistic_reg = LogisticRegression()
logistic_reg.fit(X_train, y_train)

# 在测试集上评估模型
accuracy = np.mean(logistic_reg.predict(X_test) == y_test)
print(f'Logistic Regression accuracy: {accuracy:.2f}')
```

在这个示例中,我们定义了一个 `LogisticRegression` 类,包含了逻辑回归算法的训练和预测过程。在训练阶段,我们使用 SciPy 的 `minimize` 函数来优化对数似然函数,得到权重向量 `w` 和偏置项 `b`。在预测阶段,我们使用学习得到的参数来计算 logistic 函数的值,并输出 0 或 1 的分类结果。

通过对比两个模型在 iris 数据集上的预测准确率,我们可以看到逻辑回归的表现要优于感知机,这是因为 iris 数据集中存在一些非线性可分的样本,而逻辑回归可以更好地处理这类问题。

## 5. 实际应用场景

感知机和逻辑回归作为两个经典的线性分类模型,在很多实际应用中都有广泛的使用,例如:

1. 文本分类: 可以将文本特征(词频、TF-IDF等)输入感知机或逻辑回归模型,进行文章主题、情感倾向等分类。
2. 垃圾邮件检测: 利用邮件内容、发件人等特征,训练感知机或逻辑回归模型来检测垃圾邮件。
3. 信用评估: 根据客户的个人信息