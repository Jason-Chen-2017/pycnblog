# Transformerå¤§æ¨¡å‹å®æˆ˜ åŠ è½½è‡ªå®šä¹‰æ¨¡å‹

## 1. èƒŒæ™¯ä»‹ç»

### 1.1 Transformeræ¨¡å‹çš„å‘å±•å†ç¨‹

è¿‘å¹´æ¥,è‡ªç„¶è¯­è¨€å¤„ç†(NLP)é¢†åŸŸå–å¾—äº†çªé£çŒ›è¿›çš„å‘å±•,å…¶ä¸­Transformeræ¨¡å‹åŠŸä¸å¯æ²¡ã€‚è‡ªä»2017å¹´è°·æ­Œæå‡ºTransformeræ¨¡å‹ä»¥æ¥,åŸºäºTransformerçš„å„ç§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å¦‚é›¨åæ˜¥ç¬‹èˆ¬æ¶Œç°,å¦‚BERTã€GPTã€XLNetç­‰,æå¤§åœ°æ¨åŠ¨äº†NLPæŠ€æœ¯çš„è¿›æ­¥ã€‚

### 1.2 åŠ è½½è‡ªå®šä¹‰æ¨¡å‹çš„æ„ä¹‰

éšç€Transformeræ¨¡å‹çš„ä¸æ–­å‘å±•,è¶Šæ¥è¶Šå¤šçš„ç ”ç©¶è€…å’Œå¼€å‘è€…å¼€å§‹å°è¯•è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚ç„¶è€Œ,è®­ç»ƒä¸€ä¸ªé«˜è´¨é‡çš„Transformeræ¨¡å‹éœ€è¦æµ·é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æº,å¯¹äºå¾ˆå¤šäººæ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å› æ­¤,å¦‚ä½•åŠ è½½å’Œä½¿ç”¨å·²æœ‰çš„è‡ªå®šä¹‰æ¨¡å‹å°±æ˜¾å¾—å°¤ä¸ºé‡è¦ã€‚é€šè¿‡åŠ è½½é¢„è®­ç»ƒå¥½çš„æ¨¡å‹,æˆ‘ä»¬å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒ(fine-tuning),ä»è€Œå¿«é€Ÿé€‚åº”ç‰¹å®šçš„ä»»åŠ¡,è€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚è¿™ä¸ä»…èƒ½èŠ‚çœå¤§é‡çš„æ—¶é—´å’Œè®¡ç®—èµ„æº,è¿˜èƒ½æ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚

## 2. æ ¸å¿ƒæ¦‚å¿µä¸è”ç³»

### 2.1 Transformerçš„æ ¸å¿ƒæ¦‚å¿µ

- Self-Attention:è‡ªæ³¨æ„åŠ›æœºåˆ¶,è®©æ¨¡å‹èƒ½å¤Ÿå…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„ä»»æ„ä½ç½®,æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚
- Multi-Head Attention:å¤šå¤´æ³¨æ„åŠ›,é€šè¿‡å¤šä¸ªSelf-Attentionå¹¶è¡Œè®¡ç®—,å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ 
- Positional Encoding:ä½ç½®ç¼–ç ,ä¸ºæ¨¡å‹å¼•å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„å…ˆéªŒçŸ¥è¯†ã€‚
- Feed Forward Network:å‰é¦ˆç¥ç»ç½‘ç»œ,å¯¹Self-Attentionçš„è¾“å‡ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚
- Residual Connection:æ®‹å·®è¿æ¥,ä½¿ä¿¡æ¯èƒ½å¤Ÿæ›´é¡ºç•…åœ°åœ¨ç½‘ç»œä¸­æµåŠ¨ã€‚
- Layer Normalization:å±‚å½’ä¸€åŒ–,ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚

### 2.2 é¢„è®­ç»ƒä¸å¾®è°ƒ

- é¢„è®­ç»ƒ(Pre-training):åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨è¯­æ–™ä¸Šè®­ç»ƒé€šç”¨çš„è¯­è¨€è¡¨ç¤ºæ¨¡å‹ã€‚å¸¸è§çš„é¢„è®­ç»ƒä»»åŠ¡æœ‰è¯­è¨€æ¨¡å‹ã€å»å™ªè‡ªç¼–ç å™¨ç­‰ã€‚
- å¾®è°ƒ(Fine-tuning):åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Š,ä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§è®­ç»ƒ,ä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚å¾®è°ƒä¸€èˆ¬åªéœ€è¦è¾ƒå°‘çš„è®­ç»ƒæ•°æ®å’Œè¿­ä»£è½®æ•°å°±èƒ½å–å¾—ä¸é”™çš„æ•ˆæœã€‚

### 2.3 Transformeræ¨¡å‹æ¶æ„å›¾

```mermaid
graph BT
    A[Input Embedding] --> B[Positional Encoding]
    B --> C[Multi-Head Attention]
    C --> D[Layer Normalization]
    D --> E[Feed Forward Network] 
    E --> F[Layer Normalization]
    F --> G[Output Embedding]
```

## 3. æ ¸å¿ƒç®—æ³•åŸç†å…·ä½“æ“ä½œæ­¥éª¤

### 3.1 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

- é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¸¸è§çš„é€‰æ‹©æœ‰BERTã€RoBERTaã€GPT-2ç­‰ã€‚
- ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹çš„å‚æ•°æ–‡ä»¶,å¦‚PyTorchçš„`.bin`æ–‡ä»¶æˆ–TensorFlowçš„`.ckpt`æ–‡ä»¶ã€‚
- ä½¿ç”¨å¯¹åº”çš„ä»£ç åº“åŠ è½½é¢„è®­ç»ƒæ¨¡å‹,å¦‚`transformers`åº“ã€‚

```python
from transformers import AutoModel, AutoTokenizer

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)  
model = AutoModel.from_pretrained(model_name)
```

### 3.2 å‡†å¤‡æ•°æ®é›†

- å°†åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼,å¦‚`input_ids`ã€`attention_mask`ç­‰ã€‚
- ä½¿ç”¨`Dataset`å’Œ`DataLoader`ç»„ç»‡æ•°æ®,æ–¹ä¾¿æ‰¹é‡å¤„ç†ã€‚

```python
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
        
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]
        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True)
        return encoding
        
dataset = MyDataset(data, tokenizer)        
dataloader = DataLoader(dataset, batch_size=32)
```

### 3.3 å¾®è°ƒæ¨¡å‹

- æ ¹æ®ä»»åŠ¡éœ€è¦,åœ¨é¢„è®­ç»ƒæ¨¡å‹ä¸Šæ·»åŠ æ–°çš„å±‚,å¦‚åˆ†ç±»å¤´ã€åºåˆ—æ ‡æ³¨å¤´ç­‰ã€‚
- ä½¿ç”¨æ ‡æ³¨æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ,æ›´æ–°å…¨éƒ¨æˆ–éƒ¨åˆ†å‚æ•°ã€‚é€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ã€‚
- è¯„ä¼°å¾®è°ƒåçš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½,æ ¹æ®éœ€è¦è°ƒæ•´è¶…å‚æ•°ã€‚

```python
from transformers import AdamW

# æ·»åŠ åˆ†ç±»å¤´
model.classifier = nn.Linear(model.config.hidden_size, num_labels)

# å®šä¹‰ä¼˜åŒ–å™¨
optimizer = AdamW(model.parameters(), lr=1e-5)

# è®­ç»ƒæ¨¡å‹
model.train()
for batch in dataloader:
    optimizer.zero_grad()
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    
# è¯„ä¼°æ¨¡å‹    
model.eval()
```

### 3.4 ä¿å­˜å’ŒåŠ è½½å¾®è°ƒåçš„æ¨¡å‹

- ä½¿ç”¨`save_pretrained`æ–¹æ³•ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å‚æ•°ã€‚
- ä½¿ç”¨`from_pretrained`æ–¹æ³•åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ã€‚

```python
model_path = "finetuned_model"
model.save_pretrained(model_path)

loaded_model = AutoModel.from_pretrained(model_path)
```

## 4. æ•°å­¦æ¨¡å‹å’Œå…¬å¼è¯¦ç»†è®²è§£ä¸¾ä¾‹è¯´æ˜

### 4.1 Self-Attention

Self-Attentionæ˜¯Transformerçš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€,å®ƒå…è®¸æ¨¡å‹å¯¹è¾“å…¥åºåˆ—ä¸­çš„ä»»æ„ä¸¤ä¸ªä½ç½®è®¡ç®—æ³¨æ„åŠ›æƒé‡,æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚å…·ä½“æ¥è¯´,å¯¹äºè¾“å…¥åºåˆ—$X \in \mathbb{R}^{n \times d}$,Self-Attentionçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:

$$
\begin{aligned}
Q &= XW_Q \\
K &= XW_K \\
V &= XW_V \\
\text{Attention}(Q, K, V) &= \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{aligned}
$$

å…¶ä¸­,$W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$æ˜¯å¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µ,$d_k$æ˜¯æ³¨æ„åŠ›å¤´çš„ç»´åº¦ã€‚$Q, K, V$åˆ†åˆ«ç§°ä¸ºæŸ¥è¯¢(Query)ã€é”®(Key)ã€å€¼(Value)ã€‚

é€šè¿‡è®¡ç®—$Q$å’Œ$K$çš„ç‚¹ç§¯å¹¶é™¤ä»¥$\sqrt{d_k}$,æˆ‘ä»¬å¾—åˆ°äº†æ³¨æ„åŠ›æƒé‡çŸ©é˜µã€‚è¿™ä¸ªçŸ©é˜µç»è¿‡softmaxå½’ä¸€åŒ–å,ä¸$V$ç›¸ä¹˜,å¾—åˆ°äº†Self-Attentionçš„è¾“å‡ºã€‚ç›´è§‚åœ°ç†è§£,Self-Attentionå°±æ˜¯è®©æ¨¡å‹å­¦ä¹ åˆ°è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªä½ç½®å¯¹å…¶ä»–ä½ç½®çš„é‡è¦ç¨‹åº¦,å¹¶æ ¹æ®è¿™ç§é‡è¦ç¨‹åº¦å¯¹å€¼$V$è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚

### 4.2 Multi-Head Attention

Multi-Head Attentionæ˜¯åœ¨Self-Attentionçš„åŸºç¡€ä¸Š,å¹¶è¡Œè®¡ç®—å¤šä¸ª"å¤´"(head),ç„¶åå°†ç»“æœæ‹¼æ¥èµ·æ¥ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯è®©æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸åŒçš„å­ç©ºé—´é‡Œå­¦ä¹ åˆ°ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼,æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚è®¾$h$ä¸ºæ³¨æ„åŠ›å¤´çš„æ•°é‡,Multi-Head Attentionçš„è®¡ç®—è¿‡ç¨‹ä¸º:

$$
\begin{aligned}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{aligned}
$$

å…¶ä¸­,$W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d \times d_k}, W_O \in \mathbb{R}^{hd_k \times d}$éƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µã€‚

## 5. é¡¹ç›®å®è·µï¼šä»£ç å®ä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜

ä¸‹é¢æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡,æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨`transformers`åº“åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚

### 5.1 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import BertTokenizer, BertForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)
```

è¿™é‡Œæˆ‘ä»¬é€‰æ‹©äº†BERTæ¨¡å‹,å¹¶ä½¿ç”¨`BertForSequenceClassification`ç±»,å®ƒåœ¨åŸå§‹çš„BERTæ¨¡å‹ä¸Šæ·»åŠ äº†ä¸€ä¸ªçº¿æ€§åˆ†ç±»å¤´ã€‚`num_labels`å‚æ•°æŒ‡å®šäº†åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°ã€‚

### 5.2 å‡†å¤‡æ•°æ®é›†

```python
from torch.utils.data import Dataset, DataLoader

class MyDataset(Dataset):
    def __init__(self, data, labels, tokenizer):
        self.data = data
        self.labels = labels
        self.tokenizer = tokenizer
        
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text = self.data[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(text, return_tensors='pt', padding='max_length', truncation=True)
        encoding['labels'] = torch.tensor(label)
        return encoding
        
dataset = MyDataset(data, labels, tokenizer)        
dataloader = DataLoader(dataset, batch_size=32)
```

è¿™é‡Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„`Dataset`ç±»,å®ƒæ¥å—æ–‡æœ¬æ•°æ®ã€æ ‡ç­¾ä»¥åŠåˆ†è¯å™¨ä½œä¸ºè¾“å…¥,å¹¶è¿”å›æ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚æ³¨æ„è¿™é‡Œæˆ‘ä»¬å°†æ ‡ç­¾ä¹Ÿå°è£…åˆ°äº†æœ€ç»ˆçš„å­—å…¸é‡Œ,ä»¥ä¾¿åœ¨è®­ç»ƒæ—¶ç›´æ¥è·å–ã€‚

### 5.3 å¾®è°ƒæ¨¡å‹

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=1e-5)

num_epochs = 3
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

for epoch in range(num_epochs):
    model.train()
    for batch in dataloader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].squeeze().to(device)  
        attention_mask = batch['attention_mask'].squeeze().to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        
    model.eval()    
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½
```

åœ¨å¾®è°ƒé˜¶æ®µ,æˆ‘ä»¬å®šä¹‰äº†ä¼˜åŒ–å™¨`AdamW`,å®ƒæ˜¯`transformers`åº“æ¨èçš„ä¼˜åŒ–ç®—æ³•ã€‚ç„¶åæˆ‘ä»¬éå†æ•°æ®é›†,å°†æ•°æ®ä¼ å…¥æ¨¡å‹è¿›è¡Œå‰å‘è®¡ç®—,å¹¶æ ¹æ®æŸå¤±å‡½æ•°åå‘ä¼ æ’­æ›´æ–°å‚æ•°ã€‚æ¯ä¸ªepochç»“æŸå,æˆ‘ä»¬åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½,ä»¥ä¾¿è°ƒæ•´è¶…å‚æ•°ã€‚

### 5.4 ä¿å­˜å’ŒåŠ è½½å¾®è°ƒåçš„æ¨¡å‹

```python
model_path = "finetuned_model"
model.save_pretrained(model_path)

loaded_model = BertForSequenceClassification.from_pretrained(model_path)
```

å¾®è°ƒå®Œæˆå,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`save_pretrained`æ–¹æ³•å°†æ¨¡å‹å‚æ•°ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„,å¹¶ä½¿ç”¨`from_pretrained`æ–¹æ³•é‡æ–°åŠ è½½ã€‚è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªé’ˆå¯¹ç‰¹å®šä»»åŠ¡ä¼˜åŒ–è¿‡çš„æ¨¡å‹,å¯ä»¥ç”¨äºåç»­çš„é¢„æµ‹å’Œåº”ç”¨ã€‚

## 6. å®é™…åº”ç”¨åœºæ™¯

Transformeræ¨¡å‹åœ¨NLPé¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨,ä¸‹é¢åˆ—ä¸¾å‡ ä¸ªå¸¸è§çš„åœºæ™¯:

- æ–‡æœ¬åˆ†ç±»:å¦‚æƒ…æ„Ÿåˆ†æã€åƒåœ¾é‚®ä»¶æ£€æµ‹ã€æ–°é—»åˆ†ç±»ç­‰ã€‚
- å‘½åå®ä½“è¯†åˆ«:è¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€æœºæ„åç­‰ã€‚
- é—®ç­”ç³»ç»Ÿ:æ ¹æ®ç»™å®šçš„é—®é¢˜å’Œä¸Šä¸‹æ–‡,ç”Ÿæˆç›¸åº”çš„ç­”æ¡ˆã€‚
- æœºå™¨ç¿»è¯‘:å°†ä¸€ç§è¯­è¨€çš„æ–‡æœ¬ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€ã€‚
- æ–‡æœ¬æ‘˜è¦:è‡ªåŠ¨ç”Ÿæˆæ–‡æœ¬çš„æ‘˜è¦ã€‚
- å¯¹è¯ç³»ç»Ÿ:å®ç°äººæœºå¯¹è¯,å¦‚å®¢æœèŠå¤©æœºå™¨äººç­‰ã€‚

é€šè¿‡åŠ è½½é¢„è®­ç»ƒçš„Transformeræ¨¡å‹,å¹¶ä½¿ç”¨å°‘é‡æ ‡æ³¨æ•°æ®è¿›è¡Œå¾®è°ƒ,æˆ‘ä»¬å¯ä»¥å¿«é€Ÿæ„å»ºé€‚ç”¨äºä»¥ä¸Šåœºæ™¯çš„é«˜è´¨é‡æ¨¡å‹,å¤§å¤§é™ä½äº†å¼€å‘æˆæœ¬ã€‚

## 7. å·¥å…·å’Œèµ„æºæ¨è

- [transformers](https://github.com/huggingface/transformers):ğŸ¤— Hugging Faceæ¨å‡ºçš„NLPåº“,æä¾›äº†å¤§é‡é¢„è®­ç»ƒæ¨¡å‹å’Œä¾¿æ·çš„APIã€‚
- [PyTorch](https://pytorch.org/):åŸºäºåŠ¨æ€è®¡ç®—å›¾çš„æ·±åº¦å­¦ä¹ æ¡†æ¶,åœ¨ç ”ç©¶ç•Œå¹¿æ³›ä½¿ç”¨ã€‚
- [TensorFlow](https://www.tensorflow.org/):ç”±Googleå¼€å‘çš„ç«¯åˆ°ç«¯å¼€æºæœºå™¨å­¦ä¹ å¹³å°ã€‚
- [Hugging Face Model Hub](https://huggingface.co/models):Hugging Faceæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹ä»“åº“,åŒ…å«äº†å¤§é‡ä¸åŒè¯­è¨€å’Œä»»åŠ¡çš„æ¨¡å‹ã€‚
- [Google Research](https://github.com/google-research):Google Researchçš„å®˜æ–¹GitHubä»“åº“,åŒ…å«äº†å¤šä¸ªå‰æ²¿çš„NLPæ¨¡å‹å®ç°ã€‚
- [Stanford NLP](https://nlp.stanford.edu/):æ–¯å¦ç¦å¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†ç»„,æä¾›äº†CoreNLPã€GloVeç­‰çŸ¥åNLPå·¥å…·å’Œèµ„æºã€‚

## 8. æ€»ç»“ï¼šæœªæ¥å‘å±•è¶‹åŠ¿ä¸æŒ‘æˆ˜

Transformeræ¨¡å‹çš„å‡ºç°æ ‡å¿—ç€NLPé¢†åŸŸçš„ä¸€ä¸ªé‡Œç¨‹ç¢‘,å®ƒçš„æˆåŠŸæ¿€å‘äº†ç ”ç©¶è€…çš„è¿›ä¸€æ­¥æ¢ç´¢ã€‚æœªæ¥Transformeræ¨¡å‹çš„å‘å±•è¶‹åŠ¿å¯èƒ½åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ–¹é¢:

- æ¨¡å‹çš„è¿›ä¸€æ­¥æ‰©å¤§:æ›´å¤§çš„æ¨¡å‹ã€æ›´å¤šçš„å‚æ•°,ä»¥æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
- è®­ç»ƒèŒƒå¼çš„æ”¹è¿›:å¦‚ELECTRAå¼•å…¥äº†å¯¹æŠ—è®­ç»ƒ,DeBERTaä½¿ç”¨äº†ç›¸å¯¹ä½ç½®ç¼–ç ã€‚
- çŸ¥è¯†çš„å¼•å…¥:å¦‚ä½•å°†å…ˆéªŒçŸ¥è¯†èå…¥é¢„è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸ªå€¼å¾—æ¢ç´¢çš„æ–¹å‘ã€‚
- å¤šæ¨¡æ€å­¦ä¹ :å°†