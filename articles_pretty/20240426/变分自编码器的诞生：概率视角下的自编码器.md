# 变分自编码器的诞生：概率视角下的自编码器

## 1.背景介绍

### 1.1 自编码器的起源

自编码器(Autoencoder)是一种无监督学习的人工神经网络,它的目标是学习将高维输入数据编码为低维的编码表示,然后再将该编码解码为与原始输入尽可能接近的输出。自编码器最初被提出用于降维和特征学习,后来也被广泛应用于数据去噪、生成模型等领域。

传统的自编码器由编码器(Encoder)和解码器(Decoder)两部分组成。编码器将输入数据 $\boldsymbol{x}$ 映射到隐藏表示 $\boldsymbol{z}=f(\boldsymbol{x})$,解码器则将隐藏表示 $\boldsymbol{z}$ 映射回重建数据 $\boldsymbol{\hat{x}}=g(\boldsymbol{z})$。自编码器的训练目标是最小化输入 $\boldsymbol{x}$ 与重建输出 $\boldsymbol{\hat{x}}$ 之间的重构误差,例如均方误差 $\| \boldsymbol{x} - \boldsymbol{\hat{x}} \|_2^2$。

### 1.2 自编码器的局限性

尽管自编码器在降维和特征学习方面取得了一定成功,但它们存在一些固有的局限性:

1. **确定性映射**:传统自编码器将输入数据 $\boldsymbol{x}$ 确定性地映射到隐藏表示 $\boldsymbol{z}$,这意味着相同的输入将总是产生相同的隐藏表示,从而无法捕捉数据的复杂多样性。

2. **潜在空间的限制**:传统自编码器的隐藏表示 $\boldsymbol{z}$ 通常是一个向量,这限制了它们对复杂数据分布的建模能力。

3. **生成能力有限**:由于缺乏合理的概率模型,传统自编码器无法高效地从隐藏表示 $\boldsymbol{z}$ 生成新的样本。

为了解决这些问题,变分自编码器(Variational Autoencoder, VAE)应运而生。

## 2.核心概念与联系

### 2.1 概率模型与生成模型

在介绍变分自编码器之前,我们需要先了解概率模型和生成模型的概念。

**概率模型**是一种使用概率分布来描述观测数据的模型。给定一个随机变量 $\boldsymbol{x}$,概率模型的目标是学习其概率分布 $p(\boldsymbol{x})$。

**生成模型**是一种能够从某个潜在分布 $p(\boldsymbol{z})$ 生成观测数据 $\boldsymbol{x}$ 的模型。生成模型通常定义了一个条件概率分布 $p(\boldsymbol{x}|\boldsymbol{z})$,表示在给定潜在变量 $\boldsymbol{z}$ 的条件下,观测数据 $\boldsymbol{x}$ 的概率分布。

变分自编码器正是将自编码器与概率模型和生成模型相结合,从而克服了传统自编码器的局限性。

### 2.2 变分自编码器的基本思想

变分自编码器的核心思想是将观测数据 $\boldsymbol{x}$ 看作是由某个潜在变量 $\boldsymbol{z}$ 生成的,即 $\boldsymbol{x} \sim p(\boldsymbol{x}|\boldsymbol{z})$。我们假设潜在变量 $\boldsymbol{z}$ 服从一个简单的先验分布 $p(\boldsymbol{z})$,例如高斯分布或标准正态分布。

变分自编码器由两个主要部分组成:

1. **编码器(Encoder) $q(\boldsymbol{z}|\boldsymbol{x})$**:编码器将观测数据 $\boldsymbol{x}$ 映射到潜在变量 $\boldsymbol{z}$ 的概率分布 $q(\boldsymbol{z}|\boldsymbol{x})$。这个分布被称为**近似后验分布(Approximate Posterior)**,因为我们无法直接获得真实的后验分布 $p(\boldsymbol{z}|\boldsymbol{x})$。

2. **解码器(Decoder) $p(\boldsymbol{x}|\boldsymbol{z})$**:解码器定义了在给定潜在变量 $\boldsymbol{z}$ 的条件下,观测数据 $\boldsymbol{x}$ 的概率分布 $p(\boldsymbol{x}|\boldsymbol{z})$。这个分布被称为**生成模型**。

变分自编码器的训练目标是最大化观测数据 $\boldsymbol{x}$ 的边际对数似然 $\log p(\boldsymbol{x})$。然而,由于真实的后验分布 $p(\boldsymbol{z}|\boldsymbol{x})$ 通常难以计算,我们使用变分推断(Variational Inference)的方法来近似这个目标。

## 3.核心算法原理具体操作步骤

### 3.1 变分下界(ELBO)

为了最大化观测数据 $\boldsymbol{x}$ 的边际对数似然 $\log p(\boldsymbol{x})$,我们引入一个辅助分布 $q(\boldsymbol{z}|\boldsymbol{x})$ 作为近似后验分布。根据Jensen不等式,我们可以得到如下变分下界(Evidence Lower Bound, ELBO):

$$
\begin{aligned}
\log p(\boldsymbol{x}) &\geq \mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}, \boldsymbol{z})}{q(\boldsymbol{z}|\boldsymbol{x})}\right] \\
&=\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log \frac{p(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})}{q(\boldsymbol{z}|\boldsymbol{x})}\right] \\
&=\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]-D_{\mathrm{KL}}(q(\boldsymbol{z}|\boldsymbol{x}) \| p(\boldsymbol{z}))
\end{aligned}
$$

其中 $D_{\mathrm{KL}}$ 表示KL散度(Kullback-Leibler Divergence),用于衡量两个分布之间的差异。

由于 $\log p(\boldsymbol{x})$ 是一个常数,最大化变分下界 $\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]-D_{\mathrm{KL}}(q(\boldsymbol{z}|\boldsymbol{x}) \| p(\boldsymbol{z}))$ 就等价于最大化观测数据的边际对数似然 $\log p(\boldsymbol{x})$。

变分自编码器的训练目标就是最大化这个变分下界,它包含两个项:

1. **重构项 $\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]$**:这项表示在给定潜在变量 $\boldsymbol{z}$ 的条件下,观测数据 $\boldsymbol{x}$ 的期望对数似然。它衡量了解码器 $p(\boldsymbol{x}|\boldsymbol{z})$ 对观测数据 $\boldsymbol{x}$ 的重构能力。

2. **正则项 $-D_{\mathrm{KL}}(q(\boldsymbol{z}|\boldsymbol{x}) \| p(\boldsymbol{z}))$**:这项是近似后验分布 $q(\boldsymbol{z}|\boldsymbol{x})$ 与先验分布 $p(\boldsymbol{z})$ 之间的KL散度的负值。它作为一种正则化项,鼓励编码器 $q(\boldsymbol{z}|\boldsymbol{x})$ 输出的潜在表示 $\boldsymbol{z}$ 服从简单的先验分布 $p(\boldsymbol{z})$,从而使得潜在空间具有良好的结构性质。

通过最大化这个变分下界,变分自编码器可以同时优化重构项和正则项,从而学习到一个能够高效生成观测数据的概率模型。

### 3.2 重参数技巧(Reparameterization Trick)

在训练变分自编码器时,我们需要计算重构项 $\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]$ 的期望。然而,直接对 $q(\boldsymbol{z}|\boldsymbol{x})$ 进行采样是困难的,因为它是一个复杂的高维分布。

为了解决这个问题,变分自编码器引入了**重参数技巧(Reparameterization Trick)**。这个技巧利用了一个简单的事实:如果 $\boldsymbol{z} \sim q(\boldsymbol{z}|\boldsymbol{x})$,那么就存在一个确定性映射 $\boldsymbol{z}=g(\boldsymbol{\epsilon}, \boldsymbol{x})$,使得 $\boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})$ 是一个简单的分布,例如标准正态分布 $\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$。

利用这个技巧,我们可以将期望 $\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]$ 重写为:

$$
\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[\log p(\boldsymbol{x}|g(\boldsymbol{\epsilon}, \boldsymbol{x}))\right]
$$

这样,我们只需要从一个简单的分布 $p(\boldsymbol{\epsilon})$ 采样,然后通过确定性映射 $g(\boldsymbol{\epsilon}, \boldsymbol{x})$ 得到潜在变量 $\boldsymbol{z}$,就可以计算重构项的期望了。

在实践中,编码器 $q(\boldsymbol{z}|\boldsymbol{x})$ 通常被参数化为一个对角高斯分布,其均值 $\boldsymbol{\mu}$ 和标准差 $\boldsymbol{\sigma}$ 由神经网络输出。重参数技巧可以表示为:

$$
\boldsymbol{z}=\boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{I})
$$

其中 $\odot$ 表示元素wise乘积。通过这种方式,我们可以对潜在变量 $\boldsymbol{z}$ 进行有效的采样,并计算重构项的期望。

### 3.3 变分自编码器的训练过程

变分自编码器的训练过程可以总结为以下步骤:

1. 从训练数据 $\boldsymbol{x}$ 中采样一个小批量样本。
2. 通过编码器 $q(\boldsymbol{z}|\boldsymbol{x})$ 得到潜在变量 $\boldsymbol{z}$ 的均值 $\boldsymbol{\mu}$ 和标准差 $\boldsymbol{\sigma}$。
3. 从标准正态分布 $\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ 中采样噪声 $\boldsymbol{\epsilon}$,并利用重参数技巧计算 $\boldsymbol{z}=\boldsymbol{\mu}+\boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$。
4. 通过解码器 $p(\boldsymbol{x}|\boldsymbol{z})$ 得到重构数据 $\boldsymbol{\hat{x}}$。
5. 计算重构项 $\log p(\boldsymbol{x}|\boldsymbol{z})$ 和正则项 $D_{\mathrm{KL}}(q(\boldsymbol{z}|\boldsymbol{x}) \| p(\boldsymbol{z}))$。
6. 最大化变分下界 $\mathbb{E}_{q(\boldsymbol{z}|\boldsymbol{x})}\left[\log p(\boldsymbol{x}|\boldsymbol{z})\right]-D_{\mathrm{KL}}(q(\boldsymbol{z}|\boldsymbol{x}) \| p(\boldsymbol{z}))$,通过反向传播更新编码器和解码器的参数。

通过不断迭代这个过程,