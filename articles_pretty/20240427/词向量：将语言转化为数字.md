## 1. 背景介绍

自然语言处理 (NLP) 领域近年来取得了显著进展，而词向量技术是推动这一进步的核心驱动力之一。词向量是将单词或短语映射到实数向量空间的技术，使得计算机能够理解和处理语言。

### 1.1 NLP 与词向量

NLP 致力于使计算机能够理解、解释和生成人类语言。传统方法通常将文本视为离散符号序列，忽略了单词之间的语义关系。词向量通过将单词表示为连续的向量，捕捉了单词的语义和语法信息，从而克服了传统方法的局限性。

### 1.2 词向量的作用

词向量在 NLP 任务中扮演着重要角色，包括：

* **文本分类**：将文本分类为不同的类别，例如情感分析、主题分类等。
* **机器翻译**：将一种语言的文本翻译成另一种语言。
* **信息检索**：根据用户的查询检索相关文档。
* **问答系统**：回答用户提出的问题。
* **文本摘要**：生成文本的简短摘要。

## 2. 核心概念与联系

### 2.1 分布式假设

词向量技术基于分布式假设，即具有相似上下文的单词具有相似的语义。例如，“猫”和“狗”经常出现在相似的语境中，因此它们的词向量也应该相似。

### 2.2 词嵌入

词嵌入是将单词映射到向量空间的过程，使得具有相似语义的单词在向量空间中距离更近。常见的词嵌入模型包括：

* **Word2Vec**：一种基于神经网络的模型，包括 Skip-gram 和 CBOW 两种架构。
* **GloVe**：基于全局词共现矩阵的模型。
* **FastText**：考虑单词内部结构的词嵌入模型。

## 3. 核心算法原理

### 3.1 Word2Vec

Word2Vec 是最常用的词嵌入模型之一，其核心思想是通过预测目标单词的上下文或根据上下文预测目标单词来学习词向量。

#### 3.1.1 Skip-gram

Skip-gram 模型的目标是根据目标单词预测其上下文单词。模型结构如下：

* 输入层：目标单词的 one-hot 向量。
* 隐藏层：线性层，将 one-hot 向量映射到低维向量空间。
* 输出层：softmax 层，预测上下文单词的概率分布。

#### 3.1.2 CBOW

CBOW 模型的目标是根据上下文单词预测目标单词。模型结构与 Skip-gram 相似，但输入层是上下文单词的 one-hot 向量之和或平均值。

### 3.2 GloVe

GloVe 模型利用全局词共现矩阵来学习词向量。词共现矩阵记录了每个单词对同时出现的次数。GloVe 模型的目标是最小化词向量和词共现矩阵之间的差异。

## 4. 数学模型和公式

### 4.1 Skip-gram

Skip-gram 模型的损失函数为负对数似然函数，目标是最大化目标单词和其上下文单词的共现概率：

$$
J(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j} | w_t)
$$

其中：

* $T$ 是训练样本的数量。
* $c$ 是上下文窗口大小。
* $w_t$ 是目标单词。
* $w_{t+j}$ 是上下文单词。
* $p(w_{t+j} | w_t)$ 是目标单词 $w_t$ 预测上下文单词 $w_{t+j}$ 的概率。

### 4.2 GloVe

GloVe 模型的损失函数为加权平方误差，目标是最小化词向量和词共现矩阵之间的差异：

$$
J = \sum_{i,j=1}^{V} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2
$$

其中：

* $V$ 是词汇表大小。
* $X_{ij}$ 是单词 $i$ 和单词 $j$ 的共现次数。
* $w_i$ 和 $\tilde{w}_j$ 分别是单词 $i$ 和单词 $j$ 的词向量。
* $b_i$ 和 $\tilde{b}_j$ 分别是单词 $i$ 和单词 $j$ 的偏置项。
* $f(X_{ij})$ 是一个权重函数，用于降低常见单词对的影响。 
