# *DARTS：可微架构搜索

## 1.背景介绍

### 1.1 神经网络架构设计的重要性

在深度学习领域中,神经网络架构的设计对于模型的性能和效率至关重要。合理的架构设计不仅可以提高模型的准确性,还能降低计算复杂度,从而提高推理速度和降低能耗。然而,设计高效的神经网络架构通常需要大量的人工经验和反复试验,这是一个耗时且具有挑战性的过程。

### 1.2 手工设计架构的局限性

传统上,神经网络架构主要依赖于人工设计和调整。研究人员需要根据任务需求和先验知识,手动设计网络层数、层类型、连接方式等架构超参数。这种方法存在以下局限性:

1. 人工设计需要大量的领域专业知识和经验,效率低下且容易受到偏见影响。
2. 架构搜索空间庞大,手工探索难以覆盖所有可能性。
3. 针对不同任务需要重复设计新架构,缺乏通用性。

### 1.3 神经架构搜索(NAS)的兴起

为了解决手工设计架构的局限性,神经架构搜索(Neural Architecture Search, NAS)应运而生。NAS旨在自动化地学习和优化神经网络架构,从而减轻人工设计的负担。早期的NAS方法通过搜索算法(如强化学习、进化算法等)在预定义的搜索空间中探索最优架构,取得了令人鼓舞的结果。然而,这些方法往往计算成本高昂,难以应用于大规模任务。

### 1.4 DARTS:可微架构搜索的创新

DARTS(Differentiable Architecture Search)是一种新颖的可微架构搜索方法,旨在通过梯度优化高效地搜索神经网络架构。与传统的NAS方法不同,DARTS将架构搜索问题建模为一个基于连续relaxation的双层优化问题,从而使得架构搜索过程可微,可以直接利用梯度下降进行高效优化。这种创新方法显著降低了搜索成本,使得在较大的搜索空间中进行架构搜索成为可能。

DARTS的提出为神经网络架构自动设计领域带来了新的思路和契机。本文将深入探讨DARTS的核心思想、算法细节、实现方式以及在各种任务中的应用,旨在为读者提供全面的理解和实践指导。

## 2.核心概念与联系

### 2.1 架构搜索的形式化表示

在介绍DARTS之前,我们首先需要对神经网络架构搜索问题进行形式化表示。一般来说,神经网络架构可以表示为一个有向无环图(DAG),其中节点表示不同的操作(如卷积、池化等),边表示节点之间的连接。因此,架构搜索的目标就是在给定的搜索空间中找到一个最优的DAG表示。

我们用$\mathcal{F}$表示搜索空间,即所有可能的架构集合。对于每个架构$\alpha \in \mathcal{F}$,我们定义一个映射$f_\alpha$,将输入数据$x$映射到输出$y$,即$y=f_\alpha(x)$。架构搜索的目标是找到一个最优架构$\alpha^*$,使得在训练数据集$\mathcal{D}_{train}$上的损失函数$\mathcal{L}$最小化:

$$\alpha^* = \arg\min_{\alpha \in \mathcal{F}} \mathbb{E}_{(x, y) \sim \mathcal{D}_{train}} \mathcal{L}(f_\alpha(x), y)$$

传统的NAS方法通常采用基于搜索的策略(如强化学习、进化算法等)来解决上述优化问题,但计算成本往往很高。相比之下,DARTS提出了一种全新的基于梯度优化的方法,将架构搜索问题建模为一个可微的双层优化问题,从而可以直接利用高效的梯度下降算法进行优化。

### 2.2 DARTS的核心思想

DARTS的核心思想是将离散的架构搜索问题连续化(relaxation),从而使得架构参数可以通过梯度优化的方式进行学习。具体来说,DARTS将神经网络架构表示为一个超网络(over-parameterized network),其中包含所有可能的操作和连接。然后,DARTS为每个操作和连接引入一个连续的架构参数,用于控制该操作或连接在最终架构中的重要性。

在训练过程中,DARTS同时优化两个目标:一是最小化训练损失,以学习网络权重参数;二是学习架构参数,以找到最优的子网络架构。通过交替优化这两个目标,DARTS可以有效地在超网络中搜索出高性能的子网络架构。最终,根据学习到的架构参数,DARTS将保留重要的操作和连接,剔除不重要的部分,从而得到最优的离散架构。

DARTS的创新之处在于将离散的架构搜索问题连续化,从而使得梯度优化成为可能。这种思路不仅降低了搜索成本,而且可以在更大的搜索空间中进行高效的架构探索。

### 2.3 DARTS与其他NAS方法的关系

虽然DARTS提出了全新的可微架构搜索范式,但它与其他NAS方法也存在一些联系。例如,DARTS可以看作是对基于强化学习的NAS方法(如NASNet)的改进,将离散的策略梯度优化转化为连续的梯度优化。同时,DARTS也与基于进化算法的NAS方法(如AmoebaNet)有一定关联,因为它们都试图在一个大的搜索空间中找到最优架构。

另一方面,DARTS也为后续的NAS研究开辟了新的道路。许多工作致力于改进DARTS的性能和稳定性,如通过正则化或梯度剪裁来缓解权重共享带来的不利影响。此外,一些工作尝试将DARTS推广到更广泛的场景,如对抗样本防御、模型压缩等。总的来说,DARTS为神经架构搜索领域注入了新的活力,推动了该领域的快速发展。

## 3.核心算法原理具体操作步骤

### 3.1 超网络构建

DARTS的第一步是构建一个包含所有可能操作和连接的超网络(over-parameterized network)。具体来说,超网络由一系列节点(node)和边(edge)组成,每个节点代表一个特征张量,每条边代表一种可能的操作(如卷积、池化等)。

为了构建超网络,DARTS首先定义了一个基础网络结构,包括输入节点、中间节点和输出节点。然后,DARTS为每对节点之间的边引入了多种可选操作,例如$3\times3$和$5\times5$的分离卷积、$3\times3$的最大池化、$3\times3$的平均池化等。所有这些操作都被并行执行,并通过一个加权求和的方式聚合到下一个节点。

具体来说,对于每条边$(i, j)$,其输出特征张量$o^{(i,j)}$可以表示为:

$$o^{(i,j)} = \sum_{k=1}^{K} \frac{\exp(a_k^{(i,j)})}{\sum_{k'=1}^{K}\exp(a_{k'}^{(i,j)})} \circ o_k^{(i,j)}$$

其中,$K$是可选操作的总数,$o_k^{(i,j)}$表示第$k$种操作的输出,$a_k^{(i,j)}$是对应的架构参数(architecture parameter),用于控制该操作在最终架构中的重要性。$\circ$表示元素wise操作。通过这种加权求和的方式,超网络包含了所有可能的子网络架构。

在超网络构建完成后,DARTS的目标就是学习这些架构参数$a_k^{(i,j)}$,从而找到最优的子网络架构。

### 3.2 双层优化问题

DARTS将架构搜索问题建模为一个双层优化问题。在内层优化中,DARTS以当前的架构参数为条件,优化网络权重参数$w$,以最小化训练损失:

$$\min_w \mathcal{L}_{train}(w, a)$$

其中,$\mathcal{L}_{train}$是训练损失函数,$a$表示当前的架构参数集合。

在外层优化中,DARTS以内层优化得到的网络权重参数为条件,优化架构参数$a$,以最小化验证损失:

$$\min_a \mathcal{L}_{val}(w^*(a), a)$$

其中,$w^*(a)$是内层优化得到的网络权重参数,$\mathcal{L}_{val}$是验证损失函数。

通过交替优化这两个目标,DARTS可以同时学习网络权重参数和架构参数。具体来说,在每个训练步骤中,DARTS首先在训练数据上优化网络权重参数$w$,然后在验证数据上优化架构参数$a$。这种双层优化策略可以确保网络权重参数能够很好地拟合训练数据,同时架构参数也能够在验证数据上获得良好的泛化性能。

需要注意的是,由于超网络包含了所有可能的子网络架构,因此在训练过程中,所有操作都会被计算和更新。这种权重共享机制可以极大地减少搜索成本,但也可能引入一些不利影响(如权重共享不足等)。为了缓解这个问题,DARTS采用了一些正则化策略,如梯度剪裁、辅助损失等。

### 3.3 离散架构导出

在双层优化收敛后,DARTS需要从连续的架构参数$a$中导出一个离散的最终架构。具体来说,对于每条边$(i, j)$,DARTS选择具有最大架构参数值的操作作为该边的最终操作:

$$o^{(i,j)} = o_{\hat{k}}^{(i,j)}, \text{where } \hat{k} = \arg\max_k a_k^{(i,j)}$$

通过这种硬阈值化(hard-thresholding)的方式,DARTS可以从超网络中剔除不重要的操作和连接,从而得到一个精简的最终架构。

需要注意的是,由于架构参数是通过双层优化的方式学习得到的,因此最终导出的离散架构往往能够在训练数据和验证数据上都取得良好的性能。

### 3.4 架构评估与微调

在得到最终的离散架构后,DARTS会在目标任务的训练数据上从头开始训练该架构,以获得最终的模型权重参数。在这个过程中,DARTS还可以对架构进行一些微调,如手动调整卷积核大小、通道数等,以进一步提高模型性能。

最后,DARTS会在测试数据集上评估最终模型的性能,并与其他手工设计或自动搜索的架构进行比较,以验证DARTS的有效性。

## 4.数学模型和公式详细讲解举例说明

在上一节中,我们介绍了DARTS算法的核心步骤。现在,我们将更深入地探讨DARTS中使用的数学模型和公式,并通过具体示例来说明它们的含义和作用。

### 4.1 混合操作的形式化表示

在DARTS的超网络中,每条边$(i, j)$都包含了多种可选操作,它们的输出特征张量通过加权求和的方式聚合到下一个节点。具体来说,边$(i, j)$的输出特征张量$o^{(i,j)}$可以表示为:

$$o^{(i,j)} = \sum_{k=1}^{K} \frac{\exp(a_k^{(i,j)})}{\sum_{k'=1}^{K}\exp(a_{k'}^{(i,j)})} \circ o_k^{(i,j)}$$

其中,$K$是可选操作的总数,$o_k^{(i,j)}$表示第$k$种操作的输出,$a_k^{(i,j)}$是对应的架构参数。$\circ$表示元素wise操作。

这个公式实际上是一种软加权求和(soft-weighted sum)的形式,它使用了softmax函数来计算每种操作的权重。具体来说,$\frac{\exp(a_k^{(i,j)})}{\sum_{k'=1}^{K}\exp(a_{k'}^{(i,j)})}$表示第$k$种操作的权重,它是架构参数$a_k^{(i,j)}$的softmax值