# HBase在Kylin中的应用与优化

作者：禅与计算机程序设计艺术

## 1.背景介绍
### 1.1 大数据OLAP查询的需求与挑战
### 1.2 Kylin的应运而生
#### 1.2.1 Kylin的起源与发展历程
#### 1.2.2 Kylin的核心特性与优势
### 1.3 Kylin中为何选择HBase作为存储
#### 1.3.1 HBase与Kylin的契合点
#### 1.3.2 HBase给Kylin带来的价值

## 2.核心概念与联系
### 2.1 Kylin的数据处理流程
#### 2.1.1 离线预计算
#### 2.1.2 即席查询加速
### 2.2 HBase在Kylin架构中的位置
#### 2.2.1 HBase作为预计算数据的存储
#### 2.2.2 HBase与其他组件的交互
### 2.3 Cube与HTable的映射关系
#### 2.3.1 Cube的物理存储方式
#### 2.3.2 Rowkey的设计原则
#### 2.3.3 HTable中数据的组织

## 3.HBase优化实践
### 3.1 建模阶段的优化
#### 3.1.1 聚合组合与衍生维度
#### 3.1.2 Rowkey的散列与压缩
### 3.2 数据载入阶段的优化  
#### 3.2.1 Bulk Load的应用
#### 3.2.2 预分区的设置
#### 3.2.3 压缩算法的选择
### 3.3 存储格式与二级索引优化
#### 3.3.1 CuboidHFile与CubeoidHTable
#### 3.3.2 二级索引加速Lookup
### 3.4 读取与查询阶段优化
#### 3.4.1 Scan条件下推 
#### 3.4.2 Coprocessor的运用
#### 3.4.3 Region延迟加载

## 4.性能测试与优化效果评估
### 4.1 测试数据与场景
### 4.2 性能测试指标与工具
### 4.3 优化前后对比结果
#### 4.3.1 数据载入性能提升
#### 4.3.2 存储空间节省
#### 4.3.3 查询响应时间加速

## 5.HBase在Kylin中的管理
### 5.1 表的设计与预分区
### 5.2 负载均衡与数据分布
### 5.3 监控指标与告警阈值
### 5.4 灾难恢复与高可用保障

## 6.经验总结与最佳实践
### 6.1 HBase调优的关键点
### 6.2 生产环境注意事项 
### 6.3 Kylin与HBase协同优化

## 7.未来优化方向与展望
### 7.1 当前架构的局限性
### 7.2 Spark on HBase的可能性
### 7.3 实时OLAP场景的支持
### 7.4 更智能的数据分布与Sharding

## 8.附录 
### 8.1 Kylin与Druid的对比
### 8.2 常见问题解答
### 8.3 进一步阅读的参考资料

一直以来，大数据OLAP(Online Analytical Processing)查询面临着性能与灵活性的两难困境。传统的数据仓库采用预聚合与多维建模，提供了较好的查询性能，但灵活性较差。而Hadoop生态中各种即席查询产品如Hive、Impala、SparkSQL等，虽然支持了灵活的ad-
hoc查询，但性能与传统数仓相比仍有较大差距。

Apache Kylin应运而生，在预聚合与即席查询两个极端间寻求平衡。Kylin通过预计算多维立方体(Cube)，在提供亚秒级查询响应的同时，依然能支持高度灵活的多维分析。Kylin起源于eBay，经过多年发展已成为Apache顶级项目，在离线数仓OLAP领域独树一帜。

Kylin之所以选择HBase作为预计算结果的存储引擎，主要基于以下考虑：

1. 对超大数据集的支持：Kylin要存储的是各种维度组合的预聚合结果，数据量通常是原始明细数据的数十倍，HBase在海量数据存储方面有着良好的扩展性。

2. 基于Rowkey的快速查询：Kylin生成的Cube数据，按照Rowkey组织非常紧凑，通过Rowkey的前缀过滤和搜索可以极大加速查询，而这正是HBase所擅长的。 

3. 与Hadoop生态的集成：HBase构建在HDFS之上，能够与MapReduce、Spark等大数据处理框架无缝配合，便于Kylin进行ETL和Cube构建。

4. 业界成熟度：HBase是业界发展多年的开源分布式NoSQL数据库，在可靠性、性能、运维等方面都有充分实践。

可以看到，HBase与Kylin可谓是珠联璧合，它不仅仅是一个存储组件，更是发挥并放大了Kylin加速OLAP查询的优势。下面让我们一起来探索如何让这对CP发挥出最佳状态。

## Kylin查询流程与HBase的角色

为理解HBase在Kylin中的作用，我们先回顾一下Kylin的核心处理流程。Kylin主要包括两大类操作：

1. 离线预计算：这个过程从原始明细表中通过CubeBuilder完成数据聚合，建立多个Cuboid，也就是不同维度组合下的聚合结果。这些Cuboid将被存储到HBase中，成为Cube的物理载体。

2. 即席查询：当用户提交一个多维分析查询时，Kylin会根据查询的维度组合，去HBase中找出一个最匹配的Cuboid。由于预计算结果已经materialize了，查询只需简单的过滤与聚合，即可快速返回。

在离线预计算阶段，HBase主要扮演了预聚合结果的存储角色。Kylin通过Mapper将Cuboid分片写入对应的HBase表（通常称为HTable）中。每个Cuboid会对应一个HTable，而Cuboid内部的数据按Rowkey有序存储，形成一种多维数组式的结构。

在即席查询阶段，HBase则提供了快速检索与过滤数据的能力。Kylin从上层cube元数据中找到最优Cuboid后，会将查询请求转化为一个或多个针对HTable的Scan操作。这些Scan通过设置startRow和stopRow来圈定Rowkey的查找范围，再叠加一些过滤条件，最终Coprocessor将数据聚合并返回给Kylin。

由此可见，HBase作为Kylin的数据存储核心，其性能高低直接决定了整个系统的表现。下面我们就来系统地讨论在Kylin场景下对HBase的优化。

## 建模阶段的HBase优化

Kylin建模涉及Cube的设计，这里的很多决策都会影响到底层HBase的效率。

### 聚合组合与衍生维度

Kylin的Cube会基于一个事实表和多个维度表定义。为了获得最佳的查询性能，我们要尽可能地将常用的维度组合预先计算出来。这里有两个需要权衡的点：

1. 预计算的维度组合越多，占用的存储空间就越大。但从查询的角度，我们希望命中更多的查询组合。
2. 有些维度虽然不常用，但对应的基数(Cardinality)非常大，如果都计算进去，会严重膨胀Cuboid的数量。

因此我们要对常见的查询做分析，识别出最频繁使用的维度组合。同时对那些基数过大的维度，可以考虑压缩或划分到更小粒度。比如将"用户ID"维度降低到"地区"粒度，显著减少Cuboid数量的同时，对许多分析场景已经够用。

### RowKey的散列与压缩

对于每个Cuboid，都需要将参与聚合的维度串联成Rowkey。一个好的Rowkey设计需要做到：

1. 保证数据均匀分布在各个Region上。由于Rowkey决定了数据的物理分布，如果Rowkey本身有偏斜（倾斜），会导致数据在Region上分布不均，最终造成热点问题。

2. 控制Rowkey的存储大小。虽然Rowkey可以容纳任意多维度，但Rowkey本身也会占用存储空间。过长的Rowkey一方面会浪费空间，另一方面在做Scan过滤时，也会多消耗CPU资源。

为此，Kylin在生成Rowkey时做了两点优化：

1. 对维度列的值做散列。这可以打散原来可能有序或集中的值分布。Kylin支持多种Hash函数，如MD5, Murmur等。

2. 对维度列的值做字典编码。Kylin会为每个维度值维护一个字典映射，将原始的String值转换成更紧凑的数值ID。在Rowkey中存储数值ID可以显著节省空间。

通过这些手段，Kylin在Rowkey设计上实现了很好的负载均衡与空间利用。最终每一个Cuboid会生成一个对应的HTable，数据按Rowkey切分到多个HBase Region中。

## 数据载入阶段的优化

当Cube定义就绪后，就要开始将原始数据聚合并载入到HBase中。这个ETL过程对HBase本身也有很多优化点。

### Bulk Load的应用

对于初次载入或全量重算，数据量通常非常庞大。使用HBase提供的通用写入接口如Put，一条条插入数据会非常低效。因此Kylin利用了HBase的Bulk Load机制，通过外部导入直接生成HFile。整个过程分为两步：

1. 通过MapReduce job完成数据聚合，将结果输出为HFile格式。每个Mapper负责聚合一部分Cuboid的数据，并针对每个Cuboid生成一个HFile。

2. 当所有HFile就绪后，通过HBase的LoadIncrementalHFiles接口将它们直接加载到对应的Region中。

相比普通写入，Bulk Load省去了HBase端的写入合并 (minor compaction)，因此可以获得数倍的吞吐量提升。同时对集群的资源消耗也要小得多。

### 预分区的设置

HBase为了方便数据的水平扩展，默认采用了自动分区(auto sharding)的方式。当一个Region中的数据量超过阈值，就会触发分裂(split)。但对于Kylin来说，我们其实可以根据数据特征预先设定分区。

通过预分区，可以直接将数据发布到目标Region上，减少了自动分裂的发生。由于每次分裂都会造成一定的IO开销，因此尽量避免是一个好的实践。此外分区数量的设置也是一个优化点。太少的分区可能造成单个Region数据量过大，而过多的分区又会增加整体的管理开销。通常建议的分区数量在数十到数百个Region之间。

### 压缩算法的选择

HBase支持多种压缩算法，可以在创建列簇(CF)时指定。由于Kylin的数据更新频率很低，使用高压缩率的算法更有优势。实践中，Snappy和zlib都是不错的选择。前者压缩速度较快，后者压缩率更高。

一般而言，对于需要频繁小范围查询的(如按日分区的)Cuboid，其Rowkey中往往包含日期维度，可以使用Snappy。而对于常做大范围聚合，Rowkey中不带日期的Cuboid，更推荐使用zlib。

## 存储格式与二级索引优化

### CuboidHFile与CubeoidHTable

前面提到每个Cuboid在HBase中存储为一个HTable。事实上，Kylin还做了进一步的存储优化，称为CuboidHFile。

CuboidHFile的思路是在生成HFile时，将经常一起访问的数据布局在一起，以提高查询时的读取效率。具体来说，Kylin为每个度量(Measure)也单独生成一个HFile，其中只包含该度量的值。查询时先扫描索引HFile确定Rowkey范围，然后再把多个度量的HFile做join，拼出完整的度量值。

相比直接使用HBase的CuboidHTable，这种策略有两点好处：
1. 不需要的度量数据不会被读取，避免无谓的IO。
2. 度量采用列式存储，有更好的压缩率。

当然，CuboidHFile也不是银弹，因为它引入了Join的开销。因此Kylin会根据数据的特点(Cuboid大小、度量数等)自动选择两种存储方式。

### 二级索引加速Lookup

有时用户的