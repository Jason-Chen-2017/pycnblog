
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


## ID3算法简介
ID3（Iterative Dichotomiser 3）算法是信息论中用于决策树学习的一套算法，由Donald E. Cover在1986年提出，其基本思想是用极大似然法估计每一个分枝的条件概率，然后通过迭代的方式，选取最优的特征、最佳的切分点，构造出一棵完美的决策树。目前ID3已经成为集成学习（如随机森林、Adaboost等）、机器学习、模式识别和数据挖掘等领域的一个重要工具。
## 决策树的应用场景
- 分类与回归问题
- 多标签分类问题
- 时序预测问题
- 推荐系统
- 情感分析
- 生物分类
- 图像处理

# 2.核心概念与联系
## 属性(attribute)
属性又称为特征或变量，它是一个用于描述事物的客观量，是现实世界中可以作为自变量影响因素的事物属性或者它们组合。决策树算法中的每个节点都对应于一种特定的属性，决策树通过这些属性划分空间（即特征空间），把输入的样本分配到叶子结点。
## 类(class)
类也叫做标记、目标或输出，是指能够被用来进行决策的事物的属性。在决策树算法中，目标变量往往是连续值，表示离散值的情况需要进行归一化处理。
## 父节点(parent node)
父节点是指该节点的直接前驱节点。
## 孩子节点(child node)
孩子节点是指父节点下面的子节点，也是决策树的分支结构之一。
## 内部节点(internal node)
内部节点又叫做非叶子节点，它至少有两个子节点。
## 叶子节点(leaf node)
叶子节点又叫做终端节点，它没有子节点。
## 路径长度(path length)
路径长度是指从根节点到当前节点之间的边数目，它反映了各个节点之间的相关性。
## 信息增益(information gain)
信息增益是用于评价选择某个特征来作为划分标准时，信息的损失程度。它计算的是源集合的信息熵与经过这个特征分割后的各个子集的信息熵的差值，单位信息熵表示无信息的状态。信息增益越大，意味着使用这个特征来进行分割后，信息的丢失越小；反之亦然。
## 信息增益比(gain ratio)
信息增益比是信息增益与划分前的熵的比值。当选择特征的同时还要考虑信息增益的大小，信息增益比更能体现特征的实际作用。
## 基尼系数(Gini impurity coefficient)
基尼系数衡量的是分类问题的不纯度。对于二分类问题来说，若样本点属于同一类别的概率越高，则基尼系数就越小，否则就越大。
# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
## 模型构建过程
首先，将训练集的数据按照特征维度分组，得到相应的特征向量。

对于每一层的节点（包括根节点、内部节点和叶子节点），根据信息增益最大或者最小作为划分依据。然后，对训练数据集进行遍历，根据划分结果将样本分配到相应的子节点。

在分配完毕之后，对于每一个子节点，根据样本是否为空，如果为空则置为叶子节点，反之设置为内部节点。

最后，递归地对内部节点进行相同的处理，直到所有的样本均分配到叶子结点上。

## ID3算法推导及解释
### 信息熵与互信息
给定样本集D={x1, x2,..., xn}，其中xi∈X=(X1, X2,...,Xn)，Xi是Xi-th feature的取值集合，xij ∈ Xi，表示样本在第i个feature上的取值，由如下公式定义熵：
H(X)=−∑p(xi)log2p(xi)，其中pi=|Di|/N，|Di|为特征X在第i维上取值为xi的样本数，N为样本总数。
信息熵就是对所有可能的特征取值出现的概率进行加权求和，越有可能的取值得到的熵值越低。为了衡量两个随机变量之间的相似度，互信息又引入了KL散度，其中D表示样本集，X表示第i个特征，Y表示第j个特征。对于样本集D，第i个特征的取值集合为Xi={x1i, x2i,..., xni}，第j个特征的取值集合为Yj={y1j, y2j,..., ynj}，则KL散度定义为：
KLD(P||Q)=∑pi*pjlog(pi/qj)
互信息I(X; Y)=KLD(P(X,Y)||P(X)P(Y))，其中P(X,Y)表示给定X和Y的联合分布，P(X)表示X的先验分布，P(Y)表示Y的先验分布。
可以看到，信息熵和互信息都是衡量两个变量之间关联性的方法，但是两者又存在一些联系。
### ID3算法详解
#### 1.找到最佳划分特征
对每一个特征，计算出该特征的信息增益，并找出使得信息增益最大的那个特征作为划分特征。信息增益是指使得经过该特征的划分后，类条件概率分布熵的减少值。具体的计算方法如下：

设C是特征的不同取值所对应的类别集合，比如{A,B,C}，令F为特征，那么特征F的信息增益公式如下：
Gain(F)=-∑[P(Ci)*H(Di)]/N, where Di is the set of examples with feature F equal to a particular value vi and Ci is the corresponding class label for this subset of examples.

其中，N为样本数；P(Ci)是特征F不同取值对应的类别Ci的概率；H(Di)是特征F不同取值对应的集合Di的经验熵。

#### 2.分裂子结点并递归建立决策树
当选定了最佳划分特征，就可以基于该特征的值分割数据集，并且创建叶子结点，最后在子结点继续以上两步，直到满足停止条件。具体地，根据最佳划分特征，将数据集分成若干子集，并为每个子集赋予相应的类别；对于这些子集，根据相同的划分特征递归调用函数，直到子集内只剩下一个样本，此时该子集成为叶子结点。当数据集的所有样本都分配到叶子结点后，就可以停止建树，返回决策树根节点。
#### 3.选择停止条件
最常用的停止条件是信息增益小于某个阈值（通常设置为0），或者剪枝（即对某些结点进行合并，消除掉那些不会影响决策树结果的子树）。
#### 4.例题分析
#### 例1：用西瓜数据集构造决策树
西瓜数据集包含6种属性，其中色泽(cp)、根蒂(rt)、敲声(sh)、纹理(sc)、脐部(sp)、触感(fl)五个属性，每个样本对应一种品质的西瓜。假设目标是区分好瓜和坏瓜。已知训练集，利用ID3算法构建决策树。

步骤：

1. 计算数据集的经验熵H(D)：

    H(D)=-[(30+7)/6*log2((30+7)/6)- (20+4)/6*log2((20+4)/6)- (23+3)/6*log2((23+3)/6)- (17+5)/6*log2((17+5)/6)- (19+3)/6*log2((19+3)/6)- (16+6)/6*log2((16+6)/6)]
    =0.979

2. 计算每一个特征的信息增益：

    Gain(cp)=-(30/6*0 + 7/6*1)/6*log2((30/6*(30+7)/6)+(7/6*(20+4)/6))/6=0.091
    
    Gain(rt)=-(20/6*1 + 4/6*0)/6*log2((20/6*(30+7)/6)+(4/6*(20+4)/6))/6=0.213
    
    Gain(sh)=-(23/6*0 + 3/6*1)/6*log2((23/6*(30+7)/6)+(3/6*(20+4)/6))/6=-0.064
    
    Gain(sc)=-(17/6*0 + 5/6*1)/6*log2((17/6*(30+7)/6)+(5/6*(20+4)/6))/6=0.272
    
    Gain(sp)=-(19/6*1 + 3/6*0)/6*log2((19/6*(30+7)/6)+(3/6*(20+4)/6))/6=-0.015
    
    Gain(fl)=-(16/6*1 + 6/6*0)/6*log2((16/6*(30+7)/6)+(6/6*(20+4)/6))/6=0.334
    
3. 选取信息增益最大的特征rt作为划分特征，根据该特征对数据集进行分割：

    好瓜集合：{(4,"Yes","Weak","Round","Straight","Smooth"),
               (3,"No","Weak","Peaked","Spiny","Fat"),
               (5,"No","Strong","Cupped","Calyptic","Flaky")}
    
    坏瓜集合：{(2,"Yes","Medium","Crenated","Buckled","Soft"),
              (1,"Yes","Weak","Crusty","Slippery","Smooth"),
              (6,"Yes","Weak","Plain","Musky","Zingy")}
    
    将数据集再次划分，对于好瓜集合，根据色泽(cp)有两种取值，分别为"Yes"和"No",形成两个子集，对于两个子集，递归构造子树。对于"Yes"的子集，根据根蒂(rt)有两种取值，分别为"Weak"和"Strong",分别形成两个子集，对于两个子集，递归构造子树。对于坏瓜集合，根据根蒂(rt)有两种取值，分别为"Weak"和"Strong",分别形成两个子集，对于两个子集，递归构造子树。
    
停止条件：在整个数据集中只剩下单个样本，此时该样本就是叶子结点。