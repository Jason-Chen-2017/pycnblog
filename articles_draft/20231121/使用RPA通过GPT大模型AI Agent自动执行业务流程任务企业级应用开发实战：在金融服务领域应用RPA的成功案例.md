                 

# 1.èƒŒæ™¯ä»‹ç»


è¿‘å‡ å¹´ï¼Œäººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼‰ã€æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰ã€è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰ç­‰æŠ€æœ¯åœ¨å„ä¸ªè¡Œä¸šè“¬å‹ƒå‘å±•ï¼Œç»™ä¼ä¸šæä¾›äº†å·¨å¤§çš„å•†ä¸šä»·å€¼ã€‚è€ŒRPAæŠ€æœ¯åˆ™å¯ä»¥åˆ©ç”¨AI Agentå®Œæˆå„ç§é‡å¤æ€§ã€è€—æ—¶çš„å·¥ä½œï¼Œå¤§å¹…ç¼©çŸ­ä¼ä¸šæµç¨‹å¤„ç†æ—¶é—´ï¼Œæå‡å·¥ä½œæ•ˆç‡ã€‚
åœ¨é‡‘èæœåŠ¡é¢†åŸŸï¼ŒRPAåº”ç”¨çš„æˆåŠŸæ¡ˆä¾‹å¾ˆå¤šã€‚æ¯”å¦‚ï¼Œç”±äºé«˜æˆæœ¬çš„ç†è´¢äº§å“å®¡æ‰¹è¿‡ç¨‹ï¼ŒæŸé“¶è¡Œæ¨å‡ºäº†åŸºäºRPAçš„è‡ªåŠ¨åŒ–å·¥å…·ï¼Œé€šè¿‡æ‰«æäºŒç»´ç ã€çŸ­ä¿¡éªŒè¯ç ç­‰æ–¹å¼è¿›è¡Œç†è´¢äº§å“å®¡æ‰¹ï¼ŒèŠ‚çº¦äº†å®¡æ‰¹äººå‘˜çš„æ—¶é—´æˆæœ¬ï¼›å†å¦‚ï¼Œè‡ªåŠ¨åŒ–è¥é”€è½¯ä»¶ï¼Œé€šè¿‡è·Ÿè¸ªå®¢æˆ·åé¦ˆã€æœç´¢çƒ­ç‚¹è¯é¢˜ã€æ”¶é›†åª’ä½“æ•°æ®ç­‰æ–¹å¼å¯¹å®¢æˆ·ç¾¤ä½“è¿›è¡Œè¥é”€å’Œäº’åŠ¨ï¼Œæå‡è¥é”€æ•ˆæœï¼›è¿˜æœ‰å°±æ˜¯åœ¨é‡‘èæ”¯ä»˜é¢†åŸŸï¼Œåˆ©ç”¨æœºå™¨å­¦ä¹ ç®—æ³•å¯¹é“¶è¡Œè´¦å•è¿›è¡Œæ™ºèƒ½åˆ†æï¼Œè¯†åˆ«é£é™©å¹¶åŠæ—¶è¿›è¡Œæ¬ºè¯ˆäº¤æ˜“ä¿æŠ¤ã€‚
ä½†æ˜¯åœ¨ä¼ä¸šçº§åº”ç”¨ä¸­ï¼Œå¦‚ä½•å°†RPAæŠ€æœ¯åº”ç”¨åˆ°ä¸šåŠ¡æµç¨‹ä¸­ï¼Œå¹¶é€šè¿‡ä¼˜åŒ–å‚æ•°ï¼Œä½¿å…¶è¾¾åˆ°æœ€ä¼˜æ•ˆæœï¼Œæ˜¯ä¸€ä¸ªéš¾ç‚¹ã€‚
ä¸ºäº†æ›´å¥½åœ°æŒæ¡å’Œç†è§£ä½¿ç”¨RPAæŠ€æœ¯è‡ªåŠ¨æ‰§è¡Œä¸šåŠ¡æµç¨‹ä»»åŠ¡ï¼Œæˆ‘ä»¬ä»¥é“¶è¡Œå¡ä¸­å¿ƒçº¿ä¸ŠåŠç†ä¸šåŠ¡æµç¨‹ä¸ºä¾‹ï¼Œå°†å®æ–½è¿‡ç¨‹ä¸­çš„å…³é”®ç¯èŠ‚å’ŒæŠ€æœ¯éš¾ç‚¹æ¢³ç†æ¸…æ¥šï¼Œå¸Œæœ›èƒ½æŠ›ç –å¼•ç‰ï¼Œä¿ƒè¿›æ›´å¤šä¼ä¸šèƒ½å¤Ÿä»äº‹RPAåº”ç”¨å¼€å‘ã€‚
# 2.æ ¸å¿ƒæ¦‚å¿µä¸è”ç³»
## 2.1 RPAï¼ˆRobotic Process Automationï¼‰
RPA æ˜¯ä¸€ç§é€šè¿‡è®¡ç®—æœºæ¨¡æ‹Ÿäººçš„æ“ä½œè¡Œä¸ºï¼Œé€šè¿‡å›¾å½¢ç•Œé¢æŒ‡ä»¤æ§åˆ¶è½¯ä»¶æ¥å®ç°è‡ªåŠ¨åŒ–ï¼Œå¸®åŠ©å…¬å¸è§£å†³é‡å¤æ€§ã€å¤æ‚çš„ä¸šåŠ¡æµç¨‹ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯ç”¨äººå·¥æ›¿ä»£äººå·¥ï¼Œä»è€Œæå‡å·¥ä½œæ•ˆç‡å’Œé™ä½æˆæœ¬ã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜ã€å¯æ‹“å±•æ€§å¼ºã€é€‚åº”æ€§å¼ºã€‚ä¾‹å¦‚ï¼Œé“¶è¡Œå¡ä¸­å¿ƒçº¿ä¸Šä¸šåŠ¡æµç¨‹ï¼Œéœ€è¦å¤šä¸ªéƒ¨é—¨ååŒé…åˆæ‰èƒ½åŠç†å®Œæˆï¼Œå¦‚æœä¸€ä¸ªäººæ— æ³•èƒœä»»ï¼Œå°±ä¼šé€ æˆæ•ˆç‡ä¸‹é™ã€‚å› æ­¤ï¼Œä½¿ç”¨RPAå¯ä»¥è‡ªåŠ¨åŒ–è¿™ä¸€ç¹ççš„å·¥ä½œï¼Œè®©å¤šä¸ªéƒ¨é—¨èƒ½å¤Ÿå…±åŒå‚ä¸å…¶ä¸­ã€‚
## 2.2 GPT-3ï¼ˆGenerative Pre-trained Transformerï¼‰
GPT-3æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒé€šè¿‡å¤§é‡çš„æ–‡æœ¬æ•°æ®è®­ç»ƒè€Œæˆã€‚åŸºäºGPT-3ï¼Œä¼ä¸šå¯ä»¥åœ¨ä¸æä¾›ä»»ä½•æ˜ç¡®æ ‡ç­¾çš„æƒ…å†µä¸‹ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬é€šå¸¸å…·æœ‰ç‹¬ç‰¹çš„æ„ä¹‰æˆ–å«ä¹‰ã€‚
ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾æœ‰ä¸€ä¸ªä¸šåŠ¡éœ€æ±‚æ˜¯æ ¹æ®æŠ•èµ„è€…çš„éœ€æ±‚ï¼Œå†³å®šæŠ•èµ„æŸä¸ªé¡¹ç›®ã€‚é‚£ä¹ˆï¼Œå¯ä»¥ä½¿ç”¨GPT-3è‡ªåŠ¨ç”Ÿæˆä¸€ä»½æŠ•èµ„å»ºè®®æŠ¥å‘Šã€‚
## 2.3 AI Agent
AI Agentåˆç§°ä¸ºæ™ºèƒ½ä»£ç†ï¼Œæ˜¯æŒ‡å…·æœ‰æ™ºèƒ½åŠŸèƒ½çš„è‡ªåŠ¨åŒ–è½¯ä»¶æˆ–ç¡¬ä»¶è®¾å¤‡ï¼Œèƒ½å¤Ÿç‹¬ç«‹äºä¸»ä½“æ™ºèƒ½æ´»åŠ¨ï¼Œç‹¬ç«‹äºç¯å¢ƒè¿è¡Œï¼Œå…·å¤‡è‡ªå·±çš„è§„åˆ™å’Œé€»è¾‘ã€‚å½“é‡åˆ°ä»»åŠ¡æ—¶ï¼Œå¯ä»¥æŒ‰ç…§æ—¢å®šçš„è§„åˆ™ä½œå‡ºå†³ç­–ï¼Œå¹¶æ ¹æ®ç¯å¢ƒåŠ¨æ€è°ƒæ•´å…¶è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œå½“é“¶è¡Œå¡ä¸­å¿ƒçº¿ä¸Šä¸šåŠ¡æµç¨‹éœ€è¦æ”¶æ¬¾ï¼Œå¹¶ä¸”ç³»ç»Ÿå‘ç°æœ‰äººç›—åˆ·å¡ï¼Œæ­¤æ—¶ä¼šå‘ä¸Šæ¸¸åæ´—é’±æœºæ„æŠ¥è­¦å¹¶é‡‡å–è¡ŒåŠ¨ã€‚
## 2.4 æ™ºèƒ½å®¢æœ
æ™ºèƒ½å®¢æœï¼Œå³ä¼ä¸šå†…éƒ¨çš„å®¢æœç³»ç»Ÿå¯ä»¥ä¸ç”¨æˆ·æ²Ÿé€šï¼Œå¹¶ä¸ºç”¨æˆ·æä¾›ä¼˜è´¨çš„æœåŠ¡ã€‚è€Œé‡‡ç”¨RPAæŠ€æœ¯ï¼Œå°±å¯ä»¥ç”¨èŠå¤©æœºå™¨äººçš„æ–¹å¼æ¥è‡ªåŠ¨åŒ–é—®ç­”ã€‚è¿™æ ·ï¼Œå¯ä»¥å¤§å¤§å‡å°‘äººå·¥å®¢æœäººå‘˜çš„å·¥ä½œé‡ï¼Œæé«˜å·¥ä½œæ•ˆç‡å’Œå®¢æˆ·æ»¡æ„åº¦ã€‚
# 3.æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦æ¨¡å‹å…¬å¼è¯¦ç»†è®²è§£
## 3.1 æ•°æ®è·å–ä¸æ–‡æœ¬é¢„å¤„ç†
é¦–å…ˆï¼Œéœ€è¦æ”¶é›†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åº”è¯¥åŒ…å«äº†æ‰€æœ‰ç›¸å…³é¢†åŸŸçš„æ•°æ®ï¼Œå¦‚é“¶è¡Œå¡ä¸­å¿ƒçº¿ä¸Šä¸šåŠ¡æµç¨‹ç›¸å…³çš„äº¤æ˜“æ•°æ®ã€å®¢æˆ·ä¿¡æ¯ã€äº¤æ˜“æµæ°´ç­‰ã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡æ–‡æœ¬é¢„å¤„ç†çš„æ–¹æ³•ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºæœºå™¨å¯è¯»çš„è¾“å…¥ã€‚è¿™ä¸€æ­¥æ¶‰åŠåˆ°å°†åŸå§‹æ–‡æœ¬æ•°æ®å¤„ç†æˆæœºå™¨å¯è¯»çš„è¾“å…¥å½¢å¼ã€‚
## 3.2 æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
æ¥ç€ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½¿ç”¨GPT-3æ¨¡å‹ä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œä½¿ç”¨å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œæ¥è®­ç»ƒè¯¥æ¨¡å‹ã€‚è¿™ä¸€æ­¥è¦æ±‚è®¾ç½®è®­ç»ƒè¶…å‚æ•°ï¼Œé€‰æ‹©åˆé€‚çš„è®­ç»ƒè½®æ¬¡ï¼Œæ¥ä¿è¯æ¨¡å‹çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ã€‚
## 3.3 æ¨¡å‹æ”¹è¿›
ç»è¿‡æ¨¡å‹è®­ç»ƒä¹‹åï¼Œå¯ä»¥è§‚å¯Ÿæ¨¡å‹è¾“å‡ºçš„ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦æ»¡è¶³æˆ‘ä»¬çš„è¦æ±‚ã€‚å¦‚æœæ¨¡å‹é¢„æµ‹çš„ç»“æœä¸å®é™…æƒ…å†µå­˜åœ¨åå·®ï¼Œå¯ä»¥å°è¯•è°ƒæ•´æ¨¡å‹çš„å‚æ•°æˆ–è€…æ·»åŠ æ–°çš„é¢„æµ‹ä»»åŠ¡ï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
## 3.4 æµ‹è¯•
æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œå¹¶æ¯”è¾ƒä¸åŒæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ã€‚å¦‚æœæ¨¡å‹çš„æ•ˆæœä¸ä½³ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´å¥½çš„é¢„è®­ç»ƒæ¨¡å‹æ¥æ›¿æ¢å½“å‰æ¨¡å‹ã€‚
# 4.å…·ä½“ä»£ç å®ä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜
## 4.1 æ•°æ®è·å–ä¸æ–‡æœ¬é¢„å¤„ç†
```python
import requests

def get_data():
    url = "https://banking-customer-service.com/api/transactions"
    response = requests.get(url)

    transactions = []
    for transaction in response.json()['transactions']:
        if not transaction['approved'] and not transaction['declined']:
            # only keep unapproved or declined transactions
            transactions.append(transaction)
            
    return transactions
    
def preprocess(text):
    text = re.sub('\d+', 'NUMBER', text)   # replace digits with special token NUMBER
    text = re.sub('[.,!?]', '.', text)    # replace punctuation marks with.
    
    words = nltk.word_tokenize(text)        # tokenize the sentence into words
    pos_tags = nltk.pos_tag(words)          # assign part of speech tags to each word
    lemmas = [nltk.WordNetLemmatizer().lemmatize(t[0], t[1].lower()) for t in pos_tags]  # lemmatize the words based on their POS tag
    
    return lemmas
    
transactions = get_data()
preprocessed_transactions = [preprocess(t['description']) for t in transactions]
```
## 4.2 æ¨¡å‹è®­ç»ƒä¸é¢„æµ‹
```python
from transformers import pipeline, set_seed

set_seed(42)     # fix random seed for reproducibility

nlp = pipeline('text2text-generation')

model_inputs = [["Hello I am a bank customer service bot. How can you assist me today?",
                  f"Thanks for contacting us. Please provide more information about your {t['type']}."
                 ] for t in preprocessed_transactions]

outputs = nlp(model_inputs, max_length=30, num_return_sequences=1)

for i, output in enumerate(outputs):
    print("Input:", model_inputs[i][0])
    print("Output:", output[0]['generated_text'], "\n")
```
## 4.3 æ¨¡å‹æ”¹è¿›
```python
from transformers import AutoModelForCausalLM, Trainer, TrainingArguments

class CustomTrainer(Trainer):
    def training_step(self, model, inputs):
        labels = inputs.pop("labels")
        outputs = model(**inputs)

        loss = self.args.label_smoothing * (
            -((labels[:, :-1] + labels[:, 1:]) / 2).log_softmax(dim=-1).sum(-1).mean()
        ) + torch.nn.functional.cross_entropy(
            outputs.logits.view(-1, outputs.logits.size(-1)), labels[:, 1:].contiguous().view(-1), ignore_index=self.tokenizer.pad_token_id
        )

        return loss

custom_lm = AutoModelForCausalLM.from_pretrained(
    'gpt2'
)

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    do_train=True,                   # train model flag
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=64,  # batch size per device during training
    save_steps=500,                  # save checkpoint every X updates steps
    evaluation_strategy="steps",     # evaluate at each X update steps
    eval_steps=500,                  # evaluate at each X update steps
    logging_dir='./logs',            # logs directory
)

trainer = CustomTrainer(
    custom_lm,                         # the instantiated ğŸ¤— Transformers model to be trained
    training_args=training_args,       # training arguments, defined above
    train_dataset=[list(zip(*inp)) for inp in zip([["Hello"], ["How are you"]]*len(preprocessed_transactions))]
)

# Train the model
trainer.train()
```
## 4.4 æµ‹è¯•
```python
from datasets import load_metric

rouge = load_metric("rouge")

predictions = []
references = []

for pred in outputs:
    predictions.append(pred[0]['generated_text'].split("."))
    references.append(f"{'. '.join([' '.join(preprocess(t['description'])) for t in transactions[-1:]])}.".split("."))

scores = rouge.compute(predictions=predictions, references=references)

print(scores)
```