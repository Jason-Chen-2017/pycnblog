
作者：禅与计算机程序设计艺术                    

# 1.简介
  

随着机器学习领域的不断发展，新的评估指标层出不穷，且涵盖面非常广泛。本文旨在系统地总结和梳理机器学习中的各类模型评估指标，并为读者呈现一个全面的、完整的视图。

为了给读者提供更好的阅读体验，本文将按以下的结构进行组织：第一节介绍评估指标的分类方法；第二节重点介绍了回归问题中最常用的几种评估指标；第三节则详细介绍了分类问题中常用的指标；第四节介绍了异常检测问题中常用的评估指标；第五节介绍了多标签分类问题中常用的评估指标；最后一节介绍了多任务学习问题中常用的评估指标。每章节后都将附上推荐参考资料，方便读者学习和了解更多。

# 2. 评估指标分类及其应用范围

评估指标(Evaluation Metrics)是衡量模型优劣、评价模型结果的重要工具。不同的问题又会使用不同的评估指标。因此，首先需要对评估指标进行分类。
根据模型类型、输入输出以及监督方式的不同，评估指标可以分为：

1. 监督学习模型的评估指标

   （1）回归问题——均方误差(Mean Squared Error)(MSE)，平均绝对误差(Average Absolute Error)(MAE)。
   （2）二分类问题——准确率(Accuracy)，精度(Precision/PPV)，召回率(Recall/TPR)，F1得分(F1 Score)。
   （3）多分类问题——多类别准确率(Micro-averaged Accuracy)，宏查全率(Macro-averaged Precision/PPV)，微查全率(Micro-averaged Recall/TPR)，F1得分(F1-score)。
   
2. 概率估计模型的评估指标

   （1）二分类问题——ROC曲线(Receiver Operating Characteristic Curve)、AUC(Area Under the Curve)。
   （2）多分类问题——每个类别的ROC曲线的平均值(Macro-average ROC curve)或整体ROC曲线(Weighted Macro-average ROC curve)。

3. 无监督学习模型的评估指标

   （1）聚类问题——轮廓系数(Silhouette Coefficient)。
   （2）降维问题——投影误差(Projected Errors)、确定系数(Determination Coefficient)。

4. 异常检测模型的评估指标

   （1）异常判别率(False Discovery Rate)(FDR)，精准率(Precision/PPV)，召回率(Recall/TPR)，F1得分(F1 Score)。
   （2）损失敏感度(Loss Sensitivity)、特异性(Specificity)、灵敏度(Sensibility)、鲁棒性(Robustness)。

5. 多标签分类模型的评估指标

   （1）一对所有(One-vs.-All)策略——平均准确率(Micro-averaged Precision/PPV)，宏查全率(Macro-averaged Recall/TPR)，F1得分(F1-score)。
   （2）标签平均方法(Label Average Method)——多个标签的加权平均，也可以用其它算法得到单个标签。
   （3）学习到联合概率分布的半监督学习模型——F1 score、EMD距离。
   
6. 多任务学习模型的评估指标

   （1）指标集合——任务相关的指标集合。例如，交叉验证的平均损失(Cross-validation Loss Average)，各任务之间的相似性(Task Similarity)，稀疏表示学习的模型性能(Model Performance with Sparse Representations)。
   
# 3. 回归问题的评估指标

## 3.1 MSE与MAE

### 3.1.1 MSE（Mean Squared Error，均方误差）
均方误差又称为“回归平方和误差”(Squared Error)，是一个回归问题中常用的评估指标。

$$ MSE=\frac{1}{n}\sum_{i=1}^n(y_i-\hat y_i)^2 $$

其中$y_i$是真实值，$\hat y_i$是预测值，$n$为样本容量。当$n$较大时，该指标对异常值影响很小。

### 3.1.2 MAE（Mean Absolute Error，平均绝对误差）

平均绝对误差(MAE)是回归问题中另一种常用的评估指标。它计算各预测值的绝对值之和再除以样本容量，单位是原始数据的单位。

$$ MAE=\frac{1}{n}\sum_{i=1}^n|y_i-\hat y_i| $$

同样，MAE也对异常值有一定的抗性，但是当预测值离真实值较远时，MAE仍然可能过高。

### 3.1.3 小结

MSE与MAE都是回归问题中的两种常用评估指标，但MSE还可以通过对样本值做变换来修正模型对异常值的敏感性。

## 3.2 二分类问题的评估指标

## 3.2.1 准确率(Accuracy)、精确率(Precision/PPV)、召回率(Recall/TPR)、F1得分(F1 Score)

准确率(Accuracy)、精确率(Precision/PPV)、召回率(Recall/TPR)、F1得分(F1 Score)是分类问题中常用的评估指标。它们分别代表的是：

准确率: $ Acc=\frac{TP+TN}{TP+FP+FN+TN} $
精确率(PPV): $ PPV=\frac{TP}{TP+FP} $
召回率(TPR): $ TPR=\frac{TP}{TP+FN} $
F1得分(F1 Score): $ F1=\frac{2\times precision \times recall}{precision + recall} $ 

其中，TP(True Positive，真正例）、FP(False Positive，假正例）、FN(False Negative，假负例）、TN(True Negative，真负例）分别表示正确预测的个数、错误预测为正的个数、错误预测为负的个数、正确预测为负的个数。

另外，还有其他一些指标如：

- 混淆矩阵(Confusion Matrix)
- 曲线下面积(AUC，Area Under the Curve)

这些指标在不同的场景下都有不同的作用。

### 3.2.2 宏查全率(Macro-averaged Precision/PPV)、微查全率(Micro-averaged Recall/TPR)、F1-Score

对于多分类问题，上面介绍的准确率(Accuracy)、精确率(Precision/PPV)、召回率(Recall/TPR)、F1得分(F1 Score)等指标是不是就可以满足呢？

答案是否定的。原因是上述指标都只能看出某一类别的信息，对于多分类问题，这几种指标无法区分各个类的重要程度，因此，要计算全局的表现需要一些技巧。

一般来说，宏查全率(Macro-averaged Precision/PPV)、微查全率(Micro-averaged Recall/TPR)以及F1-Score三种指标都是用于衡量多分类器的全局表现的。

**宏查全率**

宏查全率(Macro-averaged Precision/PPV)是所有类别的平均值。

$$ Macro-averaged Precision = \frac{\sum_{k=1}^{K}(precision_k \times P_k)} {\sum_{k=1}^{K}P_k} $$

其中，K为类别数量；$P_k$为第k类的正负样本数量；$precision_k$为第k类的精确率。

**微查全率**

微查全率(Micro-averaged Recall/TPR)是全局的衡量标准。

$$ Micro-averaged Recall = \frac{\sum_{k=1}^{K}(recall_k \times N_k)} {\sum_{k=1}^{K}N_k} $$

其中，N为样本总数量。

**F1-Score**

F1-Score(Harmonic Mean of Precision and Recall)也是全局的衡量标准。

$$ F1-Score = \frac{2}{\frac{1}{precision}+\frac{1}{recall}} $$

注意，以上三个指标都可以使用不同形式的形式化表达式来求取。

### 3.2.3 分类报告(Classification Report)

Scikit-learn提供了classification_report()函数来自动生成classification report。这个report包含了各种性能指标，包括精确率(Precision/PPV)、召回率(Recall/TPR)、F1-score，以及支持向量机(support vector machine, SVM)使用的类别间的多类别平均值等。使用如下的代码可以生成classification report。

```python
from sklearn import metrics

target_names = ['class A', 'class B'] # class names for target variable
y_true = [0, 1, 2, 2, 2] # true labels
y_pred = [0, 0, 2, 2, 1] # predicted labels

print("Classification report:\n")
print(metrics.classification_report(y_true, y_pred, target_names=target_names))
```

输出如下所示的classification report。

```
              precision    recall  f1-score   support

    class A       0.50      1.00      0.67         1
    class B       1.00      0.33      0.50         2

    accuracy                           0.60         3
   macro avg       0.75      0.67      0.60         3
weighted avg       0.75      0.60      0.59         3
```

### 3.2.4 小结

准确率(Accuracy)、精确率(Precision/PPV)、召回率(Recall/TPR)、F1得分(F1 Score)、宏查全率(Macro-averaged Precision/PPV)、微查全率(Micro-averaged Recall/TPR)以及F1-Score都是分类问题中常用的评估指标。