
作者：禅与计算机程序设计艺术                    
                
                
## 概述
随着通信技术的发展，信息传输越来越便捷、高速，这对社会生活也产生了积极影响。传统的信息传输模式，是通过机械或电气设备将信号或数据从源头传递到目的地，但随着信息量的增加，通信系统面临更复杂、更稳定的环境要求。因此，为了提高信息传输的效率、安全性及成本效益，人们渐渐转向了无线技术。无线通信中，主机间的信息传输依赖于随机过程（又称“熵减”）。

在这个过程中，由于信道带宽受限、传播不确定性等原因，信息的比特流在传输过程中发生变化、丢失、延迟。随机过程是信息传输和处理的基础。本文将阐述信息论中的随机过程、随机事件、熵、KL散度、香农熵、最大似然估计以及隐马尔可夫模型、条件熵等相关概念、算法以及应用。

# 2.基本概念术语说明
## 定义1：随机变量Random Variable（RV）
设X是一个实验，其结果可取的集合记做$X=\{x_1,x_2,\cdots,x_n\}$,其中x∈X表示不同的结果。则称实验X的结果x具有连续分布，记做$P(X=x)=p(x), x\in X$, p(x)为概率密度函数。若实验X的结果是离散的，即x可取值的个数不是无穷多，而只限于有限个值，则称实验X的结果x具有离散分布。随机变量的概率分布可以用概率质量函数或概率密度函数表示。如图所示。
![image](https://github.com/WenlongShen/random-process/blob/main/picture/QQ%E5%9B%BE%E7%89%8720220220235337.png?raw=true)

## 定义2：随机事件Random Event（RE）
设A是由随机变量X的取值构成的一个子集。则称A为随机事件。随机事件A上的任何事物都与其发生的概率密度无关。A上的所有事件等可能发生。则表示为P[A]=∑_{x∈A}p(x)。随机事件的概率表示为概率密度的积分。
## 定义3：联合分布Joint Distribution（JD）
设X和Y是两个随机变量，它们的联合分布可以表示为p(x,y)，其中每个元素p(x,y)表示的是当X取值为xi且Y取值为yj时，联合分布的概率。如图所示。
![image](https://github.com/WenlongShen/random-process/blob/main/picture/%E5%8C%BA%E9%97%B4%E5%BD%A2%E5%BC%8F-%E9%87%8D%E5%BA%86%E5%88%86%E5%B8%83.png?raw=true)

## 定义4：边缘分布Marginal Distribution（MD）
设X和Y是两个随机变量，它们的边缘分布分别表示为p(x)和p(y) ，其中每个元素表示的是在其他变量固定后，X或者Y出现的概率。其定义为：
$$p(x)=\sum_{y}p(x,y)$$
$$p(y)=\sum_{x}p(x,y)$$

## 定义5：条件分布Conditional Distribution（CD）
设X和Y是两个随机变量，它们的条件分布分别表示为p(x|y)和p(y|x) 。其中每个元素表示的是在已知另一个随机变量的情况下，该变量取值的概率。如果另一个随机变量的值是a，则对应概率为p(x|a)。其定义为：
$$p(x|y)=\frac{p(x,y)}{p(y)}$$
$$p(y|x)=\frac{p(x,y)}{p(x)}$$

## 定义6：独立性Independence（IN）
两个随机变量X和Y相互独立，指的是他们的联合分布和边缘分布之间的关系。如果$p(x,y)=p(x)p(y)$成立，则称X和Y相互独立。
## 定义7：期望Expectation（EV）
对于连续型随机变量X的随机变量，它表示的是其平均值。期望记做EX=E[X]。定义为：
$$EX=\int_{-\infty}^{\infty}xp(x)\mathrm{d}x$$
对于离散型随机变量X的随机变量，它表示的是各个取值的出现频率，定义为：
$$EX=\sum_{i=1}^{k}ip(x_i)x_i$$
其中，x_1,x_2,...,x_k为取值，i=1,2,...,k；p(x_i)为第i个值出现的频率。

## 定义8：方差Variance（VAR）
对于随机变量X，它的方差表示了其取值的离散程度，定义为：
$$VAR(X)=\sigma^2=\int_{-\infty}^{\infty}(x-\mu)^2p(x)\mathrm{d}x$$
其中，$\mu$表示均值，$\sigma^2$表示方差。方差反映了随机变量分布的散乱程度。方差越小，分布越集中；方差越大，分布越分散。

## 定义9：协方差Covariance（COV）
对于二维随机变量X和Y，协方差表示的是X和Y的相关程度。定义如下：
$$COV(X,Y)=\rho_{XY}=\frac{\operatorname E[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X \sigma_Y}$$
其中，$\mu_X$表示X的均值，$\mu_Y$表示Y的均值，$\sigma_X$表示X的标准差，$\sigma_Y$表示Y的标准差。协方差的绝对值越大，表明X和Y正相关；协方差的绝对值越小，表明X和Y负相关；协方差等于零时，表明X和Y不相关。

## 定义10：联合概率分布的矩（Moments）
设X和Y是两个随机变量，它们的联合分布可以表示为$p(x,y)$。则：

1.$\overline{X}$是X的均值，即$\overline{X}=E[X]$。

2.$Var(X)$是X的方差，即Var(X)=E[(X-\mu)^2]。

3.$(X-\mu)(Y-
u)$是X和Y的协方差，即cov(X,Y)=E[(X-\mu)(Y-
u)]。

4.$(E[X])^m$是X的m阶原点矩，即$(E[X])^m = \int_{\infty}^{+\infty} x^{m} p(x) dx$。

5.$(E[X Y])^{mn}$是X和Y的mn阶重心矩，即$(E[X Y])^{mn} = \int_{\infty}^{+\infty}\int_{\infty}^{+\infty} (xy)^{mn} p(x, y) dxdy$。

## 定义11：熵Entropy（ENT）
对于随机变量X，熵表示了随机变量出现的不确定性，或信息的累积度。定义如下：
$$H(X)=\operatorname E[-\log P(X)]=-\int_{-\infty}^{\infty}p(x)\log p(x)\mathrm{d}x$$
熵越大，表示随机变量的不确定性越大。
## 定义12：交叉熵Cross Entropy（XC）
给定联合分布$p(x,y)$和条件分布$q(x)$，条件熵的定义如下：
$$H(x|y)=-\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} q(x)p(x|y)\log p(x|y)\mathrm{d}x\mathrm{d}y$$
交叉熵表示不同分布之间的距离。
## 定义13：相对熵Relative Entropy（REL）
对于两组事件A和B，设$p(x)$表示事件A发生的概率，$q(x)$表示事件B发生的概率。相对熵定义如下：
$$D_    ext{KL}(p||q)=\int_{-\infty}^{\infty}p(x)\log \left(\frac{p(x)}{q(x)}\right)\mathrm{d}x$$
相对熵表示用较低分布q去拟合高分布p的困难程度。相对熵一般大于等于零，如果等于零，则用q分布完全拟合p分布。
## 定义14：KL散度Kullback-Leibler Divergence（KLD）
设$p(x)$表示分布P，$q(x)$表示分布Q，那么KLD定义如下：
$$D_{KL}(P||Q)=\int_{-\infty}^{\infty} p(x)\log \left(\frac{p(x)}{q(x)}\right)\mathrm{d}x$$
KLD衡量了两个分布之间的距离。KLD越大，表明分布Q与分布P越不匹配。
## 定义15：自然对数函数Natural Logarithm Function（LOG）
自然对数函数表示形式如下：
$$\ln(x)=\log _e x$$
## 定义16：香农熵Shannon’s entropy（SHAN）
给定随机变量X，其香农熵定义如下：
$$H(X)=\operatorname E[\ln P(X)]=-\int_{-\infty}^{\infty}p(x)\ln p(x)\mathrm{d}x$$
香农熵越大，表示随机变量的不确定性越大。
## 定义17：相对熵Relativistic Entropy（RELE）
相对熵也可以表示为KL散度的特殊情况，即相对熵定义如下：
$$R_{q,p}(\delta t)=\int_{-\infty}^{\infty} e^{-t \Delta f(x)}p(x)\mathrm{d}x=\int_{-\infty}^{\infty} e^{-t (\Delta H(p,q)+\delta_{pq})}\frac{p(x)}{q(x)}\mathrm{d}x$$
其中，$\delta_{pq}$表示两个分布是否相同，如果相同，则为1，否则为0。
## 定义18：最大似然估计MLE（Maximum Likelihood Estimation）
给定观测数据D={x1,x2,...,xn},最大似然估计用于求解参数估计问题，即确定模型的参数使得观测数据D的概率最大。定义如下：
$$\hat{    heta}=\underset{    heta}{\arg\max}\prod_{i=1}^np(x_i;    heta)$$

## 定义19：隐马尔科夫模型Hidden Markov Model（HMM）
假设状态序列X和观测序列Y都是一组隐藏的随机变量，并且存在一个状态转移矩阵A和观测概率矩阵B，使得X的下一状态和Y的当前观测值条件独立。定义如下：
$$X_k=A_kX_{k-1}+w_k$$
$$Y_k=B_kX_k+v_k$$
其中，$X=(X_1,X_2,\cdots,X_T)$,$Y=(Y_1,Y_2,\cdots,Y_T)$。

