
作者：禅与计算机程序设计艺术                    

# 1.简介
  

支持向量机(Support Vector Machine, SVM)是一种二类分类模型，它的基本想法是在空间中找到一个能够将所有点分开的超平面。其最著名的应用就是用于机器学习中的文本分类。SVM可以有效地解决小样本、非线性数据集、高维数据集上的复杂分类问题，并取得了很好的效果。

在传统的机器学习方法中，往往采用特征选择或是基于统计学习的方法进行特征提取，然后用逻辑回归、决策树等分类器对训练数据进行预测。而支持向量机是一种二类分类模型，它可以有效地处理非线性数据，并且通过核函数的方式可以有效地处理高维数据集。SVM的优点主要有以下几点:

1. 对于高维数据集来说，SVM可以有效地利用核函数将非线性映射到低维空间中进行建模，从而得到比较好的分类性能；
2. SVM可以在处理小样本数据时表现出很好的泛化能力，特别是对于那些存在较多噪声的数据集；
3. SVM是一个非常灵活的分类模型，可以根据不同的目标函数形式选用不同的损失函数和优化方式，可以对特征进行交叉验证，可以结合其他监督学习模型一起作为组合模型；
4. 在实践中，SVM经常被用于文本分类、图像识别、生物信息学和模式识别等领域。

# 2.基本概念术语说明
## 2.1 定义
支持向量机(Support Vector Machine, SVM)是一种二类分类模型，它的基本想法是在空间中找到一个能够将所有点分开的超平面。其最著名的应用就是用于机器学习中的文本分类。SVM可以有效地解决小样本、非线性数据集、高维数据集上的复杂分类问题，并取得了很好的效果。SVM的基本模型由两个最关键的组成部分构成：一是特征空间，包括输入数据的一个或多个观测变量及相应的属性值；二是超平面，它是特征空间的一条直线或者曲线，用来将特征空间划分为两个区域。


## 2.2 数据集与学习任务
### 2.2.1 数据集
给定一个数据集 $D=\{(x_i,y_i)\}_{i=1}^N$ ，其中 $x_i\in R^n$ 是输入变量（feature），$y_i\in \{-1,+1\}$ 是输出变量（label）。如果是分类问题，则输出变量等于 $-1$ 或 $+1$ 表示不同类的标记；如果是回归问题，则输出变量是一个实数值。

例如，假设输入变量是一个长度为 $d$ 的矢量，表示图像的像素值；输出变量则是图像类别，如猫或狗。图像数据集的大小一般远远大于数据集的数量，所以通常都不用一次性把所有的图像都载入内存。

### 2.2.2 学习任务
SVM的学习任务是求解最佳的超平面来划分数据集 $D$ 。为了做到这一点，需要确定正负实例的最优分离超平面。对于输入空间 $R^n$ 中的任意一点 $x$ ，它关于超平面的投影方向可以用下面的等式表示：
$$
f(x)=\text{sign}(w^Tx+\theta),
$$
其中 $w$ 和 $\theta$ 是超平面的法向量和截距项。

目标函数是使得正实例和负实例之间的距离最大化，即：
$$
\begin{split}
&\underset{\hat{w},\hat{\theta}}{\operatorname{max}}\quad &\sum_{i=1}^{N}\xi_i\\
&\text{s.t.}&&\forall i:\; y_i(\hat{w}^T x_i + \hat{\theta})\geq 1-\xi_i, \\
& &&\forall i:\; y_i(\hat{w}^T x_i + \hat{\theta})< -1+\xi_i,\; \xi_i \geq 0,i = 1,...,N.\\
\end{split}
$$
其中 $\hat{w}$, $\hat{\theta}$ 是使得目标函数极大化的参数。$\hat{w}$ 和 $\hat{\theta}$ 是SVM的判别函数，也称为超平面。注意：当样本点恰好落在分割超平面上时，目标函数的值可能是无穷大的，所以引入松弛变量 $\xi_i>0$ 来对约束条件进行“松弛”。$\xi_i$ 表示第 $i$ 个样本到分割超平面的距离。

## 2.3 核函数
### 2.3.1 引子
在一般情况下，如果输入空间 $X$ 不是欧氏空间，那么直接计算内积可能会遇到困难。举个例子，假设输入空间是椭圆空间 $E^{d}$ ，则内积一般不是满足结合律的。因此，需要引入核函数 $K(x,z)$ ，使得 $K$ 对称，且满足如下条件：
$$
K(x, z)\ge 0,~\forall~x,z.\qquad K(x,z)=K(z,x).
$$
这就要求核函数可以对称地计算任意两个输入向量之间的相似度。

### 2.3.2 核技巧
核技巧就是利用核函数来扩展特征空间。具体地，定义新的特征空间 $\phi(x)$ 为：
$$
\phi(x)=\left[\varphi(x_1),\varphi(x_2),\cdots,\varphi(x_n)\right]^T,
$$
其中 $\varphi:\mathbb{R}^n\rightarrow\mathbb{R}$ 是适合的核函数。

例如，对于径向基函数（radial basis function）核：
$$
K(x,z)=(1+\frac{\lVert x-z \rVert^2}{\sigma^2})^{\gamma}.
$$
$\gamma > 0$ 是系数，$\sigma$ 是缩放因子。

### 2.3.3 常用核函数
常用的核函数有多项式核、高斯核、字符串核、Laplacian 核等。
#### 2.3.3.1 多项式核
多项式核是将每个输入向量扩展为指数的项，常用参数为 $m$ ，定义为：
$$
K(x,z)=(\langle x,z \rangle+\sigma)^m.
$$
当 $m=2$ 时，多项式核变成了 linear kernel (线性核)，即：
$$
K(x,z)=\langle x,z \rangle.
$$
当 $m=3$ 时，多项式核变成了 polynomial kernel (多项式核)，又叫 Gaussian kernel (高斯核)。
#### 2.3.3.2 高斯核
高斯核是利用径向基函数的性质，将输入向量映射到高维空间，常用参数为 $\sigma$ （缩放因子）和 $\gamma$ （径向基函数参数）：
$$
K_{\text{gauss}}(x,z)=(1+\frac{\lVert x-z \rVert^2}{\sigma^2})^{\gamma}.
$$
当 $\gamma=0$ 时，高斯核退化为多项式核。
#### 2.3.3.3 字符串核
字符串核是一种串行匹配的核函数，假设输入向量的集合 $X=\{x_i\}_1^M$ 和 $Z=\{z_j\}_1^N$ 分别表示成字符串的集合，定义为：
$$
K_{\text{str}}(x,z)=\exp(-\gamma\|x-z\|^2_{add}),
$$
其中 $\|·\|_{add}$ 表示加权串行距离（weighted additive string distance）。
#### 2.3.3.4 Laplacian 核
Laplacian 核是另一种径向基函数，是高斯核的特殊情况，定义为：
$$
K_{\text{lap}}(x,z)=\exp(-\gamma\|x-z\|_{L}),
$$
其中 $\|·\|_{L}$ 表示 Lp 距离（Lp-norm distance）。

综上所述，核函数可以视为一种通过输入向量到特征空间的转换。它能够更好地刻画输入空间的相关性，帮助 SVM 把非线性问题转化为线性可分的问题。但是，过多的核函数并不能完全解决问题，因此，需要综合使用不同的核函数进行组合。

## 2.4 软间隔与硬间隔
### 2.4.1 软间隔与硬间隔
首先，考虑线性可分情形，即训练样本可以被划分到两类互斥的集合里。对于这样的训练数据集，目标函数可以由下面这种形式表示：
$$
\begin{split}
&\min_{w,b}\frac{1}{2}\|w\|^2+\frac{C}{N}\sum_{i=1}^{N}\xi_i\\
&\text{s.t.}&&\forall i: y_i(w^Tx_i+b)\geq 1-\xi_i,\\\
& &&\forall i:\; y_i(w^Tx_i+b)< -1+\xi_i,\;\xi_i \geq 0,i = 1,...,N.
\end{split}
$$
这里，$\frac{C}{N}$ 是正则化参数。当 $C=0$ 时，就是硬间隔SVM，否则就是软间隔SVM。

软间隔SVM 的目标函数要比硬间隔SVM 包含了惩罚项，即使某个样本点被错分，也会给予它一定的奖励，因此得到了更强的鲁棒性。但是，软间隔SVM 有更高的时间复杂度。如果希望得到更快的收敛速度，可以采用一些启发式的方法来选择待分的样本点，比如随机采样、贪心算法等。

### 2.4.2 使用核函数时的软间隔与硬间隔
线性不可分情形出现的时候，通过增加参数 $m$ 可以构造出一个新的核函数，使得它能够拟合非线性数据。具体地，当 $m=2$ 时，变为线性核：
$$
K(x,z)=\langle x,z \rangle.
$$
当 $m=3$ 时，变为多项式核：
$$
K(x,z)=(\langle x,z \rangle+\sigma)^m.
$$
如果使用核函数构造出新的目标函数，那么就要同时处理 $\phi$ 函数。这时候，目标函数可以变成：
$$
\begin{split}
&\min_{w,b}\frac{1}{2}\|w\|^2+\frac{C}{N}\sum_{i=1}^{N}\xi_i\\
&\text{s.t.}&&\forall i: y_i(\langle w,\varphi(x_i)\rangle+b)\geq 1-\xi_i,\\\
& &&\forall i:\; y_i(\langle w,\varphi(x_i)\rangle+b)< -1+\xi_i,\;\xi_i \geq 0,i = 1,...,N.
\end{split}
$$

## 2.5 序列最小最优化算法(SMO)
SMO 是一种启发式的序列最小最优化算法，它通过序列求解方法来寻找两类分类问题的最优解。具体地，每次迭代选取两个变量来固定住其他变量，从而达到优化目标的目的。在 SMO 中，变量 $a_i$ 和 $b_i$ 分别表示第 $i$ 个实例的支持向量的系数，且满足：
$$
y_i(w^Tx_i+b)-\frac{a_i}{\eta_i}-\frac{b_i}{\eta_i}=0,
$$
其中 $w$, $b$ 为 SVM 的超平面参数，$\eta_i=\sum_{j=1}^Ny_jy_jK(x_i,x_j)$ 是第 $i$ 个训练实例的松弛变量。换言之，这个等式保证了第 $i$ 个实例处于正确类别边界上，且在该边界上只有支持向量才起作用。

迭代过程如下：

1. 从训练集中任选两个样本点 $i, j$ ，选取使得 $y_iy_j$ 大的 $i$ 和 $\max\{0, a_i+\alpha_ix_i^Ty_jx_j\}$ 小的 $j$ 。

2. 用以上参数来求解两个新变量的更新值：
   $$
    \alpha_i:= \frac{y_i(wx_i+b)+\eta_i-\alpha_i\eta_i}{2\eta_i}\\
    \alpha_j:= \frac{y_j(wx_j+b)+\eta_j-\alpha_j\eta_j}{2\eta_j}
   $$

   如果 $\eta_i\leq\eta_j$，即 $\alpha_j=0$ ，则直接更新：
   $$
    b := b + y_ib(\\alpha_j-\alpha_i)\frac{x_j}{||x_j||}\\
    a_i := a_i+\alpha_i\\
    a_j := a_j-\alpha_j
   $$
   如果 $\eta_j\leq\eta_i$，则先更新：
   $$
    b := b + y_jb(\\alpha_j-\alpha_i)\frac{x_j}{||x_j||}\\
    a_j := a_j+\alpha_j
   $$
   再更新：
   $$
    b := b + y_ib(\\alpha_j-\alpha_i)\frac{x_i}{||x_i||}\\
    a_i := a_i+\alpha_i
   $$

3. 更新超平面的参数：
   $$
    w := \sum_{i=1}^Na_iy_ix_i\\
    b := (b-\sum_{i=1}^N\alpha_iy_ix_i^Tw)/\rho
   $$
   其中 $\rho$ 是 Lagrange 乘子，用于求解凸二次规划问题。

4. 如果误差减少很小，则停止，否则返回到第二步继续迭代。

最后，对于剩余的实例 $k$ ，判断它们的 $a_k$ 是否非零。如果非零，则选择 $x_k$ 使得 $a_k$ 最大，否则判断 $x_k$ 是否能成为支持向量，并加入到 $\alpha$ 序列中。

综上所述，SMO 通过局部选择和后续跟踪来逐步逼近全局最优解，从而获得快速的收敛速度。