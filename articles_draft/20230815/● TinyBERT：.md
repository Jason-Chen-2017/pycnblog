
作者：禅与计算机程序设计艺术                    

# 1.简介
  


TinyBERT 是一种 BERT（Bidirectional Encoder Representations from Transformers）的小型版本。主要目的是减少模型大小，并降低其推理时间。

为了提高效率，现有的 BERT 模型都采用浅层编码器结构，即 Transformer 中的第一阶段 encoder 和第二阶段 decoder。但是这样造成了模型复杂度很高，因此作者在该项目中探索了两种方案，尝试将深层编码器结构应用于较小规模的模型中：

① 移动网络：移动网络将整个 BERT 模型分割成两部分，第一部分就是前面所说的浅层编码器结构，第二部分是一个可训练的神经网络，用于辅助后面的预测任务。这种方式可以使得模型获得更大的收敛能力，同时也更有效地利用特征向量空间中的全局信息。

② 从词窗角度看待输入序列：由于词汇表长度一般比较大，因此作者将整个输入序列看作一个词窗，并进行滑动窗口的操作，每次只关注当前词和一定范围内的上下文。作者发现这种做法可以降低模型复杂度和推理时间，并且在保持精度的情况下取得了更好的性能。

此外，作者还设计了其他一些机制来进一步优化模型的性能：

③ 连续性注意力（Continuous attention）：对每个词或者词窗的注意力分布进行平滑处理，避免出现单个词或词窗的注意力过大或者过小的问题。

④ 深度因子化（Depth-wise convolutions）：对词窗中的元素进行卷积操作，以便更好地捕获不同位置之间的关系。

⑤ 句向量拼接（Sentence vector concatenation）：将不同层的特征向量拼接起来作为最终输出，而不是简单地取最后一层的输出。

⑥ 跨层连接（Cross-layer connections）：通过层间连接的方式来引入不同层的注意力信息。

⑦ 长期记忆单元（Long-term memory units）：在第三阶段的输出中加入长期记忆单元，以实现更长的距离感知。

综合以上几点，作者基于 BERT 的原始架构开发了一个小型版 TinyBERT，其主要特点如下：

1) 轻量级模型，计算资源占用较低；

2) 良好的推理速度；

3) 较优秀的性能指标。

值得注意的是，TinyBERT 尚处于实验阶段，仍处于不断完善、优化过程之中。

# 2.基本概念术语说明
## 2.1 Transformer 架构

Transformer 由两个模块组成——encoder 和 decoder。这两个模块由多层的自注意力机制和点式基于位置的前馈网络组成。自注意力机制负责建模输入序列的局部依赖关系，点式基于位置的前馈网络则可以捕获输入序列中全局的依赖关系。

### 2.1.1 编码器

编码器模块的目标是生成固定维度的输出表示 z ，其中 z = f(x)。encoder 首先将输入序列 x 通过词嵌入层得到输入 embedding ，然后输入到第一层自注意力机制中进行计算。第一层自注意力机制将输入embedding与上文embedding、下文embedding连接并通过线性变换得到查询 q 和键 k 。然后，q、k 会被加权求和，并经过门控线性单元激活函数得到 Attention Weights a 。之后，Attention Weights a 将输入 embedding 加权求和得到表示 o 。

每层自注意力机制的输出会被加权求和后输入到第二层自注意力机制中，且第二层自注意力机制接收三个输入参数：表示 o，第一层自注意力机制的输出结果，以及未经过第一层自注意力机制的输入。

经过第三层自注意力机制后，编码器的输出会被传入第四层点式前馈网络中，该网络会将表示 o 通过多个卷积核进行处理，并通过残差结构和层归一化处理最终的输出表示 z 。

### 2.1.2 解码器

解码器模块的目标是在给定输入序列条件下生成输出序列 y 。解码器的输入包括代表输入序列的表示 z 和代表已生成的输出序列的隐藏状态 h_t-1 。解码器首先会对输入的表示 z 进行一次线性变换，以得到一个初始的隐状态 h_0 。随后的每一步解码都会更新 h_t-1 和 h_t 的状态，h_t 是由当前时刻的输入 token 和 h_t-1 状态共同决定。解码器的更新规则如下：

h_t = g(h_{t-1},y_{t-1},c_t)，其中 c_t 为编码器的输出，g 是一个递归神经网络。

当前步的输出 token y_t 是通过采样概率分布 p_t(i|h_t) 对从各个可能的 i 中选出一个，并将当前隐状态 h_t 和所有已生成的输出序列 y_{<t} 和 c_t 输入至概率分布计算模块中得到的。p_t(i|h_t) 的计算公式如下：

p_t(i|h_t) = softmax(v^t * tanh(W[k]*[h_t;c_t] + W[q]*embed(i)))

v^t 表示门控线性单元的参数，k 和 q 分别是关键字向量和查询向量。tanh 函数和乘积的符号都是按以下方式计算：


其中 \* 表示矩阵乘法。

在确定当前步的输出 token 时，解码器使用最大似然估计来拟合概率分布，即选择使得在训练集上的损失最小的输出 i 。

# 3.核心算法原理和具体操作步骤以及数学公式讲解

## 3.1 移动网络


移动网络将模型分成两个部分：浅层编码器、移动端辅助网络。移动端辅助网络根据输入序列及相应的位置，直接计算注意力分布，再输入到前馈网络中完成分类或回归任务。移动端辅助网络在速度、资源消耗等方面都有很大优势。

当输入序列较短时，移动网络能够极大地减少计算量，仅仅需要做简单的一次前馈即可完成整个任务。例如，对于序列分类任务，在输入较短的文本序列时，移动端辅助网络可以帮助完成任务。

## 3.2 从词窗角度看待输入序列


从词窗角度看待输入序列，相比于用整个序列处理的方式，可以减少模型的复杂度和推理时间，还能提高模型的准确性。这里作者把输入序列分成 n 个词窗，每次只考虑当前词和一定范围内的上下文。这样做的原因是，当输入序列较长时，如果将整个序列作为整体输入到模型中，可能会导致内存耗尽或显存爆炸。而作者发现，每隔几个词抽取出一个词窗并进行处理，效果不会太差。而且作者观察到，每隔几十个词抽取一个词窗，模型的推理速度也会提升不少。

## 3.3 连续性注意力


为了解决单词级别的注意力调控过于局限的问题，作者设计了连续性注意力机制。它的原理是在每一层的注意力分布中，除了首尾词的注意力分数都设置为零，中间词的注意力分数可以由其左右两个词组成的窗口决定。这样就可以增强模型对于词汇和语法之间的依赖关系的理解。

## 3.4 深度因子化


深度因子化旨在减少模型的计算量。在正常的卷积层中，卷积核会在完整的输入张量上进行扫描，这就会增加计算量。而深度因子化卷积层中，卷积核只扫描输入张量的一部分，这就节省了计算量。不过，这样会丢失一些图像信息，因此深度因子化卷积层并不是完全没有损失。

## 3.5 句向量拼接


由于不同层的特征向量通常具有不同的空间分布特性，因此不能直接拼接起来作为输出。但如果以全连接的方式连接不同层的特征向量，那么特征维度过高，网络难以学习复杂的特征相关性。因此，作者采用串联的方式，在输出特征向量之前，对不同层的特征向量求均值和标准差，再输入一个线性变换层，得到的特征向量才可以输入到下游任务中。

## 3.6 跨层连接


跨层连接的意义在于引入不同层之间的注意力机制，提升模型对于全局上下文的适应性。这里，作者利用两个自注意力机制将不同层的注意力输出进行连接。

## 3.7 长期记忆单元


长期记忆单元的目的是让模型在较远距离上具有更强的表征能力。它通过在第三阶段的输出中加入 LSTM 或 GRU 单元，可以学习长期依赖信息。这里作者引入 LSTM 单元作为长期记忆单元，可以学习到全局的时间序列特征。

# 4.具体代码实例和解释说明

## 4.1 输入输出形式

TINYBERT 有以下输入输出形式：

- **input**: 一个句子（sequence）组成的序列。
- **output**: 一句话情感预测结果或者一段文本摘要。

## 4.2 数据格式

TINYBERT 使用的数据格式为 tfrecord 文件，压缩后大小约为 20G。tfrecord 文件中存储的包含句子及标签的二进制数据。数据格式参考 tensorflow 中定义的 protobuffer 格式。

## 4.3 模型配置

TINYBERT 模型的配置如下：

```json
{
    "attention_probs_dropout_prob": 0.1,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 128,
    "initializer_range": 0.02,
    "intermediate_size": 512,
    "max_position_embeddings": 512,
    "num_attention_heads": 4,
    "num_hidden_layers": 4,
    "type_vocab_size": 2,
    "vocab_size": 30522
}
```

其中 `attention_probs_dropout_prob`、`hidden_dropout_prob` 和 `hidden_size` 是模型超参数。
`vocab_size` 表示输入序列的词典大小。
`num_hidden_layers` 表示模型的层数。
`num_attention_heads` 表示注意力头的数量。
`hidden_size` 表示每个注意力头输出的维度。
`intermediate_size` 表示 FFN 神经元的大小。
`max_position_embeddings` 表示输入序列的最大长度。
`type_vocab_size` 表示输入序列的类型数，本例为 2（句子类型、段落类型）。
`initializer_range` 表示模型参数初始化的方差。
`hidden_act` 表示非线性激活函数。