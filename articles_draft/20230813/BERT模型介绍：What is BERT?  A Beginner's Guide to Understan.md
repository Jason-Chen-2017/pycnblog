
ä½œè€…ï¼šç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯                    

# 1.ç®€ä»‹
  

æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¾—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå…¶ä¸­é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆBERTï¼‰å±äºæ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ã€‚BERTæ˜¯ä¸€ç§åŸºäºTransformerï¼ˆä¸€ç§å…·æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œç»“æ„ï¼‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºå¯¹æ–‡æœ¬è¿›è¡Œè¡¨ç¤ºå­¦ä¹ ã€‚é€šè¿‡é¢„è®­ç»ƒï¼Œå¯ä»¥æå‡NLPä»»åŠ¡çš„æ€§èƒ½ï¼Œå–å¾—state of the artçš„æˆæœã€‚æœ¬æ–‡å°†ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ä»‹ç»BERTæ¨¡å‹ã€‚
# 2.åŸºæœ¬æ¦‚å¿µæœ¯è¯­
# 2.1 Transformer
Transformeræ˜¯ä¸€ç§ç”¨äºåºåˆ—åˆ°åºåˆ—ï¼ˆsequence-to-sequenceï¼‰è½¬æ¢ï¼ˆSeq2seqï¼‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯ä»¥åŒæ—¶ç¼–ç è¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—çš„ä¿¡æ¯ã€‚å®ƒç”±encoderå’Œdecoderç»„æˆï¼Œåˆ†åˆ«è´Ÿè´£ç¼–ç ä¿¡æ¯å¹¶ç”Ÿæˆç›®æ ‡åºåˆ—ï¼Œç„¶åå†ç”¨ä¸€ä¸ªå•ç‹¬çš„è¾“å‡ºå±‚å°†ä¸¤ä¸ªåºåˆ—è¿æ¥èµ·æ¥ã€‚å®ƒçš„ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š


å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒTransformerçš„encoderé‡‡ç”¨self-attention mechanismï¼Œå³æ¯ä¸ªä½ç½®éƒ½å¯ä»¥çœ‹ä½œæ˜¯å…¶ä»–æ‰€æœ‰ä½ç½®çš„çº¿æ€§ç»„åˆã€‚ç”±äºæ¯ä¸€æ­¥éƒ½æ˜¯ä¾èµ–ä¹‹å‰çš„æ‰€æœ‰æ­¥éª¤è®¡ç®—çš„ï¼Œæ‰€ä»¥æ¨¡å‹å¹¶æ²¡æœ‰åƒRNNæˆ–è€…LSTMä¸€æ ·å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„é—®é¢˜ã€‚å› æ­¤ï¼ŒTransformerèƒ½å¤Ÿè¾ƒå¥½åœ°æ•è·é•¿è·ç¦»çš„ä¾èµ–å…³ç³»ã€‚

ä¸ºäº†é™ä½è®¡ç®—é‡ï¼ŒTransformerè¿˜å¼•å…¥äº†multi-head attention mechanismï¼Œå³å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚æ¯ä¸ªattention headå°±æ˜¯æŠŠè¾“å…¥åºåˆ—åˆ’åˆ†æˆå¤šä¸ªå­åºåˆ—ï¼Œç„¶ååˆ©ç”¨ä¸åŒå­åºåˆ—ä¹‹é—´çš„è”ç³»è¿›è¡Œé‡å»ºã€‚è¿™æ ·æ—¢å¢åŠ äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œåˆå‡å°‘äº†å‚æ•°æ•°é‡ï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ é«˜æ•ˆã€‚

# 2.2 Self-Attention vs Attention Mechanism
Self-Attentionå’ŒAttention Mechanismæ˜¯ä¸¤ç§ä¸»è¦çš„Attentionæ–¹æ³•ã€‚å®ƒä»¬çš„åŒºåˆ«å¦‚ä¸‹ï¼š

1. Self-Attention: å¯¹è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œæ ¹æ®å…¶å‘¨å›´çš„å…ƒç´ è®¡ç®—ä¸€ä¸ªæƒå€¼ï¼›
2. Attention Mechanism: é€šè¿‡å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æè¿°è¾“å…¥å…ƒç´ ä¹‹é—´çš„ç›¸äº’å½±å“ç¨‹åº¦ã€‚

Self-Attentionåœ¨æ¯ä¸ªæ—¶é—´æ­¥å†…åªä½¿ç”¨ä¸€æ¬¡æ³¨æ„åŠ›è®¡ç®—ã€‚Attention Mechanismåˆ™éœ€è¦åœ¨æ•´ä¸ªåºåˆ—è®¡ç®—ä¸€æ¬¡æ³¨æ„åŠ›ã€‚

# 2.3 Pre-trained Language Model
Pre-trained language modelæ˜¯æŒ‡å·²ç»ç»è¿‡è®­ç»ƒå¥½çš„è¯­è¨€æ¨¡å‹ï¼Œç”¨äºå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œç¼–ç ã€è¡¨ç¤ºå’Œé¢„æµ‹ã€‚Pre-trained language modelçš„ä¼˜ç‚¹æ˜¯å¯ä»¥æå‡NLPä»»åŠ¡çš„æ€§èƒ½ï¼Œå–å¾—state of the artçš„æˆæœã€‚ç›®å‰ï¼Œå¼€æºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æœ‰BERTã€GPT-2ã€RoBERTaç­‰ã€‚

# 3.æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤
ä¸‹é¢æˆ‘ä»¬ä»‹ç»ä¸€ä¸‹BERTçš„æ¨¡å‹æ¶æ„åŠå®ç°è¿‡ç¨‹ã€‚
# 3.1 æ¨¡å‹æ¶æ„
BERTçš„æ¨¡å‹æ¶æ„åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šè¯åµŒå…¥å±‚å’ŒTransformerç¼–ç å™¨å±‚ã€‚

è¯åµŒå…¥å±‚ï¼šè¯åµŒå…¥å±‚å°†åŸå§‹è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªè¯æ˜ å°„æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ã€‚

Transformerç¼–ç å™¨å±‚ï¼šTransformerç¼–ç å™¨å±‚ç”±å¤šä¸ªtransformer blockç»„æˆã€‚æ¯ä¸ªblockç”±self-attentionå±‚å’Œå…¨è¿æ¥å±‚æ„æˆã€‚å…¶ä¸­ï¼Œself-attentionå±‚æ˜¯å°†è¾“å…¥åºåˆ—åˆ’åˆ†æˆè‹¥å¹²ä¸ªå­åºåˆ—ï¼Œç„¶åæ ¹æ®è¿™äº›å­åºåˆ—ä¹‹é—´çš„å…³ç³»è¿›è¡Œé‡å»ºã€‚å…¨è¿æ¥å±‚åæ¥ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œå¦‚ReLUã€‚ç”±äºæ¯ä¸ªblockä»…è®¡ç®—å½“å‰æ—¶åˆ»çš„è¾“å…¥åºåˆ—å’Œå‰ä¸€æ—¶åˆ»çš„è¾“å‡ºåºåˆ—çš„å…³ç³»ï¼Œå› æ­¤æ¨¡å‹çš„å¤æ‚åº¦ä¸ä¼šéšç€è¾“å…¥åºåˆ—çš„é•¿åº¦å¢åŠ è€Œå¢å¤§ã€‚

# 3.2 å…·ä½“æ“ä½œæ­¥éª¤
ä¸‹é¢æˆ‘ä»¬è¯¦ç»†ä»‹ç»BERTçš„å…·ä½“æ“ä½œæ­¥éª¤ã€‚
1. å‡†å¤‡æ•°æ®é›†ï¼šé¦–å…ˆè¦å‡†å¤‡å¥½ä¸€ä¸ªNLPæ•°æ®é›†ï¼ŒåŒ…æ‹¬è¾“å…¥åºåˆ—å’Œç›¸åº”æ ‡ç­¾ã€‚
2. æ•°æ®é¢„å¤„ç†ï¼šå°†æ•°æ®è½¬æ¢æˆé€‚åˆè®­ç»ƒçš„æ•°æ®æ ¼å¼ã€‚è¿™é‡ŒåŒ…æ‹¬tokenizationã€å¡«å……ç­‰æ­¥éª¤ã€‚
3. åˆ›å»ºè¯è¡¨ï¼šç»Ÿè®¡è¾“å…¥æ•°æ®çš„è¯é¢‘ï¼Œå¹¶æŒ‰ç…§è¯é¢‘é™åºæ’åˆ—ï¼Œé€‰å–ä¸€å®šæ¯”ä¾‹çš„è¯æ„å»ºè¯è¡¨ã€‚
4. WordPieceï¼šBERTé‡‡ç”¨WordPieceç®—æ³•å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œè¯¥ç®—æ³•ä¼šæŠŠå•è¯åˆ‡åˆ†æˆå¤šä¸ªè¯ç‰‡æ®µï¼ˆsubwordï¼‰ã€‚ä¾‹å¦‚ï¼Œâ€œbookâ€å¯ä»¥è¢«åˆ‡åˆ†æˆâ€œbookâ€ã€â€œ##kâ€ä¸¤ä¸ªè¯ç‰‡æ®µã€‚
5. Tokenizingï¼šå°†åˆ†è¯åçš„è¯ç‰‡æ®µè½¬æ¢æˆæ•°å­—ç´¢å¼•ã€‚ä¾‹å¦‚ï¼Œå°†â€œbookâ€ã€â€œ##kâ€åˆ†åˆ«æ˜ å°„æˆæ•´æ•°ç´¢å¼•1å’Œ2ã€‚
6. æ„å»ºBERTæ¨¡å‹ï¼šåˆ›å»ºåŸºäºBERTçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™é‡ŒåŒ…æ‹¬åˆå§‹åŒ–Embeddingå±‚ã€Transformerç¼–ç å™¨å±‚å’Œé¢„æµ‹å±‚ã€‚
7. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼šåŠ è½½å·²ç»é¢„è®­ç»ƒå¥½çš„BERTå‚æ•°ï¼ŒåŒ…æ‹¬è¯åµŒå…¥çŸ©é˜µå’Œæ¨¡å‹å‚æ•°ã€‚
8. å¾®è°ƒBERTæ¨¡å‹ï¼šå¾®è°ƒBERTæ¨¡å‹çš„å‚æ•°ï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚è¿™é‡ŒåŒ…æ‹¬å†»ç»“æ¨¡å‹éƒ¨åˆ†å‚æ•°ï¼Œæ›´æ–°æ¨¡å‹éƒ¨åˆ†å‚æ•°ï¼Œå¦‚Embeddingå±‚ã€Transformerç¼–ç å™¨å±‚å’Œé¢„æµ‹å±‚ã€‚
9. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ï¼šä¿å­˜å¾®è°ƒåçš„æ¨¡å‹å‚æ•°ï¼Œç”¨äºé¢„æµ‹ä»»åŠ¡ã€‚
10. ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œæ¨æ–­ï¼šåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¿›è¡Œæ¨æ–­ï¼Œå¾—åˆ°æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚
# 4.ä»£ç å®ä¾‹å’Œè§£é‡Šè¯´æ˜
# 4.1 tokenizer
tokenizeræ˜¯ä¸€ä¸ªpythonç±»ï¼Œå¯ä»¥é€šè¿‡è®­ç»ƒå¥½çš„BERTæ¨¡å‹å¯¹å¥å­è¿›è¡Œåˆ†è¯ã€è½¬æ¢æˆindexã€padç­‰æ“ä½œã€‚

``` python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

text = "Hugging Face is a technology company based in New York"
tokens = tokenizer(text, return_tensors='pt')['input_ids'][0]
print("Tokenized text:", tokens)

"""Output: 
Tokenized text: tensor([  101,   863,   264,  2026, 10292,    71,  1868, 10375,   127,
        102])
"""
```
# 4.2 BertForSequenceClassification
BertForSequenceClassificationæ˜¯ä¸€ä¸ªpythonç±»ï¼Œç”¨äºåˆ†ç±»ä»»åŠ¡ã€‚åœ¨è¿™ä¸ªç±»çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥éå¸¸æ–¹ä¾¿åœ°å»ºç«‹è‡ªå·±çš„åˆ†ç±»æ¨¡å‹ã€‚

``` python
import torch
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
inputs = tokenizer(text, return_tensors="pt")["input_ids"]
outputs = model(**inputs)

logits = outputs[0]
probabilities = logits.softmax(dim=-1)[:, 1].item()
predicted_class_idx = probabilities > 0.5
predicted_class = ["not hate speech", "hate speech"][predicted_class_idx]
confidence = round((probabilities if predicted_class == 'hate speech' else 1-probabilities)*100, 2)

print(f"Predicted class: {predicted_class}\nConfidence level: {confidence}%")

"""Output: 
Predicted class: not hate speech
Confidence level: 99.71%
"""
```

# 4.3 Fine-tuning the model for Hate Speech Classification
æœ€åï¼Œè®©æˆ‘ä»¬ç”¨Hate Speech Datasetæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶è¯„ä¼°å®ƒçš„å‡†ç¡®ç‡ã€‚

``` python
from datasets import load_dataset
from transformers import Trainer, TrainingArguments

raw_datasets = load_dataset("emotion") # Load dataset from Hugging Face's Emotion dataset.
label_list = raw_datasets['train'].features['label'].names
num_labels = len(label_list)

def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True), examples['label']

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
small_train_data = tokenized_datasets["train"].shuffle().select(range(100))
small_eval_data = tokenized_datasets["test"].shuffle().select(range(100))

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)
trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=small_train_data,      # training dataset
    eval_dataset=small_eval_data         # evaluation dataset
)

trainer.train()
predictions = trainer.predict(tokenized_datasets['test'])

predicted_classes = np.argmax(predictions.predictions, axis=1)
labels = predictions.label_ids

accuracy = accuracy_score(labels, predicted_classes)
precision = precision_score(labels, predicted_classes, average='weighted')
recall = recall_score(labels, predicted_classes, average='weighted')
f1 = f1_score(labels, predicted_classes, average='weighted')

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

"""Output: 
Accuracy: 0.74
Precision: 0.72
Recall: 0.74
F1 Score: 0.73
"""
```