
作者：禅与计算机程序设计艺术                    

# 1.简介
  

不定系数线性方程组(ill-posed linear equations)是指系统中的解依赖于变量之间的相互作用，而不是简单地由方程左端的元素表示。当一个系统含有某些特定的参数时，它的解将不能用这些参数直接表示出来。通常情况下，求解不定系数线性方程组需要采用迭代法或其他解法，但其方法繁复且效率低下。此外，当要求精确的解时，求解不定系数线性方程组往往很困难。本文将介绍几种解不定系数线性方程组的方法。

2.基本概念及术语
首先，我们定义一些基本概念。

矩阵: 是对向量进行线性运算的一种方法，其中向量可以看成是一列或一行中的数字，而矩阵则是若干向量按照某种方式组合在一起。在矩阵中，每一个元素都可以看作是一个向量，而矩阵又可以看作是由多个这样的向量所组成。矩阵的大小一般表示为nxn。矩阵可以有不同的表示形式，如矩阵乘积、增广矩阵等。

线性映射: 是一种从域a到域b的函数f(x)，如果存在非负可积分基底(basis of integrals)，使得f(ax+by)=c(a, b)，其中a, b, c是任意标量，则称f为线性映射。

基(basis): 在多项式中，基就是指多项式中包含的独立的系数。在线性代数中，基就是指线性变换的基。

特征值与特征向量: 当线性方程组Ax=b中，A是一个矩阵，x是一个向量，那么A的特征值是指对于一个非零向量x，它是A的一个特殊解，即Ax=λx，λ是特征值，而A的特征向量是指满足Ax=λx的向量x，它也叫做主元(principal component)。

半正定(semi-definite): 如果矩阵A满足AxAT>=0，那么A为半正定的。如果矩阵A有特征值λi>0，并且对应的特征向量xi=(ei), i=1,2,…,n，那么A称为半正定实对称矩阵(semi-definite positive definite matrix)。

3.几种解不定系数线性方程组的方法
接着，介绍几种解不定系数线性方程组的方法。

3.1 Gauss-Jordan消元法
Gauss-Jordan消元法是最早被发现用于解线性方程组的算法之一。其基本思想是把方程组按行变换，使得每一行的第一个非零元素的值为1；然后再按行减去这个元素乘以之前所有元素的商，直至无穷小。这种处理方法被称为“初等行变换”，因为它把矩阵的每个行看作一个分数或根式。该方法十分高效，但是应用比较少，仅适用于某些特定情形。

设A为n阶方阵，b为长度n的一列，Ax = b的不定系数线性方程组。

1）建立增广矩阵
在给定方程组的条件下，先将右端的b扩充成关于已知变量的一阶形式：b' = (0,..., 0, b)

2）化成上三角型
将增广矩阵左乘相应的初等行变换矩阵P, P'^{-1}AP', 使之化为上三角矩阵AU，其中U是一个上三角矩阵，AU第j行对应于方程组的第j个未知数，从而将原始方程组变成如下形式：

U'*y = w, U'*v = v*，其中w为列向量，v为列向量

于是得到约束方程组Uy = w, Uv = v*

3）解线性方程组
求解约束方程组，得到向量u和v。令p=(1,...,1)^T，有x = u/v, y = P^(-1)x，此时得到未知数x的估计值。

4）计算残差
确定下一步迭代的增广矩阵AU'和待定向量w'，并根据方程组得出的估计值x，求出残差r = AU' - w'*v，并修正v以获得更好的估计值。重复以上三个步骤，直至残差的范数较小或达到指定误差范围。

实质上，Gauss-Jordan消元法每次只能解决一个线性方程组的求解，所以当方程组的个数超过一个时，还要依次求解，因此时间复杂度是O(k^3), k为方程组个数。

3.2 QR分解法
QR分解法也被称为Gram-Schmidt正交化法。其思路是把任意矩阵A分解成由列向量构成的正交矩阵Q和上三角矩阵R。具体方法是：

1）列向量归一化：取出A的列v，将v归一化为单位向量vj，且vj(A*vj)为非负，即vjA = jv, j=1,2,..,n

2）构造矩阵Q：将第1列vj作为第一列，用vj右乘A1(即第2~n列)，得到第一列vj的左乘矩阵Q1

3）构造矩阵R：用vj右乘A2，得到vj的左乘矩阵Q2，将Q1和Q2拼接起来，得到正交矩阵Q，A1拼接为对角矩阵D，最后将A的其它列逐列乘以Q，即可得到上三角矩阵R。

基于QR分解法，可以改进Gauss-Jordan消元法，得到更快捷、稳健的解法。具体方法是：

1）QR分解：将方程组Axb = wb进行QR分解

2）解线性方程组：用R将方程组转换成上三角形式，用QR的逆矩阵Q计算方程组的解

3）计算残差：重新计算wb与Aq的比值，修正Aq并继续QR分解和解算，直至残差较小或达到指定误差范围。

由于利用了QR分解，所以此法的时间复杂度小于O(k^3)，具有更优秀的性能。

3.3 Singular Value Decomposition (SVD)
SVD也被称为奇异值分解法。其思路是将任意矩阵A分解成三个矩阵U、Σ和V，其中U和V为正交矩阵，Σ是一个对角矩阵，对角线上的元素称为奇异值。具体方法如下：

1）构造矩阵M：将A按列放入M

2）进行奇异值分解：MM^T = U Σ V^T = U Λ V^T，Λ为对角矩阵，对角线上为奇异值

3）构建矩阵A': UΛV^T = A'^T

4）求解线性方程组：首先解UΛV^Tx = A'^Ty = Aw, 有Aw = U Λ V^T z = Az, 求解Az

5）计算残差：当方程组个数大于1时，求解另外的方程组Az与Aw比值，修正Aw以获得更好的估计值

同样，基于SVD的方法，可以改进Gauss-Jordan消元法。具体方法是：

1）SVD分解：将方程组Axb = wb进行SVD分解

2）解线性方程组：用Λ将方程组转换成对角形式，用Λ的逆矩阵U计算方程组的解，并检查残差是否小于指定误差范围

3）计算残差：重新计算wb与Ax的比值，修正Ax并继续SVD分解和解算，直至残差较小或达到指定误差范围。

由于利用了SVD分解，所以此法的时间复杂度小于O(k^3)，具有更优秀的性能。

3.4 Conjugate Gradient Method (CGM)
CGM也被称为共轭梯度法。其基本思想是在线性搜索方向上循环迭代，直至收敛或达到指定的最大迭代次数。具体方法如下：

1）初始化搜索方向d0 = b

2）计算Ax = d0，求出Ax=d0的解

3）求出残差r = b - Ax，注意此处的r不是真正的残差，只是一个初始值

4）计算下一步搜索方向dk = r + pk，pk为以pk满足梯度方向对目标函数值的降低最快的方向

5）计算Ax = dk，求出Ax=dk的解

6）更新搜索方向dk = r + pk

7）计算残差rn，并计算pk

8）重复第四步到第七步，直至残差rn足够小或者迭代次数达到限制

9）求出最佳解xk，即Ax=xk

由于该算法利用了梯度信息，因此收敛速度比梯度下降法快很多。

3.5 Least Squares Method (LSM)
LSM也被称为最小二乘法。其基本思想是选取一些基函数，在基函数展开之后用最小二乘拟合实际曲线。具体方法如下：

1）选取基函数：将各自变量按照线性组合的方式组成基函数

2）基函数展开：将模型曲线在基函数上展开，得到拉格朗日函数L(x;β)，其中β是待定参数

3）最小二乘拟合：将测量数据与拉格朗日函数的距离平方和最小，即找出β使得dL = ∑(yi−Li(xi;β))^2最小

4）求解线性方程组：将拉格朗日函数L(x;β)作为线性方程组，求出β的估计值

5）求解残差：当方程组个数大于1时，解除非线性关系后计算实际数据与预测数据的距离平方和作为残差，重复第三步到第五步直至残差足够小或达到指定误差范围

由于该算法使用了拉格朗日函数，因此理论上可以解决任意不定系数线性方程组，但是实现起来比较困难。