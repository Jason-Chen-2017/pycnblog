
作者：禅与计算机程序设计艺术                    

# 1.简介
  

分词（Segmentation）是自然语言处理（NLP）中最基础、最重要的一步。它的目标就是将待分析文本按句子、词、甚至单个字符进行切割，并标注其对应的语法结构信息。
词形还原（Morphological analysis/disambiguation）又称为词干提取或词形歧义消除。它的目标是根据给定的上下文环境，把同义词或近义词按照其在句子中的实际用法转化成标准形式（一般采用缩略语），同时消除上下文语境中可能影响理解的歧义信息。
一般来说，分词器和词形还原器可以一起使用。在做语料库建设时，首先用分词器切割文本，然后用词形还原器对每个词的词性标注，确保每一个单词都对应到唯一的词类和词性标签，从而方便后续的文本分类、聚类等任务。
下面，我们将介绍四种常用的分词器和词形还原器。
# 2.相关概念
## 分词
**分词（Segmentation）**是将文档、电子邮件、网页等非结构化文本转换为结构化的有意义的词序列过程，即把原始输入的语料分解为具有一定意义的短语或单词，是自然语言处理（NLP）的基础工作。中文分词是一个庞大的研究领域，其涉及语言学、计算机科学、统计学、信息检索、语音识别等众多学科，本文仅对中文分词的基本原理及基本方法进行介绍。

### 一、分词原理
分词的目的在于将复杂的、混乱的、不规整的、冗长的无意义的文字，切分出具有特定意义的词汇单元。简单的说，就是把连续的符号或字母组成的字符串，按照一定的规则变成单独的一个词或短语，并确定其位置、含义等属性。

分词的两种方法：

① **最大匹配法**：这是一种基于字典的方法，它通过扫描字典库来找出所有词的词尾，然后根据这些词尾来切割句子。这种方法简单、高效，但是对于一些特定的语言或文本比较困难；

② **状态模型法（Hidden Markov Model，HMM）**：这是一种基于概率图模型的方法，它以概率论的观点，认为一段文字由很多独立的事件（比如字母、词语等）组成，并且这些事件之间存在着一定的先后关系。因此，它把整个文本看作一个隐马尔可夫链，其中隐藏的节点代表了各种可能的字词，不同字词之间的转移概率则表示了出现某种字词的条件下，下一个字词的可能性。HMM 的训练比较复杂，但它考虑了更多的语言学、语法学、语音学等特征，能够更好地适应不同的文本。

总结一下，中文分词的两大原理是：① 最大匹配法；② HMM 模型。

### 二、中文分词方法
#### （一）词典分词方法
词典分词方法又称“字典树”分词方法、“正向最大匹配法”分词方法。该方法使用一个词表或词典文件作为分词参考，遍历词库找到所有的词的词尾（末端词），然后根据这些词尾来切割句子，直到句子的结束。例如：“今天是个好日子”，可以由“今天”、“是”、“个”、“好”、“日子”五个词的词尾切割得到。

词典分词法优点：

1.简单有效。

2.不需要训练，无需语言模型参数调整。

词典分词方法缺点：

1.切分结果可能不准确。

2.没有考虑到语言知识、语法结构，无法处理一些复杂的情况。

#### （二）字向量方法
字向量方法是基于词嵌入的统计学习方法，利用一组预先训练好的词向量，为中文分词提供依据。该方法通过比较字与字、词与词、字与词的相似性，将整个句子转换为一个向量表示。然后，用向量空间中的算法（如 K-means 或投影矩阵）来聚类，找出其中的中心词或字。

字向量方法优点：

1.分词准确率高。

字向量方法缺点：

1.需要事先训练好的词向量，计算量很大。

2.无法适应新型词、新颖语言。

#### （三）双向最大匹配法
双向最大匹配法（Bidirectional Maximum Matching，BMM）是一种结合词典分词和 HMM 模型的分词方法。它的基本思想是首先利用词典方法查找所有的词的词头和词尾，然后再使用 HMM 方法进行分词。

双向最大匹配法优点：

1.分词准确率高。

双向最大匹配法缺点：

1.训练时间长。

2.需要词典及词典内词典。

#### （四）条件随机场方法
条件随机场方法（Conditional Random Field，CRF）是一种用于序列标注问题的强大的机器学习方法。CRF 可以从训练数据中学习到序列中各个元素之间的联系，并在测试数据中使用学习到的模型进行序列标注。它的基本思想是构造一张概率函数来描述序列中各个元素的状态间的转移概率以及当前元素的发射概率。

CRF 分词方法的步骤如下：

1. 训练阶段：首先收集大量分词数据，包括分词真值、前后邻字构成等信息。

2. 模型建立阶段：根据分词训练数据构建概率模型，其中状态表示字词序列中的每个字，状态转移概率表示两个字词间的边缘概率，发射概率表示当前字词的条件概率分布。

3. 预测阶段：输入待分词序列，对每个字词应用 CRF 模型进行状态评估，输出字词序列的状态序列。

4. 解码阶段：根据状态序列重构分词结果。

CRF 分词方法优点：

1.分词准确率高。

CRF 分词方法缺点：

1.学习过程耗时。

2.需要训练数据。

# 3.常见的分词器及词形还原器
目前主流的中文分词器及词形还原器主要分为四类：
1.基于词典的分词器：包括 全模式分词器、双模式分词器、DAG 网络分词器、CRF 分词器、混合拼音分词器、字符级分词器。
2.基于字向量的分词器：包括 字向量方法分词器、词袋模型分词器。
3.基于规则的分词器：包括 词形还原器、最大匹配分词器、双向最大匹配分词器、提前终止分词器。
4.基于深度学习的分词器：包括 双向 LSTM-CRF 分词器、BERT 分词器。
## （一）基于词典的分词器
### 1.全模式分词器
全模式分词器(Full Segmentation)是指按照汉字编码顺序，一字一字地读取文本，直到遇到不在词典中的字或符号为止，然后生成相应的词组。它是最简单也最常用的一种分词器。但是全模式分词器由于只能分出单个字构成的词，所以分词效果差，且无法处理一些复杂的句子。如："你好，欢迎光临"经过全模式分词器的分词结果为："你","好", "，", "欢迎", "光临"。

### 2.双模式分词器
双模式分词器(Double-endded Morphological Analysis)是一种比较常用的中文分词方法。它采用双向最大匹配算法，可以在精确模式分词与粗糙模式分词之间取得折衷。它的基本思路是：首先，使用正向最大匹配算法找出所有词的词尾，然后，再使用逆向最大匹配算法找出所有词的词首。双模式分词器可以有效处理一些复杂的句子，并保证分词的正确性。

### 3.DAG 网络分词器
DAG 网络分词器(Directed Acyclic Graph Network for Chinese Word Segmentation)是一种比较复杂的中文分词方法。它使用的是依存句法结构，根据语义角色、并列关系、动宾关系等等因素进行词语划分。该分词器通过构建 DAG 网络，解决句子中存在的歧义问题，并确保分词的正确性。

### 4.CRF 分词器
CRF 分词器(Conditional Random Fields for Chinese Word Segmentation)是一种比较实用的分词方法。它使用 CRF 模型实现中文分词，通过统计语言学、语法学、语音学等特征对分词做出贡献。该分词器可以处理一些比较复杂的句子，并取得良好的分词效果。

### 5.混合拼音分词器
混合拼音分词器(Hybrid Pinyin-word Segmentation Method for Chinese Texts)是一种比较复杂的分词方法。它借助汉字到拼音的转换规则，将汉字与其对应的读音并联起来，这样就可以使用拼音识别来进行分词，获得较好的分词效果。

### 6.字符级分词器
字符级分词器(Character-based Chinese Word Segmentation)是一种比较简单和易于实现的分词方法。它直接把每个中文字符视为一个词语单位，并通过空格等符号来切分句子。这种分词方式速度快，分词结果准确。但是这种分词方法不能处理一些复杂的句子。如："大漠帝国"经过字符级分词器的分词结果为："大", "漠", "帝", "国"。

## （二）基于字向量的分词器
### 1.字向量方法分词器
字向量方法分词器(Word Vector-based Chinese Word Segmentation Method)是一种基于词向量的分词方法。它可以把每个汉字转换为固定维度的向量表示，然后在向量空间中进行聚类，找出中心词或字。该分词器不需要语料库，只需要词库文件和词向量文件即可进行分词。

### 2.词袋模型分词器
词袋模型分词器(Bag of words model for Chinese word segmentation)也是一种基于词向量的分词方法。它把汉字转化为固定大小的向量，每个向量中只有一个 1 ，其他都是 0 。该方法属于无监督学习方法，不需要任何领域知识。因此，词袋模型分词器适合于新闻、微博等领域的数据，因为它没有严格的语法规则。

## （三）基于规则的分词器
### 1.词形还原器
词形还原器(Morpheme analyzer or morpheme recognizer)是一种比较通用的分词方法。它把同义词或者近义词按照其在句子中的实际用法转化成标准形式，消除上下文语境中可能影响理解的歧义信息。其基本思想是将一个词与其同义词进行比较，找出其在同义词列表中的位置，确定该词的词形。如：“我喜欢吃苹果” -> “我 / VA 喜欢 / VOB 吃 / VV 苹果 / NR”。

### 2.最大匹配分词器
最大匹配分词器(Maximum matching Chinese Word Segmentation Method)是一种比较简单但是效率很高的分词方法。它基于词典，遍历所有词的词尾，然后根据这些词尾来切割句子，直到句子的结束。这种分词器属于朴素分词方法，它能够提升分词器的性能，但是分词效果不一定完全符合用户的需求。

### 3.双向最大匹配分词器
双向最大匹配分词器(Bidirectional maximum matching Chinese word segmentation method)是一种结合词典分词和 HMM 模型的分词方法。它的基本思想是首先利用词典方法查找所有的词的词头和词尾，然后再使用 HMM 方法进行分词。双向最大匹配分词器既可以找出词的正确的词尾，也可以找出词的正确的词头。

### 4.提前终止分词器
提前终止分词器(Pre-terminal Chinese word segmentor)是一种比较复杂的分词方法。它不是通过机器学习来判断是否应该终止，而是试图通过其他手段来避免错误的断句。这种方法通常会加大分词器的运行时间，导致分词效果不佳。

## （四）基于深度学习的分词器
### 1.双向 LSTM-CRF 分词器
双向 LSTM-CRF 分词器(Bidirectional Long Short Term Memory - Conditional Random Field (BiLSTM-CRF))是一种基于深度学习的分词方法。它结合了深度神经网络和条件随机场模型，利用 BiLSTM 网络的特征抽取能力和 HMM 模型的序列标注能力，来提升分词的准确率。它可以处理长文本，而且在序列标注任务上有着极高的准确率。

### 2.BERT 分词器
BERT 分词器(BERT-based Chinese Word Segmentation)是一种基于深度学习的分词方法。它使用预训练好的 BERT 模型对文本进行编码，然后把编码后的结果输入到序列标注模型中进行分词。该分词器的预训练可以有效地解决未登录词问题。