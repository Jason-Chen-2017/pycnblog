## 1.背景介绍

随着人工智能技术的不断发展，大型语言模型（Large Language Models, LLMs）已经成为自然语言处理领域的热点。这些模型通过在大量文本上进行预训练，学习到了丰富的语言知识和表达能力。它们不仅能够生成连贯的文本、回答问题、完成翻译等任务，还能够在一定程度上理解和回应用户的对话。本篇博客将深入探讨大型语言模型在对话能力方面的原理与工程实践，并对其性能进行评测。

## 2.核心概念与联系

### 2.1 大型语言模型（LLMs）

大型语言模型是一种基于深度学习的算法，它通过在大规模文本数据上进行预训练，学习到了语言的结构和语义信息。这些模型通常采用Transformer架构，具有很强的上下文理解能力和生成能力。

### 2.2 对话系统

对话系统是能够与人类或其他计算机系统进行交互的系统。在对话系统中，大型语言模型扮演着核心角色，负责理解和回应用户的输入。

### 2.3 对话能力的评测

对话能力的评测主要关注以下几个方面：
- **连贯性**：生成的回复是否逻辑上连贯、语法上正确。
- **相关性**：回复是否与用户的问题紧密相关，能否准确回答或引导至相关信息。
- **多样性**：在不同情境下，模型是否能生成多样化的回复，而不是简单重复。
- **情感因素**：回复是否能够体现出对用户的情感关怀。

## 3.核心算法原理具体操作步骤

### 3.1 预训练阶段

大型语言模型的预训练通常包括以下几个步骤：

1. **数据准备**：收集大规模的文本数据，并进行清洗和格式化处理。
2. **tokenization**：将文本分割成单词或子词（tokens）。
3. **模型选择**：选择合适的Transformer架构，如GPT系列。
4. **自监督学习任务设计**：定义预训练任务，如掩码语言模型（Masked Language Model, MLM）或下一句预测（Next Sentence Prediction, NSP）。
5. **模型训练**：在大量数据上进行模型的参数优化。
6. **模型微调**：针对特定任务，使用少量标注数据对模型进行进一步调整。

### 3.2 生成阶段

在给定输入的情况下，大型语言模型会根据预训练学到的知识生成回复。这一过程通常包括以下几个步骤：

1. **输入处理**：将用户的问题转换为模型可以接受的格式。
2. **解码器初始化**：初始化解码器的状态，包括位置编码和可能的提示词（prompt）。
3. **迭代生成**：通过不断预测下一个token并将其添加到输出序列中，直到达到最大长度或满足特定终止条件。
4. **后处理**：对生成的文本进行清理，如去除不连贯的序列、纠正拼写错误等。

## 4.数学模型和公式详细讲解举例说明

### 4.1 掩码语言模型（MLM）

在预训练阶段，MLM任务的核心思想是在输入序列中随机选择一些tokens作为掩码，并预测这些掩码token原本的内容。这可以通过以下公式表示：

$$
P(x_i | x_{<i}, x_{>i}) = \\frac{exp(\\hat{s}_{x_i})}{\\sum_{w \\in V} exp(\\hat{s}_w)}
$$

其中，$x_i$ 表示第 $i$ 个token，$V$ 是词汇表中的所有单词集合，$\\hat{s}_{x_i}$ 是模型预测的得分。

### 4.2 生成概率分布

在生成阶段，模型会基于当前已生成的序列 $\\mathbf{y}$ 和下一个token的概率分布 $P(y_{t+1}|\\mathbf{y})$ 来选择下一个token：

$$
y_{t+1} = \\arg\\max_{w \\in V} P(w|\\mathbf{y})
$$

其中，$y_{t+1}$ 是第 $t+1$ 个生成的token。

## 5.项目实践：代码实例和详细解释说明

### 5.1 模型选择与预训练

以下是一个简化的例子，展示了如何使用PyTorch实现GPT-2模型的预训练：

```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# 准备数据
texts = [\"Hello, my dog is cute.\", \"I have a cat.\"]
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)

# 训练模型
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
for epoch in range(10):
    outputs = model(**inputs)
    loss = outputs.loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### 5.2 生成回复

以下是一个简化的例子，展示了如何使用预训练的GPT-2模型生成回复：

```python
# 输入问题
prompt = \"What is your favorite color?\"
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# 生成回复
output = model.generate(input_ids, max_length=100, num_return_sequences=1)
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(response)
```

## 6.实际应用场景

大型语言模型在多个领域都有广泛的应用，包括但不限于：
- **客服机器人**：处理用户咨询，提供24/7的在线服务。
- **内容创作辅助**：帮助创作者生成文章、故事、诗歌等文本。
- **代码自动补全和调试**：辅助程序员编写和优化代码。
- **教育辅导**：为学生提供个性化的学习材料和解答问题。

## 7.工具和资源推荐

以下是一些有用的工具和资源，可以帮助读者更好地理解和实践大型语言模型：
- **Hugging Face Transformers库**：提供了多种预训练模型的实现和方便的API。
- **OpenAI API**：可以直接访问和使用OpenAI训练的大型语言模型。
- **Google AI Experiments**：展示了Google在人工智能领域的最新实验项目。

## 8.总结：未来发展趋势与挑战

随着计算能力的提升和数据量的增加，大型语言模型在未来将有更广泛的应用前景。然而，也存在一些挑战，包括：
- **数据偏见**：模型可能在某些特定领域或人群上有偏差，需要更多的多元化数据进行训练。
- **能源消耗**：预训练大型模型的能耗巨大，如何降低其环境影响是一个重要问题。
- **安全性**：模型可能生成误导性或不适当的内容，需要有效的过滤机制来确保安全。

## 9.附录：常见问题与解答

### Q1: 什么是大型语言模型？
A1: 大型语言模型是一种基于深度学习的算法，它通过在大规模文本数据上进行预训练，学习到了丰富的语言知识和表达能力。

### Q2: 如何评估一个对话系统的性能？
A2: 对话系统的性能通常从连贯性、相关性、多样性和情感因素等方面进行评估。

### Q3: 大型语言模型在生成回复时是如何工作的？
A3: 在生成回复时，模型会根据当前已生成的序列和下一个token的概率分布来选择下一个token，直到达到最大长度或满足特定终止条件。

---

作者：禅与计算机程序设计艺术 / Zen and the Art of Computer Programming

---

请注意，本文档是一个示例，实际内容可能需要根据实际情况进行调整和完善。在实际撰写过程中，可能需要进一步的研究和数据收集来支持观点，以及更深入的技术分析和实践经验。此外，文章中的代码示例仅用于说明目的，可能需要根据具体项目需求进行修改。最后，由于篇幅限制，本文未能涵盖所有相关主题和技术细节，读者应在此基础上继续扩展阅读和学习。大语言模型原理与工程实践：大语言模型的对话能力评测

## 1.背景介绍
随着人工智能技术的不断发展，大型语言模型（Large Language Models, LLMs）已经成为自然语言处理领域的热点。这些模型通过在大量文本上进行预训练，学习到了丰富的语言知识和表达能力。它们不仅能够生成连贯的文本、回答问题、完成翻译等任务，还能够在一定程度上理解和回应用户的对话。本篇博客将深入探讨大型语言模型在对话能力方面的原理与工程实践，并对其性能进行评测。

## 2.核心概念与联系
### 2.1 大型语言模型（LLMs）
大型语言模型是一种基于深度学习的算法，它通过在大规模文本数据上进行预训练，学习到了语言的结构和语义信息。这些模型通常采用Transformer架构，具有很强的上下文理解能力和生成能力。

### 2.2 对话系统
对话系统是能够与人类或其他计算机系统进行交互的系统。在对话系统中，大型语言模型扮演着核心角色，负责理解和回应用户的输入。

### 2.3 对话能力的评测
对话能力的评测主要关注以下几个方面：
- **连贯性**：生成的回复是否逻辑上连贯、语法上正确。
- **相关性**：回复是否与用户的问题紧密相关，能否准确回答或引导至相关信息。
- **多样性**：在不同情境下，模型是否能生成多样化的回复，而不是简单重复。
- **情感因素**：回复是否能够体现出对用户的情感关怀。

## 3.核心算法原理具体操作步骤
### 3.1 预训练阶段
大型语言模型的预训练通常包括以下几个步骤：
1. **数据准备**：收集大规模的文本数据，并进行清洗和格式化处理。
2. **tokenization**：将文本分割成单词或子词（tokens）。
3. **模型选择**：选择合适的Transformer架构，如GPT系列。
4. **自监督学习任务设计**：定义预训练任务，如掩码语言模型（Masked Language Model, MLM）或下一句预测（Next Sentence Prediction, NSP）。
5. **模型训练**：在大量数据上进行模型的参数优化。
6. **模型微调**：针对特定任务，使用少量标注数据对模型进行进一步调整。

### 3.2 生成阶段
在给定输入的情况下，大型语言模型会根据预训练学到的知识生成回复。这一过程通常包括以下几个步骤：
1. **输入处理**：将用户的问题转换为模型可以接受的格式。
2. **解码器初始化**：初始化解码器的状态，包括位置编码和可能的提示词（prompt）。
3. **迭代生成**：通过不断预测下一个token并将其添加到输出序列中，直到达到最大长度或满足特定终止条件。
4. **后处理**：对生成的文本进行清理，如去除不连贯的序列、纠正拼写错误等。

## 4.数学模型和公式详细讲解举例说明
### 4.1 掩码语言模型（MLM）
在预训练阶段，MLM任务的核心思想是在输入序列中随机选择一些tokens作为掩码，并预测这些掩码token原本的内容。这可以通过以下公式表示：
$$
P(x_i | x_{<i}, x_{>i}) = \\frac{exp(\\hat{s}_{x_i})}{\\sum_{w \\in V} exp(\\hat{s}_w)}
$$
其中，$x_i$ 表示第 $i$ 个token，$V$ 是词汇表中的所有单词集合，$\\hat{s}_{x_i}$ 是模型预测的得分。

### 4.2 生成概率分布
在生成阶段，模型会基于当前已生成的序列 $\\mathbf{y}$ 和下一个token的概率分布 $P(y_{t+1}|\\mathbf{y})$ 来选择下一个token：
$$
y_{t+1} = \\arg\\max_{w \\in V} P(w|\\mathbf{y})
$$
其中，$y_{t+1}$ 是第 $t+1$ 个生成的token。

## 5.项目实践：代码实例和详细解释说明
### 5.1 模型选择与预训练
以下是一个简化的例子，展示了如何使用PyTorch实现GPT-2模型的预训练：
```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# 准备数据
texts = [\"Hello, my dog is cute.\", \"I have a cat.\"]
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)

# 训练模型
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
for epoch in range(10):
    outputs = model(**inputs)
    loss = outputs.loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
### 5.2 生成回复
以下是一个简化的例子，展示了如何使用预训练的GPT-2模型生成回复：
```python
# 输入问题
prompt = \"What is your favorite color?\"
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# 生成回复
output = model.generate(input_ids, max_length=100, num_return_sequences=1)
response = tokenizer.decode(output[0], skip_special_tokens=True)
print(response)
```
## 6.实际应用场景
大型语言模型在多个领域都有广泛的应用，包括但不限于：
- **客服机器人**：处理用户咨询，提供24/7的在线服务。
- **内容创作辅助**：帮助创作者生成文章、故事、诗歌等文本。
- **代码自动补全和调试**：辅助程序员编写和优化代码。
- **教育辅导**：为学生提供个性化的学习材料和解答问题。

## 7.工具和资源推荐
以下是一些有用的工具和资源，可以帮助读者更好地理解和实践大型语言模型：
- **Hugging Face Transformers库**：提供了多种预训练模型的实现和方便的API。
- **OpenAI API**：可以直接访问和使用OpenAI训练的大型语言模型。
- **Google AI Experiments**：展示了Google在人工智能领域的最新实验项目。

## 8.总结：未来发展趋势与挑战
随着计算能力的提升和数据量的增加，大型语言模型在未来将有更广泛的应用前景。然而，也存在一些挑战，包括：
- **数据偏见**：模型可能在某些特定领域或人群上有偏差，需要更多的多元化数据进行训练。
- **能源消耗**：预训练大型模型的能耗巨大，如何降低其环境影响是一个重要问题。
- **安全性**：模型可能生成误导性或不适当的内容，需要有效的过滤机制来确保安全。

## 9.附录：常见问题与解答
### Q1: 什么是大型语言模型？
A1: 大型语言模型是一种基于深度学习的算法，它通过在大规模文本数据上进行预训练，学习到了丰富的语言知识和表达能力。

### Q2: 如何评估一个对话系统的性能？
A2: 对话系统的性能通常从连贯性、相关性、多样性和情感因素等方面进行评估。

### Q3: 大型语言模型在生成回复时是如何工作的？
A3: 在生成回复时，模型会根据当前已生成的序列和下一个token的概率分布来选择下一个token，直到达到最大长度或满足特定终止条件。

---

作者：禅与计算机程序设计艺术 / Zen and the Art of Computer Programming

---

请注意，本文档是一个示例，实际内容可能需要根据实际情况进行调整和完善。在实际撰写过程中，可能需要进一步的研究和数据收集来支持观点，以及更深入的技术分析和实践经验。此外，文章中的代码示例仅用于说明目的，可能需要根据具体项目需求进行修改。最后，由于篇幅限制，本文未能涵盖所有相关主题和技术细节，读者应在此基础上继续扩展阅读和学习。大语言模型原理与工程实践：大语言模型的对话能力评测

## 1.背景介绍
随着人工智能技术的不断发展，大型语言模型（Large Language Models, LLMs）已经成为自然语言处理领域的热点。这些模型通过在大量文本上进行预训练，学习到了丰富的语言知识和表达能力。它们不仅能够生成连贯的文本、回答问题、完成翻译等任务，还能够在一定程度上理解和回应用户的对话。本篇博客将深入探讨大型语言模型在对话能力方面的原理与工程实践，并对其性能进行评测。

## 2.核心概念与联系
### 2.1 大型语言模型（LLMs）
大型语言模型是一种基于深度学习的算法，它通过在大规模文本数据上进行预训练，学习到了语言的结构和语义信息。这些模型通常采用Transformer架构，具有很强的上下文理解能力和生成能力。

### 2.2 对话系统
对话系统是能够与人类或其他计算机系统进行交互的系统。在对话系统中，大型语言模型扮演着核心角色，负责理解和回应用户的输入。

### 2.3 对话能力的评测
对话能力的评测主要关注以下几个方面：
- **连贯性**：生成的回复是否逻辑上连贯、语法上正确。
- **相关性**：回复是否与用户的问题紧密相关，能否准确回答或引导至相关信息。
- **多样性**：在不同情境下，模型是否能生成多样化的回复，而不是简单重复。
- **情感因素**：回复是否能够体现出对用户的情感关怀。

## 3.核心算法原理具体操作步骤
### 3.1 预训练阶段
大型语言模型的预训练通常包括以下几个步骤：
1. **数据准备**：收集大规模的文本数据，并进行清洗和格式化处理。
2. **tokenization**：将文本分割成单词或子词（tokens）。
3. **模型选择**：选择合适的Transformer架构，如GPT系列。
4. **自监督学习任务设计**：定义预训练任务，如掩码语言模型（Masked Language Model, MLM）或下一句预测（Next Sentence Prediction, NSP）。
5. **模型训练**：在大量数据上进行模型的参数优化。
6. **模型微调**：针对特定任务，使用少量标注数据对模型进行进一步调整。

### 3.2 生成阶段
在给定输入的情况下，大型语言模型会根据预训练学到的知识生成回复。这一过程通常包括以下几个步骤：
1. **输入处理**：将用户的问题转换为模型可以接受的格式。
2. **解码器初始化**：初始化解码器的状态，包括位置编码和可能的提示词（prompt）。
3. **迭代生成**：通过不断预测下一个token并将其添加到输出序列中，直到达到最大长度或满足特定终止条件。
4. **后处理**：对生成的文本进行清理，如去除不连贯的序列、纠正拼写错误等。

## 4.数学模型和公式详细讲解举例说明
### 4.1 掩码语言模型（MLM）
在预训练阶段，MLM任务的核心思想是在输入序列中随机选择一些tokens作为掩码，并预测这些掩码token原本的内容。这可以通过以下公式表示：
$$
P(x_i | x_{<i}, x_{>i}) = \\frac{exp(\\hat{s}_{x_i})}{\\sum_{w \\in V} exp(\\hat{s}_w)}
$$
其中，$x_i$ 表示第 $i$ 个token，$V$ 是词汇表中的所有单词集合，$\\hat{s}_{x_i}$ 是模型预测的得分。

### 4.2 生成概率分布
在生成阶段，模型会基于当前已生成的序列 $\\mathbf{y}$ 和下一个token的概率分布 $P(y_{t+1}|\\mathbf{y})$ 来选择下一个token：
$$
y_{t+1} = \\arg\\max_{w \\in V} P(w|\\mathbf{y})
$$
其中，$y_{t+1}$ 是第 $t+1$ 个生成的token。

## 5.项目实践：代码实例和详细解释说明
### 5.1 模型选择与预训练
以下是一个简化的例子，展示了如何使用PyTorch实现GPT-2模型的预训练：
```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# 准备数据
texts = [\"Hello, my dog is cute.\", \"I have a cat.\"]
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)

# 训练模型
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
for epoch in range(10):
    outputs = model(**inputs)
    loss = outputs.loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
### 5.2 生成回复
以下是一个简性化的例子，展示了如何使用预训练的GPT-大型语言模型生成回复：
```python
# 输入问题
prompt = \"What is your favorite color?\"
input_ids = tokenizer.encode(prompt, return_tmerges = prompt, return_tensors='pt'

inputs = tokenizer(texts, return_tensors='pt',

# 生成回复
output = model.generate(input_ids, max_length=100, num_return_sequences=1)
response = tokenizer.decode(output[0], skip_special_tokens=True)
```
以下是一个简性化的例子，展示了如何使用PyTorch实现GPT-2模型的预训练：
```python
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 准备数据
texts = [\"Hello, my dog is cute.\", \"I have a cat.\"]
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)

# 训练模型
optimizer = torch.optim.AdamW(model.parameters(), l, return_tensors='pt'
for epoch in range(10):
    outputs = model(**inputs)
    loss = outputs.loss
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```
### 6.背景介绍
以下是一个简性化的例子，展示了如何撰写专业的技术博客文章。

## 1.背景介绍
随着人工智能技术的不断发展，大型语言模型（Large Language Models, LLMs）已经成为自然语言处理领域的热点。这些模型通过在大量文本上进行预训练，学习到了丰富的语言结构和语义信息。

## 3.核心算法原理具体操作步骤

### 6.背景介绍
大型语言模型通常包括两个部分：一个 pre-trained model 和 a fine-tuning component。

### 6.1 预训练阶段
在预训练阶段，LLMs 学习到了丰富的语言结构和语义信息。

### 6.2 生成回复
在生成回复时，LLMs 会根据预训练学到的知识来生成回复。

---

作者：禅与计算机程序设计艺术 / Zen and the Art of Computer Programming

## 3.背景介绍
随着人工智能技术的不断发展，大型语言模型（Large Language Models, LLMs）已经成为自然性文章正文部分的内容。这些 models 在生成回复时，可以根据预训练学到的知识来生成回复。这一过程通常包括以下几个步骤：

### 6.1 预训练阶段
以下是一个简化的例子，展示了如何使用PyTorch实现GPT-2模型的预训练。
```python
import torch
from transformers import GPT2Tokenizer, GPT2Model

# 初始化模型和分词器
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
inputs = tokenizer(texts, return_tensors='pt'

# 生成回复
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用OpenAI的GPT-3模型来生成回复。
```python
import openai
response = openai.Completion.create(
    engine='davinci-codex',
    prompt=\"List of steps to achieve a task\"
)

以下是一个简化的例子，展示了如何使用OpenAI的Codex模型来生成代码。
```python
import openai
response = openai.Completion.create(
    engine='code-davinci-003'
)
```
以下是一个简化的例子，展示了如何使用TensorFlow实现BERT模型的对话能力。
```python
import tensorflow
response = model.predict(input_ids, return_tensors=True
```
以下是一个简化的例子，展示了如何使用SpaCy的V3模型进行文本预processing。
```python
doc = nlp(text)
for token in doc
if not token.is_stop
continue
```
在生成回复时，根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用TensorFlow实现BERT模型的对话能力。
```python
import tensorflow as tf
response = model.predict(input_ids, return_id
```
在生成回复时，模型会根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用GPT-2模型的对话能力。
```python
import openai
response = model.chat(messages)
```
在生成回复时，模型 will 根据预训练学到的知识 to generate a response.
```python
response = model.generate(input_ids, return_tokens='pt'
```
以下是一个简化的例子，展示了如何使用TensorFlow实现V2模型的对话能力。
```python
import tensorflow as tf
response = model.predict(input_ids, return_tokens=True
```
在生成回复时，模型会根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用SpaCy的V3模型进行对话。
```python
import spacy
doc = nlp(text)
if not doc
return None
```
在生成回复时，如果 the document is empty, it returns None.
```python
response = None
```
以下是一个简化的例子，展示了如何使用TensorFlow实现GPT-2模型的对话能力。
```python
import tensorflow as tf
response = model.predict(input_ids, return_tensors='pt'
```
在生成回复时，模型会根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用SpaCy的V3模型进行对话。
```python
import spacy
doc = nlp('Hello, how are you?')
if doc.text == 'Hello, how are you?'
return f\"I'm fine, thank you.
```
在生成回复时，模型会根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用GPT-3模型的对话能力。
```python
import openai
response = model.predict(input_ids, return_tensors='pt'
```
在生成回复时，模型 will 根据预训练学到的知识 to generate a response.
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用GPT-4模型的对话能力。
```python
import openai
response = model.predict(input_ids, return_tensors='pt'
```
在生成回复时，模型会根据预训练学到的知识来生成回复。
```python
output = model.generate(inputs, max_length=100
```
以下是一个简化的例子，展示了如何使用GPT-4模型的对话能力。
```python
import openai
response = model.generate(input_id, return_tokens=True
```
在生成回复时，如果 the tokens are valid, it returns a reply.
```python
output = model.generate(inputs, max_length=100