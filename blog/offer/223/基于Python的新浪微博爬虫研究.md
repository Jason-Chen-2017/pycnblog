                 

### åŸºäºPythonçš„æ–°æµªå¾®åšçˆ¬è™«ç ”ç©¶ï¼šé«˜é¢‘é¢è¯•é¢˜ä¸ç®—æ³•ç¼–ç¨‹é¢˜è§£æ

#### 1. å¦‚ä½•å®ç°æ–°æµªå¾®åšçš„ç½‘é¡µç‰ˆçˆ¬è™«ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•ä½¿ç”¨Pythonå®ç°æ–°æµªå¾®åšç½‘é¡µç‰ˆçˆ¬è™«ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨Pythonçš„`requests`åº“å’Œ`BeautifulSoup`åº“æ¥å®ç°æ–°æµªå¾®åšç½‘é¡µç‰ˆçˆ¬è™«ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

# è¯·æ±‚å¾®åšé¦–é¡µ
url = "https://weibo.com/"
response = requests.get(url)

# è§£æé¡µé¢å†…å®¹
soup = BeautifulSoup(response.text, 'lxml')

# è·å–å¾®åšå†…å®¹
weibos = soup.find_all("div", class_="WB_cardwrap S_card")

for weibo in weibos:
    weibo_text = weibo.find("div", class_="WB_text W_f14").text
    print(weibo_text)
```

**è§£æï¼š** è¯¥çˆ¬è™«é¦–å…ˆå‘é€HTTPè¯·æ±‚è·å–å¾®åšé¦–é¡µçš„HTMLå†…å®¹ï¼Œç„¶åä½¿ç”¨BeautifulSoupåº“è§£æHTMLï¼Œæ‰¾åˆ°å¾®åšå†…å®¹çš„divå…ƒç´ ï¼Œå¹¶æå–å‡ºå¾®åšæ–‡æœ¬å†…å®¹ã€‚

#### 2. å¦‚ä½•å¤„ç†ç™»å½•åçš„å¾®åšå†…å®¹ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšå†…å®¹æ—¶ï¼Œå¤„ç†ç™»å½•åçš„å¾®åšå†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“çš„Sessionå¯¹è±¡æ¥ä¿æŒç™»å½•çŠ¶æ€ï¼Œç„¶ååœ¨æ¯æ¬¡è¯·æ±‚æ—¶æºå¸¦ç™»å½•åçš„Cookieã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests

# åˆ›å»ºSessionå¯¹è±¡
session = requests.Session()

# ç™»å½•å¾®åš
session.post("https://weibo.com/login.php", data={"username": "your_username", "password": "your_password"})

# è·å–ç™»å½•åçš„å¾®åšå†…å®¹
response = session.get("https://weibo.com/")

# è§£æé¡µé¢å†…å®¹
soup = BeautifulSoup(response.text, 'lxml')

# è·å–å¾®åšå†…å®¹
weibos = soup.find_all("div", class_="WB_cardwrap S_card")

for weibo in weibos:
    weibo_text = weibo.find("div", class_="WB_text W_f14").text
    print(weibo_text)
```

**è§£æï¼š** è¯¥çˆ¬è™«é¦–å…ˆä½¿ç”¨Sessionå¯¹è±¡å‘èµ·ç™»å½•è¯·æ±‚ï¼Œå°†ç™»å½•åçš„Cookieä¿å­˜åœ¨Sessionä¸­ã€‚ç„¶åä½¿ç”¨è¯¥Sessionå¯¹è±¡è·å–ç™»å½•åçš„å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšæ–‡æœ¬ã€‚

#### 3. å¦‚ä½•å®ç°å¤šçº¿ç¨‹çˆ¬å–å¾®åšï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•ä½¿ç”¨Pythonçš„å¤šçº¿ç¨‹æŠ€æœ¯å®ç°å¤šçº¿ç¨‹çˆ¬å–å¾®åšï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`threading`åº“åˆ›å»ºå¤šä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹çˆ¬å–ä¸åŒçš„å¾®åšé¡µé¢ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import threading
import requests
from bs4 import BeautifulSoup

def crawl_weibo(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibos = soup.find_all("div", class_="WB_cardwrap S_card")
    for weibo in weibos:
        weibo_text = weibo.find("div", class_="WB_text W_f14").text
        print(weibo_text)

# çˆ¬å–å¤šä¸ªå¾®åšé¡µé¢
threads = []
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]

for url in urls:
    thread = threading.Thread(target=crawl_weibo, args=(url,))
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()
```

**è§£æï¼š** è¯¥ä»£ç åˆ›å»ºäº†ä¸‰ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹åˆ†åˆ«çˆ¬å–ä¸€ä¸ªå¾®åšé¡µé¢ã€‚ä½¿ç”¨`thread.start()`å¯åŠ¨çº¿ç¨‹ï¼Œä½¿ç”¨`thread.join()`ç­‰å¾…æ‰€æœ‰çº¿ç¨‹æ‰§è¡Œå®Œæ¯•ã€‚

#### 4. å¦‚ä½•ä½¿ç”¨ä»£ç†é¿å…IPè¢«å°ç¦ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•ä½¿ç”¨ä»£ç†æ¥é¿å…IPè¢«å°ç¦ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç†æœåŠ¡ï¼Œå¦‚X-Proxyã€FreeProxyç­‰ï¼Œæ¥ä»£ç†è¯·æ±‚ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests

# ä»£ç†æœåŠ¡å™¨
proxies = {
    "http": "http://proxyserver:port",
    "https": "http://proxyserver:port",
}

# ä½¿ç”¨ä»£ç†å‘é€è¯·æ±‚
response = requests.get("https://weibo.com/", proxies=proxies)

# è§£æé¡µé¢å†…å®¹
soup = BeautifulSoup(response.text, 'lxml')
```

**è§£æï¼š** è¯¥ä»£ç è®¾ç½®äº†ä»£ç†æœåŠ¡å™¨ï¼Œå‘é€è¯·æ±‚æ—¶ä½¿ç”¨ä»£ç†æœåŠ¡å™¨æ¥è®¿é—®ç›®æ ‡ç½‘ç«™ï¼Œä»è€Œé¿å…ç›´æ¥ä½¿ç”¨è‡ªå·±çš„IPåœ°å€ï¼Œå‡å°‘è¢«å°ç¦çš„é£é™©ã€‚

#### 5. å¦‚ä½•å¤„ç†åŠ¨æ€åŠ è½½çš„å¾®åšå†…å®¹ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•å¤„ç†æ–°æµªå¾®åšé¡µé¢ä¸ŠåŠ¨æ€åŠ è½½çš„å¾®åšå†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨Seleniumåº“æ§åˆ¶æµè§ˆå™¨ï¼ŒåŠ è½½åŠ¨æ€å†…å®¹å¹¶æå–å¾®åšå†…å®¹ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
import time

# å¯åŠ¨æµè§ˆå™¨
driver = webdriver.Chrome()

# è®¿é—®å¾®åšé¦–é¡µ
driver.get("https://weibo.com/")

# ç­‰å¾…åŠ¨æ€åŠ è½½å®Œæˆ
time.sleep(5)

# è·å–å¾®åšå†…å®¹
weibos = driver.find_elements(By.CLASS_NAME, "WB_cardwrap S_card")

for weibo in weibos:
    weibo_text = weibo.find_element(By.CLASS_NAME, "WB_text W_f14").text
    print(weibo_text)

# å…³é—­æµè§ˆå™¨
driver.quit()
```

**è§£æï¼š** è¯¥ä»£ç ä½¿ç”¨Seleniumåº“å¯åŠ¨Chromeæµè§ˆå™¨ï¼Œè®¿é—®å¾®åšé¦–é¡µå¹¶ç­‰å¾…åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆã€‚ç„¶åä½¿ç”¨Seleniumæå–å¾®åšå†…å®¹ï¼Œå¹¶æ‰“å°è¾“å‡ºã€‚

#### 6. å¦‚ä½•é¿å…æŠ“å–é¢‘ç‡è¿‡é«˜å¯¼è‡´IPè¢«å°ç¦ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•é¿å…å› ä¸ºæŠ“å–é¢‘ç‡è¿‡é«˜å¯¼è‡´IPè¢«å°ç¦ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š

* **è®¾ç½®åˆç†çš„æŠ“å–é—´éš”ï¼š** åœ¨çˆ¬å–è¿‡ç¨‹ä¸­è®¾ç½®ä¸€å®šçš„å»¶æ—¶ï¼Œé¿å…è¿ç»­å¿«é€Ÿåœ°å‘é€è¯·æ±‚ã€‚
* **ä½¿ç”¨ä»£ç†æ± ï¼š** ä¸æ–­æ›´æ¢ä»£ç†IPï¼Œåˆ†æ•£è®¿é—®å‹åŠ›ã€‚
* **éµå®ˆç½‘ç«™robots.txtè§„åˆ™ï¼š** æŸ¥çœ‹ç½‘ç«™robots.txtæ–‡ä»¶ï¼Œéµå®ˆå…¶è§„åˆ™ï¼Œé¿å…è®¿é—®å—é™é¡µé¢ã€‚
* **ä½¿ç”¨å¤šçº¿ç¨‹çˆ¬å–ï¼š** æ§åˆ¶çº¿ç¨‹æ•°é‡ï¼Œé¿å…å¤§é‡è¯·æ±‚åŒæ—¶å‘é€ã€‚

#### 7. å¦‚ä½•é¿å…çˆ¬å–é‡å¤å†…å®¹ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•é¿å…åœ¨çˆ¬å–å¾®åšæ—¶æŠ“å–é‡å¤å†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š

* **ä½¿ç”¨æ•°æ®åº“ï¼š** å°†å·²çˆ¬å–çš„å¾®åšURLå­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œæ£€æŸ¥æ–°çˆ¬å–çš„å¾®åšURLæ˜¯å¦åœ¨æ•°æ®åº“ä¸­å·²å­˜åœ¨ã€‚
* **è®¾ç½®ç¼“å­˜ï¼š** ä½¿ç”¨Redisç­‰ç¼“å­˜æ•°æ®åº“ï¼Œå°†å·²çˆ¬å–çš„å¾®åšå†…å®¹ç¼“å­˜èµ·æ¥ï¼Œé¿å…é‡å¤çˆ¬å–ã€‚
* **ä½¿ç”¨hashå‡½æ•°ï¼š** å¯¹å¾®åšå†…å®¹è¿›è¡Œhashå¤„ç†ï¼Œå°†hashå€¼å­˜å‚¨åœ¨é›†åˆä¸­ï¼Œæ£€æŸ¥æ–°çˆ¬å–çš„å¾®åšå†…å®¹hashå€¼æ˜¯å¦åœ¨é›†åˆä¸­å·²å­˜åœ¨ã€‚

#### 8. å¦‚ä½•å¤„ç†å¾®åšå›¾ç‰‡å’Œè§†é¢‘å†…å®¹ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å›¾ç‰‡å’Œè§†é¢‘å†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“ä¸‹è½½å›¾ç‰‡å’Œè§†é¢‘æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests

def download_media(url, save_path):
    response = requests.get(url)
    if response.status_code == 200:
        with open(save_path, 'wb') as f:
            f.write(response.content)

# ä¸‹è½½å¾®åšå›¾ç‰‡
download_media("https://ww1.sinaimg.cn/mw690/007R9C3qgy1gmdy3ab56xj30hs0hswg8.jpg", "weibo_image.jpg")

# ä¸‹è½½å¾®åšè§†é¢‘
download_media("https://video.weibo.com/comment/aj/v1/mini/timelineè¯„è®ºè§†é¢‘ID?sid=è¯„è®ºä¼šè¯ID&gid=è§†é¢‘ç»„ID&mid=å¾®åšID&code=è¯„è®ºcode", "weibo_video.mp4")
```

**è§£æï¼š** è¯¥ä»£ç ä½¿ç”¨`requests`åº“ä¸‹è½½å¾®åšå›¾ç‰‡å’Œè§†é¢‘ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚ä¸‹è½½è§†é¢‘æ—¶ï¼Œéœ€è¦æ ¹æ®å¾®åšçš„URLç»“æ„æå–è§†é¢‘çš„ä¸‹è½½é“¾æ¥ã€‚

#### 9. å¦‚ä½•å¤„ç†å¾®åšè¯„è®ºï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†è¯„è®ºå†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšè¯„è®ºçš„URLè·å–è¯„è®ºå†…å®¹ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_comments(weibo_url):
    url = f"{weibo_url}/comment"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    comments = soup.find_all("div", class_="WB_text W_f14")

    for comment in comments:
        comment_text = comment.text
        print(comment_text)

# çˆ¬å–å¾®åšè¯„è®º
weibo_url = "https://weibo.com/789012/comments?id=1234567890123456789"
get_comments(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšè¯„è®ºçš„URLè·å–è¯„è®ºå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æè¯„è®ºHTMLï¼Œæå–è¯„è®ºæ–‡æœ¬å†…å®¹ã€‚

#### 10. å¦‚ä½•å¤„ç†å¾®åš@ç”¨æˆ·ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„@ç”¨æˆ·ä¿¡æ¯ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„@ç”¨æˆ·ä¿¡æ¯ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_mentions(weibo_url):
    url = weibo_url
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    mentions = soup.find_all("a", class_="W_f14")

    for mention in mentions:
        mention_text = mention.text
        print(mention_text)

# çˆ¬å–å¾®åš@ç”¨æˆ·ä¿¡æ¯
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
get_mentions(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„@ç”¨æˆ·é“¾æ¥ï¼Œæå–@ç”¨æˆ·åç§°ã€‚

#### 11. å¦‚ä½•å¤„ç†å¾®åšè¶…é“¾æ¥ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è¶…é“¾æ¥ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è¶…é“¾æ¥ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_hyperlinks(weibo_url):
    url = weibo_url
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    hyperlinks = soup.find_all("a")

    for hyperlink in hyperlinks:
        hyperlink_text = hyperlink.text
        hyperlink_url = hyperlink.get("href")
        print(f"Hyperlink Text: {hyperlink_text}, Hyperlink URL: {hyperlink_url}")

# çˆ¬å–å¾®åšè¶…é“¾æ¥
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
get_hyperlinks(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„è¶…é“¾æ¥ï¼Œæå–è¶…é“¾æ¥æ–‡æœ¬å’ŒURLã€‚

#### 12. å¦‚ä½•ä½¿ç”¨å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹æé«˜çˆ¬å–é€Ÿåº¦ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•ä½¿ç”¨Pythonçš„å¤šçº¿ç¨‹æˆ–å¤šè¿›ç¨‹æŠ€æœ¯æé«˜çˆ¬å–å¾®åšçš„é€Ÿåº¦ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`threading`æ¨¡å—å®ç°å¤šçº¿ç¨‹ï¼Œæˆ–è€…ä½¿ç”¨`multiprocessing`æ¨¡å—å®ç°å¤šè¿›ç¨‹æ¥æé«˜çˆ¬å–é€Ÿåº¦ã€‚

**ä»£ç å®ä¾‹ï¼ˆå¤šçº¿ç¨‹ï¼‰ï¼š**

```python
import threading
import requests
from bs4 import BeautifulSoup

def crawl_weibo(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibos = soup.find_all("div", class_="WB_cardwrap S_card")

    for weibo in weibos:
        weibo_text = weibo.find("div", class_="WB_text W_f14").text
        print(weibo_text)

urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]

threads = []
for url in urls:
    thread = threading.Thread(target=crawl_weibo, args=(url,))
    threads.append(thread)
    thread.start()

for thread in threads:
    thread.join()
```

**ä»£ç å®ä¾‹ï¼ˆå¤šè¿›ç¨‹ï¼‰ï¼š**

```python
import multiprocessing
import requests
from bs4 import BeautifulSoup

def crawl_weibo(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibos = soup.find_all("div", class_="WB_cardwrap S_card")

    for weibo in weibos:
        weibo_text = weibo.find("div", class_="WB_text W_f14").text
        print(weibo_text)

urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]

processes = []
for url in urls:
    process = multiprocessing.Process(target=crawl_weibo, args=(url,))
    processes.append(process)
    process.start()

for process in processes:
    process.join()
```

**è§£æï¼š** å¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹éƒ½å¯ä»¥æé«˜çˆ¬å–é€Ÿåº¦ï¼Œä½†å¤šçº¿ç¨‹å—é™äºå…¨å±€è§£é‡Šå™¨é”ï¼ˆGILï¼‰ï¼Œè€Œå¤šè¿›ç¨‹å¯ä»¥å……åˆ†åˆ©ç”¨å¤šæ ¸CPUçš„ä¼˜åŠ¿ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥æ ¹æ®éœ€æ±‚é€‰æ‹©é€‚åˆçš„æ–¹å¼ã€‚

#### 13. å¦‚ä½•å¤„ç†æ–°æµªå¾®åšåçˆ¬æœºåˆ¶ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•å¤„ç†æ–°æµªå¾®åšçš„åçˆ¬æœºåˆ¶ï¼Ÿ

**ç­”æ¡ˆï¼š** å¤„ç†æ–°æµªå¾®åšçš„åçˆ¬æœºåˆ¶é€šå¸¸éœ€è¦ä»¥ä¸‹ç­–ç•¥ï¼š

1. **è½®æ¢IPä»£ç†ï¼š** ä½¿ç”¨ä»˜è´¹çš„ä»£ç†æœåŠ¡æˆ–è€…å…è´¹ä»£ç†æ± ï¼Œä¸æ–­æ›´æ¢IPä»¥é¿å…IPè¢«å°é”ã€‚
2. **æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸ºï¼š** ä½¿ç”¨Seleniumæˆ–å…¶ä»–å·¥å…·æ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸ºï¼Œå¦‚éšæœºæ—¶é—´é—´éš”ã€é¡µé¢æ»šåŠ¨ç­‰ï¼Œä»¥é¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ã€‚
3. **éµå®ˆrobots.txtè§„åˆ™ï¼š** æ£€æŸ¥å¹¶éµå®ˆæ–°æµªå¾®åšçš„robots.txtæ–‡ä»¶ï¼Œé¿å…çˆ¬å–å—é™åˆ¶çš„å†…å®¹ã€‚
4. **é™ä½è¯·æ±‚é¢‘ç‡ï¼š** é™åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…çŸ­æ—¶é—´å†…å¤§é‡è¯·æ±‚ã€‚
5. **ä½¿ç”¨å¤´éƒ¨ä¼ªè£…ï¼š** ä¿®æ”¹HTTPè¯·æ±‚çš„å¤´éƒ¨ä¿¡æ¯ï¼Œå¦‚User-Agentç­‰ï¼Œæ¨¡æ‹ŸçœŸå®çš„æµè§ˆå™¨è¡Œä¸ºã€‚
6. **åŠ å¯†è¯·æ±‚å‚æ•°ï¼š** å¯¹è¯·æ±‚å‚æ•°è¿›è¡ŒåŠ å¯†å¤„ç†ï¼Œä»¥é¿å…è¢«è¯†åˆ«ã€‚

**ä»£ç å®ä¾‹ï¼ˆä½¿ç”¨ä»£ç†å’ŒUser-Agentï¼‰ï¼š**

```python
import requests
from fake_useragent import UserAgent

# è·å–éšæœºUser-Agent
ua = UserAgent()
headers = {'User-Agent': ua.random}

# ä»£ç†æœåŠ¡å™¨
proxies = {
    "http": "http://proxyserver:port",
    "https": "http://proxyserver:port",
}

# å‘é€è¯·æ±‚
response = requests.get("https://weibo.com/", headers=headers, proxies=proxies)

# å¤„ç†å“åº”
soup = BeautifulSoup(response.text, 'lxml')
```

**è§£æï¼š** è¯¥ä»£ç ä½¿ç”¨äº†`fake_useragent`åº“è·å–éšæœºçš„User-Agentï¼Œå¹¶é€šè¿‡ä»£ç†æœåŠ¡å™¨å‘é€è¯·æ±‚ï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„æµè§ˆå™¨è¡Œä¸ºï¼Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ã€‚

#### 14. å¦‚ä½•å­˜å‚¨çˆ¬å–çš„æ•°æ®ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ•°æ®åï¼Œå°†å…¶å­˜å‚¨åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶ä¸­ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹å¼å­˜å‚¨çˆ¬å–çš„æ•°æ®ï¼Œå¦‚å°†æ•°æ®å­˜å‚¨åˆ°CSVæ–‡ä»¶ã€MongoDBæ•°æ®åº“æˆ–å…¶ä»–æ ¼å¼åŒ–æ–‡ä»¶ã€‚

**ä»£ç å®ä¾‹ï¼ˆå­˜å‚¨åˆ°CSVæ–‡ä»¶ï¼‰ï¼š**

```python
import csv
import requests
from bs4 import BeautifulSoup

def store_to_csv(data, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["å­—æ®µ1", "å­—æ®µ2", "å­—æ®µ3"])  # å†™å…¥æ ‡é¢˜è¡Œ
        writer.writerows(data)

# çˆ¬å–å¾®åšæ•°æ®
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
weibos = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibo_texts = soup.find_all("div", class_="WB_text W_f14")
    for weibo in weibo_texts:
        weibos.append([weibo.text])

# å­˜å‚¨åˆ°CSVæ–‡ä»¶
store_to_csv(weibos, "weibos.csv")
```

**ä»£ç å®ä¾‹ï¼ˆå­˜å‚¨åˆ°MongoDBæ•°æ®åº“ï¼‰ï¼š**

```python
import pymongo
import requests
from bs4 import BeautifulSoup

# è¿æ¥åˆ°MongoDB
client = pymongo.MongoClient("mongodb://username:password@localhost:27017/")
db = client["weibo_database"]
collection = db["weibo_collection"]

# çˆ¬å–å¾®åšæ•°æ®
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
weibos = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibo_texts = soup.find_all("div", class_="WB_text W_f14")
    for weibo in weibo_texts:
        weibos.append({"text": weibo.text})

# å­˜å‚¨åˆ°MongoDB
collection.insert_many(weibos)
```

**è§£æï¼š** CSVæ–‡ä»¶é€‚ç”¨äºç»“æ„åŒ–æ•°æ®å­˜å‚¨ï¼Œè€ŒMongoDBæ•°æ®åº“é€‚ç”¨äºå­˜å‚¨å¤§é‡éç»“æ„åŒ–æ•°æ®ã€‚ä¸Šè¿°ä»£ç åˆ†åˆ«å±•ç¤ºäº†å¦‚ä½•å°†çˆ¬å–çš„å¾®åšæ•°æ®å­˜å‚¨åˆ°CSVæ–‡ä»¶å’ŒMongoDBæ•°æ®åº“ã€‚

#### 15. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„JavaScriptä»£ç ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†é¡µé¢ä¸­çš„JavaScriptä»£ç ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`Selenium`åº“æ¥æ§åˆ¶æµè§ˆå™¨ï¼Œæ‰§è¡ŒJavaScriptä»£ç ï¼Œå¹¶è·å–åŠ¨æ€åŠ è½½çš„æ•°æ®ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# è®¾ç½®æ— ç•Œé¢æ¨¡å¼
options = Options()
options.add_argument("--headless")

# å¯åŠ¨æµè§ˆå™¨
driver = webdriver.Chrome(options=options)

# è®¿é—®å¾®åšé¦–é¡µ
driver.get("https://weibo.com/")

# æ‰§è¡ŒJavaScriptä»£ç ï¼Œè·å–åŠ¨æ€åŠ è½½çš„æ•°æ®
data = driver.execute_script("""
    return {
        title: document.title,
        html: document.documentElement.innerHTML
    };
""")

# å¤„ç†æ•°æ®
print("Title:", data["title"])
print("HTML:", data["html"])

# å…³é—­æµè§ˆå™¨
driver.quit()
```

**è§£æï¼š** è¯¥ä»£ç ä½¿ç”¨`Selenium`åº“å¯åŠ¨æ— ç•Œé¢Chromeæµè§ˆå™¨ï¼Œè®¿é—®å¾®åšé¦–é¡µï¼Œå¹¶æ‰§è¡ŒJavaScriptä»£ç è·å–é¡µé¢æ ‡é¢˜å’ŒHTMLå†…å®¹ã€‚ä½¿ç”¨`execute_script`æ–¹æ³•å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰§è¡ŒJavaScriptä»£ç ï¼Œå¹¶è·å–è¿”å›çš„æ•°æ®ã€‚

#### 16. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„å›¾ç‰‡ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å›¾ç‰‡é“¾æ¥å¹¶å°†å…¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“ä¸‹è½½å›¾ç‰‡ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†å›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšå›¾ç‰‡é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
images = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    img_tags = soup.find_all("img")
    for img_tag in img_tags:
        img_url = img_tag.get("data-src")
        images.append(img_url)

# ä¸‹è½½å›¾ç‰‡
for image_url in images:
    response = requests.get(image_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(image_url)[0]}.jpg"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {image_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„å›¾ç‰‡é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½å›¾ç‰‡ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†å›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 17. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è§†é¢‘ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†è§†é¢‘é“¾æ¥å¹¶å°†å…¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“ä¸‹è½½è§†é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†è§†é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšè§†é¢‘é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
videos = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    video_tags = soup.find_all("video")
    for video_tag in video_tags:
        video_url = video_tag.get("src")
        videos.append(video_url)

# ä¸‹è½½è§†é¢‘
for video_url in videos:
    response = requests.get(video_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(video_url)[0]}.mp4"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {video_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„è§†é¢‘é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½è§†é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†è§†é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 18. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è½¬å‘çš„å¾®åšå†…å®¹ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è½¬å‘å†…å®¹ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„è½¬å‘é“¾æ¥è·å–è½¬å‘å†…å®¹ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_forwarded_weibo(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    forwarded_weibo = soup.find("div", class_="WB_text W_f14")
    if forwarded_weibo:
        forwarded_text = forwarded_weibo.text
        print("è½¬å‘å†…å®¹ï¼š", forwarded_text)
    else:
        print("æ²¡æœ‰æ‰¾åˆ°è½¬å‘å†…å®¹")

# çˆ¬å–å¾®åšä¸­çš„è½¬å‘å†…å®¹
weibo_url = "https://weibo.com/789012/status/1234567890123456789?from=page_1002067890123456789&mod=WEIBO_SECONDHAND_1003&tdsourcetag=s_pcqq_aiomsg"
get_forwarded_weibo(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„è½¬å‘é“¾æ¥è·å–è½¬å‘å†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšå†…å®¹ï¼Œæå–è½¬å‘æ–‡æœ¬ã€‚

#### 19. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„@ç”¨æˆ·ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„@ç”¨æˆ·ä¿¡æ¯ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„@ç”¨æˆ·ä¿¡æ¯ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_mentions(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    mentions = soup.find_all("a", class_="W_f14")
    mention_names = [mention.text for mention in mentions]
    print("æåŠç”¨æˆ·ï¼š", mention_names)

# çˆ¬å–å¾®åšä¸­çš„æåŠç”¨æˆ·
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
get_mentions(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„@ç”¨æˆ·ä¿¡æ¯ï¼Œæå–æåŠçš„ç”¨æˆ·åç§°ã€‚

#### 20. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è¶…é“¾æ¥ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è¶…é“¾æ¥ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è¶…é“¾æ¥ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_hyperlinks(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    hyperlinks = soup.find_all("a")
    for hyperlink in hyperlinks:
        text = hyperlink.text
        href = hyperlink.get("href")
        print(f"è¶…é“¾æ¥æ–‡æœ¬ï¼š{text}, è¶…é“¾æ¥URLï¼š{href}")

# çˆ¬å–å¾®åšä¸­çš„è¶…é“¾æ¥
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
get_hyperlinks(weibo_url)
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„è¶…é“¾æ¥ï¼Œæå–è¶…é“¾æ¥æ–‡æœ¬å’ŒURLã€‚

#### 21. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è¡¨æƒ…ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è¡¨æƒ…ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¾®åšä¸­çš„è¡¨æƒ…ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„æ–‡æœ¬æˆ–å›¾ç‰‡ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import re
from bs4 import BeautifulSoup

def replace_emoticon(text):
    emoticons = {
        "ğŸ˜‚": "å¤§ç¬‘",
        "ğŸ˜‚ğŸ˜‚ğŸ˜‚": "è¶…çº§å¤§ç¬‘",
        "ğŸ˜¢": "å“­æ³£",
        "ğŸ˜¢ğŸ˜¢": "è¶…çº§å“­æ³£",
    }
    for emoticon, replacement in emoticons.items():
        text = text.replace(emoticon, replacement)
    return text

def get_weibo_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    weibo_text = soup.find("div", class_="WB_text W_f14").text
    return replace_emoticon(weibo_text)

# çˆ¬å–å¾®åšå†…å®¹
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
print(get_weibo_text(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢å¾®åšä¸­çš„è¡¨æƒ…ä¸ºå¯¹åº”çš„æ–‡æœ¬æˆ–å›¾ç‰‡ã€‚é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªè¡¨æƒ…å­—å…¸ï¼Œç„¶åä½¿ç”¨å­—å…¸ä¸­çš„é”®å€¼å¯¹æ›¿æ¢æ–‡æœ¬ä¸­çš„è¡¨æƒ…ã€‚

#### 22. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„æ ‡ç­¾ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„æ ‡ç­¾ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„æ ‡ç­¾ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_tags(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    tags = soup.find_all("a", class_="W_f14")
    tag_texts = [tag.text for tag in tags]
    return tag_texts

# çˆ¬å–å¾®åšæ ‡ç­¾
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
print(get_tags(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„æ ‡ç­¾ï¼Œæå–æ ‡ç­¾æ–‡æœ¬ã€‚

#### 23. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„éŸ³é¢‘ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„éŸ³é¢‘ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“ä¸‹è½½éŸ³é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†éŸ³é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšéŸ³é¢‘é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
audios = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    audio_tags = soup.find_all("audio")
    for audio_tag in audio_tags:
        audio_url = audio_tag.get("src")
        audios.append(audio_url)

# ä¸‹è½½éŸ³é¢‘
for audio_url in audios:
    response = requests.get(audio_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(audio_url)[0]}.mp3"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {audio_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„éŸ³é¢‘é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½éŸ³é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†éŸ³é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 24. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è§†é¢‘å¡ç‰‡ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è§†é¢‘å¡ç‰‡ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è§†é¢‘å¡ç‰‡ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšè§†é¢‘å¡ç‰‡é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
videos = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    video_card_tags = soup.find_all("div", class_="WB_media_preview")
    for video_card_tag in video_card_tags:
        video_url = video_card_tag.find("video", class_="W_video").get("src")
        videos.append(video_url)

# ä¸‹è½½è§†é¢‘
for video_url in videos:
    response = requests.get(video_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(video_url)[0]}.mp4"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {video_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„è§†é¢‘å¡ç‰‡é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½è§†é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†è§†é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 25. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è§†é¢‘é“¾æ¥ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è§†é¢‘é“¾æ¥ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è§†é¢‘é“¾æ¥ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšè§†é¢‘é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
videos = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    video_tags = soup.find_all("a", class_="W_f14")
    for video_tag in video_tags:
        video_url = video_tag.get("href")
        videos.append(video_url)

# ä¸‹è½½è§†é¢‘
for video_url in videos:
    response = requests.get(video_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(video_url)[0]}.mp4"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {video_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„è§†é¢‘é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½è§†é¢‘ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†è§†é¢‘ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 26. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è¯é¢˜ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è¯é¢˜ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è¯é¢˜ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_topics(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    topic_tags = soup.find_all("a", class_="W_f14")
    topic_texts = [topic_tag.text for topic_tag in topic_tags]
    return topic_texts

# çˆ¬å–å¾®åšè¯é¢˜
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
print(get_topics(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„è¯é¢˜ï¼Œæå–è¯é¢˜æ–‡æœ¬ã€‚

#### 27. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„å›¾ç‰‡é“¾æ¥ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„å›¾ç‰‡é“¾æ¥ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„å›¾ç‰‡é“¾æ¥ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import os
import requests
from bs4 import BeautifulSoup

# çˆ¬å–å¾®åšå›¾ç‰‡é“¾æ¥
urls = ["https://weibo.com/u/123456", "https://weibo.com/u/654321", "https://weibo.com/u/789012"]
images = []

for url in urls:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    image_tags = soup.find_all("a", class_="WB_text W_f14")
    for image_tag in image_tags:
        image_url = image_tag.get("href")
        images.append(image_url)

# ä¸‹è½½å›¾ç‰‡
for image_url in images:
    response = requests.get(image_url)
    if response.status_code == 200:
        file_path = f"{os.path.splitext(image_url)[0]}.jpg"
        with open(file_path, 'wb') as f:
            f.write(response.content)
        print(f"Downloaded {image_url} to {file_path}")
```

**è§£æï¼š** è¯¥ä»£ç é¦–å…ˆçˆ¬å–å¾®åšé¡µé¢ä¸­çš„å›¾ç‰‡é“¾æ¥ï¼Œç„¶åä½¿ç”¨`requests`åº“ä¸‹è½½å›¾ç‰‡ï¼Œå¹¶ä½¿ç”¨`os`æ¨¡å—å°†å›¾ç‰‡ä¿å­˜åˆ°æœ¬åœ°ã€‚

#### 28. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è¯„è®ºï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è¯„è®ºï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è¯„è®ºã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_comments(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    comment_tags = soup.find_all("div", class_="WB_text W_f14")
    comment_texts = [comment_tag.text for comment_tag in comment_tags]
    return comment_texts

# çˆ¬å–å¾®åšè¯„è®º
weibo_url = "https://weibo.com/789012/status/1234567890123456789?from=page_1002067890123456789&mod=WEIBO_SECONDHAND_1003&tdsourcetag=s_pcqq_aiomsg"
print(get_comments(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„è¯„è®ºï¼Œæå–è¯„è®ºæ–‡æœ¬ã€‚

#### 29. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„ç‚¹èµï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„ç‚¹èµä¿¡æ¯ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„ç‚¹èµä¿¡æ¯ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_likes(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    like_tags = soup.find_all("div", class_="W_linkb")
    like_texts = [like_tag.text for like_tag in like_tags]
    return like_texts

# çˆ¬å–å¾®åšç‚¹èµä¿¡æ¯
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
print(get_likes(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„ç‚¹èµä¿¡æ¯ï¼Œæå–ç‚¹èµæ–‡æœ¬ã€‚

#### 30. å¦‚ä½•å¤„ç†å¾®åšä¸­çš„è½¬å‘ï¼Ÿ

**é¢˜ç›®ï¼š** å¦‚ä½•åœ¨çˆ¬å–å¾®åšæ—¶ï¼Œå¤„ç†å¾®åšä¸­çš„è½¬å‘ä¿¡æ¯ï¼Ÿ

**ç­”æ¡ˆï¼š** å¯ä»¥ä½¿ç”¨`requests`åº“å’Œ`BeautifulSoup`åº“ï¼Œé€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶è§£æå¾®åšä¸­çš„è½¬å‘ä¿¡æ¯ã€‚

**ä»£ç å®ä¾‹ï¼š**

```python
import requests
from bs4 import BeautifulSoup

def get_forwarded_urls(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'lxml')
    forwarded_tags = soup.find_all("a", class_="W_linkb")
    forwarded_urls = [forwarded_tag.get("href") for forwarded_tag in forwarded_tags]
    return forwarded_urls

# çˆ¬å–å¾®åšè½¬å‘é“¾æ¥
weibo_url = "https://weibo.com/789012/status/1234567890123456789"
print(get_forwarded_urls(weibo_url))
```

**è§£æï¼š** è¯¥ä»£ç é€šè¿‡å¾®åšçš„URLè·å–å¾®åšå†…å®¹ï¼Œå¹¶ä½¿ç”¨BeautifulSoupåº“è§£æå¾®åšä¸­çš„è½¬å‘ä¿¡æ¯ï¼Œæå–è½¬å‘é“¾æ¥ã€‚

