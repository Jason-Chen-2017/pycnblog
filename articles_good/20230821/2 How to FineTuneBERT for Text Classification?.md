
ä½œè€…ï¼šç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯                    

# 1.ç®€ä»‹
  
ï¼šæœ¬æ–‡å°†ä»‹ç»åˆ©ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è¯¦ç»†æ­¥éª¤ã€‚æœ¬æ–‡ä¸»è¦é¢å‘åˆçº§è¯»è€…ï¼Œæ‰€ä»¥ä¸ä¼šæ¶‰åŠå¤ªå¤šå¤æ‚çš„æ•°å­¦å…¬å¼å’Œç®—æ³•æ¨å¯¼ï¼Œåªä¼šç®€å•çš„ç»™å‡ºç›¸å…³çš„ç»†èŠ‚ä¿¡æ¯ã€‚

## 1.èƒŒæ™¯ä»‹ç»
æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åº”ç”¨ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯å°†è¾“å…¥çš„æ–‡æœ¬åˆ’åˆ†åˆ°é¢„å…ˆå®šä¹‰å¥½çš„ç±»åˆ«ä¸­ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ–°é—»æ–‡ç« åˆ†æˆä¸åŒçš„ä¸»é¢˜æ ‡ç­¾ï¼šè´¢ç»ã€å¨±ä¹ã€ç§‘æŠ€ç­‰ï¼›å¯ä»¥æŠŠç”¨æˆ·è¯„ä»·å½’ç±»ä¸ºç§¯ææˆ–æ¶ˆæç­‰ï¼›ä¹Ÿå¯ä»¥é€šè¿‡è¯„è®ºçš„æ–‡æœ¬è‡ªåŠ¨åˆ†ææ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚

ç›®å‰ï¼Œä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ–¹æ³•å¯¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æœ‰ç€è¾ƒå¥½çš„æ•ˆæœï¼Œä½†æ˜¯åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¾€å¾€éœ€è¦æ›´é«˜æ•ˆåœ°è§£å†³åˆ†ç±»ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šã€‚å› æ­¤ï¼ŒåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„æ–¹æ³•é€æ¸å—åˆ°ç ”ç©¶çš„é‡è§†ï¼Œç‰¹åˆ«æ˜¯BERTæ¨¡å‹ï¼Œå·²ç»æˆä¸ºæœ€æµè¡Œçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¹‹ä¸€ã€‚

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯2018å¹´ç”±Google AIå®éªŒå®¤ç ”å‘çš„ä¸€ç§é¢„è®­ç»ƒæ–‡æœ¬è¡¨ç¤ºæ¨¡å‹ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿå•è¯åµŒå…¥æ¨¡å‹é¢ä¸´çš„ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼šä¸€æ˜¯ç»´åº¦ç¾éš¾ï¼›äºŒæ˜¯ä¸Šä¸‹æ–‡ä¿¡æ¯æŸå¤±ã€‚

ç›¸æ¯”äºä¼ ç»Ÿçš„å•è¯embeddingæ–¹å¼ï¼ŒBERTçš„ä¼˜åŠ¿åœ¨äºï¼š

1. ä½¿ç”¨å¤šå±‚Transformerç»“æ„å®ç°ç«¯åˆ°ç«¯çš„æ— ç›‘ç£å­¦ä¹ ï¼Œé€šè¿‡æœ‰æ•ˆå­¦ä¹ æ–‡æœ¬åºåˆ—çš„å†…éƒ¨ç»“æ„ï¼Œç”Ÿæˆå®šåˆ¶åŒ–çš„è¯å‘é‡ã€‚
2. é€šè¿‡ä½ç½®ç¼–ç å‘æ¯ä¸ªè¯æ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œè§£å†³äº†åœ¨ä¸åŒä½ç½®å‡ºç°åŒä¸€ä¸ªè¯çš„é—®é¢˜ã€‚
3. æä¾›å¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä½¿å¾—å„ä¸ªä»»åŠ¡çš„fine tuningéƒ½èƒ½å¤Ÿå–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚

é™¤æ­¤ä¹‹å¤–ï¼ŒBERTè¿˜æå‡ºäº†Masked Language Modelï¼ˆMLMï¼‰ï¼Œé€šè¿‡å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œéšæœºmaskå¹¶æ›¿æ¢ä¸ºç‰¹æ®Šçš„MASKæ ‡è®°ç¬¦ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹è¿™äº›è¢«maskçš„è¯ã€‚MLMèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹é¢„æµ‹å™ªå£°è¯æ±‡ã€è¯­æ³•é”™è¯¯ç­‰ä¿¡æ¯ï¼Œå¹¶å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚

## 2.åŸºæœ¬æ¦‚å¿µæœ¯è¯­è¯´æ˜
æœ¬æ–‡ä¼šç”¨åˆ°çš„ä¸€äº›åŸºæœ¬çš„æ¦‚å¿µå’Œæœ¯è¯­ï¼ŒåŒ…æ‹¬ï¼š

1. æ•°æ®é›†ï¼šç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•°æ®é›†åˆï¼Œé€šå¸¸ç”±æ–‡æœ¬å’Œç›¸åº”çš„æ ‡ç­¾ç»„æˆã€‚
2. æ¨¡å‹ï¼šå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œåˆ†ç±»çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¯åŸºäºå„ç§ç‰¹å¾æŠ½å–æ–‡æœ¬ç‰¹å¾å¹¶è¾“å…¥åˆ†ç±»å™¨ä¸­è¿›è¡Œè®­ç»ƒã€‚
3. BERTæ¨¡å‹ï¼šGoogle AIå›¢é˜Ÿé€šè¿‡æ·±åº¦å­¦ä¹ æŠ€æœ¯è®­ç»ƒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ç”¨äºæ–‡æœ¬åˆ†ç±»ã€è¯­è¨€å»ºæ¨¡ç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚
4. å¥å­ç¼–ç ï¼šå¯¹æ¯ä¸ªè¾“å…¥çš„å¥å­è¿›è¡ŒBERTæ¨¡å‹çš„è®¡ç®—ï¼Œå¾—åˆ°è¾“å‡ºçš„å¥å­è¡¨ç¤ºã€‚å¥å­ç¼–ç å¯ä»¥ç”¨äºåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½œä¸ºè¾“å…¥ç‰¹å¾å‘é‡ã€‚
5. Token Embeddingï¼šæ¯ä¸ªTokenç»è¿‡BERTæ¨¡å‹ç¼–ç åï¼Œè·å¾—ä¸€ä¸ªå¯¹åº”çš„å‘é‡è¡¨ç¤ºã€‚
6. Input Embeddingsï¼šè¾“å…¥çš„æ¯æ¡æ–‡æœ¬ç»è¿‡BERTæ¨¡å‹çš„å¤„ç†åï¼Œå…¶æ¯ä¸ªTokençš„å‘é‡è¡¨ç¤ºéƒ½ä¼šèåˆæˆä¸€ä¸ªæ•´ä½“çš„æ–‡æœ¬è¡¨ç¤ºã€‚
7. Labelï¼šè¾“å…¥æ–‡æœ¬å¯¹åº”çš„æ ‡ç­¾ã€‚

## 3.æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦å…¬å¼è®²è§£

### 3.1 BERTæ¨¡å‹æ¦‚è¿°

#### ï¼ˆ1ï¼‰BERTæ¨¡å‹ä»‹ç»

BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ˜¯2018å¹´ç”±Google AIå®éªŒå®¤ç ”å‘çš„ä¸€ç§é¢„è®­ç»ƒæ–‡æœ¬è¡¨ç¤ºæ¨¡å‹ï¼Œå®ƒé‡‡ç”¨äº†ä¸¤æ­¥é¢„è®­ç»ƒæ³•ï¼Œç¬¬ä¸€æ­¥æ˜¯å¯¹å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®è¿›è¡Œé¢„è®­ç»ƒï¼Œç¬¬äºŒæ­¥æ˜¯åŸºäºBERTçš„é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ(Fine Tuning)çš„æ–¹å¼è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œè¿™ç§æ–¹æ³•ä¿è¯äº†æ¨¡å‹çš„ç¨³å®šæ€§å’Œè¿ç§»æ€§ã€‚ä¸‹é¢ç®€è¦ä»‹ç»ä¸€ä¸‹BERTæ¨¡å‹çš„åŸºæœ¬ç»“æ„ã€‚

BERTæ¨¡å‹æ˜¯åŸºäºTransformersçš„Encoder-Decoderæ¡†æ¶ï¼Œå®ƒçš„ç»“æ„ç”±Encoderå’ŒDecoderä¸¤éƒ¨åˆ†ç»„æˆã€‚

**ï¼ˆaï¼‰BERTæ¨¡å‹ç»“æ„**



BERTæ¨¡å‹æ˜¯ä¸€ä¸ªåŒå‘Transformeræ¨¡å‹ï¼Œå…¶ä¸­å·¦è¾¹éƒ¨åˆ†ä¸ºEncoderï¼Œå³è¾¹éƒ¨åˆ†ä¸ºDecoderã€‚Encoderæ˜¯BERTä¸­çš„ä¸»å¹²è·¯å¾„ï¼Œå®ƒè´Ÿè´£å¯¹åŸå§‹è¾“å…¥æ–‡æœ¬è¿›è¡Œç‰¹å¾æŠ½å–ã€‚

BERTæ¨¡å‹çš„è¾“å…¥æ˜¯tokençš„idåºåˆ—ï¼Œé¦–å…ˆç»è¿‡WordPieceåˆ†è¯å™¨åˆ‡åˆ†æˆå•è¯ï¼Œç„¶åæ¯ä¸ªå•è¯åˆç»è¿‡WordEmbeddingså±‚è½¬æ¢æˆè¯å‘é‡ã€‚ä¹‹åï¼Œä½¿ç”¨Self-Attentionå±‚å¯¹è¾“å…¥è¿›è¡Œç‰¹å¾æå–ï¼Œå¹¶å°†è¯å‘é‡æ˜ å°„ä¸ºæ–°çš„è¡¨ç¤ºï¼Œå¾—åˆ°æ¯ä¸ªå•è¯çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æœ€åï¼Œä½¿ç”¨å…¨è¿æ¥å±‚å¯¹ç‰¹å¾è¿›è¡Œè½¬æ¢ï¼Œè¾“å‡ºæ•´ä¸ªå¥å­çš„è¡¨ç¤ºã€‚

BERTæ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡æ˜¯Masked Language Modeling (MLM)ï¼Œå³å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œéšæœºmaskå¹¶æ›¿æ¢ä¸º[MASK]æ ‡è®°ç¬¦ï¼Œæ¨¡å‹è¦å­¦ä¹ åˆ°å“ªäº›è¯è¢«maskï¼Œæ¨¡å‹é¢„æµ‹è¿™äº›è¯çš„å¯èƒ½å€¼ã€‚åŒæ—¶ï¼Œä¸ºäº†é¿å…æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œè¿˜åŠ å…¥äº†Dropoutå±‚ï¼Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶å…·æœ‰ä¸€å®šçš„å¥å£®æ€§ã€‚

é¢„è®­ç»ƒå®Œæˆåï¼Œå°†æ¨¡å‹å›ºå®šä½ï¼Œç„¶åå¯¹å¾…åˆ†ç±»çš„ä»»åŠ¡è¿›è¡ŒFine-tuningã€‚

#### ï¼ˆ2ï¼‰æ¨¡å‹è¶…å‚æ•°è®¾ç½®

BERTæ¨¡å‹çš„å‚æ•°å¾ˆå¤šï¼Œä¸ºäº†é€‚åº”ä¸åŒçš„ä»»åŠ¡åœºæ™¯ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´ç›¸åº”çš„è¶…å‚æ•°ã€‚

##### ï¼ˆ2.1ï¼‰BERTè¶…å‚æ•°ä»‹ç»

BERTçš„æ¨¡å‹ç»“æ„ä¸€èˆ¬åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

- Layerï¼šå±‚æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹çš„æ·±åº¦ã€‚
- Hidden sizeï¼šéšè—å•å…ƒä¸ªæ•°ï¼Œä¸€èˆ¬æ˜¯768æˆ–1024ã€‚
- Attention headsï¼šæ³¨æ„åŠ›å¤´æ•°ï¼Œé€šå¸¸è®¾ä¸º12ã€‚
- Intermediate sizeï¼šFFNä¸­é—´å±‚ç¥ç»å…ƒä¸ªæ•°ï¼Œä¸€èˆ¬æ˜¯3072ã€‚
- Dropout rateï¼šéšæœºå¤±æ´»ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä¸€èˆ¬è®¾ç½®ä¸º0.1~0.3ã€‚
- Learning rateï¼šåˆå§‹å­¦ä¹ ç‡ï¼Œä¸€èˆ¬è®¾ç½®ä¸º2e-5~5e-5ã€‚

##### ï¼ˆ2.2ï¼‰Fine-tuningè¶…å‚æ•°è®¾ç½®

åœ¨æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼ŒFine-tuningå¾€å¾€ä¸éœ€è¦ä¿®æ”¹æ¨¡å‹çš„è¶…å‚æ•°ã€‚

- Batch size: ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ—¶çš„batch sizeè®¾ç½®ä¸º16æˆ–32ï¼Œè€Œæµ‹è¯•æ—¶çš„batch sizeè®¾ç½®ä¸º8ã€‚
- Epochsï¼šè®­ç»ƒè½®æ•°ï¼Œä¸€èˆ¬è®¾ç½®ä¸º3~10ã€‚
- Learning rate schedulerï¼šå­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œæ¯”å¦‚StepLRã€CosineAnnealingLRç­‰ã€‚
- Loss functionï¼šæŸå¤±å‡½æ•°ï¼Œæ¯”å¦‚CrossEntropyLossã€NLLLossç­‰ã€‚

### 3.2 æ•°æ®é›†å‡†å¤‡

#### ï¼ˆ1ï¼‰æ•°æ®é›†ä»‹ç»

é€šå¸¸ï¼Œå¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè®­ç»ƒæ•°æ®é›†åŒ…æ‹¬è®­ç»ƒæ•°æ®é›†å’ŒéªŒè¯æ•°æ®é›†ã€‚è®­ç»ƒæ•°æ®é›†åŒ…å«è®¸å¤šå¸¦æ ‡ç­¾çš„æ ·æœ¬ï¼Œè€ŒéªŒè¯æ•°æ®é›†åˆ™ç”¨æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥ä»å¤šä¸ªæ•°æ®æºæ”¶é›†åˆ°è¿™æ ·çš„æ–‡æœ¬æ•°æ®ï¼ŒåŒ…æ‹¬æ–°é—»æ–‡ç« ã€å•†å“è¯„è®ºã€ç”¨æˆ·åé¦ˆã€è®ºæ–‡æ‘˜è¦ã€ç”µå½±è¯„è®ºç­‰ç­‰ã€‚

å¯¹äºBERTæ¨¡å‹æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«æ–‡æ¡£å’Œå¯¹åº”ç±»åˆ«æ ‡ç­¾çš„è¯­æ–™åº“ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¯¥è¯­æ–™åº“è¿›è¡Œæ¸…æ´—ï¼Œæ„å»ºä¸€ä¸ªè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

#### ï¼ˆ2ï¼‰æ•°æ®é›†é¢„å¤„ç†

åœ¨æ„å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¹‹å‰ï¼Œéœ€è¦å¯¹è¯­æ–™åº“è¿›è¡Œé¢„å¤„ç†ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ æ­¥ï¼š

1. åˆ†è¯ï¼šå°†å¥å­åˆ‡åˆ†æˆè‹¥å¹²ä¸ªè¯æˆ–è€…çŸ­è¯­ï¼Œä¹Ÿå°±æ˜¯å°†æ–‡æœ¬æŒ‰ç…§è¯ã€å­—ã€ç¬¦å·ç­‰å•ä½è¿›è¡Œæ‹†åˆ†ï¼Œç›®çš„æ˜¯ä¸ºäº†æ–¹ä¾¿è¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚
2. è¯æ€§æ ‡æ³¨ï¼šç»™æ¯ä¸ªè¯èµ‹äºˆä¸€ä¸ªè¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ï¼Œç›®çš„ä¹Ÿæ˜¯ä¸ºäº†ç»™æ¨¡å‹æä¾›æ›´å¤šçš„ä¿¡æ¯ã€‚
3. åœç”¨è¯è¿‡æ»¤ï¼šè¿‡æ»¤æ‰ä¸€äº›ä¸å½±å“åˆ†ç±»ç»“æœçš„è¯ï¼Œæ¯”å¦‚â€œtheâ€ï¼Œâ€œisâ€ç­‰ã€‚
4. è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ï¼šå°†é¢„å¤„ç†åçš„æ–‡æœ¬è½¬æ¢æˆç´¢å¼•åºåˆ—ï¼Œåºåˆ—ä¸­çš„å…ƒç´ æ˜¯è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•å€¼ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªè¯åœ¨è¯æ±‡è¡¨ä¸­çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚

#### ï¼ˆ3ï¼‰åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹

ä¸ºäº†åŠ é€Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹è¿›è¡Œåˆå§‹åŒ–ã€‚é€šè¿‡è°ƒç”¨huggingfaceçš„transformersåŒ…çš„BertModelç±»å¯ä»¥åŠ è½½é¢„è®­ç»ƒå¥½çš„BERTæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨freeze()æ–¹æ³•å†»ç»“æ¨¡å‹å‚æ•°ã€‚

```python
from transformers import BertModel, BertTokenizer

bert = BertModel.from_pretrained('bert-base-uncased')
bert.eval() # set model to evaluation mode
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
```

### 3.3 æ•°æ®å¤„ç†æµç¨‹

#### ï¼ˆ1ï¼‰åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†

é¦–å…ˆï¼Œæˆ‘ä»¬è¦è½½å…¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç„¶åæŠŠå®ƒä»¬åˆ†åˆ«è½¬æ¢æˆBERTæ¨¡å‹å¯æ¥å—çš„è¾“å…¥å½¢å¼ã€‚

```python
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

def load_and_encode(sentences):
    encoded_texts = tokenizer(sentences, padding='longest', truncation=True, return_tensors="pt")
    input_ids = encoded_texts["input_ids"]
    attention_masks = encoded_texts["attention_mask"]

    dataset = TensorDataset(input_ids, attention_masks)
    return dataset

train_dataset = load_and_encode(train_texts)
test_dataset = load_and_encode(test_texts)
```

#### ï¼ˆ2ï¼‰å®šä¹‰DataLoader

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰æ•°æ®é›†åŠ è½½å™¨ï¼Œç”¨äºå°†æ•°æ®é›†åˆ†æ‰¹æ¬¡é€å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚è¿™é‡Œé‡‡ç”¨çš„æ˜¯é»˜è®¤çš„è®­ç»ƒæ¨¡å¼RandomSamplerï¼Œå³æ¯æ¬¡éšæœºé€‰å–ä¸€å°éƒ¨åˆ†æ•°æ®é€å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

```python
train_loader = DataLoader(
            train_dataset, 
            sampler=RandomSampler(train_dataset), 
            batch_size=args.train_batch_size
        )

test_loader = DataLoader(
            test_dataset, 
            sampler=SequentialSampler(test_dataset), 
            batch_size=args.eval_batch_size
        )
```

#### ï¼ˆ3ï¼‰åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°

æˆ‘ä»¬å¯ä»¥é€šè¿‡å¾ªç¯æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ã€‚

```python
for param in bert.parameters():
    param.requires_grad = False
    
output_dim = len(label_map)
bert.classifier = nn.Linear(bert.config.hidden_size, output_dim)

if args.fp16:
    try:
        from apex import amp
    except ImportError:
        raise ImportError("Please install apex from https://www.github.com/nvidia/apex to use fp16 training.")
    
    bert, optimizer = amp.initialize(bert, optimizer, opt_level=args.fp16_opt_level)
else:
    device = torch.device("cuda" if torch.cuda.is_available() and not args.no_cuda else "cpu")
    n_gpu = torch.cuda.device_count()
    logger.info("device: {}, n_gpu: {}".format(device, n_gpu))
    bert.to(device)
```

### 3.4 è®­ç»ƒæ¨¡å‹

#### ï¼ˆ1ï¼‰è®­ç»ƒæµç¨‹

è®­ç»ƒè¿‡ç¨‹åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š

1. å°†è¾“å…¥çš„å¥å­ç¼–ç ä¸ºå‘é‡å½¢å¼ï¼›
2. åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒåˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»ã€‚

åœ¨ä»¥ä¸Šæ­¥éª¤ä¸­ï¼Œç¬¬ä¸€ä¸ªæ­¥éª¤å¯ä»¥åœ¨pytorchçš„GPUæˆ–CPUä¸Šè¿›è¡Œå¿«é€Ÿè¿ç®—ã€‚

#### ï¼ˆ2ï¼‰æ¨¡å‹è®­ç»ƒ

ä¸‹é¢ç»™å‡ºå…·ä½“çš„ä»£ç ç‰‡æ®µï¼Œå±•ç¤ºå¦‚ä½•åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒBERTæ¨¡å‹ã€‚

```python
optimizer = AdamW(filter(lambda p: p.requires_grad, bert.parameters()), lr=lr, eps=adam_epsilon)
total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

loss_values=[]
for epoch in range(epochs):
  print("")
  print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))

  bert.train()
  loss_avg = 0
  
  progressbar = tqdm(enumerate(train_loader), total=len(train_loader))
  for step, batch in progressbar:
      batch = tuple(t.to(device) for t in batch)

      inputs = {
          'input_ids':      batch[0], 
          'attention_mask': batch[1], 
          'labels':         batch[3]} 

      outputs = bert(**inputs)
      loss = outputs[0]

      if n_gpu > 1:
          loss = loss.mean() 
      if args.gradient_accumulation_steps > 1:
          loss = loss / args.gradient_accumulation_steps 

      loss_avg += loss.item()
      
      if args.fp16:
          with amp.scale_loss(loss, optimizer) as scaled_loss:
              scaled_loss.backward()
          torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)
      else:
          loss.backward()
          torch.nn.utils.clip_grad_norm_(bert.parameters(), max_grad_norm)
          
      if (step+1) % args.gradient_accumulation_steps == 0:
          optimizer.step()
          scheduler.step()
          optimizer.zero_grad()
            
      progressbar.set_description("(Epoch {}) TRAIN LOSS:{:.3f}".format((epoch+1), loss_avg/(step+1)))
      torch.save({"model":bert,"optimizer":optimizer}, os.path.join("./savedmodels/", f"{experiment}_ep{epoch}.pth"))

  bert.eval() 
  eval_loss = 0.0 
  nb_eval_steps = 0 
  preds = None  
  out_label_ids = None 

  for step, batch in enumerate(test_loader):
      batch = tuple(t.to(device) for t in batch)
      labels = batch[3].detach().cpu().numpy()

      with torch.no_grad():
          inputs = {'input_ids':      batch[0],
                    'attention_mask': batch[1],
                    'labels':         labels }
                    
          outputs = bert(**inputs)
          
      tmp_eval_loss, logits = outputs[:2]

      eval_loss += tmp_eval_loss.mean().item() 
      nb_eval_steps += 1 

      if preds is None:
          preds = logits.detach().cpu().numpy()
          out_label_ids = labels.reshape(-1,)
      else:
          preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
          out_label_ids = np.append(out_label_ids, labels.reshape(-1,), axis=0)
  
  eval_loss = eval_loss / nb_eval_steps 
  result = compute_metrics(preds, out_label_ids) 
  
  
  res = {"epoch":epoch,
         "train_loss":loss_avg/len(train_loader), 
         "eval_loss":eval_loss, 
         **result}

  wandb.log(res)

print("\nTraining complete!")

torch.save({'model':bert,'optimizer':optimizer}, os.path.join('./savedmodels/', '{}.bin'.format(experiment)))
```

#### ï¼ˆ3ï¼‰æ¨¡å‹è¯„ä¼°

æœ€åä¸€æ­¥æ˜¯è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ ‡å‡†çš„accuracyã€precisionã€recallã€F1 scoreç­‰æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„å¥½åã€‚

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def compute_metrics(pred, labels):
    acc = accuracy_score(labels, pred.argmax(axis=1))
    prec = precision_score(labels, pred.argmax(axis=1), average='weighted')
    rec = recall_score(labels, pred.argmax(axis=1), average='weighted')
    f1 = f1_score(labels, pred.argmax(axis=1), average='weighted')

    return {"accuracy":acc, "precision":prec, "recall":rec, "f1":f1}
```

## 4.ä»£ç å®ä¾‹ä¸è§£é‡Šè¯´æ˜

ä¸‹é¢ï¼Œæˆ‘ä»¬ç”¨å®é™…çš„ä»£ç ç¤ºä¾‹æ¥å±•ç¤ºBERTæ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚

### 4.1 æ•°æ®é›†è¯»å–ä¸å¤„ç†

```python
from datasets import load_dataset
from transformers import AutoTokenizer

raw_datasets = load_dataset("imdb")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", use_fast=False)

def tokenize_function(examples):
    return tokenizer(examples["text"], padding='max_length', truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

### 4.2 æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°

```python
from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

training_args = TrainingArguments(
    output_dir="./results",          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=tokenized_datasets['train'],         # training dataset
    eval_dataset=tokenized_datasets['test']             # evaluation dataset
)

# Start training
trainer.train()

# Evaluate the model on test data
trainer.evaluate()
```

### 4.3 æµ‹è¯•æ•°æ®é›†é¢„æµ‹

```python
predictions, label_ids, metrics = trainer.predict(tokenized_datasets['test'])
print(metrics)
```