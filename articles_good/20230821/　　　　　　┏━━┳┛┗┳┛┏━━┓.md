
作者：禅与计算机程序设计艺术                    

# 1.简介
  
与背景
随着人工智能领域的火热，越来越多的人加入到这个领域中，已经形成了广泛的研究群体。本文将探讨一些目前最重要的AI算法以及相关应用场景。
AI（Artificial Intelligence）是一个相对来说比较新的概念，但它的确给很多公司、组织带来了巨大的价值。目前有很多人工智能算法正在向我们招手，例如：搜索引擎、推荐系统、图像识别等，并且这些算法都在不断地进化完善。因此，掌握一些核心的AI算法并了解其背后的原理，能够帮助我们更好的理解AI带来的各类影响，做出更加科学有效的决策。
# 2.算法与模型
## 2.1.基于规则的算法
基于规则的算法是指根据一定的条件和规则，由计算机自己去推导出结果。这种算法通过简单逻辑来产生结果，对每一种输入只需要进行少量的计算就可以得到结果，并且规则可以总结成一个个的判断条件，由计算机自己去学习。比如，数独游戏就是这样一个基于规则的算法。
## 2.2.贝叶斯推理算法
贝叶斯推理算法（Bayesian Inference Algorithm），也称概率推理算法或者经典贝叶斯算法，是19世纪著名的统计学家Jaynes在其名著《Probability theory: the logic of science》中提出的，它利用贝叶斯定理（Bayes’ Theorem）和条件概率来进行概率推断。简单地说，就是根据样本数据推测某个事件发生的概率。贝叶斯推理算法虽然概念简单易懂，但却具有广泛的应用，尤其是在文本分类、信息检索、信息过滤、预测性维护以及其他一些领域。
## 2.3.支持向量机（SVM）算法
支持向量机（Support Vector Machine，SVM）是1995年由Vapnik和Chervonenkis提出的算法，其主要目的是解决二元分类问题。直观来讲，支持向量机是一个间隔最大化的线性分类器，它的目标函数是找到一个超平面，将正负两类数据的实例点分开。一旦找到了这个超平面之后，该算法便可以通过决策边界把新的数据划分到合适的类别中。SVM算法是一种非盈利型机器学习算法，其自由度高，能够处理高维数据。
## 2.4.神经网络（Neural Network）
神经网络（Neural Networks）是一种基于连接的、参数化的、多层次的模型。在早期的时代，神经网络在人工智能领域是一种新的应用方式，用于分类、预测、强化学习和模式识别等任务。现如今，神经网络在许多领域已经取得了非常突破性的成果。神经网络中的隐藏层是神经网络的核心组件之一，它含有许多神经元，可以接收外部输入信号，并生成内部信号输出，再发送给其他神经元。这样，多个隐藏层构成了多层神经网络，从而完成复杂的图像和语音识别、分类和回归任务。
## 2.5.决策树算法
决策树算法（Decision Tree Algorithm）是一种机器学习方法，它用来描述对某些输入变量进行分类的过程，即给定一组输入数据，它能够确定每个输入数据属于哪一类。决策树算法特点是简单、易用、不容易过拟合，能够快速准确地分类数据。它所创建的决策树可以表示基于特征的条件测试序列，每个条件测试对应着一个局部的判定，并且最后的结果取决于最后那个测试的判定结果。
## 2.6.随机森林算法
随机森林（Random Forest）是集成学习（Ensemble Learning）中的一种方法，它由一系列决策树组成，其思路是将多个决策树组成一个大的决策系统，通过投票机制来决定最终的结果。随机森林具有很好的容错能力、无偏估计、稳健性、适应性和鲁棒性。
## 2.7.遗传算法
遗传算法（Genetic Algorithms）是一种在计算上模拟种群进化过程的算法。通过对一群 DNA（基因）进行评估、选择、交叉重组等一系列操作，随机生成新的子代，直至找到最优解或达到收敛条件为止。遗传算法可用于各种求解最优化问题、多目标规划、自组织映射、数据挖掘等领域。

综上所述，基于规则的算法、贝叶斯推理算法、支持向量机算法、神经网络、决策树算法、随机森林算法和遗传算法都是目前最主要的AI算法。不同算法的特点、结构和应用领域也各不相同。因此，了解这些算法的基本原理、算法流程及其关键操作就显得十分重要。

# 3.核心概念及术语介绍
为了更好的理解算法背后的原理，我们首先要了解一些核心的概念。下表列出了一些AI领域的核心术语和概念。
|  术语/概念  |  说明   | 
|------------|:------| 
| 数据集     |  数据集是指一组用于训练和测试机器学习模型的数据，通常包括训练数据、验证数据和测试数据。 |  
| 模型       |  模型是对特定问题的一组公式或函数，它们定义了输入和输出之间的关系。 |   
| 标签       |  标签是数据集中每个数据项对应的正确输出值。标签也被称作“目标”或“真实值”。 |     
| 特征       |  特征是数据集中输入变量或属性。特征也被称作“属性”、“输入”、“变量”或“输入变量”。 |       
| 训练集     |  训练集是用已知的输入-输出配对来训练模型的数据集。训练集的大小往往占整个数据集的 80%～90%。|          
| 测试集     |  测试集是用未知的输入-输出配对来测试模型的数据集。测试集的大小占整个数据集的 10%~20%。 |         
| 损失函数   |  损失函数（Loss function）是衡量模型预测值的距离程度的度量标准。它表示模型的好坏，给予模型不同的损失值以寻找最佳模型。 |        
| 算法       |  算法是指实现特定任务的方法或流程，它定义了输入、输出、模型和策略之间的映射关系。 |       
| 超参数   |  超参数是模型的一些基本参数，它们不是直接被学习，而是在训练过程中通过调整参数获得最佳模型效果。 | 

# 4.算法原理和操作步骤详解
## 4.1.基于规则的算法
### 4.1.1.数独算法
数独（Sudoku）是最早由英国数学家马克·约翰·斯宾诺莎（<NAME>son）在1994年发现的九宫格逻辑游戏。它是一个约束满足问题，目标是填满由 9x9 个数字单元格组成的 9x9 表格，使每行、每列、每 3x3 宫内的所有数字均含且只有一次。数独的目标是帮助孩子熟悉消除障碍、推理、逻辑推理和运用分析工具。

数独算法起源于1979年由约翰·梅罗维尔（John Merwöhrle）提出。它是一个简单的基于规则的算法，使用数独的规则判断某个数字是否可以放入某一空格。具体操作步骤如下：

1. 检查初始状态是否合法，即检查每一行、每一列、每一 3x3 宫是否存在重复数字；
2. 如果当前空格没有数字，则使用某一固定数字（如 1-9 的任意数字）尝试填入；
   - 如果尝试成功，则转至步骤 3；
   - 如果尝试失败，则转至步骤 4；
3. 如果尝试成功，则跳至下一个空格继续尝试，直至所有空格都填满或失败结束；
4. 从上一步的结果开始，反复尝试填入可能的值，直至所有可能的值都试验完毕；
5. 从第 2 步开始，如果尝试失败，则回溯到上一步尝试过的值，重新尝试某一值，直至成功或者尝试完毕。

数独算法的运行时间长且复杂，但是它的优点在于简单、规则化，适用于小数据集。
## 4.2.贝叶斯推理算法
贝叶斯推理算法（Bayesian Inference Algorithm）是一种基于概率论的统计推理算法。它认为每件事情的发生具有随机性，并假设存在先验概率分布，根据后验概率分布对事件进行建模。其中，先验概率分布是固定的，而后验概率分布则是变化的。贝叶斯推理算法利用贝叶斯公式，通过调节先验概率分布的参数，得到后验概率分布。

贝叶斯推理算法的一般过程为：

1. 收集数据：从样本空间中抽取独立同分布的数据集合，形成训练数据集；
2. 准备模型：根据数据集构建联合概率分布 P(X)，X 为待估参数；
3. 计算似然函数：计算数据集 D 出现的似然函数 Likelihood = P(D|X)；
4. 更新参数：根据似然函数计算后验概率分布 P(X|D)，得到 X 的后验分布；
5. 根据后验概率分布进行推断：根据后验分布和实际情况对事件进行推断。

贝叶斯推理算法能够利用先验知识对数据进行归纳，同时对参数进行估计，对实际情况提供了较为准确的评估。由于贝叶斯推理算法的灵活性、广泛应用和良好的理论基础，所以被广泛使用。

## 4.3.支持向量机算法
支持向量机（Support Vector Machine，SVM）是一种监督式的机器学习方法，它可以有效地解决二元分类问题。直观地讲，SVM 是找到一个超平面将正负两类数据分开。SVM 通过最大化分离超平面的Margin最大化而得到的。SVM算法的目标函数为：

L = Σ max (0, 1−yi(WXi+b)) + λ||W||^2

其中，λ 是软间隔惩罚项，L 为损失函数，W 为权重向量，Xi 是输入向量，Yi 是对应输出类别，xi*Wi+b 为对应于 xi 的支持向量的函数值，Σ 表示求和，max (a,b) 表示最大值。

具体操作步骤如下：

1. 优化目标函数：求解 W 和 b 的参数值，使得函数值 L 最小；
2. 将正负类样本进行标记；
3. 使用核函数映射数据到高维空间，使其线性可分；
4. 采用序列最小最优化算法求解问题；

SVM算法是一种高效的机器学习算法，通过核函数的映射将非线性数据映射到高维空间中，将线性不可分的问题转换成线性可分的问题，取得了很好的效果。

## 4.4.神经网络算法
神经网络（Neural Networks）是一种基于模仿生物神经元连接的方式，模拟人的大脑神经网络行为，并根据自身环境和输入采集到的信息，进行信息处理和输出。神经网络由输入层、隐层（隐藏层）和输出层构成，并通过激活函数传递信息，共同驱动整个系统运行。

神经网络算法的过程如下：

1. 初始化参数：设置模型参数（权重矩阵 W 和偏置向量 b）；
2. 前向传播：按照输入层——隐层——输出层的顺序，依次对输入数据进行处理；
3. 求损失函数：采用损失函数对输出结果进行评估，并与标签进行比较，计算误差值；
4. 反向传播：根据损失函数对模型参数进行更新，以减轻误差；
5. 迭代训练：重复步骤 2-4，直至模型性能达到要求。

神经网络的特点是具有高度的并行化特性，能够高效地处理大量的数据，因此被广泛应用于图像识别、语音识别、机器翻译等领域。

## 4.5.决策树算法
决策树（Decision Tree）是一种机器学习方法，它主要用于分类和回归问题。它是以树状结构存储的决策 rules，每个节点表示一个属性上的测试，而每个分支代表一个判断结果。决策树算法通过判断每个测试的优劣来建立分类模型，使得决策过程变得简单直观，同时也可避免过拟合现象。

决策树算法的工作原理简单而直接，它按照一定的顺序构建若干个分支，根据每个分支上的属性和样本，对样本进行分类。因此，决策树是一个十分易用的算法，对许多实际问题都有很好的解释力。

## 4.6.随机森林算法
随机森林（Random Forest）是集成学习（Ensemble Learning）中的一种方法，它由一系列决策树组成，其思路是将多个决策树组成一个大的决策系统，通过投票机制来决定最终的结果。随机森林具有很好的容错能力、无偏估计、稳健性、适应性和鲁棒性。

随机森林算法的工作过程如下：

1. 选取多个子决策树作为基学习器；
2. 对训练数据进行扰动；
3. 用各子决策树进行训练；
4. 在测试数据上进行预测；
5. 进行投票，决定最终的分类结果。

随机森林算法通过构建多个决策树来降低模型的方差和偏差，提升预测精度。它在不剪枝的前提下，能自动选择需要的特征，并集成不同子树的预测结果，既可以缓解过拟合问题，又可以提升模型的预测能力。

## 4.7.遗传算法
遗传算法（Genetic Algorithms）是一种在计算上模拟种群进化过程的算法。通过对一群 DNA（基因）进行评估、选择、交叉重组等一系列操作，随机生成新的子代，直至找到最优解或达到收敛条件为止。遗传算法可用于各种求解最优化问题、多目标规划、自组织映射、数据挖掘等领域。

遗传算法的过程如下：

1. 初始化种群：随机生成一组个体，成为初始种群；
2. 计算适应度：为每一个个体计算适应度，适应度越高，个体越有可能被保留下来；
3. 拿到适应度最高的个体；
4. 生成子代：以一定概率进行杂交（Crossover）操作，产生两个个体的子代；
5. 进行变异操作；
6. 把子代加入到种群中；
7. 重复以上步骤，直至满足终止条件；

遗传算法有很多优点，它利用了生物进化的理念，模拟了自然界中自行演化的过程，并保证了算法的高度稳健性。因此，遗传算法是一类高级的最优化算法，具有广泛的应用价值。

# 5.具体代码实例及解释说明
为了更好的理解算法的原理和实现，我们可以借助开源框架或工具进行示例代码的编写。下面展示了基于Python语言的numpy、sklearn库的数独游戏算法、贝叶斯推理算法、支持向量机算法、神经网络算法、决策树算法、随机森林算法和遗传算法的实现。

## 5.1.数独游戏算法实现
```python
import numpy as np


def solve_sudoku():
    # 初始化一个空白的 9x9 矩阵
    board = np.zeros((9, 9), dtype=np.int)

    def fill_cell(row, col):
        # 获取当前行、列、九宫格范围的可用数字列表
        row_vals = set([board[row][c] for c in range(9)])
        col_vals = set([board[r][col] for r in range(9)])
        box_vals = set([board[(row//3)*3+r][(col//3)*3+c]
                        for r in range(3) for c in range(3)])

        # 当前位置的可用数字列表
        available = set(range(1, 10)).difference(
            list(row_vals).extend(list(col_vals)).extend(list(box_vals)))

        if not available:
            return False  # 没有可用数字了，返回 False

        # 遍历可用数字，尝试在当前位置填写
        for num in available:
            board[row][col] = num

            # 判断填入后是否有冲突
            conflicted = any([(num == board[r][c]) and
                              ((r!= row) or (c!= col))
                              for r in range(9) for c in range(9)])

            if not conflicted:
                # 不冲突，递归填充下一个空白位置
                if col < 8:
                    result = fill_cell(row, col+1)

                    if result:
                        return True

                else:
                    if row < 8:
                        result = fill_cell(row+1, 0)

                        if result:
                            return True

                    else:
                        print("solved")
                        return True

            board[row][col] = 0  # 恢复到初始状态

        return False  # 所有候选值均不可行

    start = fill_cell(0, 0)

    if not start:
        print("No solution found.")

    return board
```
## 5.2.贝叶斯推理算法实现
```python
import math


class BayesianInferenceAlgorithm:

    def __init__(self, data, alpha=1.0):
        self.data = data
        self.alpha = alpha
        self.classes = []
        self.p_y = {}  # p(Y|X) 字典
        self.p_x = {}  # p(X) 字典
        self.p_xy = {}  # p(X,Y) 字典

    def train(self):
        self._train()
        pass

    def predict(self, sample):
        """
        对测试样本进行预测
        :param sample: 测试样本
        :return: 预测的标签
        """
        label_probs = self._calculate_label_probs(sample)
        labels = sorted(label_probs, key=lambda x: label_probs[x], reverse=True)
        return labels[0]

    def _calculate_label_probs(self, sample):
        """
        计算标签概率
        :param sample: 测试样本
        :return: 标签概率字典
        """
        p_ys = {clss: 0 for clss in self.classes}
        total_count = len(self.data)
        classes_counts = {}

        for datum in self.data:
            if datum[-1] not in classes_counts:
                classes_counts[datum[-1]] = 1
            else:
                classes_counts[datum[-1]] += 1

        for clss in self.classes:
            p_ys[clss] = classes_counts[clss]/total_count

        for clss in self.classes:
            denominators = []
            numerators = [math.log(self.alpha)] * 10
            count_cls_x = sum([d[-1]==clss for d in self.data])
            
            for i, val in enumerate(sample[:-1]):
                if val is None:
                    continue
                
                try:
                    count_val_x = sum([d[-1]==clss and d[i]!=None for d in self.data])/count_cls_x
                    
                    for j in range(len(self.p_x)):
                        
                        if self.p_xy[j][i][int(val)-1]>0:
                            
                            numerators[int(val)-1] -= math.log(self.p_xy[j][i][int(val)-1])
                            
                    denominators.append(math.log(sum([math.exp(numerators[k]) for k in range(10)])) -
                                        sum([math.log(denominator) for denominator in denominators]))
                            
                    if denominators[-1]<0:
                        break
                except KeyError:
                    continue
            p_ys[clss] *= math.exp(numerators[int(sample[-1])-1]-denominators[-1])
        
        return p_ys
    
    def _train(self):
        """
        训练算法
        :return: 
        """
        features = list(range(len(self.data[0])-1))
        self.classes = set([d[-1] for d in self.data])
        count_cls_x = {}
        
        for datum in self.data:
            cls = datum[-1]
            if cls not in count_cls_x:
                count_cls_x[cls] = 1
            else:
                count_cls_x[cls] += 1
            
        for feature in features:
            f_values = set([d[feature] for d in self.data if d[feature]])
            self.p_x[feature] = {}
            self.p_xy[feature] = {}
            
            for value in f_values:
                v_count = sum([d[feature]==value for d in self.data if d[feature]])
                
                self.p_x[feature][value] = v_count / \
                                            len([d for d in self.data if d[feature]])
                
                self.p_xy[feature][value] = {}
                
                for clss in self.classes:
                    xy_count = sum([d[feature]==value and d[-1]==clss for d in self.data if d[feature]])
                    
                    self.p_xy[feature][value][clss] = xy_count / \
                                                        len([d for d in self.data if d[feature]])
                    
if __name__=="__main__":
    algorithm = BayesianInferenceAlgorithm([[1, "yes"], [2, "no"], [1, "yes"], [2, "no"]], alpha=1.0)
    algorithm.train()
    print(algorithm.predict([1, None]))
    
```
## 5.3.支持向量机算法实现
```python
from sklearn import svm


def support_vector_machine():
    X = [[0, 0], [1, 1], [-1, -1], [2, 2]]
    y = [0, 1, 1, 0]
    clf = svm.SVC()
    clf.fit(X, y)
    decision_function = clf.decision_function([[1.5, 1.5]])
    pred = clf.predict([[1.5, 1.5]])
    print('Pred:', pred)
    print('Decision Function:', decision_function)
    
    
support_vector_machine()
```
## 5.4.神经网络算法实现
```python
import tensorflow as tf
from sklearn import datasets


def neural_network():
    iris = datasets.load_iris()
    X = iris.data[:, :]
    Y = iris.target
    n_input = 4
    n_hidden = 4
    n_output = 3
    learning_rate = 0.01

    sess = tf.Session()

    # Define placeholders
    X_placeholer = tf.placeholder(tf.float32, shape=[None, n_input])
    Y_placeholer = tf.placeholder(tf.float32, shape=[None, n_output])

    # Define weights & bias for hidden layer
    W1 = tf.Variable(tf.random_normal([n_input, n_hidden]), name='weights')
    B1 = tf.Variable(tf.random_normal([n_hidden]), name='bias')

    # Define activation function
    act_func = tf.nn.sigmoid

    # Hidden layer
    Z1 = tf.add(tf.matmul(X_placeholer, W1), B1)
    A1 = act_func(Z1)

    # Output Layer
    W2 = tf.Variable(tf.random_normal([n_hidden, n_output]), name='weights')
    B2 = tf.Variable(tf.random_normal([n_output]), name='bias')

    # Predictions
    logits = tf.add(tf.matmul(A1, W2), B2)
    prediction = tf.nn.softmax(logits)

    # Cost function
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_placeholer, logits=logits))

    # Optimization method
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

    # Initialize variables
    init = tf.global_variables_initializer()
    sess.run(init)

    # Training loop
    batch_size = 100
    epochs = 5000
    display_step = 500

    for epoch in range(epochs):
        avg_cost = 0.
        total_batch = int(len(X)/batch_size)

        for i in range(total_batch):
            randidx = np.random.randint(len(X), size=batch_size)
            batch_xs = X[randidx, :]
            batch_ys = Y[randidx, :]

            _, c = sess.run([optimizer, cost], feed_dict={X_placeholer: batch_xs, Y_placeholer: batch_ys})

            avg_cost += c / total_batch

        if (epoch+1) % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1),
                  "Cost=", "{:.9f}".format(avg_cost))

    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y_placeholer, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

    acc = sess.run(accuracy, feed_dict={X_placeholer: X, Y_placehoder: Y})

    print("Accuracy:", acc)


    Xnew = np.array([[1, 2.9, 1, 0.2],[1, 0.8, 0.6, 0.2]])
    preds = sess.run(prediction, feed_dict={X_placeholer: Xnew})

    print("Predictions:", preds)

    sess.close()
    
neural_network()
```
## 5.5.决策树算法实现
```python
from sklearn import tree


def decision_tree():
    iris = datasets.load_iris()
    X = iris.data[:, :]
    Y = iris.target
    clf = tree.DecisionTreeClassifier()
    clf.fit(X, Y)
    score = clf.score(X, Y)
    print('Score:', score)
    dot_data = tree.export_graphviz(clf, out_file=None,
                                    class_names=['Setosa', 'Versicolour', 'Virginica'],
                                    filled=True, rounded=True, special_characters=True)
    graph = pydotplus.graph_from_dot_data(dot_data)

    
decision_tree()
```
## 5.6.随机森林算法实现
```python
from sklearn.ensemble import RandomForestClassifier


def random_forest():
    iris = datasets.load_iris()
    X = iris.data[:, :]
    Y = iris.target
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(X, Y)
    score = rf.score(X, Y)
    print('Score:', score)

    
random_forest()
```
## 5.7.遗传算法实现
```python
import random
import numpy as np


class GeneticAlgorithm:

    @staticmethod
    def run(population_size, mutation_prob, fitness_function, target, genes_count, max_iterations):
        population = [[random.choice(['0', '1']) for _ in range(genes_count)]
                      for _ in range(population_size)]
        fitnesses = [fitness_function(individual, target) for individual in population]

        best_individual = min(zip(population, fitnesses), key=lambda item: item[1])[0]
        best_fitness = min(fitnesses)

        iteration = 1

        while iteration <= max_iterations:
            new_population = []

            for father, mother in zip(select_parents(population, fitnesses, selection_size=int(population_size/2)),
                                      select_parents(population, fitnesses, selection_size=int(population_size/2))):
                child1 = crossover(father, mother)
                child2 = crossover(mother, father)

                mutated_child1 = mutate(child1, mutation_prob)
                mutated_child2 = mutate(child2, mutation_prob)

                new_population.append(mutated_child1)
                new_population.append(mutated_child2)

            next_generation = [*new_population[:int(population_size/2)],
                               *[mutate(best_individual, mutation_prob) for _ in range(int(population_size/2))]]

            fitnesses = [fitness_function(individual, target) for individual in next_generation]

            best_individual = min(zip(next_generation, fitnesses), key=lambda item: item[1])[0]
            best_fitness = min(fitnesses)

            if abs(best_fitness - target) < 1e-10:
                return (iteration, best_individual)

            population = next_generation
            iteration += 1

        return (iteration, best_individual)


def fitness_function(individual, target):
    binary_to_decimal = lambda s: int(''.join(str(_) for _ in s), base=2)
    decimal = binary_to_decimal(individual)
    error = abs(decimal - target)
    return round(error, 5)


def select_parents(population, fitnesses, selection_size):
    fitness_order = sorted(enumerate(fitnesses), key=lambda item: item[1])
    selected_indexes = [item[0] for item in fitness_order][:selection_size]
    parents = [population[index] for index in selected_indexes]
    return parents


def crossover(parent1, parent2):
    crossover_point = random.randint(1, len(parent1)-2)
    offspring1 = parent1[:crossover_point] + parent2[crossover_point:]
    offspring2 = parent2[:crossover_point] + parent1[crossover_point:]
    return offspring1, offspring2


def mutate(individual, mutation_prob):
    for i, gene in enumerate(individual):
        if random.uniform(0, 1) < mutation_prob:
            individual[i] = '0' if gene == '1' else '1'
    return ''.join(individual)


def main():
    target = 7
    generation_limit = 1000
    population_size = 100
    mutation_prob = 0.01
    genes_count = 10
    result = GeneticAlgorithm.run(population_size=population_size,
                                  mutation_prob=mutation_prob,
                                  fitness_function=fitness_function,
                                  target=target,
                                  genes_count=genes_count,
                                  max_iterations=generation_limit)

    print('Solution found after', result[0],
          'generations with fitness equal to', result[1])


if __name__=='__main__':
    main()
```