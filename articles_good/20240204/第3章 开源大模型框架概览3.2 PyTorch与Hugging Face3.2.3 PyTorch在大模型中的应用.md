                 

# 1.èƒŒæ™¯ä»‹ç»

3.2 PyTorchä¸Hugging Faceï¼šPyTorchåœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨
=======================================

## 3.2.1 èƒŒæ™¯ä»‹ç»

è‡ª2012å¹´AlexNetåœ¨ImageNet competionä¸Šè·èƒœä»¥æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚éšç€ç¡¬ä»¶è®¾å¤‡çš„å‘å±•ï¼Œäººä»¬å¼€å§‹è®­ç»ƒè¶Šæ¥è¶Šå¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè®­ç»ƒä¸€ä¸ªå¤§è§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦å¾ˆå¤šçš„èµ„æºå’Œæ—¶é—´ã€‚å› æ­¤ï¼Œå¼€æºå¤§æ¨¡å‹æ¡†æ¶åº”è¿è€Œç”Ÿã€‚

åœ¨å¤§æ¨¡å‹æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…éœ€è¦æ”¯æŒæ ‡å‡†çš„è®­ç»ƒå’Œæµ‹è¯•æµç¨‹ï¼Œè¿˜éœ€è¦æ›´é«˜æ•ˆçš„è®°å¿†ç®¡ç†ã€å¹¶è¡Œè®¡ç®—ã€æ•°æ®ç®¡é“ç­‰ç‰¹æ€§ã€‚å› æ­¤ï¼Œå¼€æºå¤§æ¨¡å‹æ¡†æ¶é‡‡ç”¨äº†åˆ†å¸ƒå¼è®¡ç®—ã€åŠ¨æ€è®¡ç®—å›¾ã€æ··åˆç²¾åº¦è®­ç»ƒç­‰æŠ€æœ¯ã€‚

PyTorchæ˜¯ç”±Facebook AI Researchå¼€æºçš„ä¸€ç§åŸºäºPythonçš„æ·±åº¦å­¦ä¹ åº“ã€‚å®ƒé‡‡ç”¨Pythonicçš„APIè®¾è®¡ï¼Œå¹¶ä¸”ä¸NumPyå…¼å®¹ã€‚PyTorchæä¾›äº†åŠ¨æ€è®¡ç®—å›¾ã€è‡ªå®šä¹‰æ“ä½œã€CUDAæ”¯æŒç­‰ç‰¹æ€§ã€‚

Hugging Faceæ˜¯å¦ä¸€å®¶æ·±åº¦å­¦ä¹ é¢†åŸŸçš„å…¬å¸ï¼Œä¸“æ³¨äºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„å¼€æºé¡¹ç›®ã€‚ä»–ä»¬å¼€æºäº†Transformersåº“ï¼Œæä¾›äº†PyTorchå’ŒTensorFlowçš„APIã€‚Transformersåº“æ”¯æŒBERTã€RoBERTaã€XLNetç­‰ä¼—å¤šé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶æä¾›äº†ç®€å•æ˜“ç”¨çš„APIã€‚

## 3.2.2 æ ¸å¿ƒæ¦‚å¿µä¸è”ç³»

åœ¨PyTorchä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torch.nn`æ¨¡å—æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ã€‚PyTorchæä¾›äº†å·ç§¯å±‚ã€å…¨è¿æ¥å±‚ã€å½’ä¸€åŒ–å±‚ç­‰å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå±‚ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨`torch.optim`æ¨¡å—å®šä¹‰ä¼˜åŒ–å™¨ï¼Œå¦‚SGDã€Adamç­‰ã€‚

Hugging Faceçš„Transformersåº“æä¾›äº†`AutoModel`å’Œ`AutoModelForSequenceClassification`ç­‰ç±»ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ é€’æ¨¡å‹åç§°æ¥å®ä¾‹åŒ–è¿™äº›ç±»ï¼Œä¾‹å¦‚ï¼š
```python
from transformers import AutoModel, AutoModelForSequenceClassification

model = AutoModel.from_pretrained('bert-base-uncased')
classifier = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
```
åœ¨Transformersåº“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Trainer`ç±»æ¥è®­ç»ƒæ¨¡å‹ã€‚`Trainer`ç±»æ”¯æŒå¤šGPUè®­ç»ƒã€æ··åˆç²¾åº¦è®­ç»ƒã€æ•°æ®é›†ç®¡ç†ç­‰ç‰¹æ€§ã€‚

## 3.2.3 æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦æ¨¡å‹å…¬å¼è¯¦ç»†è®²è§£

### 3.2.3.1 PyTorchåœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

#### 3.2.3.1.1 åŠ¨æ€è®¡ç®—å›¾

PyTorchçš„æ ¸å¿ƒç‰¹ç‚¹ä¹‹ä¸€æ˜¯æ”¯æŒåŠ¨æ€è®¡ç®—å›¾ã€‚è¿™æ„å‘³ç€PyTorchä¼šåœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶æ„é€ è®¡ç®—å›¾ï¼Œè€Œä¸æ˜¯åƒTensorFlowä¸€æ ·åœ¨æ„é€ å›¾æ—¶å°±ç¡®å®šå¥½è®¡ç®—å›¾ã€‚

åŠ¨æ€è®¡ç®—å›¾æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜ç‚¹ï¼š

* æ›´çµæ´»ï¼šæˆ‘ä»¬å¯ä»¥åœ¨ä»»ä½•ä½ç½®ä¿®æ”¹è®¡ç®—å›¾ï¼Œä¾‹å¦‚åœ¨å¾ªç¯ä¸­æ·»åŠ æ“ä½œï¼›
* æ›´å°‘çš„å†…å­˜å ç”¨ï¼šå› ä¸ºåªåœ¨éœ€è¦æ—¶åˆ›å»ºèŠ‚ç‚¹ï¼Œæ‰€ä»¥åŠ¨æ€è®¡ç®—å›¾å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å†…å­˜ï¼›
* æ›´å¥½çš„è°ƒè¯•ï¼šåŠ¨æ€è®¡ç®—å›¾å…è®¸æˆ‘ä»¬åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æ•è·é”™è¯¯ï¼Œè€Œä¸æ˜¯åœ¨æ„é€ å›¾æ—¶ï¼›

åŠ¨æ€è®¡ç®—å›¾çš„ç¼ºç‚¹ä¹‹ä¸€æ˜¯æ€§èƒ½æ¯”é™æ€è®¡ç®—å›¾æ…¢ã€‚ä½†æ˜¯ï¼ŒPyTorchå›¢é˜Ÿå·²ç»åšå‡ºäº†å¾ˆå¤šä¼˜åŒ–ï¼Œä¾‹å¦‚ä½¿ç”¨JITç¼–è¯‘å™¨ã€C++ backendç­‰ã€‚

#### 3.2.3.1.2 è‡ªå®šä¹‰æ“ä½œ

PyTorchå…è®¸æˆ‘ä»¬å®šä¹‰è‡ªå·±çš„æ“ä½œã€‚è¿™å¯¹äºå®šåˆ¶åŒ–çš„æ“ä½œéå¸¸æœ‰å¸®åŠ©ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæ–°çš„æ¿€æ´»å‡½æ•°ã€ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ç­‰ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torch.autograd.Function`ç±»æ¥å®šä¹‰è‡ªå·±çš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„æ“ä½œ`AddBias`ï¼Œå®ƒåœ¨è¾“å…¥å¼ é‡ä¸Šå¢åŠ ä¸€ä¸ªåç½®é¡¹ï¼š
```python
import torch
import torch.autograd as autograd

class AddBias(autograd.Function):
   def forward(self, input, bias):
       self.save_for_backward(input, bias)
       return input + bias

   def backward(self, grad_output):
       input, bias = self.saved_tensors
       return grad_output, grad_output.new_zeros(bias.size())

add_bias = AddBias()
input = torch.randn(3, 4)
bias = torch.tensor([1.0, 2.0, 3.0, 4.0])
output = add_bias(input, bias)
print(output)
```
#### 3.2.3.1.3 CUDAæ”¯æŒ

PyTorchæ”¯æŒCUDAï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨GPUä¸Šè¿è¡ŒPyTorchä»£ç ã€‚è¿™å¯¹äºè®­ç»ƒå¤§è§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹éå¸¸æœ‰å¸®åŠ©ã€‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torch.device`å‡½æ•°æ¥æŒ‡å®šè®¾å¤‡ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªå¼ é‡ç§»åˆ°GPUä¸Šï¼š
```python
import torch

x = torch.randn(3, 4)
device = torch.device('cuda')
x = x.to(device)
print(x.device)
```
#### 3.2.3.1.4 åˆ†å¸ƒå¼è®¡ç®—

PyTorchæ”¯æŒåˆ†å¸ƒå¼è®¡ç®—ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨å¤šå°æœºå™¨ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ã€‚è¿™å¯¹äºè®­ç»ƒå¤§è§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹éå¸¸æœ‰å¸®åŠ©ã€‚

PyTorchæä¾›äº†`torch.distributed`æ¨¡å—æ¥æ”¯æŒåˆ†å¸ƒå¼è®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`torch.distributed.init_process_group`å‡½æ•°æ¥åˆå§‹åŒ–è¿›ç¨‹ç»„ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`DistributedDataParallel`ç±»æ¥å®ç°åˆ†å¸ƒå¼è®­ç»ƒã€‚

### 3.2.3.2 Hugging Faceåœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

#### 3.2.3.2.1 é¢„è®­ç»ƒæ¨¡å‹

Hugging Faceçš„Transformersåº“æä¾›äº†ä¼—å¤šçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å·²ç»è¢«è®­ç»ƒäº†æ•°ç™¾å°æ—¶ç”šè‡³æ•°åƒå°æ—¶ï¼Œå¹¶ä¸”å·²ç»è¾¾åˆ°äº†SOTAæ°´å¹³ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹è®­ç»ƒã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨BERTæ¨¡å‹æ¥å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬åªéœ€è¦åŠ è½½BERTæ¨¡å‹ï¼Œç„¶åå°†æ–‡æœ¬è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæœ€åè¾“å‡ºåˆ†ç±»ç»“æœï¼š
```python
from transformers import BertTokenizer, BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

text = "This is an example text."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0]
predictions = torch.argmax(scores)
print(predictions)
```
#### 3.2.3.2.2 æ•°æ®é›†ç®¡ç†

Hugging Faceçš„Transformersåº“æä¾›äº†`Dataset`å’Œ`DataCollator`ç±»æ¥ç®¡ç†æ•°æ®é›†ã€‚

`Dataset`ç±»å¯ä»¥ç”¨æ¥åŠ è½½æ•°æ®é›†ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºPyTorchçš„`Tensor`å¯¹è±¡ã€‚`DataCollator`ç±»å¯ä»¥ç”¨æ¥å°†batchæ•°æ®è½¬æ¢ä¸ºæ¨¡å‹çš„è¾“å…¥æ ¼å¼ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Dataset`å’Œ`DataCollator`ç±»æ¥åŠ è½½IMDBç”µå½±è¯„è®ºæ•°æ®é›†ï¼š
```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

dataset = load_dataset('imdb', split='train')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

inputs = dataset[0]
encoded_inputs = tokenizer(inputs['text'], truncation=True, padding="max_length", max_length=512)
batch = data_collator(encoded_inputs)
print(batch)
```
#### 3.2.3.2.3 åˆ†å¸ƒå¼è®­ç»ƒ

Hugging Faceçš„Transformersåº“æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Trainer`ç±»æ¥è®­ç»ƒæ¨¡å‹ã€‚`Trainer`ç±»æ”¯æŒå¤šGPUè®­ç»ƒã€æ··åˆç²¾åº¦è®­ç»ƒã€æ•°æ®é›†ç®¡ç†ç­‰ç‰¹æ€§ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`Trainer`ç±»æ¥è®­ç»ƒBERTæ¨¡å‹ï¼š
```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
   output_dir='./results',         # output directory
   num_train_epochs=3,             # total number of training epochs
   per_device_train_batch_size=16,  # batch size per device during training
   warmup_steps=500,               # number of warmup steps for learning rate scheduler
   weight_decay=0.01,              # strength of weight decay
   logging_dir='./logs',           # directory for storing logs
)

trainer = Trainer(
   model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
   args=training_args,                 # training arguments, defined above
   train_dataset=dataset,              # training dataset
   eval_dataset=dataset,               # evaluation dataset
)

trainer.train()
```
## 3.2.4 å…·ä½“æœ€ä½³å®è·µï¼šä»£ç å®ä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜

### 3.2.4.1 PyTorchåœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

#### 3.2.4.1.1 åŠ¨æ€è®¡ç®—å›¾

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„ç¤ºä¾‹ï¼š
```python
import torch
import torch.nn as nn
import torch.optim as optim

class Net(nn.Module):
   def __init__(self):
       super(Net, self).__init__()
       self.fc1 = nn.Linear(784, 256)
       self.fc2 = nn.Linear(256, 10)

   def forward(self, x):
       x = F.relu(self.fc1(x))
       x = self.fc2(x)
       return x

net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

for epoch in range(10):
   running_loss = 0.0
   for i, data in enumerate(trainloader, 0):
       inputs, labels = data
       optimizer.zero_grad()
       outputs = net(inputs.view(-1, 784))
       loss = criterion(outputs, labels)
       loss.backward()
       optimizer.step()
       running_loss += loss.item()
   print('Epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))

print('Finished Training')
```
#### 3.2.4.1.2 è‡ªå®šä¹‰æ“ä½œ

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨è‡ªå®šä¹‰æ“ä½œè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„ç¤ºä¾‹ï¼š
```python
import torch
import torch.autograd as autograd

class AddBias(autograd.Function):
   def forward(self, input, bias):
       self.save_for_backward(input, bias)
       return input + bias

   def backward(self, grad_output):
       input, bias = self.saved_tensors
       return grad_output, grad_output.new_zeros(bias.size())

class Net(nn.Module):
   def __init__(self):
       super(Net, self).__init__()
       self.fc1 = nn.Linear(784, 256)
       self.fc2 = nn.Linear(256, 10)

   def forward(self, x):
       x = AddBias()(self.fc1(x), torch.tensor([1.0]))
       x = F.relu(x)
       x = self.fc2(x)
       return x

net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

for epoch in range(10):
   running_loss = 0.0
   for i, data in enumerate(trainloader, 0):
       inputs, labels = data
       optimizer.zero_grad()
       outputs = net(inputs.view(-1, 784))
       loss = criterion(outputs, labels)
       loss.backward()
       optimizer.step()
       running_loss += loss.item()
   print('Epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))

print('Finished Training')
```
#### 3.2.4.1.3 CUDAæ”¯æŒ

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨CUDAè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„ç¤ºä¾‹ï¼š
```python
import torch
import torch.nn as nn
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Net(nn.Module):
   def __init__(self):
       super(Net, self).__init__()
       self.fc1 = nn.Linear(784, 256)
       self.fc2 = nn.Linear(256, 10)

   def forward(self, x):
       x = F.relu(self.fc1(x))
       x = self.fc2(x)
       return x

net = Net().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

for epoch in range(10):
   running_loss = 0.0
   for i, data in enumerate(trainloader, 0):
       inputs, labels = data
       inputs, labels = inputs.to(device), labels.to(device)
       optimizer.zero_grad()
       outputs = net(inputs.view(-1, 784))
       loss = criterion(outputs, labels)
       loss.backward()
       optimizer.step()
       running_loss += loss.item()
   print('Epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))

print('Finished Training')
```
#### 3.2.4.1.4 åˆ†å¸ƒå¼è®¡ç®—

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„ç¤ºä¾‹ï¼š
```python
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

def train(rank, world_size):
   os.environ['MASTER_ADDR'] = 'localhost'
   os.environ['MASTER_PORT'] = '12355'

   dist.init_process_group("nccl", rank=rank, world_size=world_size)

   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

   model = MyModel().to(device)
   model = DDP(model, device_ids=[device])

   criterion = nn.CrossEntropyLoss()
   optimizer = optim.SGD(model.parameters(), lr=0.01)

   for epoch in range(10):
       running_loss = 0.0
       for i, data in enumerate(trainloader, 0):
           inputs, labels = data
           inputs, labels = inputs.to(device), labels.to(device)
           optimizer.zero_grad()
           outputs = model(inputs.view(-1, 784))
           loss = criterion(outputs, labels)
           loss.backward()
           optimizer.step()
           running_loss += loss.item()
       print('Rank: %d Epoch: %d loss: %.3f' % (rank, epoch + 1, running_loss / len(trainloader)))

   dist.destroy_process_group()

mp.spawn(train, args=(4,), nprocs=4, join=True)
```
### 3.2.4.2 Hugging Faceåœ¨å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

#### 3.2.4.2.1 é¢„è®­ç»ƒæ¨¡å‹

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å®Œæˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„ç¤ºä¾‹ï¼š
```python
from transformers import BertTokenizer, BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

text = "This is an example text."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0]
predictions = torch.argmax(scores)
print(predictions)
```
#### 3.2.4.2.2 æ•°æ®é›†ç®¡ç†

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`Dataset`å’Œ`DataCollator`ç±»ç®¡ç†æ•°æ®é›†çš„ç¤ºä¾‹ï¼š
```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

dataset = load_dataset('imdb', split='train')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

inputs = dataset[0]
encoded_inputs = tokenizer(inputs['text'], truncation=True, padding="max_length", max_length=512)
batch = data_collator(encoded_inputs)
print(batch)
```
#### 3.2.4.2.3 åˆ†å¸ƒå¼è®­ç»ƒ

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨`Trainer`ç±»è®­ç»ƒBERTæ¨¡å‹çš„ç¤ºä¾‹ï¼š
```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
   output_dir='./results',         # output directory
   num_train_epochs=3,             # total number of training epochs
   per_device_train_batch_size=16,  # batch size per device during training
   warmup_steps=500,               # number of warmup steps for learning rate scheduler
   weight_decay=0.01,              # strength of weight decay
   logging_dir='./logs',           # directory for storing logs
)

trainer = Trainer(
   model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
   args=training_args,                 # training arguments, defined above
   train_dataset=dataset,              # training dataset
   eval_dataset=dataset,               # evaluation dataset
)

trainer.train()
```
## 3.2.5 å®é™…åº”ç”¨åœºæ™¯

* **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šPyTorchå’ŒHugging Faceåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸæœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚Hugging Faceæä¾›äº†ä¼—å¤šçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›æ¨¡å‹æ¥å®Œæˆæ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç³»ç»Ÿã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚
* **è®¡ç®—æœºè§†è§‰**ï¼šPyTorchåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸä¹Ÿæœ‰å¾ˆå¤šçš„åº”ç”¨ã€‚PyTorchæä¾›äº†å·ç§¯ç¥ç»ç½‘ç»œã€ResNetç­‰å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå±‚ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è¿™äº›å±‚æ¥æ„å»ºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰æ¨¡å‹ã€‚
* **å¼ºåŒ–å­¦ä¹ **ï¼šPyTorchåœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¹Ÿæœ‰å¾ˆå¤šçš„åº”ç”¨ã€‚PyTorchæä¾›äº†TensorboardXç­‰å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç›‘æ§è®­ç»ƒè¿‡ç¨‹ã€‚

## 3.2.6 å·¥å…·å’Œèµ„æºæ¨è

* **PyTorchå®˜æ–¹ç½‘ç«™**ï¼š<https://pytorch.org/>
* **Hugging Faceå®˜æ–¹ç½‘ç«™**ï¼š<https://huggingface.co/>
* **Pytorch Tutorial**ï¼š<https://pytorch.org/tutorials/>
* **Transformersåº“æ–‡æ¡£**ï¼š<https://huggingface.co/transformers/main_classes.html>

## 3.2.7 æ€»ç»“ï¼šæœªæ¥å‘å±•è¶‹åŠ¿ä¸æŒ‘æˆ˜

### 3.2.7.1 æœªæ¥å‘å±•è¶‹åŠ¿

* **æ›´å¤§çš„æ¨¡å‹**ï¼šéšç€ç¡¬ä»¶è®¾å¤‡çš„å‘å±•ï¼Œäººä»¬å¼€å§‹è®­ç»ƒè¶Šæ¥è¶Šå¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°æ›´å¤§çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
* **æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•**ï¼šè®­ç»ƒä¸€ä¸ªå¤§è§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦å¾ˆå¤šçš„èµ„æºå’Œæ—¶é—´ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°æ›´é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•ã€‚
* **æ›´æ™ºèƒ½çš„AI**ï¼šéšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„è¿›æ­¥ï¼ŒAIå˜å¾—è¶Šæ¥è¶Šæ™ºèƒ½ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°æ›´åŠ æ™ºèƒ½çš„AIã€‚

### 3.2.7.2 æŒ‘æˆ˜

* **èµ„æºé™åˆ¶**ï¼šè®­ç»ƒä¸€ä¸ªå¤§è§„æ¨¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦å¾ˆå¤šçš„èµ„æºå’Œæ—¶é—´ã€‚æœªæ¥ï¼Œèµ„æºé™åˆ¶å¯èƒ½æˆä¸ºä¸€ä¸ªå¤§çš„æŒ‘æˆ˜ã€‚
* **æ•°æ®éšç§**ï¼šéšç€äººä»¬å¯¹æ•°æ®éšç§çš„å…³æ³¨ï¼Œæ•°æ®æ”¶é›†å’Œåˆ©ç”¨å¯èƒ½æˆä¸ºä¸€ä¸ªå¤§çš„æŒ‘æˆ˜ã€‚
* **å®‰å…¨æ€§**ï¼šéšç€AIçš„æ™®åŠï¼Œå®‰å…¨æ€§å¯èƒ½æˆä¸ºä¸€ä¸ªå¤§çš„æŒ‘æˆ˜ã€‚

## 3.2.8 é™„å½•ï¼šå¸¸è§é—®é¢˜ä¸è§£ç­”

### 3.2.8.1 å¦‚ä½•å®‰è£…PyTorchï¼Ÿ

ä½ å¯ä»¥é€šè¿‡<https://pytorch.org/get-started/locally/>æŸ¥çœ‹PyTorchçš„å®‰è£…æŒ‡å—ã€‚

### 3.2.8.2 å¦‚ä½•åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Ÿ

ä½ å¯ä»¥é€šè¿‡`transformers.BertModel.from_pretrained`å‡½æ•°åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€‚ä¾‹å¦‚ï¼š
```python
from transformers import BertModel

model = BertModel.from_pretrained('bert-base-uncased')
```
### 3.2.8.3 å¦‚ä½•ä½¿ç”¨GPUè®­ç»ƒPyTorchæ¨¡å‹ï¼Ÿ

ä½ å¯ä»¥ä½¿ç”¨`device`å˜é‡æ¥é€‰æ‹©GPUæˆ–CPUã€‚ä¾‹å¦‚ï¼š
```python
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

x = torch.randn(5, 3).to(device)
y = torch.randn(5, 4).to(device)
z = x + y
```
### 3.2.8.4 å¦‚ä½•ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—è®­ç»ƒPyTorchæ¨¡å‹ï¼Ÿ

ä½ å¯ä»¥ä½¿ç”¨`torch.distributed`æ¨¡å—æ¥å®ç°åˆ†å¸ƒå¼è®­ç»ƒã€‚ä¾‹å¦‚ï¼š
```python
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

def train(rank, world_size):
   os.environ['MASTER_ADDR'] = 'localhost'
   os.environ['MASTER_PORT'] = '12355'

   dist.init_process_group("nccl", rank=rank, world_size=world_size)

   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

   model = MyModel().to(device)
   model = DDP(model, device_ids=[device])

   criterion = nn.CrossEntropyLoss()
   optimizer = optim.SGD(model.parameters(), lr=0.01)

   for epoch in range(10):
       running_loss = 0.0
       for i, data in enumerate(trainloader, 0):
           inputs, labels = data
           inputs, labels = inputs.to(device), labels.to(device)
           optimizer.zero_grad()
           outputs = model(inputs)
           loss = criterion(outputs, labels)
           loss.backward()
           optimizer.step()
           running_loss += loss.item()
       print('Rank: %d Epoch: %d loss: %.3f' % (rank, epoch + 1, running_loss / len(trainloader)))

   dist.destroy_process_group()

mp.spawn(train, args=(4,), nprocs=4, join=True)
```
### 3.2.8.5 å¦‚ä½•ä½¿ç”¨Hugging Faceçš„Transformersåº“ï¼Ÿ

ä½ å¯ä»¥é€šè¿‡<https://huggingface.co/transformers/main_classes.html>äº†è§£Transformersåº“çš„ä½¿ç”¨æ–¹æ³•ã€‚

### 3.2.8.6 å¦‚ä½•è®­ç»ƒä¸€ä¸ªTransformersåº“ä¸­çš„æ¨¡å‹ï¼Ÿ

ä½ å¯ä»¥ä½¿ç”¨`Trainer`ç±»æ¥è®­ç»ƒä¸€ä¸ªTransformersåº“ä¸­çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼š
```python
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
   output_dir='./results',         # output directory
   num_train_epochs=3,             # total number of training epochs
   per_device_train_batch_size=16,  # batch size per device during training
   warmup_steps=500,               # number of warmup steps for learning rate scheduler
   weight_decay=0.01,              # strength of weight decay
   logging_dir='./logs',           # directory for storing logs
)

trainer = Trainer(
   model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
   args=training_args,                 # training arguments, defined above
   train_dataset=dataset,              # training dataset
   eval_dataset=dataset,               # evaluation dataset
)

trainer.train()
```