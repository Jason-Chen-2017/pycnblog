
作者：禅与计算机程序设计艺术                    
                
                
## 智能音响产品的全新潮流：从豪华到时尚、从简单到复杂
随着人们生活节奏的加快，生活中的每一个角落都成为摄像头监控的焦点。智能家居产品也受到了越来越多人的关注，特别是在智能音响领域。近年来，智能音箱产品数量的增加呈现出爆炸式增长态势，涌动着不同品牌的AI智能音箱产品，无论是智能入门级产品还是高端科技产品，都在追求突破，引领智能音响产品的新潮流。
## 智能音响产品的优势：提升人类的享受感、改善生活品质
人类已经习惯了日复一日的重复性劳作，却很少满足于简单的安逸，正如智能音响产品一样，通过自动化系统提升人类的享受感，让用户享受到完全不同的生活品质。同时，为用户提供个性化推荐，帮助用户发现独特的音乐，还可以根据个人喜好，定时播放音乐，使得音乐播放时间更多元化。另外，借助智能手机的强大计算能力，可以智能分析用户的行为习惯，个性化推送音频内容，最大限度地提升用户体验。
## 智能音响产品的限制：设备成本、不便于远程操控、无法实现自动化管理
智能音响产品所面临的主要困难之一就是设备成本的问题。由于设备制造成本高昂，而且购买过程繁琐，普通消费者很难将其购买起来。此外，智能音响产品又不能被轻易远程操控，只能在固定位置进行音频播放，而且很多时候需要人力介入才能实现最简单的管理功能。最后，许多智能音响产品并没有实现自动化的管理功能，导致用户无法享受到完整的智能家居体验。
综上所述，智能音响产品面临的主要困难是设备成本太高、物联网技术尚未普及、音响控制协议不统一、远程操控问题等。不过，在市场趋势的驱动下，各家厂商纷纷创新产品，希望为消费者提供更好的服务。因此，智能音响产品的全新潮流已经席卷全球，给普通消费者提供了更多选择。
# 2.基本概念术语说明
## AI（Artificial Intelligence）人工智能
是研究、开发用于模拟智能体的计算机科学与技术的一门新兴学科。它是指由人工神经网络、模式识别、机器学习、博弈论和规划等计算机技术组合而成的系统。其目的在于让机器具有智能，能够代替人类完成各种重复性、灵活性差的工作。
## AIoT（Artificial Intelligence on Internet of Things）物联网智能
是利用计算机技术实现对智能设备、数据资源、传感器的连接，并借助信息采集与处理能力对其状态信息进行实时跟踪、分析、预测与决策。它是基于云端计算、大数据分析、人工智能技术、生物识别、图像识别等一系列新技术的结合，构建的一种新型的智能系统。
## IOT（Internet of Things）互联网物联网
是利用计算机网络技术、通讯手段及各种传感器、电子设备、终端设备和服务支持，来实现信息采集、处理、传输、储存和应用的能力。其目的是实现物理世界和数字世界之间的通信、同步、协调、协同以及信息共享。
## HUB（Hub）中心控制器
即中央控制器，又称为集线器或总控器，通常是一个信号交换中心，负责多个设备间的信息交换和控制。
## NAS（Network Attached Storage）网络附属存储
网络附属存储（NAS）是一种可搭载于本地网络环境中的数据存储设备，能够让用户通过网络共享文件、打印机、扫描仪等，并提供网络共享服务。
## WAN（Wide Area Network）广域网
即覆盖范围较大的网络，通常是指有一定区域性的局域网。WAN通常用路由器、交换机或者光纤等方式实现，其特点是覆盖范围大，速度较慢，但是提供了低延迟的通信，适用于互相隔离的网络。
## LAN（Local Area Network）局域网
即小型LAN，又称为内网，通常是指在一定地理范围内建立起来的计算机网络。LAN通常使用集线器、交换机等方式实现，其特点是规模小，数据传输速率高，但通信距离有限，适合于单位内部的通信。
## AP（Access Point）接入点
即无线接入设备，即无线路由器。AP可以是智能无线控制器（APC），也可以是普通的无线路由器，主要功能包括身份认证、加密传输、QoS保证、蜂窝网络优化、加密防护、热点隔离、攻击检测、流量统计等。
## SDN（Software Defined Networking）软件定义网络
软件定义网络(SDN)是一种通过计算机网络自学习掌握网络拓扑结构，自动配置网络设备的网络拓扑方法。通过引入软件定义网络控制平面，可以在网络实施前预先部署网络设备，降低网络维护和运营成本，提升网络可靠性、扩展性和弹性。
## DNS（Domain Name System）域名系统
是因特网使用的域名服务器的分层命名系统。域名系统用来把文字形式的主机名转换为IP地址形式，同时也是Internet上所有网站的目录。
## Wi-Fi（Wireless Fidelity）无线高达
指无线信号传输速率达到的某一阈值。一般情况下Wi-Fi信号传输速率达到1Mbps以下。
## BT（Bluetooth）蓝牙技术
蓝牙（英语：Bluetooth）是一种无线传输技术，其功能是允许个人设备之间进行短暂的点对点通信。主要特点是快速、省电、安全。
## BLE（Bluetooth Low Energy）低功耗蓝牙
BLE是Bluetooth（蓝牙）的低功耗版本，其能耗低至几乎为同等功能的蓝牙设备所提供的功耗。
## Zigbee（ZigBee）有源蜂窝
是由国际电信联盟创建的一种低功耗无线网络技术标准。它在电池寿命方面做了很大贡献。它的数据传输速率在2.4GHz的802.15.4上可达100kb/s。
## Zwave（ZigBee Wireless Protocol）ZWave无线局域网
是一种在家庭住宅和办公室环境下，为设备提供安全、快速、智能的无线网络解决方案。ZWave协议由Crestron Electronics公司开发，为各种家用设备提供安全的、低功耗的无线链接。
## ZigBee PRO（ZigBee Personal Area Network）无线个人区域网
是一种无线局域网技术，是由英国ZigBee社区发明的一种分布式的无线网络。ZigBee PRO采用主从式的结构，并且支持近距离的节点通信。
## ZigBee SRP（Smart Routing Protocal）智能路由协议
ZigBee Smart Router Protocol (ZSRP)，是一种由ZigBee社区推出的无线技术。它是一种支持动态路由和智能网络层策略的无线传输协议，可有效地减少路由消耗、改进传输性能。
## Homekit（Home Automation Accessory Protocol）家庭自动化配件协议
是苹果公司为Apple Watch、HomePod、AirPods等HomeKit Accessory提供的协议标准。它定义了一套开放的接口规范，供第三方厂商开发符合HomeKit的新配件。该协议定义了配件需要具备哪些功能、如何和设备交互等。
# 3.核心算法原理和具体操作步骤以及数学公式讲解
## 智能音响的控制
智能音响是一种具有语音识别、语音合成、音频处理、音效渲染、音频输出等功能的智能音箱产品。它的主要功能是在户外使用，通过麦克风、耳机、外部音箱等接收外部声音输入，使用语音命令控制音响播放音乐。因此，智能音响的控制，包括语音识别、语音合成、语音识别与语音合成的匹配、音频处理、音效渲染、音频输出等多个子功能模块。其中，语音识别、语音合成、音频处理、音效渲染、音频输出等子功能分别对应如下几个算法。
### 语音识别
#### 音频采样率
目前，音频采样率最高可达48kHz。通常音频的采样率比CD、DVD的采样率低得多。采样率过低会造成信号失真、噪声、失真。常用的音频采样率有8kHz、16kHz、32kHz、48kHz。音频采样率决定了语音数据的详细程度。通常，我们认为16kHz的采样率足够精确，而8kHz的采样率则比较粗糙，所以我们通常在语音识别中使用16kHz的采样率。
#### MFCC特征
MFCC特征是语音识别的一种重要技术。它是一种将声谱转化为相关特征向量的方法。相关特征向量包含声音波形的频率成分和时变成分。使用MFCC特征，可以通过对声音的频谱进行分析，获得语音的特征。MFCC特征有着良好的语音识别效果，可以满足不同的应用场景需求。
#### 分布式FFT技术
分布式FFT技术是语音识别的一种重要技术。它将语音信号进行快速傅里叶变换，得到幅值的分布。通过分析幅值的分布，我们可以获得语音的频谱特征，这是语音识别中需要用到的特征。分布式FFT技术的优点在于快速性和准确性。它可以对语音信号进行快速傅里叶变换，并得到语音的频谱特征。但分布式FFT技术可能会产生一些噪声，这就需要后续的滤波、去除噪声的操作。
### 语音合成
#### 语言模型
语言模型是语音合成的一种重要技术。它基于概率统计理论，描述语言出现的概率及各种可能性。通过语言模型，我们可以计算出某个词出现的概率，然后按照这个概率生成对应的语音信号。语言模型的训练，依赖于大量的文本数据。语言模型训练完成后，可以根据模型生成相应的语音信号。
#### GMM-HMM模型
GMM-HMM模型是语音合成的一种重要技术。它是一种混合高斯模型，将频谱中的噪声和声学参数分离，提取出高频特征。通过高频特征，我们可以实现语音合成。GMM-HMM模型包含了统计学、数学等多种学科知识。GMM-HMM模型的训练，需要大量的训练数据。训练数据越丰富，模型效果越好。
### 音频处理
#### 时域变换
时域变换是音频处理的一个重要技术。它将音频信号从时间域转换到频率域。通过频率域分析，我们可以获取到音频的声学信息。频率域分析的结果，可以为后面的音频处理提供基础。
#### 回声消除
回声消除是音频处理的一个重要技术。它通过消除对回声的干扰，提高信号质量。通过消除回声，我们可以提升音频的清晰度。回声消除技术在视频会议、录音、直播等场景中都会有用武之地。
#### 比特率压缩
比特率压缩是音频处理的一个重要技术。它是一种无损压缩算法，可以对音频数据进行压缩。通过压缩，可以减少音频文件的大小，从而降低网络带宽占用。通常，在智能音响的音频处理中，我们会使用16位、24位、32位等位宽的编码，并在进行传输之前进行压缩。
### 音效渲染
#### OTOON插件技术
OTOON插件技术是音效渲染的一种重要技术。它是一种特殊的渲染技术，可以将声音渲染为虚拟的贴花效果。通过使用贴花效果，我们可以更好地感受音效的效果。OTOON插件技术的实现，依赖于特定的渲染插件。
#### 空间音效渲染
空间音效渲染是音效渲染的一种重要技术。它将声音渲染到物体表面上的声音效果。通过声音渲染到物体表面上，我们就可以获得更高的效果。空间音效渲染的实现，依赖于特定设备、游戏引擎、工具、算法等。
### 音频输出
#### MIDI协议
MIDI协议是音频输出的一种重要协议。它是一种通信协议，用于电子琴、键盘、钢琴等多种控制器之间的通信。通过使用MIDI协议，我们可以传输音频信号。MIDI协议可以满足多种音频输出场景的需求。
#### DAC硬件输出
DAC硬件输出是音频输出的一种重要技术。它是一种将模拟信号转换为数字信号的硬件模块。通过DAC硬件输出，我们可以直接将声音输出到音箱。DAC硬件输出的实现，依赖于特定的DAC芯片和驱动程序。
## 智能音响的管理
智能音响的管理，是为了满足用户对音乐播放的个性化需求，提高音乐播放效率和满意度。智能音响的管理，包括如下几个方面。
### 用户识别
#### 账户管理
账户管理是智能音响的管理的重要组成部分。它是智能音响管理系统的基础，用于管理用户的权限和权限限制。通过账户管理，我们可以对用户进行管理。
#### 用户识别
用户识别是智能音响的管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过用户识别，我们可以对用户的行为进行监控，及时发现异常用户。
### 数据分析
#### 日志分析
日志分析是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过日志分析，我们可以了解用户的音乐播放习惯。
#### 使用行为分析
使用行为分析是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过使用行为分析，我们可以了解用户的使用习惯和喜好。
### 音频播放
#### 音乐库管理
音乐库管理是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过音乐库管理，我们可以管理音乐库的内容。
#### 播放排行榜管理
播放排行榜管理是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过播放排行榜管理，我们可以了解用户的音乐播放热度。
### 投诉反馈
#### 投诉举报机制
投诉举报机制是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过投诉举报机制，我们可以对用户的投诉进行监控，及时处理投诉。
#### 投诉举报管理
投诉举报管理是智能音响管理的重要组成部分。它是智能音响管理系统的一个重要功能。通过投诉举报管理，我们可以统计投诉举报的次数。
## 物联网和云计算技术的应用
物联网和云计算技术的应用，是智能音响管理的另一个重要组成部分。智能音响使用物联网和云计算技术，可以实现远程管理，满足用户的实时监控和管理需求。
### 远程管理
远程管理是智能音响管理的关键。通过远程管理，我们可以让用户访问智能音响，进行设备维护、设置、调试，同时还可以远程控制智能音响，随时随地地进行音乐播放。
### 物联网技术
物联网技术是智能音响管理的关键。物联网技术可以收集大量的数据，用于智能音响的远程管理。物联网技术的应用可以大大提高智能音响的实时监控和管理能力。
### 云计算技术
云计算技术是智能音响管理的关键。云计算技术可以提供数据中心、存储空间、计算能力、网络等服务。云计算技术可以大大缩短智能音响的开发周期，提高智能音响的可靠性和可用性。
# 4.具体代码实例和解释说明
## 语音识别与合成的代码实例
### Python语音识别
```python
import speech_recognition as sr

r = sr.Recognizer()

with sr.Microphone() as source:
    print("说话...")
    audio = r.listen(source)
    
try:
    text = r.recognize_google(audio, language='zh-CN') # 指定中文的语音识别模型
    
    print('你刚才说了：' + text)
    
except LookupError:
    print("找不到与语音对应的文字")
```

### Python语音合成
```python
from gtts import gTTS

text = '你好，欢迎使用我的语音合成器。'

myobj = gTTS(text=text, lang='zh', slow=False) 

myobj.save("welcome.mp3") 
```

### C++语音识别
```c++
#include <iostream>
#include <fstream>
#include <sstream>
#include "Windows.h"
#include "tesseract    esseract.h"
#pragma comment(lib,"leptonica-1.75.dll")
#pragma comment(lib,"tesseract305.dll")

using namespace std;

int main(){
    Tesseract* ocr = new Tesseract("D:\\Program Files\\Tesseract-OCR", "", tesseract::OEM_DEFAULT); //加载Tesseract识别器
    if (!ocr){
        cout << "初始化失败!" << endl;
        return -1;
    }

    Pix *pix = pixRead("test.jpg"); //读取图片
    if (!pix){
        cout << "读取图片失败!" << endl;
        delete ocr;
        return -1;
    }

    string result; 
    int conf = ocropus::ocr(pix, result, true, false);//识别图片，并获得识别结果

    if (conf <= 0){
        cout << "识别失败!" << endl;
    } else{
        cout << "识别结果:" << result << endl;
    }

    delete ocr; //释放内存
    pixDestroy(&pix);
    return 0;
}
```

### Java语音识别
```java
import java.io.*;

import javax.sound.sampled.*;
import org.apache.commons.io.IOUtils;
import com.google.cloud.speech.v1.*;

public class SpeechRecognition {
  public static void main(String[] args) throws Exception {
    String sampleFilePath = "./resources/voice.flac"; //待识别的文件路径
    InputStream inputStream = AudioSystem.getAudioInputStream(new File(sampleFilePath));//创建音频输入流对象

    Recognizer recognizer = SpeechClient.create().getRecognizer(); //创建语音识别器

    try (BufferedInputStream bis = new BufferedInputStream(inputStream)) {
      byte[] bytes = IOUtils.toByteArray(bis);

      RecognitionConfig config =
          RecognitionConfig.newBuilder()
             .setEncoding(RecognitionConfig.AudioEncoding.FLAC).build();

      RecognitionAudio audio = RecognitionAudio.newBuilder().setContent(ByteString.copyFrom(bytes)).build();

      OperationFuture<LongRunningRecognizeResponse, LongRunningRecognizeMetadata> future =
          recognizer.longRunningRecognizeAsync(config, audio);

      while (!future.isDone()) {
        Thread.sleep(1000); //等待识别完成
      }

      final LongRunningRecognizeResponse response = future.getResult();
      for (SpeechRecognitionResult result : response.getResultsList()) {
        String transcript = result.getAlternatives(0).getTranscript();
        System.out.println(transcript);
      }
    } catch (InterruptedException e) {
      throw new RuntimeException(e);
    }
  }
}
```

### JavaScript语音识别
```javascript
const speechToText = async () => {
  const stream = await navigator.mediaDevices.getUserMedia({audio: true});

  const recognizer = new StreamingRecognitionService('/models');
  let transcript = '';

  const recorder = new MediaRecorder(stream);
  
  recorder.start();
  recorder.addEventListener('dataavailable', event => {
    const decoder = new TextDecoder('utf-8');
    const buffer = event.data.getData();
    const message = decoder.decode(buffer);

    const recognitionResult = JSON.parse(message)['results'][0]['alternatives'][0];
    const word = recognitionResult['content'];

    console.log(`Word:${word}`);
    transcript += word;
  });
  
  setTimeout(() => {
    recorder.stop();
    recognizer.close();
  }, 5000);
};
```

### Android语音识别
```kotlin
class MainActivity : AppCompatActivity(), Task.Callback<LongRunningRecognizeResponse>, Runnable {

    private var mVoiceInputStream: InputStream? = null
    private var mStreamingRequest: LongRunningRecognizeRequest? = null
    private var mTask: Task<LongRunningRecognizeResponse>? = null
    private val threadHandler by lazy { Handler(Looper.getMainLooper()) }

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        setContentView(R.layout.activity_main)

        startRecognition()
    }

    override fun onStart() {
        super.onStart()
        startRecognition()
    }

    private fun startRecognition() {
        try {
            mVoiceInputStream = assets.open("test.wav")

            val configBuilder = RecognitionConfig.newBuilder()
                   .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)
                   .setLanguageCode("en-US")

            mStreamingRequest = LongRunningRecognizeRequest.newBuilder()
                   .setConfig(configBuilder.build())
                   .setInputAudio(InputAudioConfig.newBuilder().setByteStream(mVoiceInputStream).build()).build()

            mTask = RecognitionClient.create().streamingRecognizeCallable().futureCall(mStreamingRequest, this)
        } catch (e: FileNotFoundException) {
            Log.d(TAG, "File not found.")
        } catch (e: IOException) {
            Log.d(TAG, "Failed to create input stream.", e)
        }
    }

    @SuppressLint("SetTextI18n")
    override fun onCompleted(response: Task.Result<LongRunningRecognizeResponse>) {
        if (response.isSuccessful) {
            for (result in response.result!!.resultsList) {
                val alternatives = result.alternativesList[0]
                val transcript = alternatives.transcript

                threadHandler.post(Runnable {
                    findViewById<TextView>(R.id.textView).text = transcript

                    if (mTask!= null &&!mTask!!.isCancelled) {
                        mTask!!.cancel(true)
                    }
                })
            }
        } else {
            Log.w(TAG, "Failed to recognize.", response.exception)
        }
    }

    override fun run() {}

    companion object {
        private const val TAG = "MainActivity"
    }
}
```

### iOS语音识别
```swift
let settings = [AVFoundation.AVCaptureSession.lensPositionKey: AVFoundation.AVCaptureLensPosition.front]

if UIDevice.current.orientation ==.portrait {
    session.beginConfiguration()
    session.sessionPreset = AVFoundation.AVCaptureSession.Preset.medium
    session.commitConfiguration()
} else {
    guard let layer = view.layer else { fatalError() }
    layer.videoOrientation =.landscapeRight
    session.beginConfiguration()
    session.sessionPreset = AVFoundation.AVCaptureSession.Preset.high
    session.commitConfiguration()
}

let captureDevice = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeAudio)
do {
    try captureDevice.lockForConfiguration()
    captureDevice.setActive(true, completionHandler: nil)
} catch _ { }

var running = true

captureDevice.addObserver(self,
                           forKeyPath: "active", options: NSKeyValueObservingOptions.new, context: &runningContext)

while running {
    DispatchQueue.global(qos: DispatchQueue.QoSClass.userInteractive).async {
        do {
            autoreleasepool {
                let bufferLength: Int32 = 0
                var micBuffer = AVAudioPCMBuffer(length: bufferLength, channels: UInt32(1), sampleRate: Float(settings["sampleRate"] as! Double), interleaved: true)
                
                while let data = try self.captureDevice.readSampleBuffer(ofType: AVMediaTypeAudio, timeout: kCMTimeZero)?.pcmData {
                    copyBytes(micBuffer!.bytes, count: data.count, from: data)
                    
                    var seconds = CMTimeGetSeconds(micBuffer!.duration)
                    if seconds > 0.5 {
                        var packet = AVSpeechUtterance(string: "")
                        packet.startPoint = CMTimeMakeWithSeconds(seconds - 0.5, Int32(micBuffer!.sampleRate))
                        packet.endPoint = CMTimeMake(Int64(CMTimeGetSeconds(micBuffer!.duration)), 1)
                        
                        guard let engine = AVSpeechSynthesisEngine.sharedEngine() else { break }
                        
                        engine.pauseSpeaking(true)
                        engine.speak(packet)

                        let timer = Timer.scheduledTimer(timeInterval: 1.0, target: self, selector: #selector(checkEnd), userInfo: nil, repeats: false)

                        while!engine.isSpeaking {
                            usleep(10000)
                        }

                        timer.invalidate()

                        dispatch_sync(DispatchQueue.main.queue) {
                            checkResult()
                        }
                    }
                }
            }
        } catch {
            print("\(error)")
        }
    }
}

@objc func checkEnd() {
    if engine?.isSpeaking?? false {
        debugPrint("still speaking")
        DispatchQueue.main.asyncAfter(deadline: Date().addingTimeInterval(1.0), execute: checkEnd)
    } else {
        debugPrint("not speaking anymore")
        DispatchQueue.main.async(execute: checkResult)
    }
}

func checkResult() {
    if engine?.finishedSpeech? true : false {
        let utterance: AVSpeechUtterance = manager.currentSpeechutterance?: return
        
        let transcript = utterance.currentPhonemeSymbols?.map { $0.orthographic }.joined()?? ""
        debugPrint(transcript)
        
        timer?.invalidate()
        running = false
    }
}

manager = AVSpeechSynthesisVoiceManager.default()
engine = AVSpeechSynthesizer(voice: voice, rate: float(settings["outputSampleRate"])!)
engine?.delegate = self

timer = Timer.scheduledTimer(timeInterval: 5.0, target: self, selector: #selector(initiateReading), userInfo: nil, repeats: false)

@objc func initiateReading() {
    reading = true
    
    if!voice.isPropertyAVVoice) {
        voice = manager.voice(forIdentifier: "com.apple.ttsbundle.Moira-compact")!
    }
    
    weak var engine: AVSpeechSynthesizer?
    
    do {
        try manager.requestAuthorization(completionHandler: {})
    } catch {
        debugPrint("access denied")
        exit(1)
    }
    
    device = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeAudio)
    
    DispatchQueue.main.async {
        if device?.isRunning?? false {
            device?.removeObserver(self, forKeyPath: "running")
            
            DispatchQueue.main.asyncAfter(deadline: Date().addingTimeInterval(0.1), execute: initiateReading)
        } else {
            startingUp = true
        }
    }
    
    NotificationCenter.default.addObserver(self,
                                             selector: #selector(handleRuntimeErrorNotification(_:)),
                                             name: NSNotification.Name.AVSpeechSynthesisProviderDidFinishLoadingVoice,
                                             object: nil)
}

override func handleRuntimeErrorNotification(_ notification: Notification) {
    guard let error = notification.userInfo?[AVSpeechSynthesisServerErrorMessageUserInfoKey] as? Error else { return }
    
    switch error.code {
    case AVSpeechSynthesisErrorCodeInsufficientSpeechCapacity:
        debugPrint("Insufficient speech capacity")
        
    default:
        debugPrint("Unknown error occurred.
\(error)")
    }
    
    stopReading()
}

private func stopReading() {
    reading = false
    
    if let manager = manager {
        manager.stopSpeech()
    }
    
    timer?.invalidate()
}

deinit {
    timer?.invalidate()
    
    session.stopRunning()
    if let device = captureDevice {
        device.removeObserver(self, forKeyPath: "active")
        device.unlockForConfiguration()
    }
}

extension AppDelegate: AVSpeechSynthesizerDelegate {
    func synthesizer(_ synthesizer: AVSpeechSynthesizer!, didFinishSpeechUtterance utteranceId: String!) {
        debugPrint("Finished speaking: \(utteranceId)")
        assert(reading)
        
        // This function is called synchronously on the main queue and should be minimized or moved off it's own queue. Otherwise, the app will freeze up.
        updateText(transcript)
        
        stopReading()
    }
}

