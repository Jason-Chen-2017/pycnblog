
作者：禅与计算机程序设计艺术                    

# 1.简介
         
随着科技革命的深入，人工智能（AI）已经成为科技领域的一个重要分支，其研究及应用得到全社会的广泛关注。但是，由于AI具有高度复杂性、高维数据处理能力、不确定性等特点，加之对个人隐私、财产权益等权力方面的侵害，以及算法技术的商业化和专利保护等法律限制，使得AI在产生价值、发展壮大方面面临诸多复杂问题。如何让公众更好地了解和评估AI技术，也就成为当前面临的重要课题之一。
为了解决这一问题，一方面国内外相关机构推出了专门的媒体报道、研讨会活动，传播AI技术、模型、数据、工具和方法；另一方面，一些大型企业也发布了AI产品或服务，通过对外发布的方式，吸引广大消费者的注意力。但这些方式虽然对AI技术的普及起到积极作用，但对于公众来说，并没有真正提供一个客观、公开、透明、可验证的态度。
因此，本文通过对国内外公开信息的梳理、分析、归纳和总结，提出以下六个具体建议，希望能够帮助公众更好地了解和掌握AI技术。
# 2.背景介绍
人工智能（Artificial Intelligence，AI）是一个基于机器学习和人类智慧所设计的计算机系统。它包括如图像识别、自然语言理解、语音识别、强化学习、决策树、神经网络等基本算法，可以模拟人的学习、推理行为、解决问题、计划行动等能力。它也被应用于图像、语音、文本、视频、日常生活等多个领域。近年来，随着人工智能技术的飞速发展，各种应用场景涌现出来。其中最突出的则是智能助手、机器翻译、自动驾驶等。截至目前，人工智能技术已成为经济发展的主导力量，成为各行各业的一项重要战略性技术。
另外，随着AI技术的日渐成熟和应用，也带来了对社会的深刻影响。当今世界，随着互联网的发展，智能设备迅速普及，造成物联网的爆炸式增长，给人们的生活带来了前所未有的便利。智能手机、汽车、空调等都带有大数据处理能力。同时，科技公司不断向社会开放，对人类进行监控、审查甚至人身安全威胁。公众对人工智能的认识还处于半成品状态。
作为信息技术领域的重量级事件，人工智能面临着前所未有的挑战，尤其是在面对共同的挑战时，如何让公众更好地了解和掌握AI技术成为了重点难题。如何客观、公开、透明、可验证地把握AI技术、模型、数据、工具和方法，是解决这一关键挑战的必要途径。
# 3.基本概念术语说明
## （1）AI模型、算法与数据
### 1) AI模型
AI模型（Artificial Intelligence Model），又称人工智能系统（Intelligent System），由算法与数据的集合组成，用于实现人类的某些智能功能。AI模型由多个算法组合而成，包括机器学习算法、数据挖掘算法、规则推理算法等。AI模型可以用来分类、预测、分析、推荐、生成数据、解决问题、决策等。如深度学习（Deep Learning）、强化学习（Reinforcement Learning）、贝叶斯网络（Bayesian Network）。
### 2) 数据
数据（Data）是指计算机处理后的信息。数据既可以直接获取，也可以通过计算机程序生成。数据分为结构化数据、非结构化数据、半结构化数据和无结构数据等类型。结构化数据指的是具有固定格式的数据，如电子表格中的数字和文字数据；非结构化数据指的是半结构化或无结构的文本、音频、视频等数据；半结构化数据指的是杂乱无章的文本数据；无结构数据指的是无法用有意义的方式组织和表示的数据。
### 3) 算法
算法（Algorithm）是指用来计算解决问题的方法，是一系列指令、循环、判断语句、函数等。算法用来指定输入的数据范围、数据处理顺序和输出结果。算法的性能和效率决定着算法的成功与否。算法是指计算机执行特定任务的计算指令集。
## （2）AI技术与方法论
### 1) 深度学习
深度学习（Deep Learning）是一种机器学习方法，它利用多层次结构的神经网络模型对数据进行学习，从而提升模型的准确性、效率和灵活性。深度学习方法主要包括卷积神经网络（Convolutional Neural Networks，CNN）、循环神经网络（Recurrent Neural Networks，RNN）、变压器神经网络（Transformer Networks）等。
### 2) 自然语言处理
自然语言处理（Natural Language Processing，NLP）是指计算机和人类用自然语言进行有效通信、理解、交流的能力。NLP通过对文本数据进行清洗、标记、解析、归纳、理解等处理，将文字转换为计算机可以理解的形式。NLP方法通常分为特征工程、词汇和语法分析、统计学习、模式识别、信息检索、语义理解等几个方面。
### 3) 智能推理
智能推理（Artificial Intelligence Inference）是指基于计算的计算机系统通过分析、处理和综合获得的知识、信息以及模型，从而对新数据做出推论或者决策。智能推理包括逻辑推理、概率推理、决策理论、数据挖掘、风险管理等。
### 4) 生物信息技术
生物信息技术（Bioinformatics）是指利用生物学知识、技术以及相关工具来理解、分析和描述基因、蛋白质、遗传、疾病、健康状况等信息的一门新兴学术研究领域。生物信息技术可用于医学、药学、生物制品、环境科学、健康管理、食品科学、农业等多个领域。
### 5) 可视化技术
可视化技术（Visualization Techniques）是指用图形、图像、色彩、动画等方式，将复杂的复杂系统、数据、模型、过程等信息呈现出来，达到直观、易懂、快速理解的目的。可视化技术可以帮助科研人员、数据科学家、管理人员和其他相关人员理解数据、发现问题、建立共识。
### 6) 机器学习实验平台
机器学习实验平台（Machine Learning Experiment Platforms）是一种集成的机器学习开发环境，它融合了机器学习算法、数据处理工具、开发框架、数据存储、运行环境等组件，能够提供方便快捷的实验平台。机器学习实验平台可以根据不同项目需要，采用不同的接口规范，对算法、模型、数据进行配置、训练、测试、部署，并提供模型优化、效果评估等工具。
### 7) 模型压缩与加速技术
模型压缩与加速技术（Model Compression and Acceleration Technologies）是指通过降低模型大小、提升模型运算速度、减少内存占用等方式，降低模型的复杂度和推理时间，提升计算效率。模型压缩与加速技术可以让模型在实际使用中获得更好的效果，并节省计算资源。
## （3）相关法律法规与政策
### 1) 国家发展改革委员会关于人工智能与信息技术产业国际合作机制的批复
《关于人工智能与信息技术产业国际合作机制的批复》（以下简称《批复》），是为了促进我国和其他国家的人工智能与信息技术产业发展，维护人工智能研究和应用的合法权益，增强我国企业在人工智能领域的竞争力，鼓励创新驱动发展。《批复》明确了人工智能与信息技术产业国际合作机制的基本目标和方向，以及主要工作，要求参与合作的国家和地区要遵守有关法律法规，接受国际人工智能学术会议和竞赛的邀请，尊重企业自主选择加入合作机制的权利。《批复》还规定，“深化人工智能专利保护、产业链合作、人才培养、技能配套等领域的合作”，进一步完善人工智能和信息技术产业的国际合作机制。
### 2) 中华人民共和国科技部关于进一步支持科技成果转化应用发展的若干意见
《科技部关于进一步支持科技成果转化应用发展的若干意见》（以下简称《意见》），是为了充分发挥国际学术交流与合作的平台优势，提升中国科技界人才培养、职业发展水平，推动科技成果转化应用取得新的突破，重申科技部在人工智能、机器人、智能系统、人机交互等领域的坚定信心，提出以下五条方针：

1．全面支持AI产业发展

2．支持AI与相关产业的合作布局

3．发挥AI资源和技术引进国际空间的积极作用

4．借鉴国际先进技术，加强AI研发基础设施建设

5．充分尊重企业合作意愿，开放AI研究平台，鼓励企业技术合作，加强政策制定和推动行业协作。
### 3) 《政府信息公开条例》修订草案
《政府信息公开条例》修订草案（GB/T 35278-2019）主要修改的内容包括以下几方面：

1. 增加“问责制”：未经原告或申请人同意，任何单位和个人不得泄露对采购人员、招聘人员、使用人员及其他人员有用信息，泄露者应承担相应责任；未经许可，任何单位和个人不得提供有用信息给他人。对销售、分销等情形，仍应依照《中华人民共和国政府信息公开办法》适用法律、法规严格执行。

2. 提高信息公开比例：各级政府要按照国家有关法律、法规的规定，按照紧急程度确定公开信息的级别，逐步提高信息公开比例。其中，应考虑到证据确实、有效、公开、完整、真实、正确、及时等原则，对分散收集、保密保存的信息，应按有关法律法规的规定予以公开。

3. 完善信息披露和删除制度：完善政府信息公开监督管理办法，明确各级政府在信息公开中应遵循的原则、规范、程序和制度，旨在确保政府信息公开的准确性、完整性、及时性、公开性和有效性，防止滥用、误导和违反公开自由。对于确有必要披露的政府信息，政府应通过公开信息发布渠道及时发布，不得滥用或泄露信息，确保信息真实、准确、及时。

4. 发挥信息共享作用：推进跨部门、跨部门之间、跨领域之间的信息共享，逐步形成覆盖国内外的政府信息公开和使用体系，构建信息共享、开放、分享、协同、融通、利用的经济、社会、文化、政治与民间交流互动平台。

5. 健全信息登记制度：健全地方政府信息登记制度，完善行政区域划分制度，让个人或单位只注册自己的信息，而不是用网页、邮箱等注册多个账号。设立信息管理局，严格落实信息收集、整理、发布、删除、备份和核查等环节，全面加强信息保障。

# 4.核心算法原理与具体操作步骤
## （1）深度学习
### 1) 卷积神经网络（Convolutional Neural Networks，CNN）
卷积神经网络（Convolutional Neural Networks，CNN）是深度学习中的一种类型的神经网络，可以自动提取图像特征，并有效地进行分类、检测、回归等。它的工作原理类似于人眼对图像的基本视觉运算——对图像的空间分布进行特征抽取，提取到图像的一些空间特征后再进行分类。CNN包括卷积层、池化层、激活层以及Fully Connected Layer（全连接层）等构成。CNN的主体部分是卷积层，它包括多个过滤器组成，每个过滤器从图像中提取特定像素块的特征。池化层可以对卷积层的输出结果进行池化，从而降低维度，提升模型的鲁棒性。激活层则一般采用sigmoid或ReLU函数。最后，全连接层是整个网络的输出，根据每张图片的特征进行分类。CNN的特点就是能够学习到图像的空间特性，而且可以有效地提取图像的全局特征。
#### 操作步骤如下：
首先，对原始图片进行预处理，比如裁剪、缩放、调整亮度、色彩等；然后，将图片输入到CNN中，通过卷积层提取图像的空间特征，通过池化层降低维度；再通过激活层将特征映射到下一层；最后，连接全连接层完成分类。
#### 特点：

1. 轻量化：参数少，运算快，占用的存储空间也很小。

2. 模块化：可以对图像进行各种不同的操作，并结合多种模块对图像进行特征提取。

3. 易于训练：不需要大量的标签，只需对样本的特征进行标记，然后就可以自动学习。

4. 有利于特征提取：卷积层和池化层的操作可以将低层次的图像特征学习到较高层次，从而提升模型的准确性。

5. 特征共享：特征从底层共享到高层，可以提升模型的泛化能力。

### 2) 循环神经网络（Recurrent Neural Networks，RNN）
循环神经网络（Recurrent Neural Networks，RNN）是深度学习中的一种类型，是一种特别擅长处理序列数据的神经网络。它可以从数据中学习到长期依赖关系，且具有记忆能力。RNN的基本结构可以用一个循环神经元（Recurrent Neuron）来表示，它接收上一时刻的输入、输出和内部状态，并根据这个信息进行计算。循环神经网络可以通过反向传播算法进行训练，这种算法的特点是可以保证网络中的所有参数能够学习到最佳的权重。RNN的缺点是不能够处理并行数据，且在很多情况下，RNN的训练速度比较慢。
#### 操作步骤如下：
首先，准备一个输入序列，即将待预测的词或者句子通过编码之后的向量表示。假设输入序列为t时刻到n时刻的序列，则第一步是将各个时刻的向量传入LSTM单元。然后，LSTM单元基于之前的输入信息、当前时刻的输入、上一时刻的输出、以及内部的状态，计算当前时刻的输出和状态。最后，根据所有时刻的输出计算预测的词或者句子。
#### 特点：

1. 适用于处理序列数据：能够对序列数据进行有效的建模和分析，例如文本数据、语音信号、图像数据等。

2. 长期记忆能力：循环神经网络能够记住上一时刻的信息，所以在预测新时刻的输入数据的时候，能够更好的保留之前的相关信息。

3. 可以捕捉长期依赖关系：循环神经网络在处理序列数据的时候，能够更好地捕捉到长期依赖关系。

4. 容易并行处理：循环神经网络能够并行处理数据，所以可以在多个线程、进程或机器上进行快速训练。

5. 不易出现梯度消失或爆炸的问题。

## （2）自然语言处理
### 1) 词向量
词向量（Word Embedding）是自然语言处理中经常使用的一种工具。词向量的基本思想是通过向量空间将单词转换为实数向量，向量越靠近则代表单词越相似。词向量的训练方法主要有两种，分别是基于共现矩阵的CBOW模型和基于共生矩阵的Skip-Gram模型。CBOW模型根据上下文窗口在给定的中心词周围预测中心词，而Skip-Gram模型则根据中心词预测周围的上下文词。两种方法都是通过反向传播算法训练出词向量。基于共现矩阵的CBOW模型训练过程如下：

1. 对语料库中的每个句子进行切分，得到句子中的每个单词的列表。

2. 使用窗口大小k，遍历每个中心词，构造该中心词左右k个词的共现窗口。

3. 将每个中心词及其上下文窗口中的单词构造为一条样本。

4. 将每个样本的中心词和上下文词对应的词向量加和，得到中心词的最终词向量。

5. 在所有样本上训练词向量，使用反向传播算法更新词向量的参数。

基于共生矩阵的Skip-Gram模型训练过程如下：

1. 对语料库中的每个句子进行切分，得到句子中的每个单词的列表。

2. 使用窗口大小k，遍历每个中心词，构造该中心词左右k个词的共生窗口。

3. 每个中心词及其上下文窗口中的每个单词对应着一个样本，即中心词和上下文词的词向量。

4. 在所有样本上训练词向量，使用反向传播算法更新词向量的参数。

### 2) 词嵌入方法
词嵌入（Word Embedding）是自然语言处理中的一种方法，是指用向量空间将文本中的每个词表示成实数向量。词嵌入模型能够为词向量提供更高的表达能力，能够捕获词与词之间的关联。常见的词嵌入模型有GloVe、word2vec、fastText、BERT等。
#### GloVe
GloVe（Global Vectors for Word Representation）是最早提出的词嵌入模型。GloVe的基本思路是将一段文本中的每两个相邻的单词联系起来，在两个单词之间的向量应该满足某种统计规律。统计规律可以由两个单词共现的次数来定义，假设一个词的共现次数是m，另一个词的共现次数是n，那么两个单词之间的向量就应该满足下面这个等式：

$$\overrightarrow{u}_i + \overrightarrow{v}_j = \frac{(m+n)}{Z} \cdot (\log m - \log n)\overrightarrow{\beta}$$

其中，$u_i$和$v_j$分别是第i个词和第j个词的词向量，$\beta$是控制相似度的超参数，$Z$是预先计算的值。GloVe模型能够捕捉到单词之间的全局信息，并且能够很好地处理缺失值。
#### word2vec
word2vec（Word to Vector）是google团队在2013年开源的词嵌入模型，其提出了一种简单有效的训练词嵌入模型的方法。word2vec模型的思路是考虑到单词之间的相关性，认为一个词的附近的词可能与它相似。词嵌入模型可以使用一组神经网络节点来表示词的特征，每个节点对应着一个单词。当训练词嵌入模型时，每个单词都由周围的词向量来决定，可以捕获到词与词之间的语义关系。word2vec的模型结构如图所示。
![word2vec](https://ai-studio-static-online.cdn.bcebos.com/f2e943954d4c4a7fb503faed91aa75a18d8eeaccd537f1ce4c7c7cfdc4c4be59)
word2vec模型包含两层网络，中间层与输入层的词向量有相同的维度，输出层与输入层的词向ved有不同的维度，中间层是中间层与输出层的连接，网络的训练过程包括两种学习策略，分别是连续随机梯度下降(SGD)和负采样(Negative Sampling)。
#### fastText
fastText是一个高效的文本分类算法，其将词向量的训练扩展到了包括n-gram信息。相对于word2vec模型，fastText训练更快，能处理更多的数据。fastText的模型结构与word2vec模型一致，中间层与输出层的词向量维度不一样。
#### BERT
BERT（Bidirectional Encoder Representations from Transformers）是google团队在2018年开源的词嵌入模型，其提出了一个名为预训练语言模型的技术，能够解决自然语言理解、生成、推理三个关键问题。BERT的模型结构与word2vec模型类似，中间层和输出层的词向量维度不一样，不同之处在于BERT训练了三层神经网络，其中一层是Bi-LSTM，是为了更好地捕捉单词的上下文关系。BERT在预训练过程中会使用Masked Language Model（MLM）、Next Sentence Prediction（NSP）等技术来辅助模型的训练。
### 3) 文本匹配方法
文本匹配方法（Text Matching Methods）是指采用算法对两个文本进行匹配，目的是找出两个文本之间存在的相似度。常见的文本匹配方法有编辑距离算法、cosine距离算法、Jaccard相似系数等。编辑距离算法是指计算两个字符串之间最少的编辑操作数量，用来衡量两个字符串之间的差异性。编辑距离算法分为全局编辑距离算法、局部编辑距离算法和两个端点的编辑距离算法。
#### 编辑距离算法
编辑距离算法（Edit Distance Algorithm）是指用来计算两个字符串之间的最小编辑距离的算法。编辑距离算法的一般步骤如下：

1. 创建一个二维数组dp，记录两个字符串之间的编辑距离。dp[i][j]表示第一个串的长度为i，第二个串的长度为j时的最小编辑距离。

2. 初始化dp[i][0]和dp[0][j]为i和j，i和j分别表示第一个串的长度和第二个串的长度。

3. 根据两个串的符号是否相等，选择下述四种情况中的一种：

   a. 如果两个符号相等，则dp[i][j]等于dp[i-1][j-1],即在第一个串的第i个字符和第二个串的第j个字符相同，最小编辑距离不变。
   
   b. 如果两个符号不相等，则选择dp[i-1][j]、dp[i][j-1]和dp[i-1][j-1]中的最小值+1，即选择在第一个串的第i个字符插入、在第二个串的第j个字符插入和替换，得到的最小编辑距离。
   
4. 返回dp[len(str1)][len(str2)]，即为两个字符串之间的最小编辑距离。

常用的编辑距离算法有Levenshtein距离、莱文斯坦距离和Damerau-Levenshtein距离。莱文斯坦距离又称为LCS距离，是指用来计算两个字符串之间最长公共子序列的长度。Damerau-Levenshtein距离是Levenshtein距离的一种扩展，允许相邻的字符发生一次错位。
#### cosine距离算法
余弦距离（Cosine Similarity）又称为夹角余弦，用来衡量两个向量之间的夹角大小。cosine距离算法是指计算两个向量的cosine距离，用来衡量两个文档或文本的相似度。cosine距离算法公式如下：

$$sim(\vec{x}, \vec{y})=\frac{\vec{x}\cdot\vec{y}}{\|\vec{x}\|_2\|\vec{y}\|_2}$$

其中，$\vec{x}$和$\vec{y}$分别是两个向量，$\cdot$表示矢量点乘，$\|\cdot\|_2$表示向量的2范数。如果$\vec{x}$和$\vec{y}$指向相同的方向，则cosine距离为1，如果它们指向相反的方向，则cosine距离为-1，如果两个向量平行，则cosine距离为0。cosine距离算法比较简单，计算代价也很小，适合大规模文本语料库的相似性计算。
#### Jaccard相似系数
Jaccard相似系数（Jaccard Coefficient）是指用来计算两个集合的相似度的一种方法。Jaccard相似系数的计算公式如下：

$$J(A,B)=\frac{|A\cap B|}{|A\cup B|}$$

其中，$A$和$B$是两个集合，$|$表示集合元素个数。Jaccard相似系数反映了两个集合的相似程度，当相等时为1，否则为0。Jaccard相似系数常用于文本相似度计算，可以评估两个文档或文本之间的相关程度。
### 4) 分词方法
分词方法（Tokenization Method）是指将文本转换为单词序列的算法。分词的目的是为了方便统计和搜索，找到词在文本中的位置，提取重要的词干。常见的分词方法有最大匹配法、双指针法、前缀字典法、后缀字典法、关键词提取法、条件随机场分词法等。
#### 最大匹配法
最大匹配法（Maximum Matching Method）是最简单的分词方法，它的基本思想是通过词典查找最大匹配词，然后从最大匹配词向后切割，直到切割完整个句子。最大匹配法速度较慢，适合对长文本分词。
#### 双指针法
双指针法（Two Pointer Method）是一种更复杂的分词方法，它的基本思想是设置两个指针，一个指针指向待分词文本的开始位置，另一个指针指向待分词文本的结束位置。通过移动两个指针，找到两个指针之间所有可能的词，然后判断这些词是否在词典中，最后选取词典中出现最多的那个词作为分词结果。双指针法速度较快，适合对短文本分词。
#### 前缀字典法
前缀字典法（Prefix Dictionary Method）是一种分词方法，它的基本思想是建立前缀字典，然后根据词典中的前缀来匹配。通过判断当前位置的词是否在前缀字典中，来判断是否有可用的词，然后继续往后匹配。前缀字典法的速度较慢，但精度高于双指针法。
#### 后缀字典法
后缀字典法（Suffix Dictionary Method）也是一种分词方法，它的基本思想是建立后缀字典，然后根据词典中的后缀来匹配。通过判断当前位置的词是否在后缀字典中，来判断是否有可用的词，然后继续往前匹配。后缀字典法的速度较慢，但精度高于双指针法。
#### 关键词提取法
关键词提取法（Keyphrase Extraction Method）是一种提取重要词组的方法，它的基本思想是找到一组连续的词，这些词能够描述整体文本的主题或观点。关键词提取法速度较快，但精度较低。
#### 条件随机场分词法
条件随机场分词法（Conditional Random Field Segmentation Method）是一种条件随机场（CRF）模型的分词方法，它的基本思想是建立分词标签序列，然后用CRF模型来训练分词模型，用训练好的模型对输入文本进行分词。条件随机场分词法速度较慢，但精度较高。

# 5.具体代码实例与解释说明
## （1）图像分类代码实例
```python
import numpy as np 
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential() # create model object
model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3))) # add convolution layer with relu activation function
model.add(MaxPooling2D(pool_size=(2, 2))) # add pooling layer
model.add(Flatten()) # add flatten layer
model.add(Dense(units=128, activation='relu')) # add fully connected layer with relu activation function
model.add(Dense(units=1000, activation='softmax')) # add output layer with softmax activation function

# compile the model using categorical cross entropy loss and adam optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam') 

# train the model on some data
X_train = [] # put training images here
Y_train = [] # put corresponding labels here
X_test = [] # put testing images here
Y_test = [] # put corresponding test labels here

num_epochs = 10 # number of epochs to train the model

for epoch in range(num_epochs):
    print('Epoch:', epoch+1)

    history = model.fit(np.array(X_train)/255., Y_train, batch_size=32, verbose=1, validation_split=0.2)
    
    score = model.evaluate(np.array(X_test)/255., Y_test, verbose=0)
    print('Test accuracy:', score[1])
```
## （2）文本匹配代码实例
```python
def editDistance(str1, str2):
    """
    This is the dynamic programming implementation of Edit Distance algorithm
    :param str1: first string 
    :param str2: second string
    :return: minimum number of edits needed to convert str1 into str2  
    """
    m = len(str1)    
    n = len(str2)    
 
    # Create a table to store results of subproblems  
    dp = [[0 for x in range(n + 1)] for y in range(m + 1)] 
 
    # Fill d[][] in bottom up manner 
    for i in range(m + 1): 
        for j in range(n + 1): 
            if (i == 0): 
                dp[i][j] = j    # Min. operations = j  
            elif (j == 0): 
                dp[i][j] = i    # Min. operations = i  
            elif (str1[i-1] == str2[j-1]): 
                dp[i][j] = dp[i-1][j-1]        # If current characters are same, ignore last char  
            else:         
                dp[i][j] = 1 + min(dp[i][j-1],       # Insert  
                                     dp[i-1][j],       # Remove  
                                     dp[i-1][j-1])    # Replace 
  
    return dp[m][n] 
  
  
# Driver program to test above function  
string1 = "kitten"
string2 = "sitting"
print("The edit distance between", string1, "and", string2,"is",editDistance(string1, string2))
```

