
作者：禅与计算机程序设计艺术                    

# 1.简介
  

在计算机视觉领域中，图像是每天都要处理的对象。图像安全、图片共享和数据隐私保护等方面也变得越来越关注。因此，越来越多的研究人员、工程师和公司开始着手对图像的安全性和隐私保护进行防范。然而，在实际应用中，一些研究人员发现了一种特殊的攻击方式-对抗攻击（Adversarial Attack）。近年来，基于神经网络的图像分类模型取得了显著的进步，这些模型可以高效地识别出各种各样的图像，但是同时也带来了新的安全性风险——对抗攻击。

攻击者通过构造复杂的对抗样本，将它们输入到目标模型中，从而导致其预测结果发生变化，这一行为被称为对抗攻击。与正常的图像相比，对抗样本往往具有明显不同的特征，这种现象被称为鲁棒性（Robustness）。对抗样本对于计算机视觉系统来说十分重要，因为对抗样本可以覆盖模型的泛化误差，从而提升模型的鲁棒性。目前，已经有许多对抗攻击方法被提出，如FGSM、BIM、DeepFool、CWL2、AutoAttack等等。然而，仍有很多工作需要做，才能使对抗攻击技术能够真正落地。

图像对抗攻击是对深度学习模型的一种安全威胁，对抗样本生成技术成为对抗攻击的一个重要组成部分。最近几年，有大量的研究试图解决如何生成有效的对抗样本的问题。随着对抗样本的研究日益增加，对抗攻击的防御也逐渐成为研究重点。下面就让我们一起回顾一下目前对抗攻击相关的主要工作。


# 2.ImageNet-Challenge中的对抗样本

在图像分类任务中，深度学习模型往往难以准确预测出图像的类别标签，但是通过对抗样本的攻击，即通过对图像做出微小的扰动，使得模型产生错误的预测结果，那么该模型是否仍然具备良好的鲁棒性呢？

ImageNet图像分类 challenge (ILSVRC) 是对抗攻击的基准测试赛。2017年，第一届 ImageNet-Challenge 比赛结束后，深圳大学、清华大学等机构发起的第二届 ILSVRC challenge 在伦敦举行，目标是用机器学习技术检测和揭示对抗攻击的威胁。参赛队伍不仅需要开发出新的算法和防护方案，还需要分析和理解对抗样本生成技术，并将对抗攻击用于图像分类任务。

为了评估对抗攻击方法的鲁棒性，challenge 开设了一个 adversarial validation 数据集，要求参赛队伍提交对抗样本生成算法，用于生成可疑图像并在 ImageNet 上进行验证。其中一项评价指标就是对抗样本准确率（perturbation accuracy），即对抗样本攻击后的准确率。这个准确率用来衡量攻击算法生成的对抗样本的质量。

下面让我们来看一下对抗样本准确率是如何影响到模型的性能的。


# 3.对抗样本准确率 vs 模型准确率

首先，我们需要搞清楚两个概念：模型准确率（model accuracy）和对抗样本准确率（adversarial accuracy）。

模型准确率（Model Accuracy）是指没有受到对抗样本攻击的情况下，模型所能正确分类的图片数量占总数量的比例。通常情况下，人们认为模型准确率越高，代表模型的好坏程度越强。但其实，模型准确率是一个比较模糊的概念，因为它无法体现一个模型在不同场景下的表现。例如，在原始图片上出现一个小角落的瑕疵，模型可能可以做出正确的分类，但是如果改动过大的区域，例如整个图片的某个位置，模型的准确率就可能会降低。所以，模型准确率不能直接反映模型在不同场景下表现的真实情况，只能反映模型在一个固定的训练集上的效果。

而对抗样本准确率（Adversarial Accuracy）则是在已知模型准确率的情况下，评估生成的对抗样本攻击后的模型准确率。模型准确率越高，说明模型对图像的分类能力越强，而生成的对抗样本准确率，则表示通过对抗样本攻击，模型的鲁棒性。我们希望，随着对抗样本攻击的加强，模型准确率不会再下降，也就是说，模型的攻击鲁棒性越高，它的分类性能越稳定。

一般来说，对于某张图片，假设有n个对抗样本被成功生成，每一个对抗样本的准确率为a，则可以通过对抗样本准确率的平均值计算得到最终的模型准确率。
$$
\text{模型准确率}=\frac{\text{模型正确分类图片个数}}{\text{总图片个数}} \\
\text{对抗样本准确率}=\frac{n}{\text{总图片个数}}\cdot a \\
\text{最终模型准确率}= \left\{
    \begin{array}{}
        {\text{如果对抗样本准确率 < 模型准确率:}\\
        \frac{n}{\text{总图片个数}}\cdot (\text{模型准确率}+\text{模型错误分类图片个数})\cdot a} \\[1ex]
        {\text{否则:}\\
         \text{模型准确率}}
    \end{array}\right.
$$


# 4.三种不同级别的防御

前文提到的对抗攻击，可以分为两大类，一种是黑盒攻击（White Box），另一种是白盒攻击（Black Box）。前者由攻击者知道模型结构，有能力根据模型结构选择最合适的对抗样本；后者则不需要了解模型结构，只需给定目标模型、一些攻击参数，就可以生成相应的对抗样本。

两种攻击类型虽然有所区别，但是却共同依赖于对抗样本生成算法。在本节中，我们将讨论三种不同的对抗样本生成技术。


## 4.1 对抗样本生成算法分类

### 4.1.1 基于模型的对抗样本生成算法

在基于模型的对抗样本生成算法中，对抗样本生成器（Adversarial Generator）由一个输入图像和目标标签作为输入，输出一个恶意样本。典型的基于模型的对抗样本生成算法包括梯度越升函数（Gradient-based attack）、最小化近似函数距离（Regularized Euclidean Distance）、最小化中间件距离（Middle-box Manipulation）等。

#### （1）梯度越升函数（FGSM）

FGSM（Fast Gradient Sign Method）算法利用目标模型梯度的信息，迭代生成对抗样本，即在输入图像上添加一个单位向量，调整幅度使得目标模型产生的损失函数最大化。详细的过程如下：

1. 计算目标模型在输入图像 x 上的梯度 g 。
2. 生成一个单位向量 d ，其方向与 g 的方向相同，但长度缩放为 epsilon。
3. 将 d 添加到 x 上，得到扰动后的图像 x+d 。
4. 检验生成的图像是否满足对抗样本的要求，如裁剪、旋转、调整亮度、饱和度、色调等。
5. 用扰动后的图像作为输入，计算目标模型产生的损失函数值 loss 。
6. 根据 loss 和模型的梯度 g ，计算调整幅度 alpha，并更新 x 为 x + alpha * d 。
7. 重复步骤4~6，直到达到目标攻击成功的条件。


FGSM 算法生成的对抗样本鲁棒性较低，原因在于其只利用目标模型的梯度信息，而忽略了模型内部隐藏层的特征信息。

#### （2）最小化近似函数距离（RFD）

RFD（Regularized FGSM with Augmentation）是对 FGSM 的改进，对 FGSM 使用随机噪声的方式增强对抗样本的鲁棒性。具体过程如下：

1. 使用 FGSM 方法生成初始的对抗样本 x 。
2. 对 x 使用随机噪声 z 来增强鲁棒性。
3. 计算 z 的梯度 gz ，以及 z 对 x 的导数 dg 。
4. 将 z 更新为 z - εgz / ∥dg∥ 。
5. 对 z 的梯度重新赋值为 gz ，以便于下次迭代时使用。
6. 判断生成的对抗样本是否满足对抗样本要求，如裁剪、旋转、调整亮度、饱和度、色调等。
7. 如果生成的对抗样本满足要求，返回；否则继续执行步骤 4-6 。


RFD 通过对原始对抗样本进行随机噪声的引入，可以更加充分地利用模型的局部特性。

#### （3）最小化中间件距离（MIM）

MIM（Minimal Intermediate Distance）是一种基于中间件的对抗样本生成算法，它的生成流程和 RFD 有些类似，不同的是，其采用中间件模式（middleware pattern）的方式，在中间件中加入随机噪声，并通过中间件传播到输入层，从而实现中间层和输入层之间的特征混淆。


MIM 可以有效地利用模型内部的特征，对抗样本生成效果更好。


### 4.1.2 无模型的对抗样本生成算法

无模型的对抗样本生成算法由两个独立的模块组成，分别为编码器和解码器。编码器负责将原始图像 x 编码为一个潜在空间 z ，解码器负责从潜在空间中重新生成图像。无模型的对抗样本生成算法可以分为两类，一类是基于梯度的算法，一类是基于扰动的算法。


#### （4）基于梯度的算法

基于梯度的算法包括 LBFGS（Limited Memory BFGS）、AdaGrad（Adaptive Gradient）、Adam（Adaptive Momentum Estimation）等。LBFGS 使用牛顿法和拟牛顿法，依据目标模型的梯度信息生成对抗样本，生成的对抗样本的质量与目标模型的梯度有关。AdaGrad 以自适应的方式更新梯度，通过累计梯度平方来调整步长，适用于非凸目标函数。Adam 会结合 AdaGrad 和 RMSProp 的优点，同时解决了 AdaGrad 的缺陷，适用于深度学习模型的对抗样本生成。


以上方法均属于无模型的对抗样本生成算法，它们使用的都是生成式模型，并没有考虑到输入的图片数据本身。


#### （5）基于扰动的算法

基于扰动的算法包括基于向量的扰动方法（Vector-Based Perturbation Methods）、基于神经网络的扰动方法（Neural Network Perturbation Methods）等。基于向量的扰动方法包括 Gaussian Blur、Salt & Pepper Noise、JPEG Compression、JPEG Artifacts、Gradient Masking、Color Shift 等。基于神经网络的扰动方法包括 DeepFool、CWL2、AutoAttack、HopSkipJump 等。

DeepFool 是一种最近邻搜索的方法，利用目标模型的梯度信息判断样本点属于当前邻域的哪一类。然后针对每个样本点，依照目标模型的分类结果和梯度信息，按照一定概率沿着梯度方向改变特征，直到目标模型的分类结果发生变化。

CWL2 使用交叉熵作为损失函数，优化目标模型的分类性能。相对于其他基于对抗样本的防御方法，CWL2 的攻击参数设置更灵活，可以在运行过程中动态调整参数以达到最佳的攻击效果。

HopSkipJump 是一种跳跃跳跃操作，通过连续进行多个随机扰动，来增加模型对攻击的鲁棒性。


以上方法都是无模型的对抗样本生成算法，它们直接在输入的图片数据中修改，使得模型产生错误的预测结果。


## 4.2 对抗攻击防御的分类

对抗攻击防御可以分为四大类：基于模型的防御（Model-based Defense）、基于梯度的防御（Gradient-based Defense）、基于扰动的防御（Perturbation-based Defense）和组合防御（Combination of Defenses）。


### 4.2.1 基于模型的防御

基于模型的防御指的是在模型上添加额外的约束或限制，来防止对抗样本的攻击。主要包括欺骗性（Confusing Input）和限制性（Limiting Features）的防御方法。


#### （6）欺骗性防御（Confusing Input）

欺骗性防御是指根据对抗样本的属性，对模型的预测结果造成恶意影响。常用的方法包括生成类内异常样本（INFORMED Sample Generation）、目标伪造（Target Guided）、反射样本（Reflective Sample）等。


##### （1）生成类内异常样本（INFORMED Sample Generation）

INFORMED Sample Generation (ISG) 是一种针对目标模型的通用生成异常样本的方法。ISG 方法通过对抗样本的判定结果、目标标签、样本的分布特点等信息，生成具有相似属性的异常样本，并注入到正常训练集中。


当模型对攻击样本的分类错误时，如果对抗样本具有相似的统计分布，且目标标签和原始样本一致，则可以将该对抗样本添加到正常训练集中，以增加模型的鲁棒性。


##### （2）目标伪造（Target Guided）

目标伪造 (TG) 指的是用异常样本来提升对特定目标类别的攻击能力。TG 方法通过改变目标类别的置信度，来改变模型的分类结果，从而达到提升对特定目标类的攻击能力。


TG 方法通过改变模型的分类概率，来引导模型的行为，令其对目标类别产生误判，提升对该目标类别的攻击能力。


##### （3）反射样本（Reflective Sample）

反射样本是指利用对抗样本的特点，去仿照它制作新的样本。通过对抗样本的差异性和特征，反射样本可以反映出输入图像的原始分布，并传递到模型中。


反射样本可以与正常训练集和测试集中的样本结合起来，构建训练样本集和测试样本集。


#### （7）限制性防御（Limiting Features）

限制性防御是指对模型的输入和输出施加限制，以防止对抗样本的攻击。限制性防御方法主要包括最大范数限制（Max Norm Constraints）、频率限制（Frequency Limiting）、动量约束（Momentum Contraint）、随机范数限制（Random Norm Constraints）等。

##### （4）最大范数限制（Max Norm Constraints）

最大范数限制（MNAC）是一种限制模型输入的范数的方法。MNAC 方法通过对模型的输入施加最大范数约束，来防止模型产生过大的激活值，从而阻止对抗样本的攻击。


MNAC 方法会限制输入向量的长度，令其不超过一定范围的值。


##### （5）频率限制（Frequency Limiting）

频率限制（FLC）是一种对模型的频率激活值进行限制的方法。FLC 方法通过对模型的频率激活值进行限制，来降低模型对某些频率的响应，从而提升模型的鲁棒性。


FLC 方法通过限制神经元的活动频率，来抑制模型的过早惩罚。


##### （6）动量约束（Momentum Contraint）

动量约束（MOMENTUM）是一种对模型的动量参数进行限制的方法。MOMENTUM 方法通过对模型的动量参数进行限制，来减少模型对长期影响的过早惩罚，从而提升模型的鲁棒性。


MOMENTUM 方法通过限制模型的动量参数，来避免过度惩罚那些无效的样本。


##### （7）随机范数限制（Random Norm Constraints）

随机范数限制（RANDNORM）是一种对模型的输入施加随机范数限制的方法。RANDNORM 方法通过对模型的输入施加随机范数约束，来模拟正常样本的噪声，从而提升模型的鲁棒性。


RANDNORM 方法会随机地缩放输入向量的值，达到对抗样本攻击的目的。


### 4.2.2 基于梯度的防御

基于梯度的防御指的是使用目标模型的梯度来限制对抗样本的攻击。主要包括梯度迫使（Gradient Penalty）、梯度裁剪（Gradient Clipping）、虚拟边界（Virtual Boundary）等方法。


#### （8）梯度迫使（Gradient Penalty）

梯度迫使（GP）是一种对抗样本进行限制的方法，其通过优化模型的梯度，来抑制模型的过早惩罚。GP 方法通过在损失函数中加入梯度惩罚项，来抑制模型对无效样本的惩罚。


GP 方法以目标模型的梯度的范数作为惩罚因子，来模拟对抗样本攻击对模型梯度的惩罚。


#### （9）梯度裁剪（Gradient Clipping）

梯度裁剪（GC）是一种对抗样本的防御方式，其通过裁剪模型的梯度，来限制其大小。GC 方法通过设定最大范数或者阈值，对模型的梯度进行裁剪，从而避免模型对过大的梯度的惩罚。


GC 方法通过限制模型的梯度的大小，来抑制模型对无效样本的惩罚。


#### （10）虚拟边界（Virtual Boundary）

虚拟边界（VB）是一种提升模型的鲁棒性的方法，其通过构建虚拟边界，来提升模型的抗攻击能力。VB 方法通过模拟训练样本的边界，对模型的输出进行约束，从而抑制模型对无效样本的攻击。


VB 方法通过构建虚假边界，来模拟训练样本的边界，对模型的输出进行约束，提升模型的鲁棒性。


### 4.2.3 基于扰动的防御

基于扰动的防御指的是通过对抗样本的参数进行限制，来抑制对抗样本的攻击。主要包括随机扰动（Random Perturbation）、参数裁剪（Parameter Clipping）、梯度投影（Gradient Projection）等方法。


#### （11）随机扰动（Random Perturbation）

随机扰动（RP）是一种对模型参数进行随机扰动的方法，其通过随机调整模型参数，来抑制对抗样本的攻击。RP 方法通过在训练过程中，随机添加噪声，对模型参数进行调整，从而生成新的样本，抑制对抗样本的攻击。


RP 方法通过在训练过程中，随机添加噪声，对模型参数进行调整，来生成新的样本，抑制对抗样本的攻击。


#### （12）参数裁剪（Parameter Clipping）

参数裁剪（PC）是一种限制模型参数的方法，其通过限制模型参数的取值范围，来抑制对抗样本的攻击。PC 方法通过设置参数的上下限，对模型参数进行裁剪，从而限制模型对输入数据的处理能力。


PC 方法通过限制模型参数的取值范围，来抑制对抗样本的攻击。


#### （13）梯度投影（Gradient Projection）

梯度投影（GP）是一种投射模型梯度的方法，其通过减少梯度的范数，来限制对抗样本的攻击。GP 方法通过优化模型梯度范数，对抗样本进行削弱，从而抑制对抗样本的攻击。


GP 方法通过优化模型梯度范数，将梯度投射到更容易受到攻击的方向上，从而抑制对抗样本的攻击。


### 4.2.4 组合防御

组合防御是指多种防御方法的结合，以提升模型的对抗样本攻击能力。组合防御方法包括学习防御（Learning-Based Defense）、统计防御（Statistical Defense）、信号防御（Signal Processing Defense）等。


#### （14）学习防御（Learning-Based Defense）

学习防御（LB）是一种利用深度学习技术，对防御策略进行训练的防御方法。LB 方法通过对抗样本及其对应的标注信息进行训练，来优化防御策略，从而提升模型的攻击能力。


LB 方法通过对抗样本及其对应的标注信息进行训练，来提升模型的攻击能力。


#### （15）统计防御（Statistical Defense）

统计防御（SD）是一种通过统计特征来判断对抗样本的有效性的方法。SD 方法通过统计模型在训练集和测试集上的表现，来判断对抗样本是否有效。


SD 方法通过统计模型在训练集和测试集上的表现，来判断对抗样本是否有效。


#### （16）信号防御（Signal Processing Defense）

信号防御（SPD）是一种通过信号处理技术，对输入信号进行修改，来限制对抗样本的攻击。SPD 方法通过对输入信号进行采样、过滤、维持或者剔除，来削弱对抗样本的攻击效果。


SPD 方法通过对输入信号进行采样、过滤、维持或者剔除，来削弱对抗样本的攻击效果。