
ä½œè€…ï¼šç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯                    

# 1.ç®€ä»‹
  

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå¯¹äºæ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ä¸€ç›´æ˜¯è¡¡é‡ä¸€ä¸ªæ¨¡å‹ä¼˜åŠ£çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ã€‚å¯¹äºä¸€äº›æ›´é«˜çº§çš„ä»»åŠ¡å¦‚æœºå™¨é˜…è¯»ç†è§£ï¼Œå­˜åœ¨ç€å¯¹æ¨¡å‹æ€§èƒ½çš„éœ€æ±‚ï¼Œè¿™äº›æ¨¡å‹éœ€è¦èƒ½å¤Ÿåœ¨å„ç§ç¯å¢ƒä¸‹çš„æ¡ä»¶ä¸‹è¿›è¡Œæ¨ç†ã€‚æ¯”å¦‚åœ¨æ¶åŠ£æƒ…å†µä¸‹ï¼Œæ¨¡å‹å¯èƒ½ä¼šé¢ä¸´è¯­æ–™è´¨é‡ä¸è¶³ã€æ¨¡å‹è¿‡äºå¤æ‚æˆ–è®­ç»ƒæ•°æ®è¿‡å°‘ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœºå™¨å­¦ä¹ ç§‘ç ”äººå‘˜ç»å¸¸ä¼šé‡‡ç”¨è®¸å¤šæ‰‹æ®µæ¥æå‡æ¨¡å‹çš„é²æ£’æ€§å’Œå‡†ç¡®æ€§ï¼Œæ¯”å¦‚æå‡æ•°æ®é›†ã€åŠ å…¥å™ªå£°æˆ–æ¨¡æ‹Ÿæ•°æ®ç­‰ã€‚ä½†æ˜¯è¿™äº›æ–¹å¼éƒ½å…·æœ‰å±€é™æ€§ï¼Œå®ƒä»¬æ— æ³•å®Œå…¨é¿å…æ¨¡å‹æ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚å¦ä¸€ç§æ›´åŠ æœ‰æ•ˆçš„æ–¹æ³•å°±æ˜¯åˆ©ç”¨äººå·¥åˆæˆçš„æ•°æ®è¿›è¡Œæ¨¡å‹çš„æµ‹è¯•ã€‚ç›¸è¾ƒäºçœŸå®çš„è¯­æ–™åº“ï¼Œåˆæˆæ•°æ®å¯ä»¥æä¾›æ›´å¤šçš„æµ‹è¯•ç”¨ä¾‹ï¼Œå¹¶é€šè¿‡æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿‘å¹´æ¥ï¼Œè¶Šæ¥è¶Šå¤šçš„ç ”ç©¶äººå‘˜è‡´åŠ›äºåˆ©ç”¨åˆæˆæ•°æ®æå‡è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚æœ¬æ–‡å°†é˜è¿°å¦‚ä½•ä½¿ç”¨åˆæˆæ•°æ®æå‡BERTé¢„è®­ç»ƒæ¨¡å‹çš„é²æ£’æ€§ï¼Œå¹¶è®¨è®ºå…¶ç†è®ºä¾æ®ï¼Œå¹¶ä»‹ç»äº†ä¸€äº›ç”¨äºåˆæˆæ•°æ®çš„å¼€æºå·¥å…·å’Œå¹³å°ã€‚

# 2.åŸºæœ¬æ¦‚å¿µæœ¯è¯­
## 2.1 BERTé¢„è®­ç»ƒæ¨¡å‹
BERTé¢„è®­ç»ƒæ¨¡å‹æ˜¯Googleåœ¨2018å¹´10æœˆå‘å¸ƒçš„ä¸€é¡¹é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè¡¨å¾å­¦ä¹ ã€‚è¯¥æ¨¡å‹é‡‡ç”¨Transformerï¼ˆä¸€ç§æ·±åº¦å­¦ä¹ ç½‘ç»œï¼‰ç»“æ„ï¼Œåœ¨æ–‡æœ¬ä¸Šè¿›è¡Œç²¾å¿ƒè®¾è®¡ï¼Œä½¿å¾—å®ƒå¯ä»¥åœ¨å¤šç§è¯­è¨€ä»»åŠ¡ä¸Šå–å¾—state-of-the-artçš„ç»“æœã€‚

## 2.2 æ•°æ®å¢å¹¿
æ•°æ®å¢å¹¿ï¼ˆData Augmentationï¼‰æ˜¯é€šè¿‡ç”Ÿæˆæ–°æ ·æœ¬çš„æ–¹å¼æ¥æ‰©å±•è®­ç»ƒæ•°æ®é›†çš„æ–¹æ³•ã€‚å®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹å­¦ä¹ åˆ°æ•°æ®åˆ†å¸ƒçš„ç›¸å…³ç‰¹æ€§ï¼Œå‡å°‘è¿‡æ‹Ÿåˆï¼Œå¹¶è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ•°æ®å¢å¹¿ä¸»è¦æœ‰ä¸¤ç§ç±»å‹ï¼š
1. å¥å­çº§æ•°æ®å¢å¹¿ï¼šå¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œå˜æ¢ï¼Œæ¯”å¦‚æ’å…¥ç‰¹æ®Šç¬¦å·ã€åˆ é™¤è¯æ±‡ã€éšæœºæ›¿æ¢å•è¯ç­‰ã€‚
2. ç‰¹å¾çº§æ•°æ®å¢å¹¿ï¼šåŸºäºæ–‡æœ¬çš„ç‰¹å¾ï¼Œæ¯”å¦‚token embeddingã€å¥å‘é‡ç­‰ï¼Œè¿›è¡Œå˜æ¢ã€‚

## 2.3 æ¨¡å‹è’¸é¦ï¼ˆDistillationï¼‰
æ¨¡å‹è’¸é¦ï¼ˆDistillationï¼‰æ˜¯ä¸€ç§è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥å°†ä¸€ä¸ªå¤§çš„å¤æ‚æ¨¡å‹å‹ç¼©æˆä¸€ä¸ªå°æ¨¡å‹ã€‚è’¸é¦åçš„æ¨¡å‹å¯ä»¥æ¥æ”¶å°æ¨¡å‹çš„è¾“å‡ºï¼Œç„¶åé€šè¿‡è‡ªé€‚åº”å­¦ä¹ æ³•æ¥å­¦ä¹ å°æ¨¡å‹çš„å‚æ•°ï¼Œä»è€Œå®ç°æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚

## 2.4 åŠ©æ•™æ¨¡å‹
åŠ©æ•™æ¨¡å‹æ˜¯ä¸€ç§æ–°é¢–çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå®ƒå¯ä»¥è‡ªåŠ¨ç”Ÿæˆä¸ç»™å®šè¾“å…¥ç›¸ä¼¼çš„åˆæˆæ•°æ®ã€‚è¿™æ ·å¯ä»¥æå‡æ¨¡å‹çš„é²æ£’æ€§ï¼Œå› ä¸ºæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºåˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶åšå‡ºè°ƒæ•´ã€‚

# 3.æ ¸å¿ƒç®—æ³•åŸç†åŠæ“ä½œæ­¥éª¤
## 3.1 æ•°æ®å¢å¹¿æ–¹æ³•
ç›®å‰ï¼Œæ•°æ®å¢å¹¿æ–¹æ³•æœ‰ä¸¤ç§ï¼š
1. ä½¿ç”¨è§„åˆ™ï¼ˆRule-basedï¼‰çš„æ–¹æ³•ï¼šæ¯”å¦‚é’ˆå¯¹ç‰¹å®šç±»å‹æ–‡æ¡£ï¼Œåªä½¿ç”¨ç¼©å†™ã€é”™åˆ«å­—æˆ–è¯­æ³•é”™è¯¯çš„è®­ç»ƒæ ·æœ¬ã€‚
2. ä½¿ç”¨ç»Ÿè®¡æ¨¡å‹çš„æ–¹æ³•ï¼šæ¯”å¦‚æ ¹æ®åˆ†å¸ƒæƒ…å†µã€åŒä¹‰è¯æ›¿æ¢ã€é€†åºæ›¿æ¢ç­‰æ¨¡å‹ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚

## 3.2 æ•°æ®è’¸é¦ç­–ç•¥
åœ¨æ¨¡å‹è’¸é¦ä¸­ï¼Œä¸€èˆ¬é€‰æ‹©Teacher-Studentç»“æ„ã€‚Teacheræ¨¡å‹æ¥å—çœŸå®æ•°æ®ï¼ŒStudentæ¨¡å‹æ¥å—åŠ©æ•™æ¨¡å‹çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œç„¶åè¿›è¡Œè’¸é¦å­¦ä¹ ï¼Œä»¥å‡å°‘æ¨¡å‹å¤§å°å’Œå‚æ•°æ•°é‡ã€‚è’¸é¦ç­–ç•¥é€šå¸¸åŒ…æ‹¬ä»¥ä¸‹ä¸‰ç§ï¼š

1. Knowledge Distillationï¼ˆKDï¼‰ï¼šTeacheræ¨¡å‹å°†ä¿¡æ¯ä¼ é€’ç»™Studentæ¨¡å‹ï¼Œåˆ©ç”¨ä¸¤ä¸ªæ¨¡å‹çš„è·ç¦»ï¼ˆå¦‚KLæ•£åº¦ï¼‰æ¥æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚

2. Adversarial Distillationï¼ˆADVï¼‰ï¼šTeacheræ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼Œé€šè¿‡è®©Studentæ¨¡å‹äº§ç”Ÿæ›´å¥½çš„ä¼ªæ ‡ç­¾æ¥æå‡æ¨¡å‹çš„é²æ£’æ€§ã€‚

3. Soft Label Distillationï¼ˆSLDï¼‰ï¼šTeacheræ¨¡å‹é¢„æµ‹ä¸€ç»„æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå­¦ç”Ÿæ¨¡å‹åˆ™å¯ä»¥æ¥å—ç›¸åº”çš„æ¦‚ç‡åˆ†å¸ƒä½œä¸ºæ­£ç¡®æ ‡ç­¾ã€‚


## 3.3 æ¿€æ´»å‡½æ•°æ”¹è¿›
æ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰æ˜¯ç¥ç»ç½‘ç»œçš„å…³é”®ç»„ä»¶ä¹‹ä¸€ï¼Œå®ƒçš„ä½œç”¨æ˜¯å†³å®šèŠ‚ç‚¹è¾“å‡ºçš„å€¼ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰sigmoidã€tanhã€reluã€softmaxç­‰ã€‚BERTä½¿ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯geluï¼Œå®ƒæ˜¯ä¸€ä¸ªå¹³æ»‘çš„åŒæ›²æ­£åˆ‡å‡½æ•°ã€‚ä½†æ˜¯ï¼Œgeluæ¿€æ´»å‡½æ•°çš„å¯¼æ•°å­˜åœ¨å›°éš¾ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¿€æ´»å‡½æ•°GELUï¼Œå®ƒæ˜¯sigmoidçš„å¹³æ»‘ç‰ˆæœ¬ã€‚

## 3.4 åˆ¤åˆ«å¼å­¦ä¹ 
åˆ¤åˆ«å¼å­¦ä¹ ï¼ˆDiscriminative Learningï¼‰æ—¨åœ¨ç›´æ¥é¢„æµ‹ç›®æ ‡å˜é‡è€Œä¸æ˜¯æ¡ä»¶æ¦‚ç‡ã€‚BERTé‡‡ç”¨åˆ¤åˆ«å¼å­¦ä¹ æ¥ç”Ÿæˆç›¸ä¼¼çš„å¥å­ã€‚åˆ¤åˆ«å¼æ¨¡å‹å¯ä»¥æ¥æ”¶ä¸€ä¸ªè¾“å…¥åºåˆ—å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œç„¶åå­¦ä¹ åˆ¤åˆ«å‡½æ•°ï¼Œæ˜ å°„è¾“å…¥åºåˆ—åˆ°æ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒã€‚åˆ¤åˆ«å¼æ¨¡å‹å­¦ä¹ åˆ°çš„åˆ¤åˆ«å‡½æ•°å¯ä»¥åº”ç”¨äºä¸åŒçš„ä»»åŠ¡ï¼Œä¾‹å¦‚æƒ…æ„Ÿåˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ã€å›å½’ç­‰ã€‚åˆ¤åˆ«å¼æ¨¡å‹è¿˜å¯ä»¥ç”¨äºæ›´å¤æ‚çš„åœºæ™¯ï¼Œæ¯”å¦‚è§†é¢‘åˆ†ç±»ã€å›¾åƒæ£€ç´¢ã€å›¾åƒåˆ†å‰²ç­‰ã€‚

# 4.å…·ä½“ä»£ç å®ä¾‹å’Œè§£é‡Šè¯´æ˜
ä¸‹é¢è¯¦ç»†ä»‹ç»ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨å¼€æºå·¥å…·å’Œå¹³å°å®ç°æ•°æ®å¢å¹¿å’Œæ¨¡å‹è’¸é¦ï¼Œä»¥åŠåŠ©æ•™æ¨¡å‹çš„æ„å»ºã€‚

## 4.1 æ•°æ®å¢å¹¿
æ•°æ®å¢å¹¿å·¥å…·ä¸»è¦æœ‰ä¸¤ç§ï¼š
1. nlpaugåº“ï¼šnlpaugæä¾›äº†ä¸€ç³»åˆ—æ•°æ®å¢å¹¿çš„æ–¹æ³•ï¼Œå¯ä»¥å¢å¼ºå·²æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼Œç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚
2. transformersåº“ï¼štransformersåº“ä¸­çš„Trainerç±»æä¾›äº†æ•°æ®å¢å¹¿åŠŸèƒ½ï¼Œå¯ä»¥é€šè¿‡é…ç½®å‚æ•°æ¥ä½¿ç”¨ã€‚

## 4.2 æ¨¡å‹è’¸é¦
### 4.2.1 Teacheræ¨¡å‹è®­ç»ƒ
é¦–å…ˆéœ€è¦è®­ç»ƒä¸€ä¸ªTeacheræ¨¡å‹ï¼Œå¹¶æŠŠå®ƒå›ºå®šä½ï¼Œä¸è¦è¿›è¡Œä»»ä½•æ”¹åŠ¨ã€‚æ­¤æ—¶ï¼ŒTeacheræ¨¡å‹å·²ç»å…·å¤‡è‰¯å¥½çš„æ•°æ®å¤„ç†èƒ½åŠ›ï¼Œå¯ä»¥å¾ˆå¥½åœ°å®Œæˆå„ç§ä»»åŠ¡ã€‚

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments

teacher_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

training_args = TrainingArguments(
    output_dir="output/",          # output directory
    num_train_epochs=3,             # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    save_steps=1000,                # number of updates steps before saving
    learning_rate=2e-5,            # initial learning rate for AdamW
    warmup_steps=1000,              # number of warmup steps for learning rate scheduler
    weight_decay=0.01               # strength of weight decay
)

trainer = Trainer(
    model=teacher_model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                          # training arguments, defined above
    train_dataset=tokenized_datasets["train"],   # training dataset
)

trainer.train()
```

### 4.2.2 Studentæ¨¡å‹è®­ç»ƒ
æ¥ç€ï¼Œè®­ç»ƒä¸€ä¸ªStudentæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨Teacheræ¨¡å‹çš„è¾“å‡ºä½œä¸ºè¾“å…¥ã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºæ¨¡å‹è’¸é¦ï¼Œç›®çš„æ˜¯æå‡Studentæ¨¡å‹çš„æ€§èƒ½ã€‚å¸¸ç”¨çš„æ¨¡å‹è’¸é¦æ–¹æ³•æœ‰KDã€ADVå’ŒSLDã€‚

#### KDæ–¹æ³•

```python
import torch
import torch.nn as nn
from functools import partial

class DistillBERT(nn.Module):

    def __init__(self, teacher_model: nn.Module, student_config):
        super().__init__()

        self.teacher_model = teacher_model
        
        config = teacher_model.config
        config.update({"hidden_dropout_prob":student_config['hidden_dropout_prob'], 
                       "layer_norm_eps":student_config['layer_norm_eps']})

        self.student_model = AutoModel.from_config(config)
        
    def forward(self, input_ids, attention_mask, labels=None):

        with torch.no_grad():
            outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits

        loss_fct = nn.CrossEntropyLoss()
        logits = self.student_model(input_ids=input_ids, attention_mask=attention_mask).logits
        loss = loss_fct(logits.view(-1, self.student_model.config.num_labels),
                        labels.view(-1)) * (logits.shape[-1] ** -1)

        return {'loss': loss}
    
teacher_model = AutoModel.from_pretrained('distilbert-base-uncased')
student_config = {"hidden_dropout_prob":0.1,"layer_norm_eps":1e-7}

model = DistillBERT(teacher_model, student_config)

criterion = nn.MSELoss().to(device)
optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)

for epoch in range(10):
    
    for step, batch in enumerate(data_loader):

        inputs, masks, targets = tuple(t.to(device) for t in batch[:3])
        optimizer.zero_grad()

        predictions = model(inputs, masks)['logits'].squeeze()
        soft_predictions = F.log_softmax(predictions / temperature, dim=-1)
        with torch.no_grad():
            hard_targets = teacher_outputs[step].argmax(dim=-1)
        loss = criterion(soft_predictions, target_variable)

        loss.backward()
        optimizer.step()

        if step % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, step * len(inputs), len(data_loader.dataset),
                100. * step / len(data_loader), loss.item()))
            
```

#### ADVæ–¹æ³•

```python
def calculate_kl_divergence(y_pred, y_true):
    """Calculate the kl divergence between two probability distributions"""
    p = F.log_softmax(y_pred/temperature, dim=-1)
    q = F.softmax(y_true/temperature, dim=-1)
    kl_pq = F.kl_div(p,q, reduction='sum')
    return kl_pq/(len(y_pred)*1.)

def calcualte_bce_loss(y_pred, y_true):
    bce_loss = nn.BCEWithLogitsLoss()(y_pred, y_true)
    return bce_loss*y_pred.shape[-1]**-1

class AdvBERT(nn.Module):

    def __init__(self, teacher_model: nn.Module, student_config):
        super().__init__()

        self.teacher_model = teacher_model
        
        config = teacher_model.config
        config.update({"hidden_dropout_prob":student_config['hidden_dropout_prob'], 
                       "layer_norm_eps":student_config['layer_norm_eps']})

        self.student_model = AutoModel.from_config(config)
        
    def forward(self, input_ids, attention_mask, labels=None):

        with torch.no_grad():
            outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits

        probs = F.softmax(outputs/temperature, dim=-1)
        advs = torch.cat([torch.zeros(probs.shape[:-1]+(1,), dtype=probs.dtype, device=probs.device),
                          probs[...,:-1]], axis=-1)
        adv_probs = F.softmax((advs+probs)/temperature, dim=-1)
        onehots = torch.eye(adv_probs.shape[-1], device=probs.device)[labels.long()]
        real_probs = probs*((onehots-adv_probs)*(onehots!=0)).mean()
        fake_probs = adv_probs*((1.-onehots)+adv_probs*(onehots==0)).max()
        alpha = fake_probs/real_probs

        loss_fct = nn.CrossEntropyLoss()
        logits = self.student_model(input_ids=input_ids, attention_mask=attention_mask).logits
        loss = ((1.-alpha)*calcualte_bce_loss(logits, labels)+(alpha)*loss_fct(logits.view(-1, self.student_model.config.num_labels),
                                                                                 labels.view(-1))) * (logits.shape[-1] ** -1)

        return {'loss': loss}
    
teacher_model = AutoModel.from_pretrained('bert-base-uncased')
student_config = {"hidden_dropout_prob":0.1,"layer_norm_eps":1e-7}

model = AdvBERT(teacher_model, student_config)

criterion = nn.MSELoss().to(device)
optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)

for epoch in range(10):
    
    for step, batch in enumerate(data_loader):

        inputs, masks, targets = tuple(t.to(device) for t in batch[:3])
        optimizer.zero_grad()

        predictions = model(inputs, masks)['logits'].squeeze()
        soft_predictions = F.log_softmax(predictions / temperature, dim=-1)
        with torch.no_grad():
            hard_targets = teacher_outputs[step].argmax(dim=-1)
        loss = criterion(soft_predictions, target_variable)

        loss.backward()
        optimizer.step()

        if step % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, step * len(inputs), len(data_loader.dataset),
                100. * step / len(data_loader), loss.item()))
```

#### SLDæ–¹æ³•

```python
class SLDBERT(nn.Module):

    def __init__(self, teacher_model: nn.Module, student_config):
        super().__init__()

        self.teacher_model = teacher_model
        
        config = teacher_model.config
        config.update({"hidden_dropout_prob":student_config['hidden_dropout_prob'], 
                       "layer_norm_eps":student_config['layer_norm_eps']})

        self.student_model = AutoModel.from_config(config)
        
    def forward(self, input_ids, attention_mask, labels=None):

        with torch.no_grad():
            outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits

        prob_distribution = F.softmax(outputs[:, labels][:, :, None]/temperature, dim=-1)
        true_label_index = list(range(labels.shape[0]))
        random.shuffle(true_label_index)
        pred_label_index = []
        index_count = 0
        while True:
            max_indexes = np.unravel_index(np.argmax(prob_distribution.cpu().numpy()),
                                            shape=prob_distribution.shape[:2])
            if max_indexes not in true_label_index and max_indexes not in pred_label_index \
                    and prob_distribution[max_indexes] > threshold:
                pred_label_index.append(list(max_indexes))
                prob_distribution -= prob_distribution[max_indexes]
                index_count += 1
            else:
                break
        if len(pred_label_index)!= labels.shape[0]:
            diff = abs(len(pred_label_index)-labels.shape[0])
            rand_indexes = np.random.choice(labels.shape[0], diff, replace=False)
            pred_label_index.extend([[i]*int(labels[i]>threshold) for i in rand_indexes])
        pred_label_index = np.array(pred_label_index).T
        flattened_index = [[j for j in range(l.shape[0])] for l in pred_label_index]
        unflattened_index = [x for xs in flattened_index for x in xs]
        distilled_label = prob_distribution[tuple(pred_label_index)].reshape((-1,))

        loss_fct = nn.CrossEntropyLoss()
        logits = self.student_model(input_ids=input_ids, attention_mask=attention_mask).logits
        loss = loss_fct(logits.view(-1, self.student_model.config.num_labels),
                        torch.tensor(distilled_label).long()) * (logits.shape[-1] ** -1)

        return {'loss': loss}
    
teacher_model = AutoModel.from_pretrained('roberta-large')
student_config = {"hidden_dropout_prob":0.1,"layer_norm_eps":1e-7}

model = SLDBERT(teacher_model, student_config)

criterion = nn.MSELoss().to(device)
optimizer = optim.AdamW(model.parameters(), lr=2e-5, eps=1e-8)

for epoch in range(10):
    
    for step, batch in enumerate(data_loader):

        inputs, masks, targets = tuple(t.to(device) for t in batch[:3])
        optimizer.zero_grad()

        predictions = model(inputs, masks)['logits'].squeeze()
        soft_predictions = F.log_softmax(predictions / temperature, dim=-1)
        with torch.no_grad():
            hard_targets = teacher_outputs[step].argmax(dim=-1)
        loss = criterion(soft_predictions, target_variable)

        loss.backward()
        optimizer.step()

        if step % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, step * len(inputs), len(data_loader.dataset),
                100. * step / len(data_loader), loss.item()))
```

### 4.2.3 åŠ©æ•™æ¨¡å‹æ„å»º

```python
class AuxiliaryGenerator(nn.Module):
    
    def __init__(self, student_model):
        super().__init__()
        self.student_model = student_model
        
    def forward(self, sequence_embedding, mask, labels=None):
        prediction = self.student_model(sequence_embedding, attention_mask=mask,
                                        token_type_ids=None).logits
        probabilities = torch.softmax(prediction[:, :-1, :], dim=-1)
        scores = torch.matmul(probabilities,
                              sequence_embedding[:, 1:, :] - sequence_embedding[:, :-1, :]).squeeze(-1)
        gumbel = GumbelSoftmax(dim=-1)(scores).unsqueeze(-1) + 1e-9
        auxiliary_vector = (gumbel.transpose(-1,-2) @ sequence_embedding).squeeze(-1)
        generated_seq = sequence_embedding[:, 0, :] + auxiliary_vector.unsqueeze(1)
        return {**{"generated_sequences": generated_seq}, **{"auxiliary_vectors": auxiliary_vector}}
    
generator = AuxiliaryGenerator(model.student_model)
```

## 4.3 åŠ©æ•™æ¨¡å‹æ•ˆæœè¯„ä¼°

ä¸‹é¢ä»‹ç»ä¸€ä¸‹åŠ©æ•™æ¨¡å‹åœ¨ä¸‰ç§ä¸åŒåœºæ™¯ä¸‹çš„è¡¨ç°ã€‚

### 4.3.1 æ— ç›‘ç£æ•°æ®å¢å¹¿

é¦–å…ˆï¼ŒåŠ è½½æ²¡æœ‰æ ‡è®°çš„æ•°æ®ï¼Œç”Ÿæˆæ–°çš„åˆæˆæ•°æ®ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŠ©æ•™æ¨¡å‹æ¥ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚

```python
from nlpaug.util.file.download import DownloadUtil
DownloadUtil.download_wordnet()
from nltk.corpus import wordnet
from nlpaug.augmenter.word import ContextualWordEmbsAug
aug = ContextualWordEmbsAug(model_path='bert-base-uncased', action='insert')

sentences = ['This is a good movie.', 'The book was written by John Smith.', 'He gave his thumbs up for that job.']
aug_texts = aug.augment(sentences, n=3, num_thread=4)
print(aug_texts)
```

```
[['this furiously operates at our leisure moment to keep everyone entertained','This dutifully examines some speculative details.',
  'these constantly expose us to new possibilities'],'I went to gossip on my trip around town and got lost along the way.',
 ['John snapped him out of it right away.',"They didn't even notice me looking but they knew I'd been watching them.",
  'Smith understood their concern']]
```

ä¸Šé¢ä»£ç å±•ç¤ºäº†ä¸€ä¸ªæ— ç›‘ç£æ•°æ®å¢å¹¿ä¾‹å­ï¼Œå…¶ä¸­ï¼Œä½¿ç”¨äº†Contextual Word Embeddingæ–¹æ³•ã€‚æ³¨æ„ï¼Œç”±äºåŠ©æ•™æ¨¡å‹çš„ç”Ÿæˆé€Ÿåº¦è¾ƒå¿«ï¼Œåœ¨å®é™…ç”Ÿäº§ç¯èŠ‚å¯ä»¥ç›´æ¥è°ƒç”¨ç”Ÿæˆæ¥å£ã€‚

### 4.3.2 å¼±ç›‘ç£æ•°æ®å¢å¹¿

å½“æ•°æ®é›†åªæœ‰ä¸€éƒ¨åˆ†å¸¦æœ‰æ ‡ç­¾ï¼Œå¦ä¸€éƒ¨åˆ†ä¸å¸¦æ ‡ç­¾çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¼±ç›‘ç£æ•°æ®å¢å¹¿æ–¹æ³•æ¥æ‰©å……æ•°æ®é›†ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¼±ç›‘ç£æ•°æ®å¢å¹¿å¯ä»¥äº§ç”Ÿæ½œåœ¨æœ‰ç”¨çš„ä¿¡æ¯ï¼Œè€Œä¸ä¼šç ´åæ•°æ®çš„ç¨³å®šæ€§ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1. ä»æœªæ ‡è®°æ•°æ®ä¸­ï¼Œé€‰æ‹©ä¸€éƒ¨åˆ†æœ‰æ ‡ç­¾çš„æ•°æ®ã€‚
2. å¯¹æœªæ ‡è®°æ•°æ®è¿›è¡Œæ•°æ®å¢å¹¿ã€‚
3. å°†ç”Ÿæˆçš„æ•°æ®æ‹¼æ¥åˆ°æœ‰æ ‡ç­¾æ•°æ®ä¹‹åã€‚
4. ç”¨è®­ç»ƒå¥½çš„Studentæ¨¡å‹é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

```python
labeled_texts = [['i am happy', 'positive'], ['i am sad', 'negative']]
unlabeled_texts = ["it's a beautiful day outside today.",
                   "the weather outside is so nice this morning."]
aug_texts = generator({'text': unlabeled_texts}).get('generated_sequences').tolist()

aug_labeled_texts = [(txt+' '+aug+'\n'+lbl, lbl)
                     for txt, lbl in labeled_texts for aug in aug_texts]
new_labeled_texts = labeled_texts + aug_labeled_texts

train_encodings = tokenizer([' '.join(tup[0].split('\n')[0]),
                           ''.join(tup[0].split('\n')[1])],
                           padding='max_length', truncation=True,
                           return_tensors='pt')['input_ids']
train_labels = torch.LongTensor([int(tup[1]=='positive') for tup in new_labeled_texts])

train_dataset = TensorDataset(train_encodings, train_labels)
```

### 4.3.3 æœ‰ç›‘ç£æ•°æ®å¢å¹¿

æœ‰ç›‘ç£æ•°æ®å¢å¹¿æ˜¯åœ¨å·²æœ‰æ ‡ç­¾æ•°æ®åŸºç¡€ä¸Šè¿›è¡Œå†æ¬¡å¢å¹¿ï¼Œç›®çš„æ˜¯æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥å…ˆå¯¹è®­ç»ƒé›†çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼Œç„¶åå†ç”¨ç”Ÿæˆçš„æ•°æ®è¿›è¡Œå†æ¬¡å¢å¹¿ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1. æ ¹æ®ç°æœ‰æ•°æ®å¯¹æ•°æ®é›†è¿›è¡Œå¤„ç†ï¼Œå‡†å¤‡è®­ç»ƒé›†æ•°æ®ã€‚
2. åœ¨è®­ç»ƒé›†ä¸­ï¼Œéšæœºé€‰å–éƒ¨åˆ†æ–‡æœ¬è¿›è¡Œæ•°æ®å¢å¹¿ã€‚
3. è®­ç»ƒStudentæ¨¡å‹ã€‚

```python
labeled_texts = [('i am happy positive', 1), ('i am sad negative', 0)]

aug_texts = generator({'text': ['the weather outside is very sunny today.','it\'s raining today.']}).get('generated_sequences').tolist()
aug_labeled_texts = [(txt+' '+aug+'\n'+lbl, int(lbl=='positive'))
                     for txt, lbl in labeled_texts for aug in aug_texts]

all_train_texts = [tup[0].split('\n')[0]+'\n'+tup[0].split('\n')[1]
                   for tup in labeled_texts + aug_labeled_texts]
all_train_labels = [tup[1] for tup in labeled_texts + aug_labeled_texts]
train_encodings = tokenizer(all_train_texts, padding='max_length', truncation=True,
                           return_tensors='pt')['input_ids']
train_labels = torch.LongTensor(all_train_labels)

train_dataset = TensorDataset(train_encodings, train_labels)

training_args = TrainingArguments(
    output_dir="output/",          # output directory
    num_train_epochs=3,             # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    save_steps=1000,                # number of updates steps before saving
    learning_rate=2e-5,            # initial learning rate for AdamW
    warmup_steps=1000,              # number of warmup steps for learning rate scheduler
    weight_decay=0.01               # strength of weight decay
)

trainer = Trainer(
    model=model.student_model,                        # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                             # training arguments, defined above
    train_dataset=train_dataset                     # training dataset
)

trainer.train()
```