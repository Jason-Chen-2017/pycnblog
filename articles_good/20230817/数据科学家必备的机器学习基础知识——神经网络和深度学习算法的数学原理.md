
作者：禅与计算机程序设计艺术                    

# 1.简介
  

数据科学的重要任务之一就是利用数据分析获取有效的信息，而机器学习算法则扮演了至关重要的角色。近年来，随着深度学习的兴起，机器学习领域发生了翻天覆地的变化。从传统的监督学习（Supervised Learning）到无监督学习（Unsupervised Learning），再到自然语言处理（Natural Language Processing），人工智能领域出现了多种形式的新方法论，这些方法论都建立在统计、概率论等基础上，并且应用于解决各种各样的问题。

那么，什么是机器学习呢？机器学习是指让计算机通过学习数据，自动发现数据的结构和模式，并据此进行相应的分析、预测和决策的一类技术。机器学习涉及到三个方面：

· 1.数据：机器学习所使用的训练集、测试集或者生产环境的数据，通常来源于现实世界或经过清洗、转换后的数据。

· 2.模型：机器学习模型根据数据进行训练得到一个参数模型，该参数模型定义了数据的特征表示方式，用于对输入的实例进行预测、分类或回归。常用的模型包括线性回归、逻辑回归、支持向量机、随机森林、决策树、K-means聚类、深度学习网络等。

· 3.算法：机器学习算法是指用来训练模型的具体计算过程，它决定了模型能够在给定输入情况下输出正确的结果。常用的算法有梯度下降法、牛顿法、遗传算法、模拟退火算法、马尔可夫链蒙特卡罗方法、贝叶斯估计、EM算法、PageRank算法等。

对于机器学习的研究和应用来说，重要的是理解其数学原理，了解机器学习的基本流程和工作机制。因此，本文将介绍神经网络和深度学习算法的数学原理，并结合实际例子，阐述一些最常见但又不容易理解的机器学习问题的具体解决方案。

# 2.基本概念术语说明
## 2.1 感知机（Perceptron）
感知机（Perceptron）是最简单的二类分类器，由一个输入的加权求和构成，然后通过一个激活函数（activation function）得到输出，输出为+1或-1。它是一个简单而有效的分类器，但是也存在一些局限性。

假设输入空间X中有一个维度为d的特征向量x，则感知机可以表示为：

$$f(x) = sign(\sum_{i=1}^dx_iw_i + w_0), \quad x\in X,\quad w=(w_1,...,w_d),\quad b\in R$$

其中$sign(z)$函数是符号函数，符号函数的作用是判断一个实数是否大于零，如果大于零则返回1，否则返回-1；$b$为偏置项。

这里，当输入为一个向量时，感知机的输出可以看作是该向量在每个分割面的投影长度（投影向量与输入向量的点积）。感知机的学习策略是选择适当的分割超平面，使得分类正确率最大化。为了实现这一目标，可以使用梯度上升算法，即沿着损失函数的方向更新参数，直到达到预期效果。

如下图所示，在两类数据集上的感知机学习过程：


其中，$\hat{y}$表示感知机的输出，当$|\hat{y}|=0$时，表示两个类的分类不可行，此时无法进行学习；当$|\hat{y}|=\pm1$时，表示分类结果正确，分类完成；当$\hat{y}\leq 0$时，为类1；当$\hat{y}>0$时，为类2。 

## 2.2 异或问题
异或问题是计算机科学中的经典问题。它要求对两个相同长度的字符串进行比较，若其中只有一位不同，则认为这两个字符串是不同的。例如：

$A="10101000", B="10001100" \rightarrow A\oplus B="00100100"$

异或运算满足交换律和分配律，即：

$(A\oplus B)\oplus C=A\oplus (B\oplus C),(A\oplus B)=B\oplus A$

显然，异或运算可用于构造矢量空间中的线性子空间，如二维空间中的直线。异或问题属于判定问题，也可以用其他的方法解决。

## 2.3 模型评估
模型评估是机器学习中重要的一环。在实际应用中，我们需要对模型进行性能的评估，以便确定其是否能够真正地帮助解决实际问题。通常，模型评估主要有以下几种方法：

1. 使用验证集：将数据集划分为训练集和验证集，一般取80%作为训练集，20%作为验证集。将训练集用于模型的训练，验证集用于模型的选择。

2. 交叉验证法（cross validation）：将数据集按折的方式切分成k个子集，分别训练k次模型，每次测试集为一个子集，验证集为剩余的k-1个子集的均值。

3. 留出法（holdout method）：也称留一法，将数据集划分为训练集和测试集，测试集比例较小，占总体数据的约20%左右。在训练过程中，不断调整模型的参数以获得更好的性能。

4. 准确率（accuracy）：将预测正确的样本数除以样本总数。

5. 精确率（precision）：TP/(TP+FP)，表示模型识别出所有正例的比例。

6. 召回率（recall）：TP/(TP+FN)，表示模型覆盖所有正例的比例。

7. F1值：F1值为精确率和召回率的调和平均数，值越接近1，模型效果越好。

8. 损失函数：用来衡量模型在当前参数下的表现。

以上几种方法仅是模型评估的方法，还有很多其它的方法，比如改进后的卡方检验等。

## 2.4 反向传播算法
反向传播算法（back propagation algorithm）是目前应用最广泛的深度学习算法之一。它基于一种称为BP（误差反向传递）规则的迭代式学习规则。该规则计算输出层误差时，不仅考虑输出层前一层的误差，还要考虑隐藏层的误差。这是因为隐藏层的权重和偏置可以参与计算输出层误差的影响，因此可以提供更优秀的模型。

在反向传播算法中，首先计算输出层的误差，然后从最后一层开始，逐层计算隐藏层的误差，并通过梯度下降法更新权重和偏置。具体的计算过程如下：

1. 初始化模型参数
2. 对输入的样本x进行预测
3. 通过forward pass计算输出y
4. 根据输出y和标签t计算输出层的误差δo
5. 从后往前计算隐含层的误差δk，即$\frac{\partial E}{\partial z^{(l)}} = (\delta^{l})^\top a^{(l-1)}$
6. 更新权重和偏置，即$\Delta w^{(l)} = \alpha (y - t)a^{(l-1)}^\top \cdot \sigma'(z^{(l)})\quad \Delta b^{(l)} = \alpha (y - t)\sigma'(z^{(l)})$，其中α是步长，σ'是激励函数的导数。
7. 返回第二步，重复执行4-6步，直到收敛

在实际使用过程中，采用min-batch的形式将数据集分割为多个小批量，每次更新梯度时只用当前批次数据。这样可以有效减少计算量和内存消耗。

# 3.核心算法原理和具体操作步骤以及数学公式讲解
## 3.1 感知机算法
感知机算法是最初用于二类分类的线性分类器。它是单层神经网络的一种简单且易于实现的变体。该算法包括一个输入层、一个输出层和一个隐藏层，其形式如上图所示。感知机的训练方式非常直接，每一次训练样本都被送入网络中，然后通过学习规则更新权重和偏置。其训练过程可以用以下几个步骤来描述：

1. 初始化权重w（使用任意方法，如0、标准差等）
2. 在训练数据集中输入训练数据x，对每一组训练数据进行以下操作：
   a) 对输入数据x进行处理，加上偏置项
   b) 将加偏置的数据乘以权重，得到隐含层的激活值φ（记为a）
   c) 计算输出层的输出，y（记为o），它等于φ的符号函数的值
3. 判断输出y与实际的标签t之间的关系，如果它们一致，则继续进行训练；否则，利用学习规则更新权重和偏置。
4. 如果所有的训练数据都已处理完毕，则结束训练。

感知机学习规则如下：

$$w_i:=w_i+\eta(t_i-\overline{y}_i)\phi_j, \quad j=1,2,\cdots,n, i=0,1,\cdots,m-1$$

其中，$\eta$是学习速率，$t_i$代表第i个训练样本的真实标记，$y_i$代表第i个训练样本的输出，$\overline{y}_i$代表整个训练数据集的输出平均值，$\phi_j$代表输入向量的第j个元素。当$t_i=y_i$时，更新权重；否则，没有更新。当训练数据集线性可分时，即存在一个超平面可以将数据集分割为两类时，感知机学习规则保证永远不会陷入局部最小值。

## 3.2 单隐层的神经网络算法
单隐层的神经网络（Neural Network with One Hidden Layer）是一种简单而有力的模型，具有很高的适应能力。它的输入层与输出层之间只有一个隐含层，隐藏层的节点个数可以选择。隐藏层的每一个节点都对应着输入层的一个元素，但是它们之间彼此间没有联系。这种结构下，网络可以模拟复杂的非线性函数，而且可以学习到复杂的特征，从而提升模型的能力。

1. 初始化权重w，首先随机初始化权重，再用训练数据对权重进行修正。权重可以初始化为任意值，如0、标准差等。
2. 对每个输入样本x进行处理，用激活函数处理隐含层的输出值φ。
3. 对每一个样本，计算激活值，并用激活函数处理，得到最终的输出值y。
4. 用实际的标签t和预测出的标签y之间的差距作为损失函数，来计算梯度，梯度越小，代表网络越拟合。
5. 根据梯度下降法更新权重，更新公式为：

   $$\theta:= \theta-\eta \nabla L(\theta), \quad \theta: w, b; x^i;\quad y^i$$

   $\eta$是学习速率，L($\theta$)代表损失函数，$\theta$代表网络参数，x^i是第i个训练样本的输入，y^i是第i个样本的标签。

6. 重复第4步和第5步，直到训练误差最小或者指定次数停止。

## 3.3 多隐层的神经网络算法
多隐层的神经网络（Neural Network with Multiple Hidden Layers）是一种更为复杂的模型，可以模拟复杂的非线性函数。它的输入层可以是多个变量，也可以包括隐藏层。与单隐层不同的是，多隐层的网络可以包含更多的隐藏层，每层可以有不同的节点数目，甚至可以在同一层使用不同的激活函数。多层的组合可以生成更复杂的特征，从而提升模型的能力。

与单隐层不同的是，多隐层网络会有多个参数，所以需要通过正则化方法来防止过拟合。正则化方法包括损失函数的正则化，权重衰减，添加噪声等。

1. 初始化权重w，首先随机初始化权重，再用训练数据对权重进行修正。权重可以初始化为任意值，如0、标准差等。
2. 对每个输入样本x进行处理，先经过输入层，再经过隐含层，再经过输出层，得到输出值y。
3. 用实际的标签t和预测出的标签y之间的差距作为损失函数，来计算梯度，梯度越小，代表网络越拟合。
4. 根据梯度下降法更新权重，更新公式为：

   $$\theta := \theta-\eta \nabla_{\theta} L(\theta), \quad \theta: W, b; x^i;\quad y^i$$

    $W$表示网络的权重，$\eta$是学习速率，L($\theta$)代表损失函数，x^i是第i个训练样本的输入，y^i是第i个样本的标签。

5. 重复第4步，直到训练误差最小或者指定次数停止。
6. 添加正则化项来防止过拟合，如L1范数、L2范数、dropout方法等。
   
## 3.4 BP算法的数学推导
BP算法是神经网络的一种训练算法，相比其他算法更有名气。这个算法背后的理论基础是误差反向传播。该算法在单隐层网络中可以证明是收敛的，但是在多隐层网络中，由于梯度爆炸和梯度消失的问题，导致收敛困难。为了缓解这些问题，BP算法引入了随机梯度下降算法，其基本思想是用一个随机向量来迭代计算梯度，而不是用真实的梯度来迭代计算梯度。通过引入噪声，使得更新的方向不固定，避免了梯度爆炸和梯度消失的问题。

假设网络的结构如下图所示：


BP算法的计算步骤如下：

1. 初始化权重w（使用任意方法，如0、标准差等）
2. 对每一个输入样本x，进行forward pass（前向传播），计算隐含层的激活值φ，计算输出层的输出o。
3. 计算损失函数的导数，使用BP算法的规则计算梯度。
4. 对每一个权重w，更新规则为：

   $$w:=w+\Delta w,\quad \Delta w=-\eta\nabla_\theta J(w)+\lambda\omega$$

   其中，$\eta$是学习速率，J(w)代表损失函数，\nabla_\theta J(w)代表网络参数w关于损失函数J的导数。

5. 返回第三步，重复执行，直到收敛。

## 3.5 卷积神经网络算法
卷积神经网络（Convolutional Neural Networks，CNN）是深度学习中另一种重要的模型，可以有效地处理图像、视频、语音等序列数据的特征学习。它的主要特点是在卷积层中加入池化层，可以有效地降低模型的复杂度和参数量。CNN模型的特别之处在于它可以识别多尺寸的对象，在图像中，它可以识别多个目标。

CNN算法的计算步骤如下：

1. 对每一张图片进行处理，将像素映射到各个通道，卷积核对每个通道进行滑动，产生feature map。
2. 对每个feature map，进行池化层，将特征矩阵压缩到固定大小，丢弃冗余信息。
3. 对每一个池化层的输出，进行全连接层，计算输出值。
4. 用实际的标签t和预测出的标签y之间的差距作为损失函数，来计算梯度，梯度越小，代表网络越拟合。
5. 根据梯度下降法更新权重，更新公式为：

   $$\theta := \theta-\eta \nabla_{\theta} L(\theta), \quad \theta: W, b; x^i;\quad y^i$$

    $W$表示网络的权重，$\eta$是学习速率，L($\theta$)代表损失函数，x^i是第i个训练样本的输入，y^i是第i个样本的标签。

6. 重复第4步，直到训练误差最小或者指定次数停止。

## 3.6 深度信念网络算法
深度信念网络（Deep Belief Networks，DBN）是深度学习中一类有代表性的模型。它使用了一种称为提前训练（pre-training）的手段，该手段通过无监督学习来学习特征的表示方式，然后运用学习到的特征来进行有监督学习。这种方法有效地避免了传统监督学习中的标注数据带来的偏差。

DBN算法的计算步骤如下：

1. 对训练数据集T进行预训练，其中T={x^(1)},{x^(2)},...,{x^(m)}，其中每个样本都是包含n个特征的向量x^(i)。
2. 对每个样本xi，进行降维操作，将其映射到新的高维空间。
3. 在新空间中，对训练数据集T进行监督学习。
4. 为每个输入样本x^(i)，计算其相应的输出值y^(i)。

# 4.具体代码实例和解释说明
## 4.1 感知机算法的Python代码实现

```python
import numpy as np


class Perceptron:
    
    def __init__(self):
        self._weights = None
        
    @property
    def weights(self):
        return self._weights
    
    def fit(self, data, labels):
        
        n_samples, n_features = data.shape
        if not isinstance(labels, np.ndarray):
            labels = np.array(labels).reshape(-1,)
            
        # Initialize weights to zeros
        self._weights = np.zeros(n_features + 1)
        
        for _ in range(n_samples):
            
            sample = data[_, :].tolist()
            label = labels[_]
            
            prediction = np.dot(sample, self._weights[:-1]) + self._weights[-1]
            
            error = label - int(np.where(prediction >= 0)[0][0])
            
            update = [error * xi for xi in sample] + [-error]

            self._weights += update
            
    def predict(self, x):
        
        if len(x.shape) == 1:
            x = x.reshape(1, -1)
        
        predictions = []
        
        for sample in x:
            activation = np.dot(sample, self._weights[:-1]) + self._weights[-1]
            pred = np.argmax(activation)
            predictions.append(pred)
            
        return np.array(predictions)
```

这个代码实现了一个Perceptron类，该类提供了fit和predict方法，分别用来训练模型和进行预测。

fit方法接受两个参数，data和labels。data是输入的数据，labels是输入数据的标签。该方法使用梯度上升法来训练模型，通过调整权重来改变预测值与实际值的距离。

predict方法接受一个输入数据x，返回预测的标签。如果输入数据不是1维数组，就把它转为1维数组。该方法对每个输入数据进行预测，得到其对应的输出值，然后取最大值作为预测的标签。

## 4.2 单隐层的神经网络算法的Python代码实现

```python
import numpy as np


class NeuralNetwork:
    
    def __init__(self, layers=[1, 1], lr=0.1):
        self.layers = layers
        self.lr = lr
        self.params = {}
        
    def initialize_parameters(self):

        np.random.seed(1)
        self.params['W1'] = np.random.randn(self.layers[1], self.layers[0]) / np.sqrt(self.layers[0])
        self.params['b1'] = np.zeros((self.layers[1], 1))
        self.params['W2'] = np.random.randn(self.layers[1], self.layers[1]) / np.sqrt(self.layers[1])
        self.params['b2'] = np.zeros((self.layers[1], 1))
                
    def sigmoid(self, Z):
        A = 1 / (1 + np.exp(-Z))
        cache = Z
        return A, cache
    
    def relu(self, Z):
        A = np.maximum(0, Z)
        cache = Z
        return A, cache
    
    def linear(self, A):
        return A, A
    
    def forward_propagation(self, X, params):
        
        caches = []
        A = X
        L = len(params)//2
        
        # Input layer to hidden layer 1
        Z1 = np.dot(params['W1'], A) + params['b1']
        A1, cache1 = self.relu(Z1)
        caches.append(cache1)
        
        # Hidden layer 1 to output layer
        Z2 = np.dot(params['W2'], A1) + params['b2']
        A2, cache2 = self.sigmoid(Z2)
        caches.append(cache2)
        
        assert(A2.shape == (1,X.shape[1]))
        
        return A2, caches
    
    def backward_propagation(self, AL, Y, caches):
        
        grads = {}
        L = len(caches) // 2
        m = AL.shape[1]
        
        # Output layer gradients
        dAL2 = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
        dZ2 = dAL2 * self.sigmoid(caches[L-1])[0]*(1-self.sigmoid(caches[L-1])[0])
        grads['dW2'] = (1./m)*np.dot(dZ2, caches[L-2].T)
        grads['db2'] = (1./m)*(np.sum(dZ2, axis=1, keepdims=True))
        
        # Hidden layer 1 gradients
        dA1 = np.dot(grads['dW2'], caches[L-2].T)
        dZ1 = np.multiply(dA1, np.int64(caches[L-1] > 0))
        grads['dW1'] = (1./m)*np.dot(dZ1, caches[L-3].T)
        grads['db1'] = (1./m)*(np.sum(dZ1, axis=1, keepdims=True))
        
        return grads
    
    def compute_cost(self, Y, AL, params):
        
        m = Y.shape[1]
        
        logprobs = np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y)
        cost = (-1./m)*np.nansum(logprobs)
        
        # Add regularization term to cost
        L2_term = (self.lambd/(2.*m))*np.sum([np.linalg.norm(value)**2 for key, value in sorted(params.items()) if key[0]=='W'])
        cost += L2_term
        
        return cost
    
    def train(self, X, Y, num_iterations=10000, lambd=0):
        
        costs = []
        self.initialize_parameters()
        self.lambd = lambd
        
        for i in range(num_iterations):
            
            # Forward propagation
            AL, caches = self.forward_propagation(X, self.params)
            
            # Compute cost and perform backpropagation
            cost = self.compute_cost(Y, AL, self.params)
            grads = self.backward_propagation(AL, Y, caches)
            
            # Update parameters
            self.params['W1'] -= self.lr*grads['dW1']
            self.params['b1'] -= self.lr*grads['db1']
            self.params['W2'] -= self.lr*grads['dW2']
            self.params['b2'] -= self.lr*grads['db2']
            
            # Print the cost every 1000 iterations
            if i % 1000 == 0:
                print("Cost after iteration %i: %f" %(i, cost))
                costs.append(cost)
                    
            if cost < 0.001:
                break
                
        return costs
```

这个代码实现了一个NeuralNetwork类，该类提供了train方法，用来训练模型。

train方法接受三个参数，X是输入的数据，Y是输入数据的标签，num_iterations是迭代次数，lambd是正则化系数。该方法使用随机梯度下降法来训练模型，通过调整权重来改变预测值与实际值的距离。

## 4.3 多隐层的神经网络算法的Python代码实现

```python
import numpy as np


class DeepNN:
    
    def __init__(self, input_dim, hidden_dim, num_classes, learning_rate, reg_lambda):
        self.input_dim = input_dim    # number of features
        self.hidden_dim = hidden_dim  # number of nodes in the hidden layer
        self.num_classes = num_classes        # number of classes we are predicting
        self.learning_rate = learning_rate   # learning rate for gradient descent 
        self.reg_lambda = reg_lambda         # lambda used for L2 regularization
        
        
    def init_parameters(self):
        """ Initializes weight matrices and bias vectors randomly."""
        
        np.random.seed(1)
        self.params = {}
        self.params['W1'] = np.random.randn(self.input_dim, self.hidden_dim)/np.sqrt(self.input_dim)
        self.params['b1'] = np.zeros((self.hidden_dim,1))
        self.params['W2'] = np.random.randn(self.hidden_dim, self.num_classes)/np.sqrt(self.hidden_dim)
        self.params['b2'] = np.zeros((self.num_classes,1))

        
    def ReLU(self, z):
        return np.maximum(0., z)
    
    
    def softmax(self, z):
        exp_scores = np.exp(z)
        probs = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)
        return probs
    
    
    def sigmoid(self, z):
        return 1. / (1. + np.exp(-z))
    
    
    def cross_entropy(self, logits, labels):
        m = labels.shape[0]
        loss = -1/m * np.sum(np.multiply(labels, np.log(logits)))
        return loss
    
    
    def l2_regularization(self, params):
        """Computes the L2 norm of all the weight matrices"""
        
        reg_loss = 0
        for _, W in sorted(params.items()):
            reg_loss += 0.5*(self.reg_lambda*np.sum(W**2))
        
        return reg_loss

    
    def propagate(self, inputs, is_training=True):
        """Forward propagates through our network"""
        
        W1, b1 = self.params['W1'], self.params['b1']
        W2, b2 = self.params['W2'], self.params['b2']
        
        # Linear transformations on inputs
        z1 = np.dot(inputs, W1) + b1
        a1 = self.ReLU(z1)
        z2 = np.dot(a1, W2) + b2
        logits = self.softmax(z2)
        
        if not is_training:
            return logits
        
        # Compute cost function
        cost = self.cross_entropy(logits, self.y_)

        # Regularize cost function
        reg_loss = self.l2_regularization(self.params)
        cost += reg_loss
        
        # Backward propagate derivatives
        dz2 = logits - self.y_
        dw2 = np.dot(a1.T, dz2)
        db2 = np.sum(dz2, axis=1, keepdims=True)
        da1 = np.dot(dz2, W2.T)
        dz1 = da1 * self.ReLU(z1, deriv=True)
        dw1 = np.dot(inputs.T, dz1)
        db1 = np.sum(dz1, axis=1, keepdims=True)
        
        gradients = {"dw1": dw1, "db1": db1,"dw2": dw2,"db2": db2}
        
        return cost, gradients
    
    
    def optimize(self, cost, gradients, epsilon):
        """Updates the model's parameters using Gradient Descent"""
        
        self.params["W1"] -= epsilon * gradients["dw1"]
        self.params["b1"] -= epsilon * gradients["db1"]
        self.params["W2"] -= epsilon * gradients["dw2"]
        self.params["b2"] -= epsilon * gradients["db2"]
        
        return
    
    
    def accuracy(self, y_pred, y_true):
        """Returns the accuracy of the model given predicted and true labels"""
        
        correct_preds = np.equal(np.argmax(y_pred,axis=0), np.argmax(y_true,axis=0)).sum()
        acc = float(correct_preds)/y_pred.shape[1]
        
        return acc
    
    
    def fit(self, X, y, batch_size=100, epochs=1000, epsilon=0.1):
        """Trains the neural net for specified number of epochs."""
        
        self.X_ = X
        self.y_ = onehot(y, self.num_classes)
        
        num_batches = int(len(X)/batch_size)
        
        for epoch in range(epochs):
            avg_cost = 0
            total_acc = 0
            
            for i in range(num_batches):
                start = i * batch_size
                end = min((i + 1) * batch_size, len(X))

                # Evaluate mini-batch
                feed_dict = {self.inputs_: X[start:end,:]}
                cost_val, gradients, _ = sess.run([self.cost, self.gradients, self.optimizer], 
                                                    feed_dict=feed_dict)
                
                avg_cost += cost_val/num_batches
                
                # Optimize model
                self.optimize(avg_cost, gradients, epsilon)
            
            if epoch % 10 == 0 or epoch==epochs-1:
                print("Epoch:", '%04d' % (epoch+1),
                      "cost=", "{:.9f}".format(avg_cost),
                      "train_accuracy=", "{:.5f}%".format(100*self.accuracy(self.predict(), self.y_)))
        return
    
    
    def predict(self):
        """Runs inference on test set and returns predicted class probabilities"""
        
        feed_dict = {self.inputs_: self.X_}
        y_pred, y_prob = sess.run([self.output_layer, self.probs], feed_dict=feed_dict)
        
        return y_prob


def onehot(y, K):
    """Converts integer encoded labels into one hot encoding"""
    y_onehot = np.eye(K)[y]
    return y_onehot