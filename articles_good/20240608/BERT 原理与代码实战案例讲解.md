# BERT åŸç†ä¸ä»£ç å®æˆ˜æ¡ˆä¾‹è®²è§£

## 1. èƒŒæ™¯ä»‹ç»

### 1.1 è‡ªç„¶è¯­è¨€å¤„ç†çš„å‘å±•å†ç¨‹

è‡ªç„¶è¯­è¨€å¤„ç†(Natural Language Processing, NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯,æ—¨åœ¨è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€å¤„ç†å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPæŠ€æœ¯çš„å‘å±•ç»å†äº†å‡ ä¸ªé‡è¦é˜¶æ®µ:

- 1950å¹´ä»£çš„æœºå™¨ç¿»è¯‘
- 1960å¹´ä»£çš„è‡ªç„¶è¯­è¨€ç†è§£
- 1970-1980å¹´ä»£çš„åŸºäºè§„åˆ™çš„æ–¹æ³•  
- 1990å¹´ä»£çš„ç»Ÿè®¡å­¦ä¹ æ–¹æ³•
- 2000å¹´ä»£çš„æ·±åº¦å­¦ä¹ æ–¹æ³•

### 1.2 æ·±åº¦å­¦ä¹ æ—¶ä»£çš„ NLP

2013å¹´,word2vec çš„æå‡ºæ€èµ·äº† NLP é¢†åŸŸçš„æ·±åº¦å­¦ä¹ é©å‘½ã€‚æ­¤å,CNNã€RNNã€LSTM ç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹è¢«å¹¿æ³›åº”ç”¨äº NLP ä»»åŠ¡,æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ã€‚ä½†è¿™äº›æ¨¡å‹éƒ½å­˜åœ¨ä¸€äº›å±€é™æ€§,å¦‚:

- éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–
- ç¼ºä¹å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£
- æ— æ³•è¿›è¡ŒåŒå‘å»ºæ¨¡
- éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®

### 1.3 Transformer çš„å‡ºç°

2017å¹´,Google æå‡ºäº† Transformer æ¨¡å‹[1],å®ƒåŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶,å¯ä»¥é«˜æ•ˆåœ°è¿›è¡Œå¹¶è¡Œè®¡ç®—,å…‹æœäº† RNN ç­‰æ¨¡å‹çš„ç¼ºé™·ã€‚Transformer å¾ˆå¿«æˆä¸ºäº† NLP é¢†åŸŸçš„ä¸»æµæ¨¡å‹ã€‚

### 1.4 BERT çš„è¯ç”Ÿ

2018å¹´,Google åœ¨ Transformer çš„åŸºç¡€ä¸Šæå‡ºäº† BERT(Bidirectional Encoder Representations from Transformers)æ¨¡å‹[2],è¿›ä¸€æ­¥å°† NLP æ¨å‘æ–°çš„é«˜åº¦ã€‚BERT é€šè¿‡åŒå‘è®­ç»ƒå’Œæ›´å¤§è§„æ¨¡çš„æ•°æ®,å­¦ä¹ åˆ°äº†æ›´åŠ å¼ºå¤§çš„è¯­è¨€è¡¨ç¤ºã€‚

## 2. æ ¸å¿ƒæ¦‚å¿µä¸è”ç³»

### 2.1 Transformer 

- Encoder-Decoder ç»“æ„
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
- ä½ç½®ç¼–ç 
- æ®‹å·®è¿æ¥ä¸ LayerNorm

### 2.2 é¢„è®­ç»ƒ

- æ— ç›‘ç£å­¦ä¹ 
- æµ·é‡æ— æ ‡æ³¨è¯­æ–™
- Masked Language Model(MLM)
- Next Sentence Prediction(NSP)

### 2.3 å¾®è°ƒ

- è¿ç§»å­¦ä¹ 
- ä¸‹æ¸¸ä»»åŠ¡
- å°‘é‡æ ‡æ³¨æ•°æ®
- å‚æ•°åˆå§‹åŒ–

### 2.4 Transformer ä¸ BERT çš„å…³ç³»

```mermaid
graph LR
Transformer --> BERT
```

BERT æ˜¯åŸºäº Transformer ä¸­çš„ Encoder ç»“æ„,é€šè¿‡é¢„è®­ç»ƒå’Œå¾®è°ƒä¸¤ä¸ªé˜¶æ®µå®ç°çš„è¯­è¨€æ¨¡å‹ã€‚

## 3. æ ¸å¿ƒç®—æ³•åŸç†å…·ä½“æ“ä½œæ­¥éª¤

### 3.1 BERT çš„ç½‘ç»œç»“æ„

BERT çš„ç½‘ç»œç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤º:

```mermaid
graph BT
    subgraph BERT
        Embedding[Input Embedding]
        Trm_1[Transformer Encoder 1] 
        Trm_2[Transformer Encoder 2]
        Trm_n[Transformer Encoder N]
        
        Embedding --> Trm_1
        Trm_1 --> Trm_2
        Trm_2 --> Trm_n
    end
```

BERT ç”±å¤šå±‚ Transformer Encoder å †å è€Œæˆ,æ¯ä¸€å±‚éƒ½åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚è¾“å…¥é¦–å…ˆç»è¿‡ Embedding å±‚,ç„¶åä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚ Transformer Encoder,æœ€åè¾“å‡ºæ¯ä¸ªä½ç½®çš„éšè—çŠ¶æ€ã€‚

### 3.2 è¾“å…¥è¡¨ç¤º

BERT çš„è¾“å…¥ç”±ä¸‰éƒ¨åˆ†ç»„æˆ:Token Embeddingsã€Segment Embeddings å’Œ Position Embeddingsã€‚

- Token Embeddings:å°†æ¯ä¸ªå•è¯è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å‘é‡ã€‚
- Segment Embeddings:ç”¨äºåŒºåˆ†ä¸¤ä¸ªå¥å­,ç¬¬ä¸€ä¸ªå¥å­ä¸º0,ç¬¬äºŒä¸ªå¥å­ä¸º1ã€‚
- Position Embeddings:è¡¨ç¤ºæ¯ä¸ªå•è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚

ä¸‰ç§ Embedding ç›¸åŠ åä½œä¸º BERT çš„è¾“å…¥ã€‚

### 3.3 é¢„è®­ç»ƒä»»åŠ¡

#### 3.3.1 Masked Language Model(MLM)

MLM çš„ç›®æ ‡æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹è¢«é®ç›–(mask)çš„å•è¯ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹:

1. éšæœºé®ç›–15%çš„å•è¯,æ›¿æ¢ä¸º [MASK] æ ‡è®°ã€‚ 
2. å°†é®ç›–åçš„åºåˆ—è¾“å…¥ BERTã€‚
3. é¢„æµ‹ [MASK] ä½ç½®çš„å•è¯ã€‚

MLM ä½¿ BERT èƒ½å¤Ÿå­¦ä¹ åˆ°åŒå‘çš„è¯­è¨€è¡¨ç¤ºã€‚

#### 3.3.2 Next Sentence Prediction(NSP)  

NSP çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸é‚»ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹:

1. éšæœºé€‰æ‹©è¯­æ–™åº“ä¸­çš„ä¸¤ä¸ªå¥å­ A å’Œ Bã€‚
2. 50%çš„æ¦‚ç‡ä¿æŒ A å’Œ B çš„ç›¸é‚»å…³ç³»,50%çš„æ¦‚ç‡éšæœºæ›¿æ¢ Bã€‚
3. å°†ä¸¤ä¸ªå¥å­æ‹¼æ¥åè¾“å…¥ BERTã€‚
4. é¢„æµ‹ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸é‚»ã€‚

NSP ä½¿ BERT èƒ½å¤Ÿå­¦ä¹ åˆ°å¥å­é—´çš„å…³ç³»ã€‚

### 3.4 å¾®è°ƒ

å°†é¢„è®­ç»ƒå¥½çš„ BERT æ¨¡å‹åº”ç”¨åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶,åªéœ€åœ¨é¡¶å±‚æ·»åŠ ä¸€ä¸ªä¸ä»»åŠ¡ç›¸å…³çš„è¾“å‡ºå±‚,ç„¶ååœ¨å°‘é‡æ ‡æ³¨æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒå³å¯ã€‚å¾®è°ƒæ—¶ä¸€èˆ¬ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡,ä»¥é˜²æ­¢é¢„è®­ç»ƒçš„å‚æ•°è¢«ç ´åã€‚

## 4. æ•°å­¦æ¨¡å‹å’Œå…¬å¼è¯¦ç»†è®²è§£ä¸¾ä¾‹è¯´æ˜

### 4.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶

è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒ,å®ƒå¯ä»¥è®¡ç®—åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„å…³è”åº¦ã€‚å‡è®¾è¾“å…¥åºåˆ—ä¸º $X \in \mathbb{R}^{n \times d}$,è‡ªæ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹:

$$
\begin{aligned}
Q &= XW^Q \\
K &= XW^K \\
V &= XW^V \\
Attention(Q,K,V) &= softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{aligned}
$$

å…¶ä¸­,$Q$,$K$,$V$ åˆ†åˆ«ä¸ºæŸ¥è¯¢çŸ©é˜µã€é”®çŸ©é˜µå’Œå€¼çŸ©é˜µ,$W^Q$,$W^K$,$W^V$ ä¸ºå¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µã€‚$Attention(Q,K,V)$ å³ä¸ºæ³¨æ„åŠ›çŸ©é˜µã€‚

ä¾‹å¦‚,å‡è®¾è¾“å…¥åºåˆ—ä¸º ["I","love","NLP"],å¯¹åº”çš„ Token Embeddings ä¸º:

$$
X=
\begin{bmatrix} 
0.1 & 0.2 & 0.3\\
0.4 & 0.5 & 0.6\\ 
0.7 & 0.8 & 0.9
\end{bmatrix}
$$

å‡è®¾ $W^Q$,$W^K$,$W^V$ éƒ½ä¸ºå•ä½çŸ©é˜µ,åˆ™:

$$
\begin{aligned}
Q=K=V &=
\begin{bmatrix}
0.1 & 0.2 & 0.3\\  
0.4 & 0.5 & 0.6\\
0.7 & 0.8 & 0.9
\end{bmatrix}\\
QK^T &=
\begin{bmatrix}
0.14 & 0.32 & 0.50\\ 
0.32 & 0.77 & 1.22\\
0.50 & 1.22 & 1.94
\end{bmatrix}\\
Attention &= softmax(\frac{QK^T}{\sqrt{3}})V
\end{aligned}
$$

æœ€ç»ˆå¾—åˆ°çš„ $Attention$ çŸ©é˜µå°±è¡¨ç¤ºäº†åºåˆ—ä¸­æ¯ä¸ªä½ç½®ä¸å…¶ä»–ä½ç½®çš„å…³è”åº¦ã€‚

### 4.2 BERT çš„æŸå¤±å‡½æ•°

BERT çš„é¢„è®­ç»ƒæŸå¤±å‡½æ•°ç”± MLM å’Œ NSP ä¸¤éƒ¨åˆ†ç»„æˆ:

$$
\mathcal{L} = \mathcal{L}_{MLM} + \mathcal{L}_{NSP}
$$

å…¶ä¸­,$\mathcal{L}_{MLM}$ ä¸ºé®ç›–è¯­è¨€æ¨¡å‹çš„æŸå¤±:

$$
\mathcal{L}_{MLM} = -\sum_{i=1}^{n}m_i\log p(w_i|w_{\backslash i})
$$

$n$ ä¸ºåºåˆ—é•¿åº¦,$m_i$ ä¸ºé®ç›–æ ‡è®°,$w_i$ ä¸ºç¬¬ $i$ ä¸ªä½ç½®çš„å•è¯,$p(w_i|w_{\backslash i})$ ä¸ºæ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹ $w_i$ çš„æ¦‚ç‡ã€‚

$\mathcal{L}_{NSP}$ ä¸ºä¸‹ä¸€å¥é¢„æµ‹çš„æŸå¤±:

$$
\mathcal{L}_{NSP} = -\log p(y|\text{[CLS]})
$$

$y \in \{0,1\}$ è¡¨ç¤ºä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸é‚»,$\text{[CLS]}$ ä¸ºå¥å­å¼€å¤´çš„ç‰¹æ®Šæ ‡è®°ã€‚

## 5. é¡¹ç›®å®è·µ:ä»£ç å®ä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜

ä¸‹é¢æ˜¯ä½¿ç”¨ PyTorch å®ç° BERT çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹:

```python
import torch
import torch.nn as nn

class BertEmbedding(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_len, dropout):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(max_len, hidden_size)
        self.segment_embedding = nn.Embedding(2, hidden_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, seg):
        seq_len = x.size(1)
        pos = torch.arange(seq_len, dtype=torch.long).to(x.device)
        pos = pos.unsqueeze(0).expand_as(x)
        
        e = self.token_embedding(x) + self.position_embedding(pos) + self.segment_embedding(seg)
        return self.dropout(e)
    
class BertLayer(nn.Module):
    def __init__(self, hidden_size, num_heads, ff_size, dropout):
        super().__init__()
        self.attention = nn.MultiheadAttention(hidden_size, num_heads, dropout)
        self.ff = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.ReLU(),
            nn.Linear(ff_size, hidden_size)
        )
        self.norm1 = nn.LayerNorm(hidden_size)
        self.norm2 = nn.LayerNorm(hidden_size)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, x, mask):
        a, _ = self.attention(x, x, x, attn_mask=mask)
        x = x + self.dropout1(a)
        x = self.norm1(x)
        
        f = self.ff(x)
        x = x + self.dropout2(f)
        x = self.norm2(x)
        return x

class Bert(nn.Module):
    def __init__(self, vocab_size, hidden_size, max_len, num_layers, num_heads, ff_size, dropout):
        super().__init__()
        self.embedding = BertEmbedding(vocab_size, hidden_size, max_len, dropout)
        self.layers = nn.ModuleList([BertLayer(hidden_size, num_heads, ff_size, dropout) for _ in range(num_layers)])
        
    def forward(self, x, seg, mask):
        x = self.embedding(x, seg)
        for layer in self.layers:
            x = layer(x, mask)
        return x
```

ä»£ç è§£é‡Š:

- `BertEmbedding` å®ç°äº† BERT çš„è¾“å…¥è¡¨ç¤º,åŒ…æ‹¬ Token Embeddingsã€Position Embeddings å’Œ Segment Embeddingsã€‚
- `BertLayer` å®ç°äº† Transformer Encoder çš„ä¸€ä¸ªå­å±‚,åŒ…æ‹¬å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚
- `Bert` å®ç°äº†å®Œæ•´çš„ BERT æ¨¡å‹,ç”± Embedding å±‚å’Œå¤šä¸ª BertLayer ç»„æˆã€‚
- å‰å‘ä¼ æ’­æ—¶,è¾“å…¥çš„ `x` ä¸º token çš„ç´¢å¼•åºåˆ—,`seg` ä¸º segment çš„ç´¢å¼•åºåˆ—,`mask` ä¸ºæ³¨æ„åŠ›æ©ç ã€‚

åœ¨å®é™…åº”ç”¨ä¸­,è¿˜éœ€è¦åœ¨ BERT çš„è¾“å‡ºä¸Šæ·»åŠ ç‰¹å®šçš„ä»»åŠ¡å±‚,å¹¶åŠ è½½é¢„è®­ç»ƒçš„å‚æ•°è¿›è¡Œå¾®è°ƒã€‚

## 6. å®é™…åº”ç”¨åœºæ™¯

BERT å¯ä»¥åº”ç”¨äºå‡ ä¹æ‰€æœ‰çš„ NLP ä»»åŠ¡,ä¸‹é¢åˆ—ä¸¾å‡ ä¸ªå¸¸è§çš„åº”ç”¨åœºæ™¯:

### 6.1 æ–‡æœ¬åˆ†ç±»

å°† BERT çš„è¾“å‡ºæ¥ä¸€ä¸ªåˆ†ç±»å™¨,å¯ä»¥ç”¨äºæƒ…æ„Ÿåˆ†æã€æ–°é—»åˆ†ç±»ã€æ„å›¾è¯†åˆ«ç­‰ä»»åŠ¡ã€‚

### 6.2 å‘½åå®ä½“è¯†åˆ«

å°† BERT çš„è¾“å‡ºæ¥ä¸€ä¸ª CRF å±‚,å¯ä»¥ç”¨äºè¯†åˆ«æ–‡æœ¬ä¸­çš„äººåã€åœ°åã€æœºæ„åç­‰å®ä½“ã€‚

### 6.3 é—®ç­”ç³»ç»Ÿ

å°†é—®é¢˜å’Œæ–‡ç« æ‹¼æ¥åè¾“å…¥ BERT,ç„¶ååœ¨ BERT çš„è¾“å‡ºä¸Šé¢„æµ‹ç­”æ¡ˆçš„èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®,å¯ä»¥å®ç°ä¸€ä¸ªç®€å•çš„é˜…è¯»ç†è§£å¼é—®ç­”ç³»ç»Ÿã€‚

### 6.4 æ–‡æœ¬ç›¸ä¼¼åº¦

å°†ä¸¤ä¸ªæ–‡æœ¬åˆ†åˆ«è¾“å…¥ BERT,ç„¶åè®¡ç®—å®ƒä»¬çš„è¾“å‡ºå‘é‡çš„ç›¸ä¼¼åº¦,å¯ä»¥ç”¨äºè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ã€é‡å¤æ–‡æœ¬æ£€æµ‹ç­‰ä»»åŠ¡ã€‚

### 6.5 æœºå™¨ç¿»è¯‘

å°† BERT ä½œä¸ºç¼–ç å™¨,å†åŠ ä¸€ä¸ª Transformer è§£ç å™¨,å¯ä»¥å®ç°ä¸€ä¸ªåŸºäº BERT çš„ Seq2Seq æœºå™¨ç¿»è¯‘æ¨¡å‹ã€‚

## 7. å·¥å…·å’Œèµ„æºæ¨è

- [transformers](https://github.com/huggingface/transformers):ğŸ¤— Hugging Face å¼€æºçš„ Transformer åº“,æä¾›äº† BERT ç­‰é¢„è®­ç»ƒæ¨¡å‹çš„å®ç°å’Œé¢„è®­ç»ƒå‚æ•°ã€‚
- [bert-as-service](https://github.com/hanxiao/bert-as-service):åŸºäº BERT çš„å¥å‘é‡ç¼–ç æœåŠ¡ã€‚
- [BERT-NER](https://github.com/kamalkraj/BERT-NER):åŸºäº BERT çš„å‘½åå®ä½“è¯†åˆ«ã€‚
- [keras-bert](https://github.com/CyberZHG/keras-bert):åŸºäº Keras çš„ BERT å®ç°ã€‚

## 8. æ€»ç»“:æœªæ¥å‘å±•è¶‹åŠ¿ä¸æŒ‘æˆ˜

BERT çš„æˆåŠŸå¼€å¯äº† NLP é¢†åŸŸçš„é¢„è®­ç»ƒæ—¶