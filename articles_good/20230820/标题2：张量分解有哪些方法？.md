
作者：禅与计算机程序设计艺术                    

# 1.简介
  

张量分解（Tensor Decomposition）是指将一个大的矩阵分解成两个或多个较小的子矩阵，并且在保持原矩阵性质的情况下对新矩阵进行有效的运算。张量分解对于很多高维数据分析、机器学习等领域都非常重要。张量分�作为一种有效的降维方法，可以提高运算效率并简化模型建立过程，所以在该领域的研究工作越来越多。

张量分解方法一般可分为全局核张量分解（Global Tensor Decomposition）和局部核张量分解（Local Tensor Decomposition）。本文主要讨论的是全局核张量分解。全局核张量分解的特点是考虑了矩阵元素之间的依赖关系，同时还考虑到矩阵各个特征向量之间可能存在相关性。

全局核张量分解算法通常包括如下四个步骤：

1. 对输入矩阵A进行奇异值分解SVD，得到三个矩阵U、S和V^T，其中V^T表示的就是矩阵A的特征向量；
2. 对矩阵A的特征向量按照某种规则进行重新排列组合，如保留重要的特征向量或是仅选择部分特征向量等；
3. 通过选定的特征向量构建一个新的张量，这个张量与原始矩阵A具有相同的秩，但与其余特征向量之间的关系发生变化；
4. 使用得到的张量对原始矩阵A进行相应的运算。

张量分解技术的发展历史可追溯至1972年，当时Fisher发表了著名的Hosvd方法。随后，张量分解方法逐渐形成完整体系。2014年诺贝尔物理学奖获得者J.L.Greenwood首次将全局核张量分解引入了物理学中，并取得突破性的成果。

# 2.基本概念术语说明
## 2.1 矩阵
首先需要对矩阵有一个基本的了解。矩阵是一个方阵，由若干行若干列的实数元素组成。矩阵相乘可以定义为矩阵对应位置上的元素是两个矩阵的对应元素之积，记作$AB=C$，则称$B$是$A$的左因子，$A$是$B$的右因子，$C$是$A$和$B$的乘积。若$k$不是$A$的秩，则称矩阵$A$处于秩不足状态。

## 2.2 特征值与特征向量
设$A\in \mathbb{R}^{n\times n}$是$n$阶方阵，那么它的一组最高次线性无关向量构成它的特征空间$\mathcal{F}(A)$。如果$x_1,\cdots, x_m \in \mathcal{F}(A)$，使得$Ax=\lambda x_i(i=1,\cdots, m)$，其中$\lambda=\frac{\sigma_i}{\sqrt{n}}$，那么$x=(x_1,\cdots, x_m)^T$被称为矩阵$A$的特征向量，而$\lambda=\frac{\sigma_i}{\sqrt{n}}$被称为矩阵$A$的特征值。满足$det(\frac{\partial A}{\partial \vec{x}})=0$的$\vec{x}\in \mathcal{F}(A)$被称为正交特征向量，即$A^{-1}=Q\Lambda Q^{-1}$,其中$Q=[q_1, \cdots, q_m]$，$\Lambda = diag(\lambda_1,\cdots, \lambda_m)$，$q_i\cdot q_j=\delta_{ij}$, $\delta_{ij}=1$ if $i=j$, else $\delta_{ij}=\epsilon$.

## 2.3 标准型与秩
设$A\in \mathbb{R}^{n\times n}$是$n$阶方阵，那么$A$的标准型是方阵$A^{*}$，$A^{*}=(a_{ij})$，$a_{ii}>0$(主对角线元素均大于零)，$|a_{ij}|<=a_{jj}/\sqrt{n}$ $(i\neq j)$，且每一行的绝对值的和等于$1$(对称)。

如果$A$有$r$个不同的非零特征值，那么$A$的秩$rank(A)=r$；如果$A$的所有特征值都是零，那么$rank(A)=0$；如果$A$秩不满，那么$rank(A)<r$。

## 2.4 协同变换与内积
设$\phi: V\rightarrow W$是一个映射，$X,Y\in \mathbb{R}^d$，则$dX=d[X]=X+\varepsilon$，其中$\varepsilon$是一个实数，称为$X$的散度。设$\alpha, \beta \in V$，则$\langle \alpha, \beta\rangle_{\mathrm{L}}=\int_\Omega \alpha(\xi)\beta(\xi) d\xi$，称为$\alpha$和$\beta$在参考曲面$\Omega$上关于拉普拉斯度量的内积。

## 2.5 张量
设$f: \mathbb{R}^{n_1}\times \cdots \times \mathbb{R}^{n_p} \rightarrow \mathbb{R}$是$p$-形式函数，$X_1,\cdots, X_p \in \mathbb{R}^{n_1}\times \cdots \times \mathbb{R}^{n_p}$是分别属于$\mathbb{R}^{n_1},\cdots, \mathbb{R}^{n_p}$的一个基，则以下命题成立：
- 定理1：令$\overline{X}_1,\cdots, \overline{X}_{p+1}$是$X_1,\cdots, X_p,\overline{\nabla}f$关于基$e_1,\cdots, e_{p+1}$的张量场，则$f(X_1,\cdots, X_p)$可以写为下面的形式：

  $$
  f(X_1,\cdots, X_p)=\sum_{\mu\in I_p}(\mathrm{sgn}(\mu))^{\frac{|I_p|-|\mu|}{2}}\prod_{i=1}^p (\overline{X}_i)_{(\mu)}
  $$
  
  其中$\mu$是一个$p$-集，$I_p$表示$\{1,\cdots, p\}$，$|\mu|=s$表示$\mu$的大小为$s$，$\mathrm{sgn}(\mu)$表示符号函数，$\{(\overline{X}_i)_{(\mu)}\}$表示张量$((\overline{X}_i)_{\mu})$关于基$e_1,\cdots, e_{p+1}$的展开，写作$(\overline{X}_i)_{(\mu)}$。
  
- 定理2：假设$X_1,\cdots, X_p$是一组向量场，$\eta_i\in \mathbb{R}^{n_i}$是$\mathbb{R}^n$的一组基，那么下述关系成立：

  $$\left.\begin{aligned}
    \nabla^\mu &= \nabla_{\mu'}\\
    \text{div}\vec{X} &= \text{div}\eta\\
    \text{curl}\vec{X} &= \text{curl}\eta
  \end{aligned}\right\}$$
  
- 定理3：假设$X_1,\cdots, X_p$是一组向量场，$\rho_1,\cdots, \rho_p$是连续映射，那么下述关系成立：

  $$\left.\begin{aligned}
    \text{rot}\vec{X} &=-\nabla_{\nu^{(1)}}\rho_{\mu_1}\dotsb\rho_{\mu_p}\\
    \rho_i X_j &= -\dfrac{1}{h_j}\left(\text{div}\rho_i Y_j-\text{div}\rho_j Y_i\right)\\
    \text{lap}\vec{X} &= h_{\mu'}\rho_{\mu'_1}\dotsb\rho_{\mu'_p}\text{div}\eta
  \end{aligned}\right\}$$
  
  其中$h_{\mu'}$是规范张量，$Y_i$是全微分算子，$\rho_{\mu_1}\dotsb\rho_{\mu_p}$是导数算子，$\text{div}\eta$是克氏矩阵的第$i$列，$\text{rot}\vec{X}$是$X$的旋转张量，$\text{div}\vec{X}$是$X$的投影张量。

# 3.核心算法原理和具体操作步骤以及数学公式讲解
## 3.1 全局核张量分解法
### 3.1.1 算法流程
全局核张量分解法的基本思想是在对矩阵进行奇异值分解SVD之后，根据不同的特征向量选择策略将其选取出来并重新排布顺序，然后通过选取出的特征向量构建一个新的张量，从而得到所需的张量分解结果。具体的流程如下图所示：


<center>全局核张量分解法的算法流程</center><br>

### 3.1.2 SVD分解
奇异值分解（Singular Value Decomposition，SVD）是指将任意矩阵分解为三个矩阵相乘的形式，其中的第一个矩阵为正交矩阵U，第二个矩阵为奇异值矩阵S，第三个矩阵为逆正交矩阵V。具体地，若矩阵A有$m\times n$维，则有如下的等价性质：

$$
A=U\Sigma V^T \\
A=USV^T
$$

其中$U\in \mathbb{R}^{m\times m}$是正交矩阵，$V\in \mathbb{R}^{n\times n}$是正交矩阵，而$\Sigma \in \mathbb{R}^{m\times n}$为对角矩阵，其对角元$\sigma_i$称为矩阵A的奇异值。若对角线元素全部大于零，则矩阵A是列满秩的；若对角线元素全部为零，则矩阵A是行满秩的。

### 3.1.3 特征向量选择策略
张量分解过程中，特征向量选择策略也会起到重大作用。对于某些复杂系统来说，不同特征向量可能有着不同的意义，因此选择它们中的一些作为张量分解的基础，可以避免无效的特征向量对系统建模产生影响。常用的特征向量选择策略包括：

- 一半保留：只保留矩阵A的前$k$个特征向量，这也是常用的方法。
- 混合保留：先根据某种策略选择一些特征向量，再保留其他特征向量。例如，可以先保留重要的特征向量，再保留其他稀疏的特征向量。
- 抽象特征：抽象出某些特征的概念，用这些概念来描述原始矩阵A中的特征向量。例如，可以将3D图像中的颜色、纹理、结构等抽象为“红色”、“绿色”、“蓝色”，再使用这些抽象特征来刻画矩阵A中的特征向量。

### 3.1.4 张量构建方法
根据实际情况选择的特征向量构成了一个张量。常用的张量构建方法包括：

- 张量积：使用特征向量构成的张量与原始矩阵A的对应元素进行矩阵乘积。
- 复共轭张量积：通过对特征向量进行一些变换，使得张量积可以按如下方式表示：

  $$
  [A,B]_{\gamma}=(A_\gamma B_{\gamma'})_{[\mu',\gamma]}
  $$
  
  其中$A_{\gamma}\in \mathbb{R}^{m\times n}$，$B_{\gamma'}\in \mathbb{R}^{n\times r}$，$[\mu',\gamma]=(\mu',\gamma')$，则$[A,B]_{\gamma}$是矩阵$[(A_\gamma B_{\gamma'}),([A_{\gamma'},B_{\gamma}],\vec{0})\cdots ([A_{\gamma'},B_{\gamma}],\vec{0}),(-\vec{0},\vec{0})\cdots (-\vec{0},\vec{0})]^T$的形式。$\gamma$为特征向量的组合，$\vec{0}$表示零向量，此时$\gamma$称为复共轭张量。

- 小波核张量：对每个特征向量构造一个小波核，把每个特征向量变换到另一个基，再利用小波近似插值的方法计算张量积。这种张量分解方法不需要对原始矩阵进行奇异值分解，而且能够处理非对称的张量积。

## 3.2 代数与几何基础
### 3.2.1 范数
范数又称为向量的长度或者大小，是衡量一个向量大小的度量。在线性代数中，向量范数有以下几类：

- $L^2$范数：$\parallel v\parallel_2=\sqrt{\sum_{i=1}^n |v_i|^2}$，称为二范数。
- $L^\infty$范数：$\parallel v\parallel_\infty=\max_{i}|v_i|$，称为无穷范数。
- $L^\pmb{\alpha}$范数：$\parallel v\parallel_\pmb{\alpha}=\left(\sum_{i=1}^n|v_i|^\alpha\right)^{1/\alpha}$，对于$\alpha\ge 1$，称为偶点范数。
- $L^\pmb{\beta}$范数：$\parallel v\parallel_\pmb{\beta}=\left(\sum_{i=1}^n|\log(|v_i|)^\beta\right)^{1/\beta}$，对于$\beta>0$，称为谱范数。

一般来说，$L^\pmb{\inf}$范数和$L^\pmb{\sup}$范数没有明显的界。

### 3.2.2 单位化
设$\vec{u}$为$\mathbb{R}^n$的一个向量，其范数为$||\vec{u}||$，则有$||\vec{u}||=1$。特别地，若$\forall i,j:(1\le i,j\le n), u_i\ne 0$，则$\vec{u}$的单位化$\hat{u}$是指：

$$
\hat{u}_i=\frac{u_i}{||\vec{u}||}
$$ 

### 3.2.3 内积与标量积
在线性代数中，内积（inner product）又称为向量积，是一个泛函，具有以下几何意义：给定两个向量$\vec{u}$和$\vec{v}$，它们的内积$\langle \vec{u},\vec{v}\rangle$是一个标量，满足如下条件：

$$
\langle \vec{u},\vec{v}\rangle=\langle v,\vec{u}\rangle=\mathrm{Tr}[\vec{u},v]
$$ 

其中$\mathrm{Tr}[\vec{u},v]$表示矩阵$\vec{u}$和$\vec{v}$的迹，即对所有$i,j:\vec{u}_{i,j}$，$\vec{v}_{i,j}$，有$i=j$时的条目。由于迹的存在，使得向量积具有封闭性，即：

$$
\langle \vec{u}+\vec{v},\vec{w}\rangle=\langle \vec{u},\vec{w}}\tag{1}
$$ 

$$
\langle \alpha \vec{u},\vec{w}\rangle=\alpha\langle \vec{u},\vec{w}}\tag{2}
$$ 

以上两个式子对任何两个向量$\vec{u},\vec{v},\vec{w}$都成立。若向量$\vec{v}$是$\mathbb{R}^m$中的列向量，则$\langle \vec{u},\vec{v}\rangle$被称为列向量$\vec{v}$在向量$\vec{u}$上的投影。类似地，若$\vec{u}$是$\mathbb{R}^n$中的行向量，则$\langle \vec{u},\vec{v}\rangle$被称为行向量$\vec{u}$在向量$\vec{v}$上的投影。

### 3.2.4 张量与张量积
设$X_1,\cdots, X_p\in \mathbb{R}^n$是$p$维线性空间中的$p$个矢量，$\otimes : \mathbb{R}^{n_1}\times \cdots \times \mathbb{R}^{n_p}\times \mathbb{R}^{m}\rightarrow \mathbb{R}^{(n_1+n_2+\cdots +n_p)\times m}$是一个张量积映射。对于任意$x_1,\cdots, x_p\in \mathbb{R}^{n_1}\times \cdots \times \mathbb{R}^{n_p}$和$y\in \mathbb{R}^m$，有：

$$
(x_1\otimes y)(i,j)=\sum_{k=1}^px_ki_jy_j\quad (i,j=1,\cdots, m)
$$

若$\Delta_{ijk}=1$当且仅当$i=j$时，$Z_{ijkl}=1$当且仅当$i=l$且$j=k$时，那么对于$\rho_1,\cdots, \rho_p\in \mathbb{R}^m$，有：

$$
[X_1,\cdots, X_p]\rho=[\rho,(X_1\otimes \rho),(\rho\otimes X_1),\cdots, (X_1\otimes \rho)\otimes (\rho\otimes X_1)]
$$ 

注意这里的张量积的形式只是一种记法。事实上，张量积可以有多种解释，例如：

- 当两组矢量都是矩阵，那么张量积表示的是两个矩阵的乘积；
- 当两组矢量分别是矩阵和向量，那么张量积表示的是矩阵的向量积；
- 当两组矢量分别是向量和向量，那么张量积表示的是两个向量的内积；
- 当两组矢量都是张量，那么张量积表示的是按张量乘积的记号；
- ……

### 3.2.5 流形与测度
在希腊语里，流形（manifold）和测度（metric）经常一起出现。流形是一个连通的矢量空间，它由一个集合$M$和一个距离度量$\rho$组成。$\rho$是一个距离函数，使得对于任意$x,y\in M$和$\theta\in [0,1]$，有：

$$
\rho(x,y)=\rho(\theta x+(1-\theta)y)
$$ 

特别地，$\rho$是一个双射，即对于任意$x,y\in M$，有$x\neq y$或$\rho(x,y)=\rho(y,x)$。在$\rho$确定的情况下，$\rho$的分割（section）$\operatorname{sec}(\rho)$是一个由曲线或曲面等组成的凸集。测度定义为：

$$
ds^2=\rho(dx,dy)=\|\operatorname{grad}\varphi(t)\|^2dt
$$ 

其中$\varphi$是曲线$dx=du$在$\rho$下的切线，$d^2s^2$是二次曲率。由于$\rho$是一个距离函数，$\rho$对向量场的支持也可以用来定义测度。更准确地说，测度定义为：

$$
\mu=\int_{\operatorname{vol}(M)}\rho^2(x,y)ds^2
$$ 

其等价定义为：

$$
\mu=\lim_{\eps\to 0}\frac{1}{\pi^2\eps^2}\int_{G_{\eps}(M)}\left[\int_{\Gamma_{\eps}}|\operatorname{grad}\psi(s)|^2ds\right]ds
$$ 

其中$\eps$是任意的测度，$G_{\eps}(M)$是一个球状体，半径为$\eps$，中心在$M$的某个点$c$上。$\Gamma_{\eps}$是$G_{\eps}(M)$上包含于边界$\partial M$的曲面。$\psi(s)$是位于$\Gamma_{\eps}$上、距$c$最近的点的曲线。

如果$M$是一个曲线，那么测度就等于路径（path）长度。如果$M$是一个曲面，那么测度就等于曲面区域面积除以路径长度。