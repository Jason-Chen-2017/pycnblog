                 

# 1.èƒŒæ™¯ä»‹ç»

ç¬¬7ç«  å¤§æ¨¡å‹çš„æ•°æ®ä¸æ ‡æ³¨-7.1 æ•°æ®é‡‡é›†ä¸å¤„ç†-7.1.1 æ•°æ®é‡‡é›†ç­–ç•¥
======================================================

ä½œè€…ï¼šç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯

## 7.1 æ•°æ®é‡‡é›†ä¸å¤„ç†

### 7.1.1 æ•°æ®é‡‡é›†ç­–ç•¥

#### 1. èƒŒæ™¯ä»‹ç»

åœ¨AIæŠ€æœ¯å‘å±•çš„å½“ä»Šï¼Œå¤§æ¨¡å‹å·²æˆä¸ºä¸€ä¸ªé‡è¦ä¸”å¹¿æ³›ä½¿ç”¨çš„è¯é¢˜ã€‚è¿™äº›æ¨¡å‹éœ€è¦æµ·é‡çš„æ•°æ®å’Œå‡†ç¡®çš„æ ‡æ³¨æ¥è®­ç»ƒï¼Œä»è€Œäº§ç”Ÿå‡ºå¼ºå¤§çš„é¢„æµ‹èƒ½åŠ›ã€‚å› æ­¤ï¼Œæ•°æ®é‡‡é›†ä¸å¤„ç†æˆä¸ºäº†ä¸€ä¸ªè‡³å…³é‡è¦çš„ç¯èŠ‚ï¼Œä¹Ÿæ˜¯æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ—¶é—´å’Œç²¾åŠ›ä¸Šæœ€å¤§çš„æ¶ˆè€—ç‚¹ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å…³æ³¨æ•°æ®é‡‡é›†ç­–ç•¥ï¼Œå³å¦‚ä½•æœ‰æ•ˆåœ°æ”¶é›†é«˜è´¨é‡çš„æ•°æ®æ¥è®­ç»ƒå¤§æ¨¡å‹ã€‚è¿™æ¶‰åŠåˆ°è®¸å¤šå› ç´ ï¼Œå¦‚æ•°æ®æ¥æºã€é‡‡æ ·æ–¹æ³•ã€æ•°æ®è´¨é‡æ§åˆ¶ç­‰ã€‚æˆ‘ä»¬å°†é€ä¸€ä»‹ç»è¿™äº›å› ç´ ï¼Œå¹¶æä¾›ä¸€äº›æœ€ä½³å®è·µå’Œå·¥å…·æ¨èï¼Œå¸®åŠ©æ‚¨åœ¨å®é™…åº”ç”¨ä¸­é€‰æ‹©æœ€é€‚åˆè‡ªå·±éœ€æ±‚çš„ç­–ç•¥ã€‚

#### 2. æ ¸å¿ƒæ¦‚å¿µä¸è”ç³»

* **æ•°æ®é‡‡é›†**ï¼šæŒ‡ä»å¤–éƒ¨æºä¸­è·å–æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æœ¬åœ°æˆ–äº‘ç«¯æœåŠ¡å™¨ä¸Šï¼Œä»¥å¤‡åç»­å¤„ç†å’Œåˆ†æã€‚
* **æ•°æ®å¤„ç†**ï¼šæŒ‡å¯¹åŸå§‹æ•°æ®è¿›è¡Œæ¸…æ´—ã€è½¬æ¢ã€å½’ä¸€åŒ–ç­‰æ“ä½œï¼Œä»¥ä¾¿ä½¿å…¶é€‚ç”¨äºç‰¹å®šçš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚
* **æ•°æ®é‡‡æ ·**ï¼šæŒ‡ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªå­é›†ï¼Œä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚
* **æ•°æ®æ ‡æ³¨**ï¼šæŒ‡å°†äººç±»çŸ¥è¯†æ³¨å…¥åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ç‰¹å®šä»»åŠ¡ã€‚
* **æ•°æ®è´¨é‡æ§åˆ¶**ï¼šæŒ‡è¯„ä¼°æ•°æ®é›†çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¹¶é‡‡å–é€‚å½“çš„æªæ–½æ¥æ”¹å–„å®ƒä»¬ã€‚

#### 3. æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦æ¨¡å‹å…¬å¼è¯¦ç»†è®²è§£

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»æ•°æ®é‡‡é›†ç­–ç•¥çš„æ ¸å¿ƒç®—æ³•å’Œæ“ä½œæ­¥éª¤ï¼Œè€Œéæ•°å­¦æ¨¡å‹å…¬å¼ã€‚

##### 3.1 æ•°æ®é‡‡æ ·ç®—æ³•

æ•°æ®é‡‡æ ·æ˜¯æŒ‡ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªå­é›†ï¼Œä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚å¸¸è§çš„æ•°æ®é‡‡æ ·ç®—æ³•åŒ…æ‹¬ç®€å•éšæœºé‡‡æ ·ã€ stratified samplingã€ cluster samplingã€ systematic samplingç­‰ã€‚

* **ç®€å•éšæœºé‡‡æ ·**ï¼šä»æ€»ä½“Nä¸­éšæœºé€‰æ‹©nä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ è¢«é€‰ä¸­çš„æ¦‚ç‡ç›¸ç­‰ã€‚
* **stratified sampling**ï¼šé¦–å…ˆå°†æ€»ä½“åˆ†æˆKä¸ªä¸ç›¸äº¤çš„å­é›†ï¼ˆstrataï¼‰ï¼Œç„¶åä»æ¯ä¸ªsubé›†ä¸­ç‹¬ç«‹åœ°é€‰æ‹©n_kä¸ªå…ƒç´ ï¼Œä½¿å¾—æ€»ä½“ä¸­æ¯ä¸ªstrataçš„æ¯”ä¾‹å¾—åˆ°ä¿ç•™ã€‚
* **cluster sampling**ï¼šé¦–å…ˆå°†æ€»ä½“åˆ’åˆ†ä¸ºMä¸ªäº’æ–¥ä¸”æ— é‡å çš„å­é›†ï¼ˆclusterï¼‰ï¼Œç„¶åä»Mä¸ªclusterä¸­éšæœºé€‰æ‹©Lä¸ªclusterï¼Œå¹¶å°†æ‰€æœ‰clusterä¸­çš„å…ƒç´ éƒ½çº³å…¥é‡‡æ ·é›†ä¸­ã€‚
* **systematic sampling**ï¼šé¦–å…ˆå¯¹æ€»ä½“æŒ‰ç…§æŸç§é¡ºåºæ’åˆ—ï¼Œç„¶åä»ç¬¬iä¸ªå…ƒç´ èµ·é€‰æ‹©æ¯éš”kä¸ªå…ƒç´ ä½œä¸ºé‡‡æ ·å…ƒç´ ã€‚

##### 3.2 æ•°æ®æ ‡æ³¨ç®—æ³•

æ•°æ®æ ‡æ³¨æ˜¯æŒ‡å°†äººç±»çŸ¥è¯†æ³¨å…¥åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ç‰¹å®šä»»åŠ¡ã€‚å¸¸è§çš„æ•°æ®æ ‡æ³¨ç®—æ³•åŒ…æ‹¬Active Learningã€ Transfer Learningã€ Semi-Supervised Learningç­‰ã€‚

* **Active Learning**ï¼šé€‰æ‹©æœªæ ‡æ³¨æ•°æ®ä¸­æœ€æœ‰ä»·å€¼çš„å­é›†è¿›è¡Œæ ‡æ³¨ï¼Œä»¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚
* **Transfer Learning**ï¼šåˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯¹æ–°ä»»åŠ¡è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘éœ€è¦æ ‡æ³¨çš„æ•°æ®é‡ã€‚
* **Semi-Supervised Learning**ï¼šåˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ã€‚

##### 3.3 æ•°æ®è´¨é‡æ§åˆ¶ç®—æ³•

æ•°æ®è´¨é‡æ§åˆ¶æ˜¯æŒ‡è¯„ä¼°æ•°æ®é›†çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¹¶é‡‡å–é€‚å½“çš„æªæ–½æ¥æ”¹å–„å®ƒä»¬ã€‚å¸¸è§çš„æ•°æ®è´¨é‡æ§åˆ¶ç®—æ³•åŒ…æ‹¬Cleaningã€ Data Augmentationã€ Regularizationç­‰ã€‚

* **Cleaning**ï¼šç§»é™¤æ•°æ®é›†ä¸­çš„å™ªå£°ã€é”™è¯¯å’Œå¼‚å¸¸å€¼ï¼Œä»¥æé«˜æ•°æ®è´¨é‡ã€‚
* **Data Augmentation**ï¼šé€šè¿‡å¢åŠ æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
* **Regularization**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ å…¥æ­£åˆ™é¡¹ï¼Œä»¥æƒ©ç½šæ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚

#### 4. å…·ä½“æœ€ä½³å®è·µï¼šä»£ç å®ä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æä¾›ä¸€äº›å…³äºæ•°æ®é‡‡é›†ç­–ç•¥çš„æœ€ä½³å®è·µå’Œä»£ç ç¤ºä¾‹ã€‚

##### 4.1 æ•°æ®é‡‡æ ·

```python
import numpy as np
import random

def simple_random_sampling(data, n):
   """
   ç®€å•éšæœºé‡‡æ ·
   :param data: ndarray or list, æ•°æ®é›†
   :param n: int, é‡‡æ ·æ•°é‡
   :return: ndarray or list, é‡‡æ ·ç»“æœ
   """
   if isinstance(data, list):
       data = np.array(data)
   sample_idx = np.random.choice(len(data), size=n, replace=False)
   return data[sample_idx]

def stratified_sampling(data, n, strata):
   """
   åˆ†å±‚é‡‡æ ·
   :param data: ndarray or list, æ•°æ®é›†
   :param n: int, é‡‡æ ·æ•°é‡
   :param strata: list or ndarray, æ•°æ®é›†çš„å±æ€§
   :return: dict, {'strata_name': [sample_data]}
   """
   if isinstance(data, list):
       data = np.array(data)
   strata_num = len(set(strata))
   sample_num_per_strata = n // strata_num
   sample_dict = {i: [] for i in set(strata)}
   for i in range(strata_num):
       cur_strata_idx = np.where(strata == i)[0]
       cur_strata_sample_idx = np.random.choice(cur_strata_idx, size=sample_num_per_strata, replace=False)
       cur_strata_sample = data[cur_strata_sample_idx]
       sample_dict[i].extend(cur_strata_sample.tolist())
   return sample_dict

def cluster_sampling(data, n, clusters):
   """
   èšç±»é‡‡æ ·
   :param data: ndarray or list, æ•°æ®é›†
   :param n: int, é‡‡æ ·æ•°é‡
   :param clusters: list or ndarray, æ•°æ®é›†çš„èšç±»ä¿¡æ¯
   :return: ndarray or list, é‡‡æ ·ç»“æœ
   """
   if isinstance(data, list):
       data = np.array(data)
   cluster_num = len(set(clusters))
   sample_cluster_num = n // cluster_num
   sample_cluster_idx = np.random.choice(cluster_num, size=sample_cluster_num, replace=False)
   sample_idx = np.unique(np.concatenate([clusters == i for i in sample_cluster_idx]))
   return data[sample_idx]

def systematic_sampling(data, n):
   """
   ç³»ç»Ÿé‡‡æ ·
   :param data: ndarray or list, æ•°æ®é›†
   :param n: int, é‡‡æ ·æ•°é‡
   :return: ndarray or list, é‡‡æ ·ç»“æœ
   """
   if isinstance(data, list):
       data = np.array(data)
   sample_step = len(data) // n
   sample_idx = np.arange(0, len(data), step=sample_step)
   return data[sample_idx]
```

##### 4.2 æ•°æ®æ ‡æ³¨

```python
from sklearn.model_selection import train_test_split
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

def active_learning(model, data_loader, num_labels, batch_size, n_iter):
   """
   ä¸»åŠ¨å­¦ä¹ 
   :param model: Model, é¢„è®­ç»ƒæ¨¡å‹
   :param data_loader: DataLoader, æ•°æ®é›†åŠ è½½å™¨
   :param num_labels: int, ç±»åˆ«æ•°
   :param batch_size: int, æ‰¹æ¬¡å¤§å°
   :param n_iter: int, è¿­ä»£æ¬¡æ•°
   :return: Model, å¾®è°ƒåçš„æ¨¡å‹
   """
   train_data, val_data = train_test_split(data_loader, test_size=0.2, random_state=42)
   train_data, unlabelled_data = train_test_split(train_data, test_size=0.5, random_state=42)
   for _ in range(n_iter):
       model.train()
       optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
       training_args = TrainingArguments(
           output_dir='./results',         # output directory
           num_train_epochs=3,             # total number of training epochs
           per_device_train_batch_size=batch_size,  # batch size per device during training
           warmup_steps=500,               # number of warmup steps for learning rate scheduler
           weight_decay=0.01,              # strength of weight decay
           logging_dir='./logs',           # directory for storing logs
           logging_steps=10,
       )

       trainer = Trainer(
           model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
           args=training_args,                 # training arguments, defined above
           train_dataset=train_data,            # training dataset
           eval_dataset=val_data
       )

       trainer.train()

       model.eval()
       predictions = []
       with torch.no_grad():
           for batch in tqdm(unlabelled_data):
               input_ids = batch['input_ids'].to(device)
               attention_mask = batch['attention_mask'].to(device)
               labels = batch['labels']
               outputs = model(input_ids, attention_mask=attention_mask)
               logits = outputs.logits
               preds = torch.argmax(logits, dim=-1)
               predictions.extend(preds.cpu().numpy().tolist())

       sorted_idx = np.argsort(predictions)[::-1][:len(unlabelled_data)]
       labelled_data = unlabelled_data[sorted_idx]
       train_data = train_data + labelled_data
       unlabelled_data = unlabelled_data[~sorted_idx]

   return model

def transfer_learning(model, data_loader, num_labels, batch_size):
   """
   è¿ç§»å­¦ä¹ 
   :param model: Model, é¢„è®­ç»ƒæ¨¡å‹
   :param data_loader: DataLoader, æ•°æ®é›†åŠ è½½å™¨
   :param num_labels: int, ç±»åˆ«æ•°
   :param batch_size: int, æ‰¹æ¬¡å¤§å°
   :return: Model, å¾®è°ƒåçš„æ¨¡å‹
   """
   model.num_labels = num_labels
   model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)

   model.train()
   optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
   training_args = TrainingArguments(
       output_dir='./results',         # output directory
       num_train_epochs=3,             # total number of training epochs
       per_device_train_batch_size=batch_size,  # batch size per device during training
       warmup_steps=500,               # number of warmup steps for learning rate scheduler
       weight_decay=0.01,              # strength of weight decay
       logging_dir='./logs',           # directory for storing logs
       logging_steps=10,
   )

   trainer = Trainer(
       model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
       args=training_args,                 # training arguments, defined above
       train_dataset=data_loader,          # training dataset
       eval_dataset=None
   )

   trainer.train()

   return model

def semi_supervised_learning(model, data_loader, num_labels, batch_size, labeled_ratio):
   """
   åŠç›‘ç£å­¦ä¹ 
   :param model: Model, é¢„è®­ç»ƒæ¨¡å‹
   :param data_loader: DataLoader, æ•°æ®é›†åŠ è½½å™¨
   :param num_labels: int, ç±»åˆ«æ•°
   :param batch_size: int, æ‰¹æ¬¡å¤§å°
   :param labeled_ratio: float, æ ‡æ³¨æ¯”ä¾‹
   :return: Model, å¾®è°ƒåçš„æ¨¡å‹
   """
   labeled_data, unlabelled_data = random_split(data_loader, [int(len(data_loader)*labeled_ratio), len(data_loader)-int(len(data_loader)*labeled_ratio)])
   model.num_labels = num_labels
   model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)

   model.train()
   optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
   training_args = TrainingArguments(
       output_dir='./results',         # output directory
       num_train_epochs=3,             # total number of training epochs
       per_device_train_batch_size=batch_size,  # batch size per device during training
       warmup_steps=500,               # number of warmup steps for learning rate scheduler
       weight_decay=0.01,              # strength of weight decay
       logging_dir='./logs',           # directory for storing logs
       logging_steps=10,
   )

   trainer = Trainer(
       model=model,                       # the instantiated ğŸ¤— Transformers model to be trained
       args=training_args,                 # training arguments, defined above
       train_dataset=labeled_data,         # training dataset
       eval_dataset=unlabelled_data
   )

   trainer.train()

   return model
```

##### 4.3 æ•°æ®è´¨é‡æ§åˆ¶

```python
import pandas as pd
import numpy as np
import torch
from sklearn.impute import SimpleImputer

def cleaning(data):
   """
   æ•°æ®æ¸…æ´—
   :param data: ndarray or list, æ•°æ®é›†
   :return: ndarray or list, æ¸…æ´—åçš„æ•°æ®é›†
   """
   if isinstance(data, list):
       data = pd.DataFrame(data)

   # åˆ é™¤é‡å¤è¡Œ
   data.drop_duplicates(inplace=True)

   # åˆ é™¤ç¼ºå¤±å€¼è¶…è¿‡é˜ˆå€¼çš„è¡Œ
   missing_thresh = 0.8
   drop_rows = data.isnull().sum(axis=1) > (len(data.columns) * missing_thresh)
   data.dropna(inplace=True)

   # æ›¿æ¢ç¼ºå¤±å€¼
   imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
   data = pd.DataFrame(imputer.fit_transform(data))

   return data.to_numpy()

def data_augmentation(data, augment_ratio=0.2):
   """
   æ•°æ®å¢å¼º
   :param data: ndarray or list, æ•°æ®é›†
   :param augment_ratio: float, å¢å¼ºæ¯”ä¾‹
   :return: ndarray or list, å¢å¼ºåçš„æ•°æ®é›†
   """
   if isinstance(data, list):
       data = pd.DataFrame(data)

   aug_data = []
   for i in range(int(len(data) * augment_ratio)):
       cur_row = data.iloc[np.random.randint(0, len(data))]
       aug_row = cur_row.copy()
       aug_row[:] = np.nan
       aug_data.append(aug_row)

   data = data.append(aug_data)

   return data.to_numpy()

def regularization(model, lambda_val):
   """
   æ­£åˆ™åŒ–
   :param model: Model, é¢„è®­ç»ƒæ¨¡å‹
   :param lambda_val: float, æ­£åˆ™ç³»æ•°
   :return: Model, æ­£åˆ™åŒ–åçš„æ¨¡å‹
   """
   optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=lambda_val)
   model.train()
   loss_fn = torch.nn.CrossEntropyLoss()

   for epoch in range(3):
       for batch in tqdm(train_dataloader):
           input_ids = batch['input_ids'].to(device)
           attention_mask = batch['attention_mask'].to(device)
           labels = batch['labels']

           outputs = model(input_ids, attention_mask=attention_mask)
           logits = outputs.logits
           preds = torch.argmax(logits, dim=-1)

           loss = loss_fn(logits, labels) + lambda_val * sum([torch.norm(param) for param in model.parameters()])

           optimizer.zero_grad()
           loss.backward()
           optimizer.step()

   return model
```

#### 5. å®é™…åº”ç”¨åœºæ™¯

* **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ•°æ®é‡‡æ ·å’Œæ•°æ®æ ‡æ³¨æ˜¯è‡³å…³é‡è¦çš„æ­¥éª¤ã€‚å¯ä»¥é€šè¿‡åˆ†å±‚é‡‡æ ·æ¥ä¿è¯æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹ï¼Œå¹¶é€šè¿‡ä¸»åŠ¨å­¦ä¹ æ¥é€‰æ‹©æœªæ ‡æ³¨æ•°æ®ä¸­æœ€æœ‰ä»·å€¼çš„å­é›†è¿›è¡Œæ ‡æ³¨ã€‚
* **è®¡ç®—æœºè§†è§‰**ï¼šåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œæ•°æ®å¢å¼ºæ˜¯ä¸€ä¸ªå¸¸è§çš„æŠ€å·§ï¼Œå¯ä»¥é€šè¿‡æ—‹è½¬ã€ç¼©æ”¾ã€ç¿»è½¬ç­‰æ–¹å¼æ¥å¢åŠ æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ã€‚åŒæ—¶ï¼Œé€šè¿‡æ­£åˆ™åŒ–æŠ€æœ¯æ¥é¿å…æ¨¡å‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
* **å£°éŸ³è¯†åˆ«**ï¼šåœ¨å£°éŸ³è¯†åˆ«ä¸­ï¼Œç”±äºæ•°æ®é›†çš„è§„æ¨¡è¾ƒå°ï¼Œå¯ä»¥é€šè¿‡åŠç›‘ç£å­¦ä¹ æ¥åˆ©ç”¨å°‘é‡æ ‡æ³¨æ•°æ®å’Œå¤§é‡æœªæ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œä»¥é™ä½æ ‡æ³¨æˆæœ¬ã€‚

#### 6. å·¥å…·å’Œèµ„æºæ¨è


#### 7. æ€»ç»“ï¼šæœªæ¥å‘å±•è¶‹åŠ¿ä¸æŒ‘æˆ˜

éšç€å¤§æ¨¡å‹çš„ä¸æ–­å‘å±•ï¼Œæ•°æ®é‡‡é›†ä¸å¤„ç†å°†æˆä¸ºæ›´åŠ å…³é”®çš„ç¯èŠ‚ã€‚æœªæ¥çš„å‘å±•è¶‹åŠ¿åŒ…æ‹¬æ›´é«˜æ•ˆçš„æ•°æ®é‡‡æ ·ç®—æ³•ã€æ›´æ™ºèƒ½çš„æ•°æ®æ ‡æ³¨ç®—æ³•ã€æ›´å¤æ‚çš„æ•°æ®è´¨é‡æ§åˆ¶ç®—æ³•ç­‰ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿé¢ä¸´ç€æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®éšç§é—®é¢˜ã€æ•°æ®æ ‡æ³¨æˆæœ¬é—®é¢˜ã€æ¨¡å‹çš„è§£é‡Šæ€§é—®é¢˜ç­‰ã€‚

#### 8. é™„å½•ï¼šå¸¸è§é—®é¢˜ä¸è§£ç­”

**Q:** ä»€ä¹ˆæ˜¯æ•°æ®é‡‡æ ·ï¼Ÿ

**A:** æ•°æ®é‡‡æ ·æ˜¯æŒ‡ä»å¤§è§„æ¨¡æ•°æ®é›†ä¸­é€‰æ‹©ä¸€ä¸ªå­é›†ï¼Œä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦å¹¶æé«˜è®­ç»ƒæ•ˆç‡ã€‚

**Q:** ä»€ä¹ˆæ˜¯æ•°æ®æ ‡æ³¨ï¼Ÿ

**A:** æ•°æ®æ ‡æ³¨æ˜¯æŒ‡å°†äººç±»çŸ¥è¯†æ³¨å…¥åˆ°æ•°æ®é›†ä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹å­¦ä¹ ç‰¹å®šä»»åŠ¡ã€‚

**Q:** ä»€ä¹ˆæ˜¯æ•°æ®è´¨é‡æ§åˆ¶ï¼Ÿ

**A:** æ•°æ®è´¨é‡æ§åˆ¶æ˜¯æŒ‡è¯„ä¼°æ•°æ®é›†çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§å’Œæœ‰æ•ˆæ€§ï¼Œå¹¶é‡‡å–é€‚å½“çš„æªæ–½æ¥æ”¹å–„å®ƒä»¬ã€‚

**Q:** å¦‚ä½•è¯„ä¼°æ•°æ®é›†çš„å®Œæ•´æ€§ï¼Ÿ

**A:** å¯ä»¥é€šè¿‡è®¡ç®—æ•°æ®é›†ä¸­ç¼ºå¤±å€¼çš„æ¯”ä¾‹ã€é‡å¤å€¼çš„æ¯”ä¾‹ç­‰æŒ‡æ ‡æ¥è¯„ä¼°æ•°æ®é›†çš„å®Œæ•´æ€§ã€‚

**Q:** å¦‚ä½•è¯„ä¼°æ•°æ®é›†çš„å‡†ç¡®æ€§ï¼Ÿ

**A:** å¯ä»¥é€šè¿‡å¯¹æ•°æ®é›†ä¸­çš„æ•°æ®è¿›è¡Œäººå·¥æ£€æŸ¥æˆ–ä½¿ç”¨ä¸“ä¸šå·¥å…·è¿›è¡ŒéªŒè¯æ¥è¯„ä¼°æ•°æ®é›†çš„å‡†ç¡®æ€§ã€‚

**Q:** å¦‚ä½•è¯„ä¼°æ•°æ®é›†çš„æœ‰æ•ˆæ€§ï¼Ÿ

**A:** å¯ä»¥é€šè¿‡å¯¹æ•°æ®é›†è¿›è¡Œç»Ÿè®¡åˆ†æã€ç‰¹å¾é€‰æ‹©ã€æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ç­‰æ“ä½œæ¥è¯„ä¼°æ•°æ®é›†çš„æœ‰æ•ˆæ€§ã€‚