
ä½œè€…ï¼šç¦…ä¸è®¡ç®—æœºç¨‹åºè®¾è®¡è‰ºæœ¯                    

# 1.ç®€ä»‹
  

éšç€è¿‘å‡ å¹´çš„æŠ€æœ¯é©å‘½ï¼Œè‡ªåŠ¨åŒ–æ‰‹æ®µçš„åº”ç”¨è¶Šæ¥è¶Šå¤šï¼Œå°¤å…¶æ˜¯åœ¨ç”Ÿäº§é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œåœ¨é›¶å”®é¢†åŸŸï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æå‡æ•ˆç‡ï¼Œå‡å°‘å·¥ä½œé‡ï¼›åœ¨åˆ¶é€ é¢†åŸŸï¼Œé€šè¿‡é¢„æµ‹åœäº§æ—¶é—´ï¼Œå¯ä»¥èŠ‚çœæ—¶é—´ï¼Œæé«˜ç”Ÿäº§è´¨é‡ã€‚ä½†æ˜¯è¿™äº›æŠ€æœ¯å¾€å¾€åŸºäºæŸäº›é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶ä¾èµ–äºè¾ƒä¸ºç²¾ç¡®çš„ç»Ÿè®¡æ¨¡å‹ï¼Œå› æ­¤å¯¹äºè®¸å¤šéä¸“ä¸šäººå‘˜æ¥è¯´ä»ç„¶ååˆ†éš¾ç”¨ã€‚å¦å¤–ï¼Œå¾ˆå¤šæŠ€æœ¯è¿˜å­˜åœ¨ç€è¾ƒå¤§çš„å±€é™æ€§ï¼Œä¾‹å¦‚ï¼Œå¯¹äºæ–°é—»æ–‡æœ¬çš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆæ¥è¯´ï¼Œç›®å‰è¿˜æ²¡æœ‰ä¸€ä¸ªå¯ä»¥å®Œå…¨æ›¿ä»£äººçš„ç³»ç»Ÿã€‚å› æ­¤ï¼Œå¦‚ä½•åˆ©ç”¨äººç±»çš„è¯­è¨€æŠ€å·§è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„ç”Ÿæˆå°±æ˜¯ä¸€ä¸ªéœ€è¦è§£å†³çš„é‡è¦é—®é¢˜ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å°†ä»‹ç»ä¸€ç§æ–°çš„æŠ€æœ¯â€”â€”åŸºäºé¢„è®­ç»ƒç¼–ç å™¨ï¼ˆPre-trained encodersï¼‰çš„æ–‡æœ¬æ‘˜è¦ç”Ÿæˆæ–¹æ³•ã€‚ä¸ä¼ ç»Ÿçš„åŸºäºè§„åˆ™çš„æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œæœ¬æ–‡é‡‡ç”¨é¢„è®­ç»ƒç¼–ç å™¨ä»å¤§å‹çš„æ–‡æœ¬æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨ç»™å®šè¾“å…¥æ–‡æœ¬åï¼Œå°†å…¶ç¼–ç æˆå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºï¼Œå†ç”±æ­¤è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„ç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å…‹æœäº†ä¼ ç»Ÿæ–¹æ³•çš„ä¸¤ä¸ªä¸»è¦ç¼ºé™·ï¼šé¦–å…ˆï¼Œå®ƒä¸ä»…å¯ä»¥å­¦ä¹ åˆ°æœ‰æ•ˆçš„è¯­è¨€æ¨¡å‹ï¼Œè€Œä¸”èƒ½å¤Ÿæ•è·å…¨å±€çš„ã€é•¿æœŸçš„ä¿¡æ¯ï¼Œè¿™ä½¿å¾—ç”Ÿæˆçš„æ‘˜è¦æ›´åŠ ç‹¬ç‰¹å’Œå‡†ç¡®ï¼›å…¶æ¬¡ï¼Œå®ƒä¸éœ€è¦é¢å¤–çš„æ•°æ®é›†ï¼Œåªéœ€æ ¹æ®å¤§é‡çš„è®­ç»ƒæ–‡æœ¬æ•°æ®è¿›è¡Œé¢„è®­ç»ƒå³å¯ï¼Œå› è€Œå¯ä»¥åœ¨å°‘é‡çš„è®­ç»ƒæ•°æ®ä¸‹äº§ç”Ÿå¾ˆå¥½çš„æ•ˆæœã€‚

# 2.åŸºæœ¬æ¦‚å¿µæœ¯è¯­è¯´æ˜
## 2.1 æ–‡æœ¬æ‘˜è¦ä¸è‡ªåŠ¨æ‘˜è¦
æ–‡æœ¬æ‘˜è¦(text summarization)æ˜¯ä»ä¸€æ®µé•¿æ–‡æœ¬ä¸­è‡ªåŠ¨åœ°åˆ›å»ºç®€çŸ­ã€æ˜äº†ä¸”ç»“æ„æ¸…æ™°çš„ç‰ˆæœ¬ï¼Œé€šå¸¸åªä¿ç•™å…³é”®ä¿¡æ¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°ä¼ è¾¾é‡ç‚¹ã€‚è‡ªåŠ¨æ‘˜è¦çš„ç›®çš„æ˜¯å°†å¤æ‚è€Œå†—é•¿çš„ä¿¡æ¯è½¬åŒ–ä¸ºç®€æ´æ˜“è¯»çš„å½¢å¼ï¼Œä»è€Œæé«˜é˜…è¯»æ•ˆç‡ã€‚

æ–‡æœ¬æ‘˜è¦çš„ä¸¤ç§ä¸»è¦ç±»å‹ï¼š

1.å•æ–‡æ¡£æ‘˜è¦ï¼šæŒ‡çš„æ˜¯å¯¹å•ä¸ªæ–‡æ¡£è¿›è¡Œæ‘˜è¦ã€‚é€šå¸¸æ˜¯é’ˆå¯¹å¾®åšã€åšå®¢æˆ–è®ºæ–‡ç­‰å°æ–‡è¿›è¡Œæ‘˜è¦ã€‚

2.å¤šæ–‡æ¡£æ‘˜è¦ï¼šæŒ‡çš„æ˜¯å¯¹å¤šä¸ªæ–‡æ¡£è¿›è¡Œæ‘˜è¦ã€‚é€šå¸¸æ˜¯å¯¹æœç´¢ç»“æœæˆ–æ–°é—»ç½‘ç«™ä¸Šçš„ç›¸å…³æ–°é—»æ¡ç›®ç­‰å¤§é‡æ–‡æ¡£è¿›è¡Œæ‘˜è¦ã€‚

è‡ªåŠ¨æ‘˜è¦æ–¹æ³•å¤§è‡´å¯åˆ†ä¸ºä¸¤ç±»ï¼š

1.åŸºäºè§„åˆ™çš„æ–¹æ³•ï¼šé€šè¿‡æŒ‡å®šä¸€å®šçš„è§„åˆ™æˆ–æ ‡æ³¨æ•°æ®ï¼Œç³»ç»Ÿåœ°ä»æ–‡æœ¬ä¸­æŠ½å–å‡ºé‡è¦çš„ä¿¡æ¯ç‰‡æ®µï¼Œç„¶åå°†å®ƒä»¬ç»„ç»‡æˆæ‘˜è¦ã€‚ä¾‹å¦‚ï¼Œå…³é”®è¯æŠ½å–æ–¹æ³•ã€ä¸»é¢˜æ¨¡å‹æ–¹æ³•ç­‰ã€‚

2.åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼šé€šè¿‡æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåˆ©ç”¨å…ˆéªŒçŸ¥è¯†ã€ç»Ÿè®¡æ–¹æ³•ç­‰ç­‰æå–æ–‡æœ¬ç‰¹å¾ï¼Œç„¶åä½¿ç”¨æ¦‚ç‡æ¨¡å‹æˆ–å…¶ä»–æ–¹æ³•ç”Ÿæˆæ‘˜è¦ã€‚

## 2.2 é¢„è®­ç»ƒç¼–ç å™¨ï¼ˆPre-trained encodersï¼‰
é¢„è®­ç»ƒç¼–ç å™¨æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€é¡¹é‡è¦æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨å¤§è§„æ¨¡æ— ç›‘ç£æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå»ºç«‹é€šç”¨çš„ã€è¡¨å¾æ€§çš„è¯­è¨€æ¨¡å‹ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥ä»åŸå§‹æ–‡æœ¬ä¸­å­¦åˆ°ä¸°å¯Œçš„ã€æ³›åŒ–èƒ½åŠ›å¼ºçš„ç‰¹å¾è¡¨ç¤ºã€‚é¢„è®­ç»ƒç¼–ç å™¨åŒ…æ‹¬ä¸¤ç§ç±»å‹ï¼š

1.è¯åµŒå…¥ï¼ˆWord embeddingsï¼‰ï¼šé€šè¿‡è¯å‘é‡ï¼ˆword vectorsï¼‰çš„æ–¹å¼è¡¨ç¤ºæ¯ä¸ªè¯çš„ä¸Šä¸‹æ–‡å…³ç³»ã€‚è¯å‘é‡èƒ½å¤Ÿæ•è·è¯æ±‡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å¸®åŠ©æ¨¡å‹ç†è§£ä¸Šä¸‹æ–‡å«ä¹‰ã€‚

2.ç¼–ç å™¨ï¼ˆEncodersï¼‰ï¼šé€šè¿‡ä¸€ç³»åˆ—çš„ç¥ç»ç½‘ç»œå±‚å°†è¯å‘é‡ç¼–ç æˆå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿæ•è·é•¿æœŸçš„æ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸ä¼šä¸¢å¤±ä»»ä½•å…³é”®ä¿¡æ¯ã€‚

## 2.3 Transformer
Transformeræ˜¯Googleæå‡ºçš„ä¸€ç§ç”¨äºæ–‡æœ¬åºåˆ—å¤„ç†çš„æ¨¡å‹ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨å¤šä¸ªè‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å®ç°ç«¯åˆ°ç«¯çš„ç‰¹å¾æŠ½å–ï¼Œå¹¶å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚Transformeræœ€åˆè¢«ç”¨äºNLPä»»åŠ¡ï¼Œä½†ä¹Ÿå¯ä»¥ç”¨äºç”Ÿæˆä»»åŠ¡ã€‚Transformeræ˜¯åœ¨encoder-decoderç»“æ„åŸºç¡€ä¸Šå‘å±•èµ·æ¥çš„ï¼Œå³transformeræœ‰è‡ªå·±çš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼Œé€šè¿‡æŠŠè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸ä½ç½®ç¼–ç ä¸€èµ·ä½¿ç”¨ï¼Œå¯ä»¥å°†åŸå§‹åºåˆ—ç¼–ç ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºï¼Œå¹¶ä¿æŒä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

# 3.æ ¸å¿ƒç®—æ³•åŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦å…¬å¼è®²è§£
## 3.1 æ¨¡å‹ç»“æ„
æœ¬æ–‡é‡‡ç”¨çš„æ¨¡å‹åŸºäºTransformerï¼Œå…¶ç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š


æ¨¡å‹ç”±encoderå’Œdecoderç»„æˆã€‚Encoderæ˜¯ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒæ¥å—è¾“å…¥åºåˆ—å¹¶è¾“å‡ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºï¼Œå…¶ä¸­åŒ…å«äº†æ•´ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚Decoderä¹Ÿæ˜¯ä¸€ä¸ªå¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒæ¥æ”¶ä¹‹å‰ç”Ÿæˆçš„å¥å­å¹¶è¾“å‡ºä¸‹ä¸€ä¸ªå¯èƒ½å‡ºç°çš„è¯ã€‚ç”±äºé‡‡ç”¨äº†attentionæœºåˆ¶ï¼Œæ¨¡å‹èƒ½å¤ŸåŒæ—¶å…³æ³¨æ•´ä¸ªåºåˆ—åŠå…¶å½“å‰éƒ¨åˆ†ï¼Œä»è€Œæé«˜ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚

## 3.2 æ•°æ®é›†
### 3.2.1 æ•°æ®é›†ä»‹ç»

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®ã€‚å¯¹äºæ–‡æœ¬æ‘˜è¦ï¼Œéœ€è¦å‡†å¤‡å¤§é‡çš„æ–‡æœ¬æ•°æ®ï¼ŒåŒ…æ‹¬æ–°é—»ã€è®ºå›å¸–å­ã€ç½‘é¡µæ–‡ç« ç­‰ç­‰ã€‚è¿™äº›æ•°æ®é›†åˆæˆä¸€ä¸ªå¤§å‹çš„æ— ç›‘ç£æ•°æ®é›†ã€‚

ç¬¬äºŒæ­¥ï¼Œå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ç”±äºæ¨¡å‹éœ€è¦å¤„ç†çš„æ–‡æœ¬é•¿åº¦ä¸åŒï¼Œå› æ­¤éœ€è¦å¯¹æ•°æ®è¿›è¡Œåˆ‡å‰²ï¼Œå°†é•¿æ–‡æœ¬åˆ‡å‰²æˆé€‚åˆæ¨¡å‹è¾“å…¥çš„é•¿åº¦ã€‚åŒæ—¶ï¼Œç”±äºæ‘˜è¦ç”Ÿæˆä»»åŠ¡éœ€è¦è€ƒè™‘é•¿æ–‡æœ¬çš„é—®é¢˜ï¼Œå› æ­¤éœ€è¦å°†æ–‡æœ¬æ‹†åˆ†æˆå¤šä¸ªçŸ­å¥ï¼Œç„¶åå°†çŸ­å¥åˆå¹¶ä¸ºæ•´ä½“ã€‚

ç¬¬ä¸‰æ­¥ï¼Œåˆ©ç”¨é¢„è®­ç»ƒç¼–ç å™¨ï¼ˆå¦‚BERTã€RoBERTaã€ALBERTç­‰ï¼‰è¿›è¡Œè®­ç»ƒã€‚é¢„è®­ç»ƒç¼–ç å™¨èƒ½å¤Ÿæ•è·æ–‡æœ¬ä¸­ä¸°å¯Œçš„æ¨¡å¼å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

æœ€åä¸€æ­¥ï¼Œåˆ©ç”¨ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ‘˜è¦ç”Ÿæˆã€‚ç”Ÿæˆæ¨¡å‹é‡‡ç”¨æŒ‡é’ˆæœºåˆ¶è¿›è¡Œè®­ç»ƒï¼Œå³é€‰æ‹©æ€§åœ°å¤åˆ¶æºæ–‡æœ¬ä¸­çš„è¯æˆ–çŸ­è¯­ï¼Œæˆ–è€…ç”Ÿæˆæ‘˜è¦ä¸­çš„æ–°çš„è¯ã€‚

## 3.3 æ­å»ºç”Ÿæˆæ¨¡å‹
ç”Ÿæˆæ¨¡å‹æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—ï¼ˆSeq2Seqï¼‰æ¨¡å‹ï¼Œå®ƒå°†è¾“å…¥åºåˆ—æ˜ å°„åˆ°è¾“å‡ºåºåˆ—ã€‚æœ¬æ–‡é‡‡ç”¨çš„æ˜¯åŸºäºTransformerçš„ç”Ÿæˆæ¨¡å‹ã€‚

### 3.3.1 å•è¯çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶
æœ¬æ–‡é‡‡ç”¨å•è¯çº§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå³æ¯ä¸ªè¯éƒ½åˆ†é…ä¸€ä¸ªæ³¨æ„åŠ›æƒé‡ã€‚å•è¯çº§çš„æ³¨æ„åŠ›æƒé‡é€šè¿‡å¯¹æºæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬çš„è¯å‘é‡è¿›è¡Œç‚¹ä¹˜å¾—åˆ°ã€‚ç‚¹ä¹˜ä¹‹åï¼Œæ‰€æœ‰è¯å‘é‡éƒ½ä¼šè·å¾—ç›¸åŒçš„æƒé‡ï¼Œåªæœ‰é‡è¦çš„è¯æ‰ä¼šè·å¾—æ›´å¤§çš„æƒé‡ã€‚

### 3.3.2 å¥å­çº§åˆ«çš„æ³¨æ„åŠ›æœºåˆ¶
æœ¬æ–‡é‡‡ç”¨å¥å­çº§çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå³æ¯å¥è¯éƒ½åˆ†é…ä¸€ä¸ªæ³¨æ„åŠ›æƒé‡ã€‚å¥å­çº§çš„æ³¨æ„åŠ›æƒé‡é€šè¿‡å¯¹æºæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬çš„å¥å‘é‡è¿›è¡Œç‚¹ä¹˜å¾—åˆ°ã€‚ç‚¹ä¹˜ä¹‹åï¼Œæ‰€æœ‰å¥å‘é‡éƒ½ä¼šè·å¾—ç›¸åŒçš„æƒé‡ï¼Œåªæœ‰é‡è¦çš„å¥æ‰ä¼šè·å¾—æ›´å¤§çš„æƒé‡ã€‚

### 3.3.3 å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå…è®¸æ¨¡å‹å­¦ä¹ ä¸åŒæ–¹é¢çš„ä¿¡æ¯ã€‚æœ¬æ–‡é‡‡ç”¨äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå³å°†ç›¸åŒçš„è®¡ç®—èµ„æºåˆ†é…åˆ°ä¸åŒçš„å­ç©ºé—´ä¸Šï¼Œä»¥ä¾¿æé«˜æ¨¡å‹çš„èƒ½åŠ›ã€‚

### 3.3.4 Positional Encoding
Positional Encodingæ˜¯ä¸€ç§å¢åŠ ä½ç½®ä¿¡æ¯çš„æ–¹å¼ã€‚å®ƒå¯ä»¥é€šè¿‡å¼•å…¥ä¸€äº›çŸ­æœŸçš„ã€å›ºå®šçš„å€¼æ¥å®ç°ã€‚å…·ä½“æ¥è¯´ï¼ŒPositional Encodingå¯ä»¥çœ‹ä½œæ˜¯å¸¦æœ‰æ—¶é—´æŒ‡æ ‡çš„ä½ç½®ç¼–ç ï¼Œå®ƒèƒ½å¤Ÿè®©æ¨¡å‹è·å¾—å…³äºæ–‡æœ¬ä½ç½®çš„æ›´å¤šä¿¡æ¯ã€‚

## 3.4 æŒ‡é’ˆæœºåˆ¶
æŒ‡é’ˆæœºåˆ¶æ˜¯ç”Ÿæˆæ¨¡å‹çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€‰æ‹©æ€§åœ°å¤åˆ¶æºæ–‡æœ¬ä¸­çš„è¯æˆ–çŸ­è¯­ï¼Œæˆ–è€…ç”Ÿæˆæ‘˜è¦ä¸­çš„æ–°çš„è¯ã€‚æŒ‡é’ˆæœºåˆ¶èƒ½å¤Ÿè®©ç”Ÿæˆæ¨¡å‹é€¼çœŸï¼Œé¿å…é‡å¤å‡ºç°é‡è¦çš„å†…å®¹ã€‚

### 3.4.1 è´ªå¿ƒç­–ç•¥
è´ªå¿ƒç­–ç•¥æ˜¯æŒ‡é’ˆæœºåˆ¶çš„ä¸€ç§ç­–ç•¥ã€‚å®ƒé€‰æ‹©å½“å‰æ¦‚ç‡æœ€å¤§çš„è¯æˆ–çŸ­è¯­ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å‡ºï¼Œè€Œä¸æ˜¯ç›´æ¥å¤åˆ¶è¯ã€‚è´ªå¿ƒç­–ç•¥èƒ½å¤Ÿé¿å…ç”Ÿæˆé”™è¯¯çš„è¾“å‡ºï¼Œä»è€Œæå‡æ¨¡å‹çš„ç”Ÿæˆè´¨é‡ã€‚

### 3.4.2 è”åˆæ³¨æ„åŠ›æœºåˆ¶
è”åˆæ³¨æ„åŠ›æœºåˆ¶æ˜¯æŒ‡é’ˆæœºåˆ¶çš„å¦ä¸€ç§ç­–ç•¥ã€‚å®ƒé€šè¿‡æŸ¥è¯¢æºæ–‡æœ¬å’Œæ‘˜è¦ä¸­çš„è¯ï¼Œæ¥ç¡®å®šå“ªäº›è¯æˆ–çŸ­è¯­æ˜¯é‡è¦çš„ã€‚è”åˆæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿè¯†åˆ«å‡ºè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ½œåœ¨è”ç³»ï¼Œä»è€Œç”Ÿæˆæ›´å…·æœ‰æ„ä¹‰çš„è¾“å‡ºã€‚

## 3.5 æŸå¤±å‡½æ•°è®¾è®¡
æŸå¤±å‡½æ•°å®šä¹‰äº†ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡ã€‚ç”±äºç”Ÿæˆæ¨¡å‹çš„è¾“å…¥è¾“å‡ºéƒ½æ˜¯åºåˆ—ï¼Œæ‰€ä»¥æŸå¤±å‡½æ•°å¯ä»¥é‡‡ç”¨seq2seqæ¨¡å‹å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

é™¤äº†äº¤å‰ç†µæŸå¤±å‡½æ•°ä¹‹å¤–ï¼Œæœ¬æ–‡è¿˜è®¾è®¡äº†å¦å¤–ä¸€ç§æŸå¤±å‡½æ•°ï¼Œå³æ‘˜è¦çš„æƒ©ç½šé¡¹ã€‚ç”±äºç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€å®šåŒ…å«å®Œæ•´çš„å¥å­ï¼Œå› æ­¤ä¸åº”è¯¥å…¨é¢åœ°åæ˜ è¾“å…¥çš„ç»†èŠ‚ã€‚å› æ­¤ï¼Œæ‘˜è¦çš„æƒ©ç½šé¡¹è®¾è®¡æˆé¼“åŠ±ç”Ÿæˆçš„æ‘˜è¦å°½å¯èƒ½æ¥è¿‘è¾“å…¥çš„æ‘˜è¦ã€‚

## 3.6 ç”Ÿæˆè¿‡ç¨‹
å½“æ¨¡å‹æ¥æ”¶åˆ°è¾“å…¥æ–‡æœ¬æ—¶ï¼Œé¦–å…ˆå°†å…¶è½¬æ¢æˆå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡encoderç”Ÿæˆæ•´ä½“çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚æ¥ç€ï¼Œç”Ÿæˆæ¨¡å‹ä½¿ç”¨å‰é¢çš„éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æœºåˆ¶æ¥ç”Ÿæˆç¬¬ä¸€ä¸ªè¯æˆ–çŸ­è¯­ã€‚ç„¶åï¼Œé€šè¿‡è§£ç å™¨ç”Ÿæˆä¸‹ä¸€ä¸ªè¯æˆ–çŸ­è¯­ã€‚ç”±äºæ¨¡å‹é‡‡ç”¨æŒ‡é’ˆæœºåˆ¶ï¼Œæ‰€ä»¥ä¼šæ ¹æ®å†å²ç”Ÿæˆçš„è¯æˆ–çŸ­è¯­æ¥å†³å®šä¸‹ä¸€ä¸ªè¾“å‡ºã€‚

æ¨¡å‹è®­ç»ƒæ—¶ï¼Œéœ€è¦æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼Œå³é¼“åŠ±æ¨¡å‹ç”Ÿæˆçš„æ‘˜è¦æ¥è¿‘äºè¾“å…¥çš„æ‘˜è¦ã€‚åŒæ—¶ï¼Œç”±äºç”Ÿæˆçš„æ‘˜è¦å¯èƒ½ä¸è¾“å…¥çš„æ‘˜è¦ç›¸å·®ç”šè¿œï¼Œå› æ­¤éœ€è¦è®¾è®¡ä¸€ç§æƒ©ç½šé¡¹ï¼Œé¼“åŠ±ç”Ÿæˆçš„æ‘˜è¦ä¸è¾“å…¥çš„æ‘˜è¦å°½é‡æ¥è¿‘ã€‚

# 4.å…·ä½“ä»£ç å®ä¾‹å’Œè§£é‡Šè¯´æ˜
## 4.1 æ•°æ®é›†ä¸‹è½½
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Hugging Faceçš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥è‡ªå·±æ”¶é›†æ–‡æœ¬æ•°æ®ã€‚è¿™é‡Œæˆ‘ä»¬é€‰ç”¨<NAME>å¼€åˆ›æ€§çš„Multi30kæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«è‹±æ–‡ç»´åŸºç™¾ç§‘çš„æ‰€æœ‰æ–‡ç« ï¼ŒåŒ…æ‹¬å¤è¯—ã€æ‚²å‰§ç”µå½±è„šæœ¬ã€ç§‘å¹»å°è¯´ç­‰ç­‰ã€‚

```python
from datasets import load_dataset

dataset = load_dataset('multi30k', 'en')
train_data = dataset['train']['text'][:10] # only use the first ten articles for demo purpose
val_data = dataset['validation']['text'][:10] # only use the first ten articles for demo purpose
test_data = dataset['test']['text'][:10] # only use the first ten articles for demo purpose
```

## 4.2 æ•°æ®é¢„å¤„ç†
æ•°æ®é¢„å¤„ç†åŒ…å«å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡å‰²ï¼Œå°†é•¿æ–‡æœ¬åˆ‡å‰²æˆé€‚åˆæ¨¡å‹è¾“å…¥çš„é•¿åº¦ï¼ŒåŒæ—¶å°†æ–‡æœ¬æ‹†åˆ†æˆå¤šä¸ªçŸ­å¥ã€‚

```python
import re
from transformers import RobertaTokenizer

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def preprocess_function(examples):
    inputs = []
    targets = []

    for text in examples:
        sentences = re.split('[.!?]', text)[:-1] # split into multiple sentences
        tokens = [tokenizer.tokenize(sent) for sent in sentences]
        
        max_len = max([len(t) for t in tokens]) + 2 # add two to include special tokens

        input_tokens = [[tokenizer.cls_token_id]+[tokenizer.pad_token_id]*max_len+[tokenizer.sep_token_id]]*len(sentences)
        target_tokens = [[tokenizer.pad_token_id]*max_len+[tokenizer.sep_token_id]]*len(sentences)
        
        for i, sentence in enumerate(sentences):
            tokenized_sentence = tokenizer.encode(sentence, truncation=True, max_length=max_len)[1:-1]
            
            input_tokens[i][:len(tokenized_sentence)+2] = tokenized_sentence
            target_tokens[i][1:len(tokenized_sentence)+1] = tokenized_sentence
            
        inputs += [" ".join([str(t) for t in token]) for token in input_tokens]
        targets += [" ".join([str(t) for t in token]) for token in target_tokens]
    
    return {'inputs': inputs, 'targets': targets}

train_data = train_data.map(preprocess_function, batched=True, batch_size=-1)
val_data = val_data.map(preprocess_function, batched=True, batch_size=-1)
test_data = test_data.map(preprocess_function, batched=True, batch_size=-1)
```

## 4.3 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Hugging Faceçš„Transformersåº“ï¼Œæ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å¯¹æ¨¡å‹è¿›è¡Œfine-tuningã€‚

```python
from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer

model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=1)

training_args = TrainingArguments(output_dir='./results',          # output directory
                                    learning_rate=2e-5,              # learning rate
                                    per_device_train_batch_size=16,   # batch size for training
                                    per_device_eval_batch_size=16,    # batch size for evaluation
                                    num_train_epochs=3,               # number of training epochs
                                    weight_decay=0.01,               # strength of weight decay
                                    warmup_steps=500,                # number of warmup steps 
                                    logging_dir='./logs',            # directory for storing logs
                                    logging_steps=10,
                                    save_total_limit=3)               # limit the total amount of checkpoints

trainer = Trainer(
                        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
                        args=training_args,                  # training arguments, defined above
                        train_dataset=train_data,             # training dataset
                        eval_dataset=val_data                 # evaluation dataset
                    )
```

## 4.4 æ‰§è¡Œè®­ç»ƒ
è®­ç»ƒæ¨¡å‹éœ€è¦èŠ±è´¹ä¸€æ®µæ—¶é—´ã€‚å½“æ¨¡å‹è®­ç»ƒå®Œæ¯•ï¼Œä¿å­˜æ¨¡å‹å‚æ•°å’Œæ¨¡å‹é…ç½®ã€‚

```python
trainer.train()
trainer.save_model('my_model')
```

## 4.5 æ‰§è¡Œæ¨ç†
æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è¯»å–æµ‹è¯•æ•°æ®ï¼Œå¹¶ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¥ç”Ÿæˆæ‘˜è¦ã€‚

```python
from transformers import pipeline

generator = pipeline('summarization', model='my_model', tokenizer='roberta-base')

for article in test_data['inputs']:
  print('\n\nArticle:', article)
  
  summary = generator(article, min_length=70, max_length=100, do_sample=False)[0]['summary_text']

  print('\nSummary:', summary)
```

è¾“å‡ºç¤ºä¾‹ï¼š

```
Article: From a family that's become less accepting of her new boss, Lucy is adamant about taking on any challenge she faces in her future role as head of her school board. But what will it take to replace this highly visible and prestigious position in an environment where many other heads are struggling to meet societal expectations? 

Lucy Graduated from Indiana University with a degree in Finance. She was promoted to assistant principal after six years. Her manager wanted someone who had experience running schools or teachers and could see the value of different approaches. After graduation, she joined John Perry Academy, which has been at the forefront of education reform since its founding by advocating for social justice principles and investing in low-income students. As senior administrator, she led efforts to promote equity, community involvement, and inclusion of all students within the system. The result was widespread support for academics and educators, while funds were invested in scholarships and grants meant to help provide financial aid for low-income students. However, the charismatic leader left school at age twenty, leaving behind Lucyâ€™s daughter and her husband, both professionals who have worked closely with young children over the past decades. Now, five years later, there are more challenges than ever before for high school principals in Baltimore. In an increasingly competitive and demanding world, how can a busy midshipman handle such responsibilities without falling victim to the stereotypes of ruthlessness and obsession?