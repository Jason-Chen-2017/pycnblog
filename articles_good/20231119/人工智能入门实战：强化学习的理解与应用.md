                 

# 1.èƒŒæ™¯ä»‹ç»


å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨è®©æ™ºèƒ½ä½“è‡ªåŠ¨å­¦ä¹ ä»ŽçŽ¯å¢ƒä¸­èŽ·å–å¥–åŠ±å¹¶æŒ‰ç…§é¢„å®šä¹‰çš„ç­–ç•¥è¿›è¡Œå†³ç­–ï¼Œä»¥èŽ·å¾—æœ€å¤§åŒ–çš„æ”¶ç›Šã€‚å¼ºåŒ–å­¦ä¹ åœ¨ç”µè„‘æ¸¸æˆã€æœºå™¨äººæŽ§åˆ¶ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å¯¹æŠ—æ€§é—®é¢˜ä¸Šè¡¨çŽ°ä¼˜å¼‚ã€‚éšç€æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å‘å±•å’Œå¼€æºå·¥å…·çš„å‡ºçŽ°ï¼Œå¼ºåŒ–å­¦ä¹ çš„ç ”ç©¶å·¥ä½œä¹Ÿè¶Šæ¥è¶Šçƒ­é—¹ã€‚æœ¬æ–‡å°†ä»¥â€œçŽ©æ‰‘å…‹â€ä¸ºä¾‹ï¼Œä»‹ç»å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸Žæ•°å­¦æ¨¡åž‹çš„åŸºæœ¬çŸ¥è¯†ï¼Œå¹¶ç”¨ä»£ç å®žçŽ°ä¸€ä¸ªç®€å•çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¥è®­ç»ƒä¸€åªç»è¿‡è¶…çº§æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•è®­ç»ƒå‡ºæ¥çš„éšæœºç‰Œé€‰æ‰‹ã€‚é€šè¿‡é˜…è¯»æœ¬æ–‡ï¼Œè¯»è€…å¯ä»¥äº†è§£ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Œä»¥åŠå¦‚ä½•ç”¨å¼ºåŒ–å­¦ä¹ è§£å†³å®žé™…é—®é¢˜ã€‚
# 2.æ ¸å¿ƒæ¦‚å¿µä¸Žè”ç³»
å¼ºåŒ–å­¦ä¹ ä¸»è¦ç”±å››ä¸ªè¦ç´ æž„æˆï¼šçŽ¯å¢ƒï¼ˆEnvironmentï¼‰ã€æ™ºèƒ½ä½“ï¼ˆAgentï¼‰ã€åŠ¨ä½œï¼ˆActionï¼‰ã€å¥–åŠ±ï¼ˆRewardï¼‰ã€‚å…¶ä¸­ï¼ŒçŽ¯å¢ƒæ˜¯ä¸€ä¸ªå¤–éƒ¨ä¸–ç•Œï¼Œæ™ºèƒ½ä½“æ˜¯å¯ä»¥ä¸ŽçŽ¯å¢ƒäº¤äº’çš„ä¸»ä½“ï¼Œå¯ä»¥æ‰§è¡ŒæŸç§åŠ¨ä½œï¼Œä»Žè€Œæ”¹å˜è‡ªèº«çŠ¶æ€ã€‚æ™ºèƒ½ä½“ä¸ŽçŽ¯å¢ƒçš„äº¤äº’æ–¹å¼å°±æ˜¯é€šè¿‡çŽ¯å¢ƒç»™å‡ºçš„åé¦ˆä¿¡å·ï¼Œå³å¥–åŠ±ã€‚

## æ™ºèƒ½ä½“
æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªå¯ä»¥ä¸ŽçŽ¯å¢ƒäº¤äº’çš„ä¸»ä½“ï¼Œå®ƒå…·æœ‰å†³ç­–èƒ½åŠ›å’Œè¡ŒåŠ¨èƒ½åŠ›ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ™ºèƒ½ä½“é€šå¸¸è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªå‡½æ•°æˆ–ä¸€ä¸ªæ¨¡åž‹ï¼ŒæŽ¥å—å½“å‰çš„çŽ¯å¢ƒçŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºæœ€ä½³çš„åŠ¨ä½œã€‚

## åŠ¨ä½œ
åŠ¨ä½œæ˜¯æŒ‡æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œã€‚åœ¨ä¸€èˆ¬çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼ŒåŠ¨ä½œä¸€èˆ¬å–å†³äºŽæ™ºèƒ½ä½“å†…éƒ¨çŠ¶æ€ï¼Œå³æ™ºèƒ½ä½“å¯èƒ½é‡‡å–ä¸åŒçš„åŠ¨ä½œï¼Œä½†æ˜¯å®ƒä»¬çš„æ‰§è¡Œè¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œåœ¨çŽ©æ‰‘å…‹æ¸¸æˆä¸­ï¼Œæ™ºèƒ½ä½“çš„åŠ¨ä½œå¯ä»¥æ˜¯é€‰æ‹©ä¸åŒèŠ±è‰²çš„å¡ç‰‡æˆ–è€…ä¸åšä»»ä½•åŠ¨ä½œã€‚

## å¥–åŠ±
å¥–åŠ±æ˜¯æŒ‡çŽ¯å¢ƒç»™äºˆæ™ºèƒ½ä½“çš„åé¦ˆä¿¡æ¯ï¼Œå®ƒæ˜¯æ™ºèƒ½ä½“å­¦ä¹ ã€æ”¹å–„è¡Œä¸ºçš„é‡è¦å› ç´ ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–ç´¯è®¡å¥–èµã€‚åœ¨å…·ä½“çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œå¥–åŠ±å¯ä»¥æ˜¯æŒ‡ç¤ºä»»åŠ¡å®Œæˆæƒ…å†µçš„æŒ‡æ ‡ï¼ˆå¦‚æ¸¸æˆå¾—åˆ†ï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯è¡¡é‡ä»»åŠ¡å¤æ‚åº¦ã€å¥–åŠ±ç¨€ç¼ºæ€§ã€é¿å…å±€éƒ¨æœ€ä¼˜ä»¥åŠæœŸæœ›ç´¯ç§¯å›žæŠ¥çš„åå¥½ç¨‹åº¦ç­‰å› ç´ ã€‚å¥–åŠ±æ—¢å¯ä»¥ç›´æŽ¥ç»™äºˆæ™ºèƒ½ä½“ï¼Œä¹Ÿå¯ä»¥é—´æŽ¥åœ°å½±å“æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œæ¯”å¦‚è®­ç»ƒä¸€ä¸ªå¼ºå¤§çš„æ¨¡åž‹å¯èƒ½éœ€è¦æ”¶é›†å¾ˆå¤šçš„æ•°æ®æ‰èƒ½å¾—åˆ°è¶³å¤Ÿæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ã€‚

## æ¨¡åž‹
æ¨¡åž‹æ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ä¸€ç§æ•°å­¦å»ºæ¨¡æ–¹æ³•ã€‚å®ƒç”¨æ¥æè¿°æ™ºèƒ½ä½“ä¸ŽçŽ¯å¢ƒä¹‹é—´çš„å…³ç³»ï¼ŒåŒ…æ‹¬çŠ¶æ€è½¬ç§»æ¦‚çŽ‡åˆ†å¸ƒã€çŠ¶æ€ä»·å€¼å‡½æ•°ã€åŠ¨ä½œä»·å€¼å‡½æ•°ã€çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ç­‰ã€‚å…¶ä¸­ï¼ŒçŠ¶æ€è½¬ç§»æ¦‚çŽ‡åˆ†å¸ƒè¡¨å¾äº†æ™ºèƒ½ä½“ä»Žå½“å‰çŠ¶æ€åˆ°ä¸‹ä¸€çŠ¶æ€çš„è½¬æ¢æ¦‚çŽ‡ï¼›çŠ¶æ€ä»·å€¼å‡½æ•°å’ŒåŠ¨ä½œä»·å€¼å‡½æ•°åˆ†åˆ«ä»£è¡¨äº†æ™ºèƒ½ä½“å¤„äºŽæŸä¸ªçŠ¶æ€æ—¶äº§ç”Ÿçš„æœŸæœ›å›žæŠ¥å’Œæ ¹æ®å½“å‰ç­–ç•¥äº§ç”Ÿçš„æœ€å¤§å›žæŠ¥ã€‚

## ä»·å€¼å‡½æ•°
ä»·å€¼å‡½æ•°æ˜¯ä¸€ä¸ªéžå¸¸é‡è¦çš„æ¦‚å¿µï¼Œå®ƒåˆ»ç”»çš„æ˜¯åœ¨ä¸€ä¸ªç»™å®šçŠ¶æ€ä¸‹ï¼Œæ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œæ‰€äº§ç”Ÿçš„ä»·å€¼ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œä»·å€¼å‡½æ•°å¾€å¾€ä¾èµ–äºŽçŠ¶æ€è½¬ç§»æ¦‚çŽ‡åˆ†å¸ƒå’Œå¥–åŠ±ï¼Œé€šå¸¸å¯ä»¥ç”¨ä»¥ä¸‹çš„å½¢å¼è¡¨ç¤ºï¼š

V(s)=E[R|s] + \gamma E[V(sâ€™)|s] ï¼ˆè´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼‰

è¿™é‡Œï¼ŒV(s)è¡¨ç¤ºæ™ºèƒ½ä½“å¤„äºŽçŠ¶æ€sæ—¶çš„ä»·å€¼ï¼›E[R|s]è¡¨ç¤ºæ™ºèƒ½ä½“ä»ŽçŠ¶æ€så¼€å§‹ï¼Œä¸€ç›´åˆ°ç»ˆæ­¢ï¼Œæ‰€èŽ·å¾—çš„ç´¯ç§¯å¥–åŠ±æœŸæœ›ï¼›\gamma è¡¨ç¤ºå»¶è¿Ÿæƒ©ç½šå› å­ï¼ˆä¹Ÿç§°ä¸ºæŠ˜æ‰£å› å­ï¼‰ï¼Œå®ƒæŽ§åˆ¶äº†æœªæ¥å¥–åŠ±å€¼å¯¹å½“å‰å¥–åŠ±å€¼çš„å½±å“ç¨‹åº¦ï¼›E[V(sâ€™)|s]è¡¨ç¤ºæ™ºèƒ½ä½“ä»ŽçŠ¶æ€sè½¬ç§»åˆ°çŠ¶æ€s'æ—¶çš„ä»·å€¼æœŸæœ›ã€‚

# 3.æ ¸å¿ƒç®—æ³•åŽŸç†å’Œå…·ä½“æ“ä½œæ­¥éª¤ä»¥åŠæ•°å­¦æ¨¡åž‹å…¬å¼è¯¦ç»†è®²è§£
## Q-learningç®—æ³•
Q-learningç®—æ³•æ˜¯ä¸€ç§åŸºäºŽè¡¨æ ¼çš„æ–¹æ³•ï¼Œå±žäºŽå€¼è¿­ä»£ç®—æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯å»ºç«‹Qå‡½æ•°ï¼Œè®°å½•äº†æ¯ä¸€ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹çš„ä»·å€¼ï¼Œå¹¶æ ¹æ®å·²çŸ¥ä»·å€¼å‡½æ•°å’ŒçŽ¯å¢ƒï¼Œé‡‡ç”¨è´ªå©ªç­–ç•¥ï¼ˆå³ä½¿å½“å‰ç­–ç•¥ä¸ä¸€å®šæ˜¯æœ€ä¼˜ç­–ç•¥ï¼‰æ¥é€‰æ‹©åŠ¨ä½œã€‚Qå‡½æ•°å¯ä»¥è®¤ä¸ºæ˜¯çŽ¯å¢ƒå’Œæ™ºèƒ½ä½“çš„å†…åœ¨å…³è”ã€‚

Q-learningç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. åˆå§‹åŒ–Qå‡½æ•°ï¼šç»™æ¯ä¸ªçŠ¶æ€åŠ¨ä½œå¯¹èµ‹äºˆåˆå§‹å€¼ï¼Œé€šå¸¸ä½¿ç”¨0ä½œä¸ºåˆå§‹åŒ–å€¼ã€‚
2. å¾ªçŽ¯éåŽ†å¤šæ¬¡:
   a. åœ¨æ¯è½®è¿­ä»£å¼€å§‹å‰ï¼Œæ›´æ–°æ™ºèƒ½ä½“ç­–ç•¥ã€‚
   b. åœ¨å½“å‰ç­–ç•¥ä¸‹ï¼ŒæŒ‰ç…§è´ªå¿ƒç­–ç•¥é€‰å–åŠ¨ä½œAã€‚
   c. æ‰§è¡ŒåŠ¨ä½œAå¹¶å¾—åˆ°å¥–åŠ±rå’Œä¸‹ä¸€çŠ¶æ€s'.
   d. æ›´æ–°Qå‡½æ•°ï¼š
      i. å¦‚æžœä¸å­˜åœ¨Q(s',A)ï¼Œåˆ™å°†Q(s',A)è®¾ä¸º0.
      ii. æ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ›´æ–°Q(s,A):
         V(s') = E[R|s'] + gamma * max_a Q(s',a) ï¼ˆç¬¬ä¸€é¡¹ä¸ºæ”¶ç›Šï¼Œç¬¬äºŒé¡¹ä¸ºä»·å€¼æœŸæœ›ï¼‰
   e. æ›´æ–°ç­–ç•¥ï¼šå½“ä¸‹ä¸€æ­¥ç§»åŠ¨åˆ°çŠ¶æ€s'æ—¶ï¼Œé‡æ–°è®¡ç®—ç­–ç•¥ï¼Œä½¿å¾—æ™ºèƒ½ä½“æœ‰æ›´å¥½çš„è¡Œä¸ºç­–ç•¥ã€‚
   f. å›žåˆ°ç¬¬2æ­¥ç»§ç»­è¿­ä»£ã€‚

## è¶…å‚æ•°è®¾ç½®
è¶…å‚æ•°åŒ…æ‹¬åŠ¨ä½œç©ºé—´å¤§å°ã€çŠ¶æ€ç©ºé—´å¤§å°ã€å­¦ä¹ çŽ‡ã€æŠ˜æ‰£å› å­ã€æ›´æ–°æ¬¡æ•°ã€æŽ¢ç´¢ç³»æ•°ç­‰ã€‚ç”±äºŽQ-learningçš„è¿­ä»£æ¬¡æ•°å¾ˆå¤§ï¼Œä¸ºäº†é˜²æ­¢é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œéœ€è¦è®¾å®šåˆé€‚çš„åœæ­¢æ¡ä»¶ã€‚åŒæ—¶ï¼Œè¿˜éœ€è¦é€‰æ‹©åˆé€‚çš„å­¦ä¹ çŽ‡ã€æŠ˜æ‰£å› å­å’Œæ›´æ–°æ¬¡æ•°æ¥ä¿è¯ç®—æ³•æ”¶æ•›ã€‚

## åŠ¨ä½œå€¼å‡½æ•°
åœ¨Q-learningç®—æ³•ä¸­ï¼ŒQå‡½æ•°ç”¨çš„æ˜¯è¡¨æ ¼æ³•å­˜å‚¨çŠ¶æ€åŠ¨ä½œå¯¹çš„ä»·å€¼ã€‚ç„¶è€Œï¼Œåœ¨å®žé™…åº”ç”¨ä¸­ï¼Œå¾€å¾€å­˜åœ¨ä¸€äº›ä¸å¯è§‚æµ‹çš„çŠ¶æ€ï¼Œæ— æ³•ç”¨çŠ¶æ€åŠ¨ä½œå¯¹ç›´æŽ¥è¡¨ç¤ºã€‚å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨åŠ¨ä½œå€¼å‡½æ•°ï¼ˆAction Value Functionï¼‰æ¥è¿‘ä¼¼è¡¨ç¤ºçŠ¶æ€çš„ä»·å€¼ï¼Œé€šè¿‡ä¼°è®¡åŠ¨ä½œçš„åŽç»­çŠ¶æ€å€¼ä¸Žå½“å‰çŠ¶æ€å€¼ä¹‹é—´çš„å·®è·æ¥è¿‘ä¼¼ä»·å€¼ã€‚å¯¹äºŽæ¯ä¸ªçŠ¶æ€så’ŒåŠ¨ä½œaï¼ŒåŠ¨ä½œå€¼å‡½æ•°Qaè¡¨ç¤ºçš„æ˜¯æ™ºèƒ½ä½“ä»¥åŠ¨ä½œaè¿›å…¥çŠ¶æ€sä¹‹åŽï¼Œæ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ä»·å€¼ä¹‹å’Œï¼Œå³:

Qa(s,a)=âˆ‘_{s'} P(s'|s,a)[r+Î³V(s')] (è’™ç‰¹å¡æ´›æ–¹æ³•)

## DQNç®—æ³•
DQNç®—æ³•æ˜¯æ·±åº¦ç¥žç»ç½‘ç»œï¼ˆDNNï¼‰ç»“åˆQ-learningç®—æ³•çš„å˜ä½“ï¼Œå®ƒå¯ä»¥è§£å†³è¿žç»­åŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚DQNçš„åŸºæœ¬æ€è·¯æ˜¯åˆ©ç”¨æ·±åº¦ç¥žç»ç½‘ç»œé€æ­¥é¢„æµ‹å„ä¸ªåŠ¨ä½œçš„ä»·å€¼ï¼Œç„¶åŽä¾æ®Q-learningç®—æ³•çš„æ€è·¯ï¼Œé€‰æ‹©ä»·å€¼æœ€é«˜çš„åŠ¨ä½œæ¥æ›´æ–°ç­–ç•¥ã€‚

DQNç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. ä½¿ç”¨DQNç½‘ç»œæ‹ŸåˆQ-learningç›®æ ‡å‡½æ•°ã€‚
2. åˆå§‹åŒ–Qå‡½æ•°ï¼šQå‡½æ•°ç”¨DQNç½‘ç»œä¼°è®¡çš„åŠ¨ä½œå€¼å‡½æ•°æ›¿ä»£ï¼Œåˆå§‹å€¼ä¹Ÿå¯ä»¥è®¾ç½®ä¸º0ã€‚
3. æŒ‰ç…§DQNç½‘ç»œçš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œAã€‚
4. æ‰§è¡ŒåŠ¨ä½œAå¹¶å¾—åˆ°å¥–åŠ±rå’Œä¸‹ä¸€çŠ¶æ€s'.
5. å°†s,a,r,s'çš„ç»éªŒä¿å­˜åˆ°è®°å¿†åº“ä¸­ã€‚
6. ä»Žè®°å¿†åº“ä¸­æŠ½å–ä¸€æ‰¹ç»éªŒï¼Œç”¨DQNç½‘ç»œå­¦ä¹ çŠ¶æ€è½¬ç§»æ¦‚çŽ‡åˆ†å¸ƒå’Œå¥–åŠ±ã€‚
7. é‡å¤æ­¥éª¤3-6ï¼Œç›´è‡³è¾¾åˆ°è®¾å®šçš„è®­ç»ƒæ—¶é—´ã€‚

## è¶…å‚æ•°è®¾ç½®
è¶…å‚æ•°åŒ…æ‹¬ç¥žç»ç½‘ç»œç»“æž„ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ çŽ‡ã€åŠ¨ä½œé€‰æ‹©æ–¹æ³•ã€åŠ¨ä½œè´ªå©ªç³»æ•°ã€æŸå¤±å‡½æ•°ã€ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘çŽ‡ã€æœ€å°æ ·æœ¬æ•°é‡ç­‰ã€‚è¶…å‚æ•°è°ƒä¼˜çš„å…³é”®æ˜¯å¯»æ‰¾æœ€åˆé€‚çš„å‚æ•°ç»„åˆï¼Œèƒ½ä½¿å¾—æ¨¡åž‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ çŽ¯å¢ƒï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨é‡åˆ°æ–°æƒ…å†µæ—¶å¿«é€Ÿå“åº”ã€‚

# 4.å…·ä½“ä»£ç å®žä¾‹å’Œè¯¦ç»†è§£é‡Šè¯´æ˜Ž
## ä»£ç å®žçŽ°Q-learningçŽ©æ‰‘å…‹
æœ¬èŠ‚åŸºäºŽKeraså’ŒOpenAI Gymåº“ï¼Œç”¨Q-learningç®—æ³•æ¥è®­ç»ƒä¸€åªç®€å•ç‰Œé€‰æ‰‹ï¼ŒçŽ©ä¸€è½®æ ‡å‡†æ‰‘å…‹æ¸¸æˆã€‚å…ˆå¼•å…¥ç›¸å…³çš„åŒ…ï¼š

``` python
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from gym.envs.registration import register
```

é¦–å…ˆå®šä¹‰ç‰Œé¢æ•°ç›®ï¼Œç”¨æ•°å­—0~9æ¥ä»£è¡¨ä¸åŒçš„ç‰Œï¼Œé»‘æ¡ƒAã€é»‘æ¡ƒKå’Œçº¢æ¡ƒJã€çº¢æ¡ƒQã€æ–¹å—2ã€æ–¹å—8ã€é’»çŸ³Xã€æ˜Ÿå·*å’ŒçŽ‹ç‰ŒWï¼š

``` python
num_card = 13
```

å®šä¹‰ç‰Œåž‹åˆ†ç±»å­—å…¸ï¼š

```python
suit_dict = {'C': 'â™ ï¸',
             'D': 'â™¦ï¸',
             'H': 'â™¥ï¸',
             'S': 'â™£ï¸'}
rank_dict = {str(i): str(i) for i in range(2, 11)}
rank_dict['A'] = 'ðŸ‚¡'
rank_dict['K'] = 'ðŸ‚®'
rank_dict['J'] = 'ðŸ‚«'
rank_dict['Q'] = 'ðŸ‚¿'
rank_dict['T'] = 'ðŸ‚¾'
rank_dict['2'] = '2ï¸âƒ£'
rank_dict['8'] = '8ï¸âƒ£'
rank_dict['X'] = 'ðŸƒ'
rank_dict['*'] = '*'
rank_dict['W'] = ''
```

åˆ›å»ºå¼ºåŒ–å­¦ä¹ çš„OpenAI GymçŽ¯å¢ƒï¼Œå¹¶æ³¨å†Œç‰Œé€‰æ‰‹å‚ä¸Žæ¸¸æˆã€‚è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªé»‘åº•çº¢å­—çš„æ‰‘å…‹æ¸¸æˆï¼Œç”¨çº¢è‰²ä»£è¡¨è¯¥çŽ©å®¶çš„ç‰Œï¼Œé»‘è‰²ä»£è¡¨å¯¹æ‰‹çš„ç‰Œã€‚

``` python
register(id='Poker-v0',
         entry_point='gym.envs.toy_text:DiscreteEnv',
         kwargs={'nS': num_card ** 4,
                 'nA': num_card ** 2,
                 'P': None,
                 'isd': lambda: np.random.uniform(size=num_card),
                'rewards': {},
                'reset_probs': {}},
         max_episode_steps=np.inf)


class BlackjackHand():
    def __init__(self):
        self.cards = []

    def add_card(self, card):
        if len(self.cards) == 2:
            raise ValueError("Blackjack is over")

        self.cards.append(card)

    def get_value(self):
        value = sum([card[0] for card in self.cards])
        
        if 'A' in [card[1] for card in self.cards]:
            values = [1, 11]
            Acount = list(filter(lambda x: x=='A', [card[1] for card in self.cards]))

            while True:
                value += random.choice(values)

                if Acount and value > 21:
                    value -= 10
                    Acount.pop()

                    if not Acount:
                        break
                    
        return min(max(value, 1), 21)

    def get_reward(self):
        value = self.get_value()

        if value >= 21:
            reward = -1
        elif any(['*' in card[1] for card in self.cards]):
            reward = -1
        else:
            reward = 0

        return reward
```

å®šä¹‰Q-learningç®—æ³•è®­ç»ƒæ¨¡åž‹ï¼š

``` python
def qlearn(env, episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.1):
    nS = env.observation_space.n
    nA = env.action_space.n
    
    model = Sequential()
    model.add(Dense(128, input_dim=nS))
    model.add(Dense(nA, activation='linear'))
    model.compile(loss='mse', optimizer='adam')
    
    Q = np.zeros((nS, nA))
    for episode in range(episodes):
        state = env.reset()

        hand = BlackjackHand()
        done = False

        # decide action using epsilon greedy method
        if np.random.rand() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state, :])

        while not done:
            next_state, reward, done, _ = env.step(action)

            # update Q function
            best_next_action = np.argmax(Q[next_state, :])
            
            td_target = reward + gamma * Q[next_state][best_next_action] 
            td_error = td_target - Q[state][action] 
            
            Q[state][action] += alpha * td_error 

            state = next_state

            # decide action using epsilon greedy method
            if np.random.rand() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(Q[state, :])

        print('Episode:', episode, '| Score:', hand.get_value())
        
    return Q, hand
    
env = gym.make('Poker-v0')
Q, hand = qlearn(env)
print('Final score:', hand.get_value(), '\n')
```

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°Qå‡½æ•°å’Œæœ€ç»ˆçš„ç‰Œåˆ†æ•°æ‰“å°å‡ºæ¥ã€‚