
作者：禅与计算机程序设计艺术                    

# 1.简介
         
机器翻译（MT）作为一个重要的自然语言处理任务之一，具有广泛的应用场景。目前，机器翻译模型主要包括统计概率模型、非参与型模型、基于神经网络的模型等。本文将对卷积神经网络(CNN)在机器翻译中的应用进行综述。

传统的机器翻译方法通常采用统计或规则方法构建翻译模型。例如，统计方法可以基于计数词频、语言模型等，通过统计概率来计算翻译概率。而非参与型方法，如翻译编辑距离模型、基于感知机的统计机器翻译模型等，则利用强大的特征抽取能力和基于规则的翻译逻辑来构造翻译模型。近年来，基于神经网络的方法也逐渐受到关注。由于神经网络的高度自动化特性及其良好的表达能力，它们在大量文本数据上的预训练与微调方面都取得了突破性的进展。最近，越来越多的研究工作试图将卷积神经网络(CNN)引入到机器翻译中。CNN能够学习到高级的表征特征，并能有效地捕获长距离依赖关系和结构化信息。CNN在NLP领域已经得到了广泛的应用，如图像分类、序列标注等任务。因此，CNN在机器翻译中的应用可以帮助提升性能、降低成本，并实现更准确、更优雅的翻译结果。

CNN在机器翻译中的应用主要包括三个方面：编码器-解码器框架、单步解码器、连贯解码器。下面我们对这三个方面逐一进行阐述。

1.编码器-解码器框架
传统的机器翻译方法一般分为编码阶段和解码阶段，即输入序列->编码器->隐藏状态序列->解码器->输出序列。其中编码器用于从输入序列中提取高层次的表示，隐藏状态序列代表待翻译的句子的内部结构，解码器用于从隐藏状态序列中生成翻译后的句子。

CNN被设计用来解决语义建模和局部重排序的问题，并且通过自动形成特征空间来区分不同位置之间的关系。这样，CNN就可以作为编码器，用于从源语言句子中提取高层次的特征表示。在隐藏状态序列中，每一步只能看到一部分源码序列的信息，因此只能通过由上一步产生的隐状态向下传递。这种上一层到当前层信息的通信被称为上下文依赖关系。

为了解决解码过程中的循环依赖问题，连贯解码器(CaR Decoder)被提出，它在解码过程中使用注意力机制来融合不同解码路径的信息。

相比于传统的编码-解码方法，CNN在编码时可以同时考虑整个源语句，而在解码时仅需要考虑当前目标词对应的上下文信息即可。

2.单步解码器
传统的机器翻译方法使用独立的LSTM单元作为解码器，每次翻译一个单词。但是，这种方法容易受到梯度消失或爆炸现象的影响，导致训练困难、推断困难。

为了克服这一缺陷，Singh等人提出了单步解码器(Stepwise Decoder)，该方法在一次解码迭代中同时翻译多个单词。它允许LSTM单元在不改变LSTM状态的情况下，接受完整的输入序列并同时生成输出序列。使用这种方法，LSTM单元一次只能处理一部分输入信息，但是不需要反复更新状态，减少了梯度消失或爆炸的问题。

单步解码器同样适用于其他RNN结构，比如GRU等。但是，使用LSTM的单步解码器在训练和推断时间上均优于其他RNN结构。

3.连贯解码器
单步解码器虽然能避免梯度消失或爆炸现象，但仍然存在重复翻译相同单词的问题。为了解决这个问题，Bahdanau等人提出了连贯解码器(Concise Decoder)。该方法不仅保证在序列结束时生成正确翻译，而且还在连续生成翻译时保持一致性。

连贯解码器最初的提出者就是贺建奎。他提出了一种名为贪心搜索(Greedy Search)的方法，即每次只选择概率最大的翻译结果，直至遇到EOS标记停止。贪心搜索方法的效果不好，因为它没有考虑到整体翻译计划的全局性，可能造成不必要的错误翻译。

为了克服贪心搜索方法的局限性，Kiros等人提出了一种动态规划(Dynamic Programming)的方法，即使用Viterbi算法来求解最优翻译路径。Viterbi算法维护一个状态序列概率的动态转移矩阵，对于每个位置，按照所有可能前面的状态组合计算到当前位置的所有后续状态的概率，然后找到概率最大的组合路径作为最优路径。因此，Viterbi算法可以根据历史信息优化翻译策略，使得翻译结果尽可能准确。

实践中，双向LSTM+贪心搜索的方法效果较好。它使用两条LSTM编码器，分别对源语句左右两个方向的表示进行建模；然后，将这两条编码器的输出拼接起来作为候选翻译的输入；最后，使用贪心搜索算法从候选翻译集中选择概率最大的翻译作为最终的翻译结果。

# 2.相关术语说明
- 源语言(Source Language):英文
- 目标语言(Target Language):中文
- 文本编码(Text Encoding):UTF-8
- 句子对(Sentence Pair):一对一的英文-中文句子组成的数据
- 概率分布(Probability Distribution):某变量的取值范围和出现的概率构成的函数。如汉字到拼音的映射，一个字的概率可以用一个概率分布来描述，而不是直接给出概率的值。
- 词汇表(Vocabulary):一组用来表示文本数据的集合。
- 数据集(Dataset):一组句子对，每个句子对包含一个源语言句子和一个目标语言句子。
- 上下文窗口大小(Context Window Size):当把当前词及周围的固定窗口大小的词一起考虑时，所考虑的词的个数。
- 嵌入维度(Embedding Dimension):词向量的维度。
- 深度(Depth):神经网络的层数。
- 宽度(Width):每一层神经元的个数。
- 循环神经网络单元(Recurrent Neural Network Unit)(RNN unit):用来记录之前状态和输入序列的神经网络单元。
- LSTM(Long Short Term Memory):一种特殊的RNN单元。
- 门控机制(Gate Mechanism):控制信息流动的神经网络结构。
- 双向LSTM(Bidirectional LSTM):使用正向和反向两条LSTM，在编码阶段同时考虑源语句左右两个方向的信息。
- 贪心搜索(Greedy Search):在搜索过程中，只选择概率最大的翻译结果。
- Viterbi算法(Viterbi Algorithm):用于寻找最优路径的动态规划算法。
# 3.核心算法原理和具体操作步骤
## 3.1　编码器-解码器框架
主要思想是先使用CNN编码器将输入序列映射为一个固定长度的向量，再使用RNN作为解码器，将生成的向量传入模型解码得到翻译后的句子。

### 3.1.1　CNN编码器
CNN编码器的主要功能是从输入序列中提取出结构化信息，并转换为固定长度的向量。

#### CNN卷积层
CNN编码器的第一层是卷积层，它主要用来提取文本的局部特征。卷积核是一个矩形的、具有一定权重的卷积核，可以检测输入文本中的特定模式。在CNN卷积层中，输入文本经过若干个不同的卷积核，从而提取出各种各样的特征，如边缘、颜色、纹理等。卷积核的大小可以从小到大，也可以从大到小，从而提取出不同粒度的局部特征。

#### CNN池化层
CNN编码器的第二层是池化层，它主要用来缩小特征图的尺寸。池化层一般采用最大池化，即从输入的特征图中取出区域内最大值的那一个元素，去除其他元素。池化层的目的是减少参数数量，以便降低过拟合风险。

#### CNN全连接层
CNN编码器的第三层是全连接层，它主要用来连接上一层的输出向量和全连接层。在全连接层中，特征向量经过一系列的线性变换，得到一个向量。该向量的维度与标签集中的类别个数相同。

### 3.1.2　RNN解码器
RNN解码器的主要功能是根据前面一步的生成结果生成后一步的生成结果，由此达到依据历史信息进行翻译的目的。

#### RNN门控机制
RNN解码器的第二层是门控机制，它主要用于控制信息流动。RNN解码器的门控机制包括遗忘门、输出门和更新门。遗忘门用于决定应该遗忘哪些信息；输出门用于决定应该输出什么信息；更新门用于决定如何更新门控单元的状态。

#### RNN循环神经网络单元
RNN解码器的第一层是循环神经网络单元，它主要用于存储和处理历史信息。循环神经网络单元接收前一步的生成结果和当前输入词，并进行信息更新和输出。

### 3.1.3　CNN编码器-解码器框架总结
CNN编码器-解码器框架主要包括以下几部分：

- CNN卷积层：输入的文本经过若干个卷积核提取出不同粒度的局部特征，并转换为固定长度的向量。
- CNN池化层：缩小特征图的尺寸，减少参数数量。
- CNN全连接层：将特征向量与标签集中的类别个数相同的全连接层连接，得到一个向量。
- RNN门控机制：控制信息流动，记录历史信息。
- RNN循环神经网络单元：接收前一步的生成结果和当前输入词，并进行信息更新和输出。

## 3.2　单步解码器
单步解码器是一种RNN结构，可以在一次迭代中生成多个单词。它的主要思想是在一次迭代中同时生成多个单词，并确保生成的结果是连续的。

### 3.2.1　LSTM门控机制
单步解码器的门控机制包括遗忘门、输出门和更新门。遗忘门用于决定应该遗忘哪些信息；输出门用于决定应该输出什么信息；更新门用于决定如何更新门控单元的状态。

### 3.2.2　单步解码器网络结构
单步解码器的网络结构如下图所示：

![single_step_decoder](https://pic4.zhimg.com/v2-2cbdb9d4d6cf6f71a9688cf7bfdd5b3f_r.jpg)

图中，输入为当前输入词（x_t），LSTM状态为h_t-1，输出为候选翻译集C_t。遗忘门决定遗忘多少信息；输出门决定应该输出什么信息；更新门决定如何更新门控单元的状态。其中，$s_{ij}$表示第i层的第j个门控单元的状态；$W^{xy}, b^{xy}$为第i层的第j个门控单元的参数；$g(.)$表示激活函数；$h_t = \sigma(W^{hh} h_{t-1} + W^{hx} x_t + b^h)$表示LSTM单元的新状态。

### 3.2.3　单步解码器训练过程
单步解码器的训练过程如下：

1. 初始化网络参数：将网络结构中的参数随机初始化。
2. 从初始输入词开始生成句子：初始化LSTM状态为0，将输入词送入网络中，并将输出作为当前输出词。
3. 对每个位置t=2...T-1，按照以下步骤进行：
   - 根据当前输入词x_t和h_{t-1}，得到当前候选翻译集C_t。
   - 计算当前目标词softmax概率分布p_{ct}(y_t|x_t, C_t)。
   - 使用交叉熵损失函数计算目标词p_{ct}(y_t|x_t, C_t)的梯度。
   - 更新LSTM状态h_t，并将h_t送入到下一时间步，作为新的输入。
4. 使用梯度下降法对网络参数进行更新。

### 3.2.4　单步解码器推断过程
单步解码器的推断过程如下：

1. 初始化网络参数：将网络结构中的参数设置为已训练好的参数。
2. 从初始输入词开始生成句子：初始化LSTM状态为0，将输入词送入网络中，并将输出作为当前输出词。
3. 在时间步t=1...T-1上，对每个位置t，按照以下步骤进行：
   - 根据当前输入词x_t和h_{t-1}，得到当前候选翻译集C_t。
   - 将h_t送入到下一时间步，作为新的输入。
   - 判断当前输出是否为EOS标记，如果是，结束生成。
4. 生成的句子中，除了EOS外的词都要加上EOS作为分隔符。

## 3.3　连贯解码器
连贯解码器是一种RNN结构，是在贪心搜索算法的基础上，改进而来的。它的主要思想是动态地维护历史信息，并对生成的结果进行调整，从而达到更准确的翻译结果。

### 3.3.1　Viterbi算法
Viterbi算法是一种动态规划算法，用于寻找最优路径。在机器翻译中，Viterbi算法用来计算某词的最佳前驱词序列，也就是找到最短的翻译序列。

### 3.3.2　贪心搜索
贪心搜索算法是指在每次迭代中，只选择概率最大的翻译结果，直至遇到EOS标记停止。

### 3.3.3　连贯解码器网络结构
连贯解码器的网络结构如下图所示：

![concise_decoder](https://pic3.zhimg.com/v2-fccefbaf87df054d05d955f3e5ee9eb7_r.png)

图中，输入为当前输入词x_t，前面n步的生成结果为$\{C_k\}_{k=1}^n$，候选翻译集C_t由LSTM解码器生成，输出为$\{y_t\}_t$。

### 3.3.4　连贯解码器训练过程
连贯解码器的训练过程如下：

1. 初始化网络参数：将网络结构中的参数随机初始化。
2. 从初始输入词开始生成句子：初始化LSTM状态为0，将输入词送入网络中，并将输出作为当前输出词。
3. 计算当前候选翻译集C_t，其中C_t = $\{C_k\}_{k=1}^{n}$ U {$<eos>$}，U表示集合的并集。
4. 通过Viterbi算法计算当前单词的最佳前驱词序列$\hat{z}_{t-1}$。
5. 用当前输入词x_t和最佳前驱词序列$\hat{z}_{t-1}$，计算$P(    heta|\hat{z}_{t-1})$。
6. 如果$\hat{z}_{t-1}$中包含EOS，结束生成。否则，更新当前输出词。
7. 使用梯度下降法对网络参数进行更新。

### 3.3.5　连贯解码器推断过程
连贯解码器的推断过程如下：

1. 初始化网络参数：将网络结构中的参数设置为已训练好的参数。
2. 从初始输入词开始生成句子：初始化LSTM状态为0，将输入词送入网络中，并将输出作为当前输出词。
3. 创建空列表，用于保存生成的句子。
4. 在时间步t=1...T-1上，对每个位置t，按照以下步骤进行：
   - 根据当前输入词x_t和h_{t-1}，得到当前候选翻译集C_t。
   - 用Viterbi算法计算当前单词的最佳前驱词序列$\hat{z}_{t-1}$。
   - 用当前输入词x_t和最佳前驱词序列$\hat{z}_{t-1}$，计算$P(    heta|\hat{z}_{t-1})$。
   - 如果$\hat{z}_{t-1}$中包含EOS，结束生成。否则，更新当前输出词。
   - 把当前输出词添加到生成的句子中。
5. 生成的句子中，除了EOS外的词都要加上EOS作为分隔符。

# 4.具体代码实例与解释说明
这里给出一些具体的代码实例和解释说明。

## 4.1 CNN编码器
### 4.1.1 安装
```python
!pip install torchtext==0.8.0
!pip install jieba
!pip install transformers==4.5.1
```

### 4.1.2 数据集处理
```python
import torchtext
from torchtext import data
import jieba
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') # 初始化中文BERT tokenizer
TEXT = data.Field(sequential=True, tokenize=lambda s: [tok for tok in jieba.cut(s)]) # 定义Field
train_data, val_data = torchtext.datasets.SIGHAN2005.splits(
    TEXT, fields=(("src", TEXT), ("trg", TEXT))
)
vocab_size = len(TEXT.vocab) # 获取词表大小
print("vocab size:", vocab_size)
batch_size = 32
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # 选择设备类型
```

### 4.1.3 模型构建
```python
import torch.nn as nn
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.input_dim = input_dim
        self.emb_dim = emb_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.dropout = dropout
                
        self.embedding = nn.Embedding(input_dim, emb_dim) # embedding layer
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, bidirectional=True) # lstm layer

    def forward(self, src):

        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.rnn(embedded)
        
        return hidden, cell

encoder = Encoder(vocab_size, 256, 128, 2, 0.5).to(device) # 定义CNN编码器
optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001) # 定义优化器
criterion = nn.CrossEntropyLoss() # 定义损失函数
```

### 4.1.4 训练模型
```python
def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src = batch.src.to(device)
        
        optimizer.zero_grad()
        
        output, _ = encoder(src)
        
        loss = criterion(output[0], src[:,:-1])
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):

            src = batch.src.to(device)

            output, _ = encoder(src)
            
            loss = criterion(output[0], src[:,:-1])

            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

N_EPOCHS = 20
CLIP = 1
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    train_loss = train(encoder, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(encoder, val_iterator, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = time.strftime('%M:%S', time.gmtime(end_time - start_time)), time.strftime('%S', time.gmtime(end_time - start_time))

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'    Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'     Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
```

## 4.2 RNN解码器
### 4.2.1 数据集处理
```python
import torch
import random
import numpy as np
from torch.autograd import Variable
import time
import math

TEXT = data.Field(sequential=True, tokenize='spacy', lower=True, fix_length=MAX_LEN)
train_data, test_data = datasets.IMDB.splits(TEXT)
TEXT.build_vocab(train_data, min_freq=1)

word_embeddings = TEXT.vocab.vectors
vocab_size = len(TEXT.vocab)

# create batches iterators
train_iterator, test_iterator = data.BucketIterator.splits((train_data, test_data), 
                                                            batch_size=BATCH_SIZE, 
                                                            sort_key=lambda x: len(x.text),
                                                            device=device)
```

### 4.2.2 模型构建
```python
class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        
        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, DEC_HID_DIM)
        self.v = nn.Parameter(torch.rand(DEC_HID_DIM))
        
    def forward(self, hidden, encoder_outputs):
        
        batch_size = encoder_outputs.shape[0]
        src_len = encoder_outputs.shape[1]
        
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        
        energy = energy.permute(0, 2, 1)
        
        v = self.v.repeat(batch_size, 1).unsqueeze(1)
        
        attention = torch.bmm(v, energy).squeeze(1)
        
        return F.softmax(attention, dim=1)
        
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):
        super().__init__()

        self.emb_dim = emb_dim
        self.enc_hid_dim = enc_hid_dim
        self.dec_hid_dim = dec_hid_dim
        self.output_dim = output_dim
        self.dropout = dropout
        self.attention = attention
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)
        
        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, encoder_outputs):

        seq_len = input.shape[0]
        batch_size = input.shape[1]
        src_len = encoder_outputs.shape[1]
        
        # input = [seq_len, batch_size]
        # hidden = [n layers * dir, batch_size, dec hid dim]
        # encoder_outputs = [batch_size, src_len, enc_hid_dim * 2]
        
        embeddings = self.embedding(input)
        
        embeddings = self.dropout(embeddings)
        
        a = self.attention(hidden, encoder_outputs)
        
        a = a.unsqueeze(1)
        
        # a = [batch_size, 1, src_len]
        
        weighted = torch.bmm(a, encoder_outputs)
        
        # weighted = [batch_size, 1, enc_hid_dim * 2]
        
        rnn_input = torch.cat((weighted, embeddings), dim=2)
        
        # rnn_input = [batch_size, 1, (enc_hid_dim * 2) + emb_dim]
            
        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))
        
        # output = [seq_len, batch_size, dec_hid_dim]
        # hidden = [n layers * dir, batch_size, dec_hid_dim]
        
        assert (output == hidden).all().item()
        
        embedded = embeddings.squeeze(0)
        
        output = output.squeeze(0)
        
        weighted = weighted.squeeze(1)
        
        prediction = self.out(torch.cat((output, weighted, embedded), dim=1))
        
        # prediction = [batch_size, output_dim]
        
        return prediction, hidden.squeeze(0)
    
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        
        # src = [src sent len, batch size]
        # trg = [trg sent len, batch size]
        # teacher_forcing_ratio is probability to use teacher forcing
        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time
        
        batch_size = src.shape[1]
        max_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # tensor to store decoder outputs
        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)
        
        # last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        
        # first input to the decoder is the <sos> tokens
        input = trg[0,:]
        
        for t in range(1, max_len):
            
            output, hidden = self.decoder(input, hidden, cell)
            outputs[t] = output
            
            # decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            top1 = output.argmax(1)
            
            input = (trg[t] if teacher_force else top1)
            
        return outputs    
            
INPUT_DIM = len(TEXT.vocab)
OUTPUT_DIM = len(TEXT.vocab)
ENC_EMB_DIM = 256
DEC_EMB_DIM = 256
ENC_HID_DIM = 512
DEC_HID_DIM = 512
ENC_DROPOUT = 0.5
DEC_DROPOUT = 0.5
TRG_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_NUM_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)

model = Seq2Seq(enc, dec, device).to(device)

optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)

criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)
```

### 4.2.3 训练模型
```python
def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src = batch.src.to(device)
        trg = batch.trg.to(device)
        
        optimizer.zero_grad()
        
        output = model(src, trg)
        
        # trg = [trg sent len, batch size]
        # output = [trg sent len, batch size, output dim]
        
        output_dim = output.shape[-1]
        
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        
        # trg = [(trg sent len - 1) * batch size]
        # output = [(trg sent len - 1) * batch size, output dim]
        
        loss = criterion(output, trg)
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(iterator):

            src = batch.src.to(device)
            trg = batch.trg.to(device)

            output = model(src, trg, 0) # turn off teacher forcing

            # trg = [trg sent len, batch size]
            # output = [trg sent len, batch size, output dim]

            output_dim = output.shape[-1]

            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            # trg = [(trg sent len - 1) * batch size]
            # output = [(trg sent len - 1) * batch size, output dim]

            loss = criterion(output, trg)

            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

N_EPOCHS = 10
CLIP = 1
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    
    start_time = time.time()
    
    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()
    
    epoch_mins, epoch_secs = time.strftime('%M:%S', time.gmtime(end_time - start_time)), time.strftime('%S', time.gmtime(end_time - start_time))

    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
    print(f'    Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'     Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'tut6-model.pt')
```

