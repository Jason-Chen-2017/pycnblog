
作者：禅与计算机程序设计艺术                    

# 1.简介
  

模拟退火 (Simulated Annealing) 是一种基于概率论的优化算法,它利用随机化的方法对目标函数进行寻优,其优化的过程看上去像一个不断的冷却与升温的过程,每次迭代都从当前状态逐渐向目标状态靠拢,最终收敛到最佳状态,因此被称为模拟退火算法。虽然模拟退火算法在许多领域都有应用,但由于其高维空间复杂度及计算量大等特点,目前还没有成为主流的分布式计算优化算法。 

为了解决这一问题,研究者们提出了一些改进版本,即分布式模拟退火算法 (Distributed Simulated Annealing, DS-SA)。DS-SA 使用集群资源将系统中的各个目标函数分割成多个子问题,并分别用不同服务器上的模拟退火算法来求解这些子问题,最后再根据各个子问题的最优解来合并得到全局最优解。

本文将详细阐述分布式模拟退火算法及其相关概念和算法原理,包括问题建模、参数设置、模拟退火算法原理和具体操作步骤、程序实现、代码分析和可视化展示、实验结果、扩展思考、未来发展方向与挑战、参考文献。读者可以仔细阅读,并思考如何运用模拟退火算法来求解复杂的分布式计算优化问题。
# 2.问题建模
首先我们需要将待优化的问题转化为计算机可运行的形式,假设我们有n个物理节点和m条边,每个节点具有相应的处理能力cp,每条边连接两个节点,两两之间存在一条信息流通的线路,当两个节点之间的信息量越小,则线路传输速度越快;当两个节点之间的信息量越大时,线路传输速度就越慢。

我们定义图中的节点为V={v1,v2,...,vn},而边为E={(u,v),...,(uk,vk)}.每条边连接两个节点,记作(u,v). u和v是邻居关系,且对于每个节点v,记其所有邻居的集合为N(v);同时记Vi=[c(vi),u(vi)],其中c(vi)表示节点vi的处理能力,u(vi)表示节点vi的所有输入边数量。为了简单起见,假设所有的节点处理能力cp均相同,且所有输入边均相等。

假设我们希望找出一组权值w=(w1,w2,...,wn)和传输速率p,使得两个节点间的通信代价最小,即结点间的通信延迟最小。则在给定任一参数组合w,p下,问题可以表述为:

min sum_{i=1}^n d(vi,vj)=sum_{i<j} min(p_ij + w_i, p_ji + w_j), i,j ∈ V-{vi,vj} 

s.t. di+1 = ci*ui*di/(ci+cj)*uj/(ci+cj)*dj

d1 = c1*u1*d1/c2*u2*d2/.../cn*un*dn

where pij is the communication delay of edges (i,j), ui and uj are input degree of nodes vi and vj respectively.

为了解决该问题,我们可以采用分布式模拟退火算法来求解。
# 3.参数设置
一般来说,分布式模拟退火算法的参数设置有以下几方面:
1. Tmax: 最大退火次数,即算法执行的次数。

2. Tmin: 停止退火的温度值。

3. alpha: 温度变化率。

4. k: 每隔k次更新参数,选择一次最好的解作为新的参数起始点。

5. gamma: 生成新解的概率。

6. m: 将整个网络划分成m份,每份负责不同的子问题。

这里,我们要注意的是,对于不同子问题的优化目标函数,并不需要非常精确,通常只需估计出一个初步的估计值即可。因此,在此处选择合适的初始温度T0可能是关键所在。例如,我们可以在测试数据集上预先训练好机器学习模型或基于经验选择一个合适的值。

除了参数设置之外,还需要确定每个子问题的具体分配方案,即每个服务器上的任务范围。一般地,我们会按照节点处理能力的大小对节点进行划分,并让负责节点处理能力最大的服务器来承担处理能力最大的子问题,以此类推。

# 4.模拟退火算法原理和具体操作步骤
## 4.1 算法流程
DS-SA 的主要思想是利用分布式并行计算环境来解决问题。在算法的执行过程中,我们将整个网络划分为m份,分别由不同的服务器来承担不同的子问题。每个服务器都运行一个独立的模拟退火算法,通过自己的处理资源和交换链路来执行子问题的求解。如下图所示:

DS-SA 算法的具体操作步骤如下:
1. 初始化参数：在第一轮的迭代中,随机生成若干初始解作为初始参数,用于产生下一轮的搜索点。
2. 在第k轮迭代中,对每份子网,服务器选择适应度较好的解作为新的起始点。
3. 根据新的起始点生成新的解。
4. 判断是否达到终止条件。若达到终止条件,则退出算法；否则进入下一轮迭代。
5. 返回最优解。

## 4.2 局部搜索策略
在模拟退火算法的执行过程中,每个服务器都会执行一个局部搜索策略,其目的是找到一组解,使得该解能够使目标函数取得一定程度的减少。换句话说,就是希望找到一组解,使得问题的可行解空间在一定时间内快速扩张,并逐渐缩小到能够解决实际问题的可行解空间。

在每个服务器上的模拟退火算法可以分为两个阶段：(1) 搜索阶段,即探索并尝试引入更优秀的解；(2) 接受阶段,即接受或丢弃当前解。搜索阶段的工作过程如下:

1. 随机选取一个解xi作为当前解,并计算其对应可行解空间Ei。

2. 对每一条与当前解xi相邻的解x'i,计算其对应的可行解空间Ei',并判断其是否比当前解Xi更优。若是,则将Ei'替换为Ei。

3. 以一定概率接受或拒绝当前解xi。若接受,则继续搜索阶段;若拒绝,则随机生成另一个解作为新的搜索点。

4. 当搜索阶段的轮数达到一定的阈值后,如果当前解的可行解空间Ei仍然比之前的可行解空间更小,则进行接受阶段。

5. 在接受阶段,如果当前解已经可行,则直接返回当前解。

6. 如果当前解不太可行,则减少一定概率,重新生成另一个解作为新的搜索点。直至满足终止条件。

# 5.程序实现
## 5.1 数据准备
首先,我们需要准备好数据集,其中包含了节点的处理能力和输入边数量。以本文的例子,假设我们有两个物理节点v1和v2,分别具有处理能力100和150,以及四条输入边分别连接到这两个节点。那么我们的数据可以这样表示:
```python
data = {
    'v1': {'capacity': 100, 'input': [2]},
    'v2': {'capacity': 150, 'input': [2]}
}
```

## 5.2 函数定义
然后,我们可以定义几个用于运算和可视化的函数:

### 距离计算函数dist()
用于计算两个节点之间的距离。由于距离一般都用负的代价表示,因此在计算距离时,可以用两个节点之间的通信延迟来替代。假设我们有一条边(v1,v2)和一条边(v2,v3),这两条边的通信延迟分别为pi和pj。那么我们可以定义distance()函数如下:

```python
def distance(data, x, y):
    '''
    Compute the distance between two nodes using their communication delays.

    Args:
        data: A dictionary containing node information such as capacity and input degrees.
        x: The first node id in tuple format.
        y: The second node id in tuple format.
    
    Returns:
        The total communication delay along all links connecting x and y.
    '''
    dist = []
    for z in data[y]['input']:
        if z == x:
            continue
        else:
            # Use pi to represent the communication delay between (v1,v2) and (v2,z)
            dist += [-data['links'][(x, y)][('v2', z)]]
    return abs(sum(dist))
```

### 可视化函数visualize()
用于绘制网络结构图和节点位置图。由于数据中只有节点id和输入边的信息,因此无法绘制具体的网络结构图。但是可以通过节点位置信息来粗略描绘网络结构图。假设我们有三个物理节点v1、v2、v3,并且知道v1和v2之间的距离为50,则可以定义visulize()函数如下:

```python
import networkx as nx
import matplotlib.pyplot as plt

def visualize(data):
    '''
    Draw a simple network diagram with node positions based on link distances.

    Args:
        data: A dictionary containing node information such as capacity and input degrees.
        
    Returns:
        None
    '''
    G = nx.Graph()
    pos = {}
    labels = {}
    for n in data:
        G.add_node(n)
        labels[n] = n
        pos[n] = (-len(data[n]['input']) / 2 * 200, len([l for l in data if n in data[l]['input']]))
    edge_colors = ['gray' for e in range(nx.number_of_edges(G))]
    nx.draw(G, pos=pos, node_color='white', edge_color=edge_colors, width=2)
    nx.draw_networkx_labels(G, pos=pos, labels=labels)
    plt.axis('off')
    plt.show()
```

### 参数初始化函数init_params()
用于初始化模拟退火算法的参数。假设我们设置的子网数量为m=2,则可以定义init_params()函数如下:

```python
from random import randint, uniform

def init_params(num_subnets, num_nodes, max_degree, cap_range):
    '''
    Initialize parameters used by distributed simulated annealing algorithm.

    Args:
        num_subnets: The number of subnetworks to split the network into.
        num_nodes: The total number of nodes in the network.
        max_degree: The maximum allowed degree of each node.
        cap_range: A list representing the range of possible node capacities.
    
    Returns:
        A dictionary containing initialized parameters including temperature, initial solutions, cooling rate, and subset size.
    '''
    params = {}
    temp_range = (0.01, 1)
    subnet_size = int(num_nodes / num_subnets)
    net_cap = float((cap_range[1] - cap_range[0]) / subnet_size)
    cap_dict = {}
    for i in range(subnet_size):
        cap_dict[(i, 0)] = cap_range[0] + i * net_cap
    solution_dict = {}
    for s in range(num_subnets):
        for j in range(subnet_size):
            n_id = ('sub{}'.format(s), j)
            if n_id not in solution_dict:
                candidate_list = [(s_, j_) for s_ in range(num_subnets) for j_ in range(subnet_size) if (s!= s_ or j!= j_) and s_ > s]
                selected_list = sorted([(s_, j_) for s_ in range(num_subnets) for j_ in range(subnet_size) if (s!= s_ or j!= j_) and s_ < s], key=lambda x: -distance(data, n_id, ('sub{}_{}'.format(*x))) ** 2)[0]
                solution_dict[n_id] = ('sub{}_{}'.format(*selected_list), randint(-max_degree, max_degree))
                solution_dict[n_id][1] *= (randint(0, 1) * 2 - 1) + 1
            cap_dict[n_id] = cap_dict[(s, j)]
            for s_ in range(num_subnets):
                for j_ in range(subnet_size):
                    if s_ == s and j_ >= j:
                        break
                    else:
                        neighbor_id = ('sub{}_{}'.format(s_), j_)
                        cap_dict[neighbor_id] = cap_dict[(s_, j_)]
    soln_set = set(solution_dict.items())
    params['temp'] = uniform(*temp_range)
    params['solutions'] = [{'node': '{}'.format(*k), 'degree': v[1]} for k, v in soln_set]
    params['cooling_rate'] = 0.8
    params['subset_size'] = subnet_size // 2
    return params
```

### 更新参数函数update_params()
用于根据模拟退火算法的执行情况,调整参数。假设我们设置的子网数量为m=2,则可以定义update_params()函数如下:

```python
from copy import deepcopy

def update_params(old_params, iteration, best_solution, worst_cost):
    '''
    Update parameters used by distributed simulated annealing algorithm according to execution results.

    Args:
        old_params: A dictionary containing original parameters.
        iteration: The current iteration of the algorithm.
        best_solution: The current best solution found so far.
        worst_cost: The cost of the current best solution plus one tenth its standard deviation.
    
    Returns:
        A new dictionary containing updated parameters.
    '''
    new_params = deepcopy(old_params)
    if iteration % new_params['subset_size'] == 0:
        partition_count = defaultdict(int)
        part_soln_dict = {}
        for soln in new_params['solutions']:
            key = soln['node'].split('_')[0]
            partition_count[key] += 1
            part_soln_dict[key].append({'node': soln['node'], 'degree': soln['degree']})
        parts = [[{'node': item['node']} for item in part_soln_dict[str(part)]] for part in range(new_params['subset_size'])]
        new_parts = []
        for part in parts:
            while True:
                start = randint(0, len(part)-1)
                end = randint(start, len(part)-1)
                if start!= end:
                    break
            part[start:end+1] = reversed(part[start:end+1])
            new_parts.extend(partition_partitions(part, new_params['subset_size']))
        merged_parts = merge_partitions(deepcopy(new_parts))
        nodes_per_part = defaultdict(int)
        part_deg_dict = defaultdict(float)
        for part in merged_parts:
            for node in part['nodes']:
                nodes_per_part[str(part['idx'])] += 1
                part_deg_dict[str(part['idx'])] += node['degree']
        for part in merged_parts:
            part_size = nodes_per_part[str(part['idx'])]
            average_deg = part_deg_dict[str(part['idx'])] / part_size
            std_dev = sqrt(worst_cost / (iteration / new_params['subset_size'] * math.log(iteration)))
            max_delta = std_dev * math.exp((-average_deg - worst_cost) / (std_dev**2))
            delta_deg = round(uniform(-max_delta, max_delta))
            deg_prob = exp(-abs(delta_deg) / max_delta)
            accept_prob = pow(math.e, -deg_prob * ((best_solution['degree'] - average_deg)**2)/(2*(std_dev**2)))
            if accept_prob > uniform(0, 1):
                new_params['solutions'][:] = [soln for soln in new_params['solutions'] if soln['node'] not in str(part['idx'])]
                for node in part['nodes']:
                    new_params['solutions'].append({'node': node['node'], 'degree': node['degree'] + delta_deg})
        sort_func = lambda soln: soln['node'].split('_')[-1]
        new_params['solutions'] = sorted(new_params['solutions'], key=sort_func)
    else:
        pass
    new_params['temp'] *= new_params['cooling_rate']
    return new_params
```

### 模拟退火主函数sa()
用于执行模拟退火算法。假设我们设置的子网数量为m=2,则可以定义sa()函数如下:

```python
from collections import defaultdict
from math import log

def sa(data, max_iter=1000, num_subnets=2, max_degree=2, cap_range=(50, 500)):
    '''
    Run distributed simulated annealing algorithm.

    Args:
        data: A dictionary containing node information such as capacity and input degrees.
        max_iter: Maximum iterations allowed for the algorithm.
        num_subnets: Number of subnetworks to split the network into.
        max_degree: Maximum allowed degree of each node.
        cap_range: Range of valid node capacities.
    
    Returns:
        A dictionary containing final solutions obtained during the search process.
    '''
    def objective_function(solns):
        costs = []
        for i in range(num_subnets):
            src_ids = [('sub{}_{}'.format(i, j), j) for j in range(subnet_size)]
            dst_ids = [('sub{}_{}'.format(i, j), j) for j in range(subnet_size)]
            src_ids.remove(('sub{}_{}'.format(i, src_index), src_index))
            dst_ids.remove(('sub{}_{}'.format(i, dst_index), dst_index))
            for j in range(src_index+1, subnet_size):
                dst_ids.remove(('sub{}_{}'.format(i, j), j))
            for j in range(dst_index+1, subnet_size):
                src_ids.remove(('sub{}_{}'.format(i, j), j))
            for j in range(num_subnets):
                if j!= i:
                    src_ids.extend([('sub{}_{}'.format(j, k), k) for k in range(subnet_size)])
                    dst_ids.extend([('sub{}_{}'.format(j, k), k) for k in range(subnet_size)])
            for j in range(subnet_size):
                src_id = ('sub{}_{}'.format(i, j), j)
                for k in range(subnet_size):
                    dst_id = ('sub{}_{}'.format(i, k), k)
                    if src_id[1] <= dst_id[1]:
                        dist = -data['links'][src_id][dst_id]
                    else:
                        dist = -data['links'][dst_id][src_id]
                    for s in range(num_subnets):
                        if s!= i:
                            if src_id[0][:4] =='sub{}_'.format(s):
                                other_id = (dst_id[0][:4]+'_'+src_id[0][5:], dst_id[1]-src_id[1]+other_index+1)
                            elif dst_id[0][:4] =='sub{}_'.format(s):
                                other_id = (src_id[0][:4]+'_'+dst_id[0][5:], src_id[1]-dst_id[1]+src_index+1)
                            else:
                                continue
                            if other_id[1] < 0 or other_id[1] >= subnet_size:
                                continue
                            if src_id[1] <= dst_id[1]:
                                real_dist = -data['links'][src_id][other_id]
                            else:
                                real_dist = -data['links'][dst_id][other_id]
                            dist += real_dist
                    costs.append(dist)
        return sum(costs)

    def partition_partitions(part_list, num_parts):
        '''
        Partition partitions recursively until we have num_parts pieces.

        Args:
            part_list: List of partitions to be partitioned.
            num_parts: Final desired number of partitions after partitioning.
        
        Returns:
            Partitions after recursive partitioning.
        '''
        if len(part_list) == num_parts:
            return part_list
        midpoint = len(part_list) // 2
        left_parts = [item for idx, item in enumerate(part_list[:midpoint]) if idx%2==0]
        right_parts = [item for idx, item in enumerate(part_list[:midpoint]) if idx%2!=0]
        if len(left_parts) > num_parts//2:
            left_parts = partition_partitions(left_parts, num_parts//2)
        if len(right_parts) > num_parts-len(left_parts):
            right_parts = partition_partitions(right_parts, num_parts-len(left_parts))
        combined_parts = []
        for left in left_parts:
            for right in right_parts:
                combined_parts.append({'nodes': [], 'idx': part_list.index(left)+part_list.index(right)})
                combined_parts[-1]['nodes'].extend(left['nodes'])
                combined_parts[-1]['nodes'].extend(right['nodes'])
        return combined_parts

    def merge_partitions(part_list):
        '''
        Merge partitions that are too small.

        Args:
            part_list: List of partitions to be merged.
        
        Returns:
            Merged partitions without any small ones.
        '''
        merged_parts = []
        visited = [False]*len(part_list)
        while False in visited:
            curr_visited = visited[:]
            for i in range(len(part_list)):
                if not visited[i] and not curr_visited[i]:
                    for j in range(i+1, len(part_list)):
                        if not visited[j] and not curr_visited[j] and len(part_list[i]['nodes']) < len(part_list[j]['nodes']):
                            target_idx = i
                            source_idx = j
                        elif not visited[j] and not curr_visited[j] and len(part_list[i]['nodes']) > len(part_list[j]['nodes']):
                            target_idx = j
                            source_idx = i
                    if target_idx == i and source_idx == j:
                        diff = abs(len(part_list[target_idx]['nodes']) - len(part_list[source_idx]['nodes']))
                        if diff > 1:
                            move_num = randint(diff-1, diff)
                            nodes_to_move = part_list[source_idx]['nodes'][:move_num]
                            del part_list[source_idx]['nodes'][:move_num]
                            part_list[target_idx]['nodes'].extend(nodes_to_move)
                            curr_visited[target_idx] = True
                            curr_visited[source_idx] = True
                        else:
                            visited[source_idx] = True
                            curr_visited[target_idx] = True
                    elif target_idx == j and source_idx == i:
                        diff = abs(len(part_list[target_idx]['nodes']) - len(part_list[source_idx]['nodes']))
                        if diff > 1:
                            move_num = randint(diff-1, diff)
                            nodes_to_move = part_list[target_idx]['nodes'][-move_num:]
                            del part_list[target_idx]['nodes'][-move_num:]
                            part_list[source_idx]['nodes'].extend(nodes_to_move)
                            curr_visited[target_idx] = True
                            curr_visited[source_idx] = True
                        else:
                            visited[target_idx] = True
                            curr_visited[source_idx] = True
                    elif target_idx == i:
                        move_num = len(part_list[source_idx]['nodes'])
                        nodes_to_move = part_list[source_idx]['nodes']
                        part_list[target_idx]['nodes'].extend(nodes_to_move)
                        del part_list[source_idx]
                        curr_visited[target_idx] = True
                    elif source_idx == i:
                        move_num = len(part_list[target_idx]['nodes'])
                        nodes_to_move = part_list[target_idx]['nodes']
                        part_list[source_idx]['nodes'].extend(nodes_to_move)
                        del part_list[target_idx]
                        curr_visited[source_idx] = True
                    else:
                        raise ValueError('Partition merging failed.')
            for i in range(len(curr_visited)):
                if curr_visited[i]:
                    visited[i] = True
        return part_list

    def perturbation():
        nonlocal other_index, src_index, dst_index
        chosen_soln = choice(new_params['solutions'])
        other_index = randint(0, subnet_size-1)
        src_index = randint(0, subnet_size-1)
        dst_index = randint(0, subnet_size-1)
        while dst_index == src_index:
            dst_index = randint(0, subnet_size-1)
        for s in range(num_subnets):
            if s == chosen_soln['node'].split('_')[0]:
                node_ids = [('sub{}_{}'.format(chosen_soln['node'].split('_')[0], j), j) for j in range(subnet_size) if j!= src_index]
                node_ids.append(('sub{}_{}'.format(chosen_soln['node'].split('_')[0], other_index), other_index))
            else:
                node_ids = [('sub{}_{}'.format(s, j), j) for j in range(subnet_size)]
            add_probs = [pow(1.1**(subnet_size-j-1), 1/float(max_degree)) if j!= other_index else pow(1.1**(max_degree+1), 1/float(max_degree)) for j in range(subnet_size)]
            remove_probs = [pow(1.1**(j), 1/float(max_degree)) for j in range(subnet_size)]
            for step in range(2):
                chosen_id = choice(node_ids)
                if step == 0 and chosen_id[1] == src_index:
                    if chosen_id[0][:4] == chosen_soln['node'].split('_')[0]+'_':
                        alt_id = (chosen_id[0][5:], src_index-other_index+1)
                    else:
                        alt_id = (chosen_soln['node'].split('_')[0]+'_'+chosen_id[0][4:], src_index+1)
                    try:
                        new_params['solutions'].remove({'node': '_'.join(map(str, chosen_id)), 'degree': chosen_soln['degree']})
                        new_params['solutions'].append({'node': '_'.join(map(str, alt_id)), 'degree': alt_id[1]})
                        return
                    except ValueError:
                        pass
                elif step == 1 and chosen_id[1] == other_index:
                    try:
                        new_params['solutions'].remove({'node': '_'.join(map(str, chosen_id)), 'degree': 0})
                    except ValueError:
                        pass
                else:
                    prob = add_probs[chosen_id[1]] if step == 0 else remove_probs[chosen_id[1]]
                    change = randint(-max_degree, max_degree)
                    change = change if step == 0 else -change
                    if changed_id := ('sub{}_{}'.format(s, chosen_id[1]), chosen_id[1]):
                        if step == 0:
                            new_params['solutions'].append({'node': '_'.join(map(str, changed_id)), 'degree': changed_id[1]+change})
                        else:
                            try:
                                new_params['solutions'].remove({'node': '_'.join(map(str, changed_id)), 'degree': changed_id[1]})
                            except ValueError:
                                pass

    solve_time = timeit.default_timer()
    subnet_size = int(len(data) / num_subnets)
    print('Initializing parameters...')
    new_params = init_params(num_subnets, len(data), max_degree, cap_range)
    solve_times = [solve_time]
    iter_counts = [0]
    best_objectives = [objective_function(new_params['solutions'])]
    old_obj = best_objectives[0]
    print('Start searching...')
    for t in range(max_iter):
        iter_counts.append(t+1)
        obj_values = []
        best_solns = []
        for i in range(num_subnets):
            for j in range(subnet_size):
                if (i, j) not in cap_dict or (i, j) in problematic_nodes:
                    continue
                soln_set = {'node': '{}_{}'.format(i, j), 'degree': new_params['solutions'][i*subnet_size+j]['degree']}
                while True:
                    tmp_degree = new_params['solutions'][i*subnet_size+j]['degree'] + randint(-max_degree, max_degree)
                    if 0 <= tmp_degree <= max_degree:
                        break
                try:
                    proposed_params = update_params(new_params, t, {'degree': new_params['solutions'][i*subnet_size+j]['degree']}, worst_cost)
                    proposed_params['solutions'][i*subnet_size+j]['degree'] = tmp_degree
                    prev_cost = proposed_params['temp'] * objective_function(proposed_params['solutions'])
                    new_cost = proposed_params['temp'] * objective_function(tmp_degree, i, j, proposed_params['temp'])
                    metropolis_ratio = exp((prev_cost - new_cost)/proposed_params['temp'])
                    if metropolis_ratio > uniform(0, 1):
                        obj_values.append(new_cost)
                        best_solns.append(proposed_params['solutions'])
                        if new_cost < old_obj:
                            new_params = proposed_params
                            best_objectives.append(new_cost)
                            old_obj = new_cost
                            worst_cost = new_cost + np.std(np.array(obj_values))/10.0 if len(obj_values)>1 else new_cost*2
                            perturbation()
                            break
                    else:
                        obj_values.append(prev_cost)
                        best_solns.append(copy.deepcopy(new_params['solutions']))
                except IndexError:
                    problematic_nodes.add((i, j))
        solve_times.append(timeit.default_timer()-solve_time)
    print('Search complete!')
    print('Best objectives:', best_objectives)
    plot_fig(iter_counts, solve_times, best_objectives)
    return new_params['solutions']
```