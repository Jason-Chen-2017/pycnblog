
作者：禅与计算机程序设计艺术                    

# 1.背景介绍


## 一、问题的提出及背景介绍
智慧城市的兴起，信息化、网络化的进程让人们生活在数字化的时代，传统行业的应用方式已经发生巨变。智慧城市带给人们生活的便利，特别是在人力资源管理、房地产开发、政务服务等领域，都要面临新的复杂场景。对于政务服务领域来说，人们可以通过多种渠道获取政务数据并进行智能分析处理，以此为基础制定出一系列政策措施。

目前各类智能城市的建设也都聚焦于政务服务领域，例如百度地图“环违”检测城市中不规范行为的召集活动、腾讯的“急救地图”提醒人们不要驾车撞人、阿里巴巴的“公交闸机”预警出租车司机开门。但如何将这些在线政务工具结合到大数据平台，构建出具有智能决策功能的决策系统呢？ 

基于当前政务决策需求，本文将从以下几个方面进行阐述：

1. 数据采集
2. 数据存储
3. 数据清洗
4. 数据计算
5. 数据可视化
6. 智能决策
7. 决策系统运维

## 二、关键技术要素解析

首先需要明确的是，政务决策是一个综合性的问题，涉及到了政务数据采集、存储、清洗、计算、可视化、智能决策以及决策系统运维等多个关键技术要素。

### （1）数据采集

政务决策涉及到多种类型的数据源，包括微博、微信、社交媒体、门户网站、电子邮件、报纸、政府部门提供的专业数据等。数据的获取一般通过爬虫的方式实现，利用网站的API接口或SDK。同时还可以采用云计算平台将数据中心和业务系统相连接，实时同步变化。

### （2）数据存储

数据采集完成后需要存储，政务数据经过清洗之后通常存放在关系型数据库（RDBMS）中，以方便数据统计、分析、查询等。对于大量的文本数据，建议选择分布式文件系统Hadoop进行存储，可实现海量数据的快速查询，并支持MapReduce计算框架。

### （3）数据清洗

数据清洗指的是对原始数据进行结构化和非结构化处理，清除无效、不准确的数据，并转换成标准化的格式。政务数据经常存在各种噪声和错误，如数据缺失、异常、不全、不一致等，因此需要对数据进行清洗处理。数据清洗的一个重要工作就是地址的识别与标准化，将不同地名表达方式统一转换为标准地址。另外，由于政务数据一般存在较长的文本长度，因此可以使用机器学习算法对文本进行自动分类、摘要等处理。

### （4）数据计算

数据计算主要解决数据特征识别、相似性计算、关联规则挖掘等问题。其中，数据特征识别是最重要的一环。数据特征识别涉及到统计概率方法、向量空间模型、判别分析等技术。统计概率方法主要用于文本分类、聚类、主题模型等任务；向量空间模型则可以用来发现语义相近的文本；判别分析可以用来区分不同类的文本。

### （5）数据可视化

数据可视化是指将数据以图表、图像等形式展现出来，能够直观呈现数据特征。政务数据展示的前提是将数据转化成易于理解的图表，并通过热力图、词云等可视化手段进行呈现。通过可视化的手段，就可以方便地对数据进行分析、总结、得出规律。

### （6）智能决策

智能决策的核心是建立一个模型，根据历史数据和当前环境情况，对政务决策进行分析、判断，提升决策效率。目前国内外的政务决策模型通常由专家系统、决策树、随机森林等构成。随着政务决策模型越来越复杂，决策过程也会越来越智能化。

### （7）决策系统运维

决策系统运维包括两个层面的工作：模型的维护和数据源的更新。模型的维护意味着对模型参数进行持续优化，以提高模型精度、降低误差，并在必要时进行模型重训练。而数据源的更新则包括新的数据的采集、清洗、入库等。

# 2.核心概念与联系
## 一、数据采集
数据采集：包括微博、微信、社交媒体、门户网站、电子邮件、报纸、政府部门提供的专业数据等。
## 二、数据存储
数据存储：在关系型数据库(RDBMS)中存储数据，采用分布式文件系统Hadoop进行存储。
## 三、数据清洗
数据清洗：数据清洗的目的是为了将原始数据中的不准确和噪音数据去除，并将其转换成标准化的格式，以便后期数据分析。数据清洗的关键步骤是地址的识别与标准化、文本的自动分类、摘要生成、数据格式转换等。
## 四、数据计算
数据计算：数据计算主要解决数据特征识别、相似性计算、关联规则挖掘等问题。
## 五、数据可视化
数据可视化：将数据以图形的形式展现出来，以便更好地呈现政务数据的特征。
## 六、智能决策
智能决策：建立模型，使用历史数据和当前环境信息，分析和判断政务决策。
## 七、决策系统运维
决策系统运维：包括模型的维护和数据源的更新。模型的维护是指对模型参数进行持续优化，以提高模型精度、降低误差，并在必要时进行模型的重新训练。而数据源的更新则包括新的数据的采集、清洗、入库等。

# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
## 一、数据采集
### 1.微博、微信、社交媒体、门户网站、电子邮件、报纸、政府部门提供的专业数据等
利用API接口或SDK进行数据采集。
```python
import tweepy # twitter API interface
from selenium import webdriver # python bindings for web scraping tool Chrome/Firefox etc
import re # regular expression module to process text data
```
### 2.云计算平台实时同步
利用云计算平台将数据中心和业务系统相连接，实时同步变化。
```python
from aws import S3Bucket # Amazon Web Services (AWS) S3 bucket service class
import boto3 # AWS SDK library in Python
```
## 二、数据存储
### 1.关系型数据库存储
政务数据经过清洗之后通常存放在关系型数据库中。
```SQL
CREATE TABLE policy_data(
    id INT PRIMARY KEY AUTO_INCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    name VARCHAR(255),
    address VARCHAR(255),
    type VARCHAR(255),
    content TEXT);
INSERT INTO policy_data VALUES("","2021-11-12 19:50:35","","","",""),(NULL,"2021-11-12 19:51:46","王者荣耀分部管委会","北京市朝阳区东三环北路26号院2号楼3单元104室","","");
```
### 2.分布式文件系统HDFS存储
对于大量的文本数据，建议选择分布式文件系统HDFS进行存储。
```bash
hdfs dfs -mkdir /user/username/input_data
hdfs dfs -put *.csv /user/username/input_data
```
## 三、数据清洗
### 1.地址的识别与标准化
对于政务数据来说，地址信息十分重要，因此需要对其进行自动识别和标准化。
地址识别的方法主要有基于正则表达式、基于机器学习、基于Web API等。
```python
def parse_address(text):
    pattern = r'\d{6}\s*\d*[号|街|路|巷](\s*\d+)?'
    matchObj = re.search(pattern, text)
    if matchObj is None:
        return ""
    else:
        result = matchObj.group()
        return result
```
### 2.文本的自动分类、摘要生成
对于政务数据来说，文本的数量往往很大，文本自动分类和摘要生成能够帮助用户快速了解政务数据的大致情况。
文本分类的方法有基于自然语言处理的分类器、基于机器学习的分类器等。
```python
import nltk
nltk.download('stopwords') # download stop words list
nltk.download('punkt') # download tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words=nltk.corpus.stopwords.words('english'))
X = vectorizer.fit_transform(['Hello world', 'Python programming'])
print(vectorizer.get_feature_names())
```
### 3.数据格式转换
为了使不同数据源的政务数据之间能够相互兼容，需要进行数据格式的转换。
```python
df = pd.read_excel("./example.xlsx") # read Excel file into pandas DataFrame
df['timestamp'] = df['日期'].apply(lambda x: datetime.datetime(*xlrd.xldate_as_tuple(x, 0))) # convert date string to datetime object
df = df[['日期','内容']] # select only the necessary columns and rename them as required by our system schema
df.to_sql('policy_data', engine, index=False, if_exists='append') # write cleansed dataset back to database table
```
## 四、数据计算
### 1.数据特征识别
对于政务数据来说，数据特征识别十分重要，不同的文本形式对应着不同的结构特征。
数据特征识别的方法主要有基于统计概率方法、基于向量空间模型、基于判别分析等。
```python
import numpy as np
import math
class TextProcessor:

    def __init__(self):
        pass
    
    def count_word_frequency(self, wordlist):
        """
        Given a list of strings, calculate their frequency counts using dictionary.
        
        Arguments:
            wordlist {List} -- List of strings
        
        Returns:
            Dict -- Dictionary containing word frequencies
        """

        freq_dict = {}
        for word in wordlist:
            if word not in freq_dict:
                freq_dict[word] = 0
            freq_dict[word] += 1
        return freq_dict
    
    def tfidf(self, corpus):
        """
        Given a set of documents, compute TFIDF scores for each term in vocabulary.
        
        Arguments:
            corpus {Set} -- Set of documents
        
        Returns:
            Tuple -- A tuple containing two dictionaries, one with term frequencies per document,
                     another with inverse document frequencies.
        """

        # Calculate total number of documents in corpus
        num_docs = len(corpus)

        # Compute term frequencies per document
        docfreq = {}
        for docid, doc in enumerate(corpus):
            freq = self.count_word_frequency(doc)
            for token in freq:
                if token not in docfreq:
                    docfreq[token] = [0]*num_docs
                docfreq[token][docid] = freq[token]
        
        # Compute inverse document frequencies
        invdocfreq = {}
        for token in docfreq:
            df = sum([bool(x) for x in docfreq[token]])
            invdocfreq[token] = math.log(float(num_docs)/max(df, 1)) + 1
        
        # Compute TFIDF scores for terms in vocabulary
        tfidf = {}
        for token in docfreq:
            weights = []
            for i, freq in enumerate(docfreq[token]):
                if freq > 0:
                    weight = freq * invdocfreq[token]
                    weights.append((i, weight))
            sorted_weights = sorted(weights, key=lambda x: x[1], reverse=True)[:10] # top n docs by score
            docids = [sorted_weight[0] for sorted_weight in sorted_weights]
            tfidf[token] = {'tf': {}, 'idf': {}}
            for docid in docids:
                tf = float(docfreq[token][docid])/(sum(docfreq[token]))
                idf = float(invdocfreq[token])
                tfidf[token]['tf'][docid] = tf*idf
            
        return tfidf, docfreq
```
### 2.相似性计算
相似性计算是指比较两个文档之间的相似程度。相关性算法主要有Cosine Similarity、Pearson Correlation Coefficient、Jaccard Index等。
```python
from scipy.spatial.distance import cosine
def similarity(doc1, doc2):
    vec1 = vectorizer.transform([doc1]).todense().tolist()[0]
    vec2 = vectorizer.transform([doc2]).todense().tolist()[0]
    sim = 1 - cosine(vec1, vec2) # Cosine similarity
    return round(sim, 4)
```
### 3.关联规则挖掘
关联规则挖掘是指找到文档中的关系模式。关联规则的形式是IF THEN，描述了购买A商品，会出现B商品这种行为。
```python
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
def find_association_rules():
    frequent_itemsets = apriori(dataset, min_support=0.001, use_colnames=True)
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.2)
    print(rules)
```
## 五、数据可视化
### 1.热力图
热力图是一种矩阵图表示法，它以颜色编码显示出矩阵元素的大小。热力图常用于展示多变量数据的相关性。
```python
import seaborn as sns
sns.set(font_scale=1.2) # increase font size
matrix = [[1, 0, 1, 1],
          [1, 1, 1, 1],
          [1, 0, 0, 1]]
mask = np.zeros_like(matrix)
mask[np.triu_indices_from(mask)] = True
with sns.axes_style('white'):
    ax = sns.heatmap(matrix, mask=mask, annot=True, fmt=".2g", cmap="YlOrRd", square=True, linewidths=.5, cbar=True)
ax.tick_params(labelsize=12)
plt.yticks(rotation=0)
plt.title("Example Heat Map")
plt.show()
```
### 2.词云
词云是一种词频统计结果的可视化手段。词云可以将所有文本数据按照词频统计，将词频较高的词汇用大字体显示，反之用小字体显示。
```python
from wordcloud import WordCloud
wordcloud = WordCloud(background_color="white").generate(" ".join(data))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()
```
## 六、智能决策
### 1.专家系统
专家系统是一种决策理论，基于知识库、规则引擎、推理系统等技术，能够做出全局的、客观的决策。
```xml
<expert>
    <premise>每年都有不少新闻发布。</premise>
    <conclusion>假设为真。</conclusion>
</expert>
<expert>
    <premise>经济学的发展速度和速度是一样快的。</premise>
    <conclusion>假设为假。</conclusion>
</expert>
```
### 2.决策树
决策树是一种机器学习算法，基于特征选择、分割数据集等，构建一系列条件测试，最终决定是否做出决定。
```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import graphviz 
from sklearn.externals.six import StringIO  
from IPython.display import Image  
dot_data = StringIO()  
tree.export_graphviz(dtc, out_file=dot_data, feature_names=['Age', 'Gender'], filled=True, rounded=True, special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
```
### 3.随机森林
随机森林是一种集成学习算法，基于Bootstrap Aggregation，基于决策树构建多棵树，通过平均来减少方差。
```python
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50, max_depth=None, random_state=0)
rfc.fit(train_features, train_labels)
predictions = rfc.predict(test_features)
accuracy = accuracy_score(test_labels, predictions)
print("Accuracy:", accuracy)
```
# 4.具体代码实例和详细解释说明
## 一、数据采集
### （1）微博数据采集
利用tweepy、beautifulsoup4等包抓取微博数据并保存到MySQL数据库。

```python
import tweepy
import pymysql
from bs4 import BeautifulSoup


class WeiboDataCollector(object):

    def __init__(self, consumer_key, consumer_secret, access_token, access_token_secret):
        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
        auth.set_access_token(access_token, access_token_secret)
        api = tweepy.API(auth)
        self.api = api

    def get_tweets(self, keywords, pages):
        tweets = []
        for keyword in keywords:
            for page in range(pages):
                try:
                    public_tweets = self.api.search(q=keyword, lang='zh-CN', rpp=100, page=page)
                    for tweet in public_tweets:
                        soup = BeautifulSoup(tweet._json['text'], "html.parser")
                        text = ''
                        for s in soup.stripped_strings:
                            text += s+' '
                        tweets.append({'created_at': str(tweet.created_at),
                                       'screen_name': tweet.author.screen_name,
                                       'retweet_count': tweet.retweet_count,
                                        'favorite_count': tweet.favorite_count,
                                        'text': text})
                except Exception as e:
                    continue
        return tweets

    def save_to_mysql(self, tweets):
        connection = pymysql.connect(host='localhost', user='root', password='', db='weibo')
        cursor = connection.cursor()
        sql = '''insert into weibo_data(created_at, screen_name, retweet_count, favorite_count, text)
                 values(%s,%s,%s,%s,%s)'''
        try:
            for tweet in tweets:
                params = (str(tweet['created_at']),
                          tweet['screen_name'],
                          tweet['retweet_count'],
                          tweet['favorite_count'],
                          tweet['text'])
                cursor.execute(sql, params)
            connection.commit()
        finally:
            connection.close()
        

if __name__ == '__main__':
    consumer_key = 'YOUR_CONSUMER_KEY'
    consumer_secret = 'YOUR_CONSUMER_SECRET'
    access_token = 'YOUR_ACCESS_TOKEN'
    access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'
    collector = WeiboDataCollector(consumer_key, consumer_secret, access_token, access_token_secret)
    keywords = ['肿瘤', '医生']
    pages = 20
    tweets = collector.get_tweets(keywords, pages)
    collector.save_to_mysql(tweets)
    
```

## 二、数据清洗
### （1）微博数据清洗
利用BeautifulSoup包解析微博文本，移除HTML标签和特殊字符。
```python
import pymongo
from bson.objectid import ObjectId
from bs4 import BeautifulSoup
import re
client = pymongo.MongoClient(host='localhost', port=27017)
db = client.weibo_data
collection = db.weibo_data

for item in collection.find():
    oid = item['_id']
    created_at = item['created_at']
    screen_name = item['screen_name']
    retweet_count = item['retweet_count']
    favorite_count = item['favorite_count']
    text = item['text']
    soup = BeautifulSoup(text, "html.parser")
    cleaned_text = ''
    for s in soup.stripped_strings:
        cleaned_text += s+' '
    new_item = {'created_at': created_at,
               'screen_name': screen_name,
               'retweet_count': retweet_count,
                'favorite_count': favorite_count,
                'cleaned_text': cleaned_text}
    del item['_id']
    update_result = collection.update_one({'_id':ObjectId(oid)}, {"$set":new_item}, upsert=False)
    
```

## 三、数据计算
### （1）微博数据特征识别
利用TF-IDF计算词语权重，通过权重排序展示热度榜单。
```python
import jieba
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

mongo_uri ='mongodb://localhost:27017/'
db_name = 'weibo_data'
coll_name = 'weibo_data'

client = MongoClient(mongo_uri)
db = client[db_name]
collection = db[coll_name]

results = list(collection.find({}, {'cleaned_text': 1}))
texts = [row['cleaned_text'] for row in results]

# Vectorize texts into matrix of token counts
cv = CountVectorizer()
matrix = cv.fit_transform(texts).toarray()

# Apply TF-IDF transformation
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(matrix).toarray()

# Show weighted ranking of tokens based on TF-IDF value
terms = cv.get_feature_names()
ranks = [(term, tfidf[i].sum()) for i, term in enumerate(terms)]
ranks = sorted(ranks, key=lambda x: x[1], reverse=True)

df = pd.DataFrame([(r[0], '{:.2f}'.format(r[1]), i+1) for i, r in enumerate(ranks)],
                  columns=['Term', 'Weight', 'Rank'])

fig, ax = plt.subplots()
ax.scatter(df['Rank'], df['Weight'])
for i, txt in enumerate(df['Term']):
    ax.annotate(txt, (df['Rank'][i]+0.2, df['Weight'][i]-0.01))
ax.set_xlabel('Rank')
ax.set_ylabel('Weight')
ax.set_title('Weighted Ranking Based on Term Frequency-Inverse Document Frequency')
plt.show()
```

## 四、数据可视化
### （1）微博热点图
利用seaborn热力图模块绘制微博数据热度图。
```python
import pandas as pd
import seaborn as sns
import numpy as np

mongo_uri ='mongodb://localhost:27017/'
db_name = 'weibo_data'
coll_name = 'weibo_data'

client = MongoClient(mongo_uri)
db = client[db_name]
collection = db[coll_name]

results = list(collection.find({}).sort([('_id', -1)]).limit(100))
users = [row['screen_name'] for row in results]
counts = [row['retweet_count'] for row in results]

# Create dataframe with user names and hotness ratings
df = pd.DataFrame({'User': users,
                   'Retweets': counts})

# Generate heatmap from dataframe
sns.set(font_scale=1.2)
sns.set_palette("PuBuGn_d")
hm = sns.heatmap(df.pivot('User', 'Retweets'), cmap="YlOrRd")
hm.invert_yaxis()
hm.set_title('Most Retweeted Users')
plt.show()
```

### （2）微博词云
利用jieba分词、wordcloud生成微博热词词云。
```python
import jieba
import pandas as pd
from wordcloud import WordCloud
from PIL import Image
import os
import matplotlib.pyplot as plt

mask = np.array(Image.open(image_path))

mongo_uri ='mongodb://localhost:27017/'
db_name = 'weibo_data'
coll_name = 'weibo_data'

client = MongoClient(mongo_uri)
db = client[db_name]
collection = db[coll_name]

results = list(collection.find({}, {'cleaned_text': 1}).limit(10000))
joined_text = ''.join([row['cleaned_text'] for row in results])

# Extract top 100 most common words
words = jieba.analyse.extract_tags(joined_text, topK=100, allowPOS=('ns', 'n', 'vn', 'v'))

# Generate word cloud based on extracted words
wc = WordCloud(background_color='white',
               mask=mask,
               font_path='/Library/Fonts/Songti.ttc').generate(joined_text)

# Display generated word cloud
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()
```

## 五、智能决策
### （1）专家系统决策
利用Python编写的专家系统模块，判断是否为假设为真的事物。
```python
import xml.etree.ElementTree as ET
from pprint import pprint

def check_assumption(filename):
    tree = ET.parse(filename)
    root = tree.getroot()
    for child in root:
        premises = child.findall('premise')
        conclusion = child.find('conclusion').text.lower()
        assumptions = [{'text': p.text.lower(), 'is_true': False} for p in premises]
        while assumptions[-1]['text']!= '':
            input("Press Enter to add assumption...")
            premise = input("Enter new assumption:\n").lower()
            is_true = bool(int(input("Is it true? (1 or 0)\n")))
            assumptions.append({'text': premise, 'is_true': is_true})
        pprint(assumptions)
        response = assumptions[0]['is_true']
        for assumption in assumptions[1:-1]:
            if assumption['is_true']:
                response &= assumption['is_true']
            elif assumption['text'].startswith(('not ', 'n\'t ')):
                response |= not assumption['is_true']
            else:
                response &= assumption['is_true']
        if assumptions[-1]['text'].startswith(('not ', 'n\'t ')):
            response |= not assumptions[-1]['is_true']
        else:
            response &= assumptions[-1]['is_true']
        print("Does it hold that", assumptions[0]['text'], "?")
        answer = input("(y or n):\n").lower()
        if answer == 'y':
            print(response, "(Correct)")
        else:
            print(not response, "(Incorrect)")
```

### （2）决策树决策
利用Scikit-learn和GraphViz库实现决策树。
```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import graphviz 


# Load data into Pandas dataframe
data = pd.read_csv('dataset.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[[column for column in data.columns if column!='target']],
                                                    data['target'], test_size=0.2, random_state=42)

# Build decision tree classifier
clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)
clf.fit(X_train, y_train)

# Visualize decision tree
dot_data = StringIO()  
tree.export_graphviz(clf, out_file=dot_data, feature_names=[column for column in data.columns if column!='target'], filled=True, rounded=True, special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  

# Test model performance
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
```

### （3）随机森林决策
利用Scikit-learn和Random Forest实现随机森林。
```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

# Load iris dataset into Pandas dataframe
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, dtype='category')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Build random forest classifier
rfc = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=5,
                             bootstrap=True, oob_score=True, random_state=42)
rfc.fit(X_train, y_train)

# Evaluate model performance on training and testing sets
train_acc = accuracy_score(y_train, rfc.predict(X_train))
test_acc = accuracy_score(y_test, rfc.predict(X_test))
print("Training Accuracy:", train_acc)
print("Testing Accuracy:", test_acc)

cm = confusion_matrix(y_test, rfc.predict(X_test))
print("Confusion Matrix:")
print(cm)
```