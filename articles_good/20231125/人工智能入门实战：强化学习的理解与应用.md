                 

# 1.背景介绍


在日常生活中，人们常用到的很多应用都离不开人工智能技术的帮助。比如自动驾驶汽车、虚拟现实、图像识别等。而强化学习（Reinforcement Learning）就是其中一种非常重要的技术。它通过对环境进行探索、学习和反馈的方式，从而促使机器更好地适应环境、解决问题。

强化学习有两个基本要素，一个是状态（State），另一个是动作（Action）。状态描述当前所处的场景，动作则是对状态施加的影响。通常来说，环境会给出一个奖励（Reward）作为反馈，告知机器接下来的行为是否正确。如果出现了错误的行为，机器就会受到惩罚，并收敛到一个局部最优解或全局最优解。

我们以宠物狗狗狗币赚钱游戏为例，来介绍强化学习。这个游戏里，玩家需要训练一个机器狗狗币机器人，通过给它不同的输入，它会给予不同的输出——每次给它加一枚硬币时，它会给予一个奖励；但是，如果给它超过一定次数的正面反馈，它可能会采取一些措施，比如撤退或逃跑，从而使得自己的损失增多。

这里还有另一个应用案例。在上世纪五六十年代，IBM的贝尔实验室曾开发出贴纸识别软件，但由于开发难度过高，很少有人能够真正运用它。而机器学习模型的训练可以极大的提升它的识别能力。那么，如何将机器学习模型应用于贴纸识别领域呢？我们就可以利用强化学习的方法，让机器自己学习识别不同种类的贴纸。

所以，强化学习是机器学习的一大分支，拥有广阔的应用前景。想要真正掌握强化学习技术，首先需要理解其本质，了解背后的一些核心概念和算法原理，并熟练运用。本文将带您进入强化学习的世界，从基础知识入手，结合实际案例，引导您快速上手，真正掌握强化学习技术！

# 2.核心概念与联系
## 2.1 马尔科夫决策过程（MDP）
首先，我们需要明确一下什么是马尔科夫决策过程（Markov Decision Process，简称MDP）。顾名思义，马尔科夫决策过程是一个由马尔可夫随机场（Markov Random Field）推导出来的框架。这里的马尔可夫随机场指的是一个状态转移概率分布矩阵，用来刻画状态间的相互关系。它是一个非负概率分布矩阵，满足如下的性质：

1. 状态转移概率分布矩阵中的每一个元素的值都应该是非负的。
2. 在任意时刻t，如果一个状态s被观察到，那么随着时间的推移，它的行为将一直保持不变，也就是说，即便是在其他时刻，这个状态依然是s。
3. 如果有一个状态s1，并且根据转移矩阵M，在t时刻转移到了s2，那么在t+1时刻必然又会回到s1，而不是其他的状态。

换句话说，在一个马尔科夫决策过程中，所有可能的状态都是确定的，系统的状态只有一个，即当前所在的位置，其他情况一概不考虑。每一次行动都会导致状态的改变，同时还会给出一个奖励（Reward），表示该行动对环境造成的影响。我们的目标就是最大化累计奖励。

## 2.2 Q-learning 算法
Q-learning算法是强化学习的经典算法之一。它基于值函数Q，也就是我们对每一个状态的预期奖励。Q值代表了当做一个特定的行动在当前状态下的长期收益，它可以由贝尔曼方程计算出来。对于一个给定状态（S）和动作（A），它的Q值可以定义为：

$$
Q(S_t, A_t) = R_{t+1} + \gamma \max _{a}{Q(S_{t+1}, a)} 
$$

其中，R表示奖励，\gamma表示折扣因子（Discount Factor），\max 表示取最大值的动作。

Q-learning算法分为两个阶段：第一阶段是初始阶段，也就是环境随机生成初始状态；第二阶段是持续进行阶段，系统根据策略选择动作，执行动作后得到反馈，更新Q值。

## 2.3 策略（Policy）
策略描述了当做一个特定的状态，我们的行为应该是怎样的。在强化学习中，策略一般采用函数形式表示，其接受当前的状态作为输入，输出对应的动作。策略定义了我们的决策方式，决定了系统的行动。

不同策略之间的差异主要体现在它们的目标。有些策略更侧重于短期利益（比如价值函数），而有些策略则更侧重于长期利益（比如价值函数）。而且，也有一些策略能够承受一定的错误率。因此，策略的选择可以影响系统的表现。

常用的策略包括：

1. 最优策略：最优策略是指在当前情况下，具有最高可能性的行为序列。在Q-learning算法中，最优策略对应的Q值为：

   $$
   policy^\star(S_t) = \arg \max _{a}{Q(S_t, a)} 
   $$
   
2. ε-贪婪策略：ε-贪婪策略是指在一定概率范围内，随机选择行为。在Q-learning算法中，ε-贪婪策略对应的Q值为：

   $$
   Q^{\epsilon\text{-greedy}} (S_t, A_t) = (1-\epsilon)\times Q(S_t, A_t) + \epsilon\times max\{q:q\neq{}Q(S_t, A_t), q=max_a[Q(S_{t+1}, a)]\}
   $$

   ε的大小控制了ε-贪婪策略的鲁棒性。

3. β-概率策略：β-概率策略是指在某一特定状态下，采用固定概率向前搜索，但有一定的概率向后退却。β-概率策略对应的Q值为：

   $$
   Q_{\beta}(S_t, A_t) = (1-\beta)\times Q(S_t, A_t) + \beta\times max_a'Q(S_{t+1}, a')
   $$

    β控制了向后退的概率。

# 3.核心算法原理和具体操作步骤以及数学模型公式详细讲解
## 3.1 MDP 建模
### 3.1.1 猫狗局部环境（猫狗吃土豆、石头剪刀布）
我们先看一组具体的猫狗局部环境，如图所示。图中左边是猫和狗的局部环境，右边是猫和狗的全环境。猫面前有一堆土豆，狗面前有一堆石头。在这个环境下，猫和狗可以进行以下的四种动作：

1. 站立不动
2. 随机选择动作
3. 拿起土豆吃掉
4. 拿起石头剪掉

如果猫和狗都不做任何事情，那么这个局部环境就结束了。

<div align="center">
</div>

### 3.1.2 MDP 模型
为了能够用强化学习方法解决这个问题，我们需要建立一个马尔科夫决策过程（Markov Decision Process，简称MDP）。在此，我们首先要确定状态空间和动作空间，即局部环境中所有可能的状态和动作集合。在这里，状态空间为$S=\left \{ s_1, s_2,..., s_{n_s}\right \}$，动作空间为$A=\left \{ a_1, a_2,..., a_{n_a}\right \}$，其中$n_s$和$n_a$分别表示状态和动作数量。

然后，我们定义转移概率矩阵$\mathcal{T}$和奖励函数$\mathcal{R}$，表示状态之间的转换关系和奖励获得量。$\mathcal{T}_{s, s'}^{a}=\Pr\left (S_{t+1}=s'|S_t=s, A_t=a\right )$表示当状态为$s$，动作为$a$时，转移到状态为$s'$的概率。$\mathcal{R}_{s}^{a}=r_s^a$表示当状态为$s$，动作为$a$时，获得的奖励。

最后，我们把$S$和$A$以及$\mathcal{T}$和$\mathcal{R}$联系起来，形成一个马尔可夫随机场（MRF）。

### 3.1.3 MDP 示例
假设在一个迷宫环境中，我们希望训练一个智能代理，使其能够走出迷宫。那么，我们需要构建一个马尔可夫随机场，来描述迷宫的状态空间和动作空间，以及状态转移概率和奖励。

例如，迷宫的状态空间可以分为$n_x \times n_y$个网格点，每个网格点对应一个状态。我们可以定义每个网格点的状态表示为$(i,j)$，其中$i$和$j$分别表示坐标轴上的横纵坐标值。迷宫的动作空间可以有上下左右四个方向，因此动作空间可以表示为$A=\{\text{'up'},\text{'down'},\text{'left'},\text{'right'}\}$.

迷宫中每个网格点的状态转移概率可以由迷宫地图给出。我们假设迷宫中存在一条通路，可以从迷宫入口出去。迷宫地图可以记录每个网格点的上下左右四个方向的移动是否可达，或者某个方向的移动能否走出迷宫。我们可以用矩阵来表示迷宫地图。矩阵中的每个元素表示网格点的上下左右四个方向的移动是否可达。如果某个网格点的某个方向不可达，那么这个元素的值设置为False。

奖励函数可以由迷宫地图给出。迷宫地图中的特殊符号可以标注出终止状态。我们可以判断一个网格点是否是一个终止状态，如果是，那么这个网格点对应的奖励值设置为终止奖励值。否则，奖励函数的值设置为步进奖励值。

综上所述，我们可以构造一个马尔可夫随机场，用于描述一个迷宫环境，其中状态空间为$(n_x \times n_y)^2$，动作空间为$A=\{\text{'up'},\text{'down'},\text{'left'},\text{'right'}\}$。状态转移概率矩阵由迷宫地图给出，奖励函数由迷宫地图和特殊符号给出。

## 3.2 Q-learning 算法
### 3.2.1 智能代理（Agent）
首先，我们需要定义一个智能代理。在这里，智能代理就是我们的学习系统。智能代理的输入是状态$s$，输出是一个动作$a$。它接收当前状态，根据策略来选取动作，并执行相应的动作。如果策略是最优策略，则该策略一定是最优的。

### 3.2.2 Q-table
接着，我们需要定义一个Q表。Q表存储了所有可能的状态和动作组合及其对应的Q值。当智能代理接收到一个状态$s$，我们可以通过查阅Q表来获取在该状态下，各个动作的Q值，然后选择相应的动作。

### 3.2.3 策略评估与改善
在策略评估阶段，我们将根据Q表来评估策略。在这里，我们要保证两种策略之间的差距尽可能小，这样才能使学习过程更有效。通常，我们可以使用均方误差（Mean Squared Error，MSE）来衡量两个策略之间差别。

在策略改善阶段，我们通过模拟来学习策略。在这里，我们不断地运行智能代理，模仿它进行探索。我们收集经验数据，然后根据这些数据来改善Q表，使得智能代理越来越像人类一样。

### 3.2.4 ε-贪婪策略
ε-贪婪策略是一种特殊的策略，其概率向随机选择动作和选择最优动作之间进行权衡。具体实现时，ε-贪婪策略会根据ε的值，来决定采用哪种策略。ε越小，则在对当前策略的评估过程中，会更多地依赖随机策略；ε越大，则在对当前策略的评估过程中，会更多地依赖当前最优策略。

### 3.2.5 β-概率策略
β-概率策略是一种特殊的策略，其会考虑到未来的奖励，并根据这一信息来调整动作的选择。具体实现时，β-概率策略会根据β的值，来决定对未来的奖励的折现率。β=0意味着完全忽略未来的奖励；β=1意味着完全考虑未来的奖励。

# 4.具体代码实例和详细解释说明
这里，我们结合代码和实例，为读者呈现强化学习算法的具体操作步骤。

## 4.1 猫狗狗币赚钱游戏示例
```python
import random

class DogCoin:
    
    def __init__(self):
        self.coins = 1
        
    def eat(self):
        if self.coins > 0:
            print("The dog eats the coin and gets one.")
            self.coins -= 1
            
    def play(self):
        print("The dog plays with the ball and receives reward.")
        self.coins += 1
        
def train():
    agent = DogCoin()
    num_episodes = 1000
    
    for i in range(num_episodes):
        
        # initial state
        if agent.coins >= 10:
            action = "eat"
        else:
            action = random.choice(["play", "wait"])

        print("\nEpisode:", i)
        print("Dog coins:", agent.coins)
        print("Action taken by the agent:", action)
        
        # take action and receive reward
        if action == "eat":
            agent.eat()
        elif action == "play":
            agent.play()
            
        print("Dog coins after taking an action:", agent.coins)

if __name__=="__main__":
    train()
```
```
Episode: 0
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 1
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 2
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 3
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 4
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 5
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 6
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 7
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 8
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 9
Dog coins: 1
Action taken by the agent: wait
Dog coins after taking an action: 1

Episode: 10
Dog coins: 1
Action taken by the agent: play
Dog coins after taking an action: 2

Episode: 11
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 12
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 13
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 14
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 15
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 16
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 17
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 18
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 19
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 20
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 21
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 22
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 23
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 24
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 25
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 26
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 27
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 28
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 29
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 30
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 31
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 32
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 33
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 34
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 35
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 36
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 37
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 38
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 39
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 40
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 41
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 42
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 43
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 44
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 45
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 46
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 47
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 48
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 49
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 50
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 51
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 52
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 53
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 54
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 55
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 56
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 57
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 58
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 59
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 60
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 61
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 62
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 63
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 64
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 65
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 66
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 67
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 68
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 69
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 70
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 71
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 72
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 73
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 74
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 75
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 76
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 77
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 78
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 79
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 80
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 81
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 82
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 83
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 84
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 85
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 86
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 87
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 88
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 89
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 90
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 91
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 92
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 93
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 94
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 95
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 96
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 97
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 98
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2

Episode: 99
Dog coins: 2
Action taken by the agent: wait
Dog coins after taking an action: 2
```

## 4.2 贴纸识别示例
```python
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

def preprocess_image(im):
    """Preprocess image."""
    im = im.resize((28, 28))   # resize to fit model input size
    im = np.array(im).astype('float32') / 255    # normalize pixel values to [0, 1]
    return im.reshape(-1, 28*28)/255
    
def plot_digit(im):
    """Plot digit."""
    plt.imshow(np.hstack([im]), cmap='gray', interpolation='none')
    plt.xticks([])
    plt.yticks([])
    plt.show()
    
def load_dataset(path):
    """Load dataset from file."""
    images = []
    labels = []
    with open(path, 'r') as f:
        for line in f:
            label, path = line.strip().split(',')
            img = Image.open('{}/{}'.format(path, label)).convert('L')
            im = preprocess_image(img)
            images.append(im)
            labels.append(int(label))
    return np.array(images), np.array(labels)

def softmax(logits):
    exp_scores = np.exp(logits)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    return probs

def cross_entropy(probs, y_true):
    n_samples = len(y_true)
    y_pred = np.argmax(probs, axis=1)
    loss = -np.log(probs[range(n_samples), y_true])
    return np.mean(loss)

def accuracy(y_true, y_pred):
    return np.mean(y_true==y_pred)

class Model:
    
    def __init__(self):
        self.W = None
        self.b = None
        
    def build(self, input_size, output_size):
        np.random.seed(0)
        self.W = np.random.randn(input_size, output_size)*0.01
        self.b = np.zeros((1, output_size))
        
    def forward(self, X):
        Z = np.dot(X, self.W)+self.b
        logits = np.maximum(Z, 0)
        probs = softmax(logits)
        return probs
    
    def backward(self, X, Y, lr=0.01):
        m = Y.shape[0]
        AL = self.forward(X)
        dZ = AL - Y
        self.W -= lr * (1/m) * np.dot(X.T, dZ)
        self.b -= lr * (1/m) * np.sum(dZ, axis=0, keepdims=True)
        
    def predict(self, x):
        prob = self.forward(x)[0]
        return np.argmax(prob)
    
    
train_data, train_labels = load_dataset('../mnist/train.csv')
test_data, test_labels = load_dataset('../mnist/test.csv')


model = Model()
model.build(input_size=28*28, output_size=10)

epochs = 100
batch_size = 100
lr = 0.001

for epoch in range(epochs):
    idx = np.random.permutation(len(train_data))[:batch_size]
    X_batch = train_data[idx]
    Y_batch = np.eye(10)[train_labels[idx]]
    cost = model.backward(X_batch, Y_batch, lr=lr)
    acc = accuracy(train_labels[idx], model.predict(X_batch))
    if epoch % 10 == 0:
        print("Epoch: {}, Cost: {:.4f}, Acc: {:.4f}".format(epoch, cost, acc))

print("Train Accuracy: {:.4f}".format(accuracy(train_labels, model.predict(train_data))))
print("Test Accuracy: {:.4f}".format(accuracy(test_labels, model.predict(test_data))))
```